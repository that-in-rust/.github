

# **Beyond the Browser: A Strategic Analysis of Platform-Independent Runtimes for Concurrent Rust Applications**

## **Section 1: The State of the Union: Rust Concurrency on WebAssembly Today**

The promise of WebAssembly (WASM) is to provide a portable, high-performance compilation target for languages like Rust, enabling them to run securely in any environment, most notably the web browser. For a language renowned for its "fearless concurrency," the ability to leverage this strength in a platform-independent manner is a critical goal. However, an in-depth analysis of the current state of multithreading for Rust on WASM reveals an ecosystem in a functional but deeply compromised transitional state. The existing model is not a native expression of Rust's capabilities but rather a complex adaptation, heavily constrained by the architecture and limitations of its primary host environment: the JavaScript-centric web platform. This section will dissect this baseline, establishing the foundational mechanisms, the intricate toolchain, and the significant architectural and performance trade-offs that define concurrent Rust on WASM today.

### **1.1. The Foundation: Web Workers and SharedArrayBuffer**

The fundamental architecture for multithreading in WebAssembly on the web is not an intrinsic feature of the WASM specification itself but rather an extension of the pre-existing concurrency model provided by web browsers.1 This model is built upon two key JavaScript technologies: Web Workers and

SharedArrayBuffer. Understanding this foundation is crucial, as it dictates the capabilities and limitations of any Rust program compiled for this target.

The primary mechanism for parallel execution is the Web Worker API. A Web Worker allows a script to be run in a background thread, separate from the main browser UI thread, preventing the user interface from becoming unresponsive during long-running computations. In the context of WASM, when a new thread is spawned from Rust code, the underlying system creates a new Web Worker. This worker then instantiates its own, separate instance of the WebAssembly module.1 This "instance-per-thread" model is a defining characteristic of the architecture. Each thread is, in effect, an independent program execution, running the same code but with its own stack and set of globals. This means that WASM globals are not truly "global" in a multithreaded context; they are effectively thread-local storage, as each instance maintains its own copy that is inaccessible to others.1

For these separate instances to function as threads of a single cohesive application, they must be able to share state. This is enabled by SharedArrayBuffer, a JavaScript object that represents a fixed-length raw binary data buffer that can be shared across multiple Web Workers and the main thread.1 When a Rust program is compiled for multithreading, its linear memory—the contiguous block of memory that the WASM instance uses for its heap, stack, and static data—is backed by a

SharedArrayBuffer. This shared buffer is then passed to each new Web Worker during its creation. Consequently, while each thread has its own WASM instance, all instances operate on the exact same block of linear memory, allowing them to read and write to the same data structures and collaborate on a computation.1

To manage safe access to this shared memory and prevent data races, the WebAssembly threads proposal introduced a set of low-level synchronization primitives directly into the instruction set. These include atomic memory access instructions, which correspond to Rust's std::sync::atomic types like AtomicUsize. On single-threaded WASM targets, these types are lowered to simple non-atomic operations. However, when the threads feature is enabled, they compile to actual atomic hardware instructions, guaranteeing that operations like increments or compare-and-swaps are indivisible.1 Furthermore, the proposal adds

wait and notify instructions, which provide a futex-like mechanism for building higher-level synchronization constructs. These primitives allow a thread to sleep on a memory location until it is woken up by another thread via a notification, forming the essential building blocks for implementing mutexes, condition variables, and channels directly within Rust/WASM code.2

### **1.2. The Rust Ecosystem: The wasm-bindgen-rayon Adapter**

While the underlying browser primitives provide the necessary building blocks for concurrency, they are low-level and cumbersome to use directly. The Rust ecosystem has bridged this gap with libraries that provide high-level, ergonomic abstractions. For data parallelism, Rayon is the de facto standard in the Rust community, celebrated for its ability to convert sequential iterators into parallel ones with minimal code changes, often just by replacing .iter() with .par\_iter().4 However, Rayon is built for native OS threads and is not inherently aware of the Web Worker-based architecture of WASM.

To solve this, the wasm-bindgen-rayon crate was developed. It is crucial to understand that this crate is not a native implementation of Rayon for WASM but an *adapter*.5 Its primary role is to provide the JavaScript "glue" code required to make Rayon's work-stealing scheduler operate on top of the Web Worker infrastructure. When a Rayon-based computation is initiated,

wasm-bindgen-rayon intercepts the thread spawning requests and translates them into the necessary JavaScript calls to create Web Workers, instantiate the WASM module within them, and initialize the shared memory.5 It effectively creates and manages a thread pool of Web Workers that Rayon can then utilize to execute tasks in parallel.

Achieving this functional bridge, however, imposes a significant and fragile configuration burden on the developer, revealing the architectural friction between Rust's native threading model and the WASM target. The setup process is far from trivial and involves several non-standard steps:

1. **Nightly Rust Toolchain**: The standard library (std) for the default wasm32-unknown-unknown target is intentionally built without thread support to ensure maximum portability and minimal binary size.5 To access thread-safe APIs like  
   std::sync, developers are required to use a specific nightly version of the Rust compiler. This is a major barrier for production environments where stability is paramount.2  
2. **Recompilation of the Standard Library**: Using the nightly toolchain is not sufficient on its own. The developer must instruct Cargo to recompile the standard library from source using the unstable \-Z build-std flag. This process enables the necessary threading features within std for the WASM target.5  
3. **WASM Feature Flags**: The compilation process must be further configured via rustflags to enable the specific WebAssembly features that multithreading depends on. The \-C target-feature=+atomics,+bulk-memory flags instruct the compiler to emit atomic instructions and use bulk memory operations, which are essential for the underlying synchronization and memory initialization logic.5  
4. **Server-Side Configuration**: A final, and often overlooked, requirement exists outside the Rust toolchain. For SharedArrayBuffer to be available in a web browser, the web server hosting the application must be configured to send specific Cross-Origin Opener Policy (COOP) and Cross-Origin Embedder Policy (COEP) headers. This "cross-origin isolation" is a security measure browsers enforce to mitigate side-channel attacks like Spectre. Configuring these headers correctly can be a complex task, especially in existing web applications.5

While this configuration chain is complex, the ecosystem tooling provides some relief. The wasm-bindgen-rayon adapter is designed to work with popular JavaScript bundlers like Webpack (v5 and later) and Parcel (v2), which can automatically recognize and package the necessary Web Worker scripts, simplifying the deployment process.5 Nevertheless, the overall setup underscores that multithreading in Rust-WASM is not a first-class, stable feature but an experimental add-on that requires deep technical expertise and a tolerance for instability.

### **1.3. Architectural Limitations and Performance Overheads**

The reliance on the Web Worker model imposes fundamental architectural limitations and performance overheads that are absent in native Rust applications. These issues stem directly from the fact that the concurrency model is an emulation built on a higher-level system, rather than a direct mapping to OS primitives.

A primary architectural constraint is the "instance-per-thread" model. As previously noted, this design makes true global state impossible to share beyond the linear memory buffer.1 Any state managed by the WASM instance itself, such as function tables or non-shared globals, is duplicated for each thread. This can complicate the design of systems that rely on dynamically modifying shared state, such as through

dlopen-style plugin mechanisms, which would not behave as expected.9

The cost of thread creation is a significant performance bottleneck. Spawning a native OS thread is a relatively fast operation, allowing for fine-grained task parallelism. In contrast, spawning a Web Worker is an expensive, asynchronous process that involves setting up a new JavaScript execution environment, loading, and instantiating the WASM module. Reports indicate that creating just ten Web Workers can take one to two seconds.2 This high startup cost makes the model entirely unsuitable for workloads that depend on the rapid creation and destruction of short-lived threads, a common pattern in parallel algorithms. The performance model instead favors creating a fixed-size thread pool at application startup and dispatching tasks to these long-lived workers.

Perhaps the most critical flaw in the current architecture is the existence of memory leaks by design. In a native Rust application, the RAII (Resource Acquisition Is Initialization) paradigm ensures that when a thread's handle goes out of scope, its resources, including its stack, are properly cleaned up by the operating system. The Web Worker model provides no equivalent mechanism. When a worker finishes its task and is terminated, there is no standard way for the WASM runtime to deallocate the memory that was provisioned for that thread's stack and thread-local storage.1 This results in a guaranteed memory leak for every thread that is spawned and subsequently terminates. By default, this can be at least 1 MiB per worker, a severe problem for any long-running or resource-intensive application.8 This is a known and acknowledged shortcoming of the WebAssembly specification as it stands today, with potential solutions still in the design and proposal stages.8

Finally, the boundary between JavaScript and WebAssembly remains a source of significant performance overhead. While wasm-bindgen provides a relatively ergonomic way to pass data and call functions between the two environments, every such call incurs a cost. This "FFI overhead" involves marshalling data types and transitioning between the JS and WASM execution contexts.10 For functions that perform very little computation, this overhead can dominate the execution time, making a WASM implementation orders of magnitude slower than an equivalent pure JavaScript version.11 The performance benefits of WASM are only realized for coarse-grained, computationally intensive tasks where a large amount of work is performed within the WASM module for each boundary crossing, thereby amortizing the cost of the call.12 This dynamic fundamentally shapes how high-performance WASM applications must be architected, favoring fewer, larger data transfers over frequent, small interactions.

### **1.4. Section Insights & Implications**

The state of concurrent Rust on WebAssembly today is a testament to the ingenuity of the ecosystem's developers, who have built a functional system on a foundation not originally designed for it. However, a deeper analysis reveals that this functionality comes at a high cost, and the underlying architecture is fundamentally misaligned with the principles that make Rust's native concurrency "fearless" and efficient.

The first critical realization is that WASM's concurrency model is an *emulation*, not a native feature. Rust's native concurrency model is built on the assumption of direct, low-cost access to OS-level threads and system memory management. The std::thread::spawn function maps closely to primitives like pthread\_create, and RAII ensures deterministic resource cleanup. The WebAssembly model offers none of this. It forces Rust's sophisticated abstractions into the rigid and restrictive container of the Web Worker API. The result is a system where thread creation is prohibitively expensive for many parallel algorithms 2, where shared state is confined to a single linear memory buffer 1, and where fundamental resource management guarantees are absent, leading to unavoidable memory leaks.8 Consequently, design patterns and performance intuitions from native Rust do not translate. An approach that is efficient in native code, such as spawning many fine-grained tasks, becomes an anti-pattern in WASM due to the high cost of worker instantiation. Developers cannot simply recompile their concurrent native Rust code and expect it to be performant; they must re-architect it specifically for the constraints of the emulated environment.

Secondly, the sheer complexity of the toolchain required for multithreading serves as a clear indicator of profound architectural friction. In a mature and well-integrated ecosystem, enabling a core feature should be a straightforward process, typically involving a stable toolchain and a simple configuration flag. The reality for multithreaded WASM in Rust—requiring a specific nightly compiler, the unstable \-Z build-std feature, and a collection of RUSTFLAGS—signals that this is an experimental, bolted-on capability.5 This complexity arises because the standard

wasm32-unknown-unknown target is intentionally optimized for the single-threaded MVP, prioritizing maximum portability and minimal code size above all else.5 The convoluted build process is the necessary "glue" to force a shared-memory threading model onto a target that was not designed for it. This impedance mismatch between the desired feature and the target's core design principles implies that, for the foreseeable future, developing and maintaining multithreaded Rust applications for the web will remain a niche, expert-level task. It carries a high maintenance burden due to its reliance on unstable features and is ill-suited for projects that demand the stability and predictability of the standard Rust release channel.

## **Section 2: The Architectural Horizon: WASI, the Component Model, and Composable Concurrency**

While the browser-centric model for concurrent Rust on WASM is fraught with limitations, it represents only the past and present of the technology. The true potential for leveraging Rust's systems-level capabilities in a platform-independent manner lies in the evolution of WebAssembly beyond the browser, an effort spearheaded by the WebAssembly System Interface (WASI) and the ambitious Component Model. This trajectory marks a profound architectural shift, moving away from the emulated threading of Web Workers toward a native, composable concurrency paradigm designed for server-side, edge, and other non-web environments. This section traces this evolution, from the initial steps of WASI to the forward-looking proposals that promise to redefine what is possible with concurrent, portable applications.

### **2.1. The Genesis of WASI: Beyond the Browser**

The core WebAssembly specification defines a pure computation engine. A vanilla WASM module is a sandboxed binary that can perform calculations on data within its linear memory but has no standardized way to interact with the outside world.14 It cannot open a file, make a network request, or even get the current time. This makes it secure but also functionally useless for any standalone application outside the browser, where such interactions are mediated by JavaScript APIs.

The WebAssembly System Interface (WASI) was created to solve this problem. WASI is a modular, standardized set of system call-like APIs that provide WASM modules with controlled access to host system resources.14 Crucially, this access is governed by a capability-based security model. A WASM module cannot, for example, access the entire filesystem; instead, the host environment must explicitly grant it a handle (a capability) to a specific directory, and the module can only operate within that directory. This preserves WASM's security principles while enabling it to become a viable target for general-purpose applications.

The initial version of this standard, known as wasi\_snapshot\_preview1, provided a foundational, POSIX-like layer, offering APIs for filesystem access, environment variables, and clocks.16 However, Preview 1 had significant limitations that hindered its adoption for complex server-side applications. Most notably, it deliberately omitted APIs for creating network sockets and lacked a robust, standardized model for multithreading.14 This forced developers to rely on non-standard extensions provided by specific runtimes. For example, Wasmer created its own superset of WASI called WASIX, which added support for sockets,

fork(), and other POSIX-style features to meet user demand, leading to a risk of ecosystem fragmentation.14

### **2.2. The Legacy wasi-threads Proposal: A Stepping Stone**

The first formal attempt to standardize multithreading for standalone WASM was the wasi-threads proposal. This proposal was designed to augment the core WebAssembly threads specification (which provided atomics and shared memory) with a single, crucial missing piece: a standardized API for spawning new threads.9

Architecturally, wasi-threads made a pragmatic but ultimately limiting choice: it adopted the same "instance-per-thread" model used by browsers and Web Workers.3 When a program called the

wasi\_thread\_spawn function, the host runtime was expected to create a completely new instance of the WASM module. This new instance would be given access to the same shared linear memory as the parent, but it would have its own separate function tables, globals, and other module-level state.9

This design had the advantage of maintaining compatibility with the browser model, but it also inherited its fundamental flaws. The high memory consumption from duplicating the instance for every thread and the inability to share state beyond linear memory (such as dynamically loaded functions in a shared function table) made it a suboptimal long-term solution.9 Recognizing these drawbacks, the WebAssembly community has officially designated

wasi-threads as a **legacy proposal**. It is now retained only for backward compatibility with older runtimes that support WASI Preview 1, while future development has shifted to a more advanced and architecturally sound model.9

### **2.3. The Paradigm Shift: The Component Model and shared-everything-threads**

The future of WASI, and indeed of WebAssembly as a whole, is inextricably linked to the Component Model. This is a paradigm-shifting proposal that elevates WASM from a target for single, monolithic binaries to a platform for composing small, language-agnostic, and securely sandboxed "components".14 Components communicate not through low-level memory pointers and integer types, but through high-level, well-defined interfaces specified in the WebAssembly Interface Type (WIT) language. A WIT file acts as a contract, defining the functions, types, and resources that a component exports and imports. This enables a Rust component that needs key-value storage to be seamlessly linked at runtime with a Python component that provides access to Redis, without either component needing to know the implementation language of the other and without any manually written FFI glue code.18

This new, component-oriented world requires a more sophisticated concurrency model than the legacy "instance-per-thread" approach. The successor to wasi-threads is the **shared-everything-threads** proposal, which represents a fundamental architectural break with the past. Its goal is to enable a true shared-memory model that is far closer to the native threading experience that Rust developers are accustomed to.9

The core innovation of this proposal is the extension of shared annotations. While the original threads proposal allowed linear memory to be marked as shared, this new proposal applies the shared attribute to *all* WebAssembly objects: tables, globals, functions, and even garbage-collected data.20 This is a game-changer. It means that multiple threads can exist within a

*single WASM instance* and can safely access and modify shared function tables, shared global state, and other resources. The WASM validator ensures at load time that a shared function, for example, only calls other shared functions and accesses other shared state, providing static guarantees of thread safety.20 This completely eliminates the performance and memory overhead of the "instance-per-thread" model.

Under this new model, thread spawning will become a standardized, low-level, built-in function of the Component Model itself, providing a common foundation for all languages to build upon.20 The

shared-everything-threads proposal is explicitly designed to fill the gaps left by its predecessors. It aims to introduce support for weaker memory ordering models (e.g., release-acquire), which can offer significant performance improvements over the strict sequential consistency of the initial atomics proposal, and to finally enable multithreading for languages that rely on garbage collection (WasmGC) by allowing references to be shared across threads.20

### **2.4. WASI 0.3 and Beyond: Composable, Native Async**

The evolution of WASI's concurrency story does not stop with shared-memory threads. The upcoming WASI 0.3 release, with previews expected in August 2025, is centered on another transformative feature: **native asynchronous support** integrated directly into the Component Model.21

This proposal introduces explicit future\<T\> and stream\<T\> types as first-class citizens of the WebAssembly Canonical ABI.18 This means a component can define in its WIT interface that a function is asynchronous and returns a future, or that it accepts or returns a stream of data. The profound implication of this design is that the management of concurrency—the event loop, polling for I/O readiness, and task scheduling—is lifted out of the component's implementation and into the host runtime.

This architecture offers a powerful solution to the "function coloring" problem that plagues many native async ecosystems, including Rust's. In a traditional async model, a function that becomes async "infects" its entire call stack, requiring all callers to also become async.18 The WASI 0.3 model breaks this chain. A component's internal logic can be written in a simple, synchronous, blocking style. When it performs an operation that would block, like reading from a network socket, the WASI runtime can transparently suspend the entire component instance, perform the I/O operation non-blockingly on the host, and then resume the component with the result when it is ready. From the component's perspective, it made a simple blocking call; from the host's perspective, it was part of a highly efficient, non-blocking, concurrent workflow.18

The end goal of this roadmap is clear. WASI is evolving from a simple set of POSIX-like system calls into a sophisticated, reactive, and composable application framework. The future of server-side WASM is not about running single, large programs in a sandbox, but about assembling complex applications from a collection of small, secure, language-agnostic, and inherently concurrent components, linked together like LEGO blocks.14 The initial focus is on cooperative, async-style concurrency, with preemptive multithreading planned as a subsequent, compatible addition.21

### **2.5. Section Insights & Implications**

The trajectory of WASI and its associated proposals reveals a deep and deliberate architectural pivot. The ecosystem is moving from a model centered on emulating a "sandboxed process" to one focused on enabling "composable micro-components." WASI Preview 1 and the legacy wasi-threads proposal were concerned with running a single, isolated, POSIX-like program. The concurrency model reflected this, treating each thread as a separate "process" that happened to share a memory space.16 The Component Model introduces a fundamentally different philosophy. It is not about running one large application, but about linking many small, independent, and potentially polyglot components into a cohesive whole.14 The

shared-everything-threads and native async proposals are the critical enabling technologies for this vision. They provide the fine-grained sharing and high-level communication primitives that are essential for these components to collaborate efficiently, without the crippling overhead and architectural limitations of the instance-per-thread model. This implies a significant shift in how developers should think about architecting applications for this target. The optimal approach in the future will not be to compile a single, monolithic Rust binary, but to decompose the application's logic into a set of reusable, independently deployable components, each defined by a clear WIT interface.

Furthermore, the design of WASI's upcoming async model is architecturally superior to many native async models because it elevates concurrency from a mere implementation detail to a formal, contractual part of a component's public interface. While Rust's native async capabilities are powerful, the ecosystem is fragmented by competing runtimes (e.g., Tokio vs. async-std) and constrained by the "function coloring" problem. The WASI 0.3 model, by making future\<T\> and stream\<T\> part of the ABI and delegating event loop management to the host runtime, solves this at an architectural level.18 A component's business logic is decoupled from the mechanics of concurrency. A Rust component can interact asynchronously with a Python component, which in turn streams data to a Go component, all participating in the same concurrent workflow orchestrated by the host, without any of them needing to share a common async runtime or even be aware of each other's implementation details. This offers the promise of a more robust, interoperable, and less complex solution to concurrency than what is often possible even in native ecosystems today. It is a compelling, long-term architectural advantage that positions the future of WASI as a premier platform for building complex, concurrent systems.

## **Section 3: An Alternative Paradigm: The GraalVM LLVM Runtime**

While the WebAssembly ecosystem evolves towards a future of composable concurrency, a mature and powerful alternative already exists that embodies a completely different architectural philosophy. The GraalVM LLVM runtime, developed by Oracle Labs, offers a platform-independent execution environment not by defining a new, sandboxed instruction set like WASM, but by leveraging a standard compiler intermediate representation—LLVM bitcode—and executing it within a high-performance, polyglot Java Virtual Machine (JVM). This approach trades the Ahead-of-Time (AOT) compilation and minimal runtime of WASM for the dynamic Just-in-Time (JIT) compilation and rich feature set of a world-class managed runtime.

### **3.1. Architecture: JIT Compilation of LLVM Bitcode**

The core mechanism of the GraalVM LLVM runtime is fundamentally different from that of a WASM runtime. Instead of AOT compiling source code to a specific binary format (.wasm) which is then executed, GraalVM operates on LLVM bitcode, the intermediate representation (IR) used by the LLVM compiler toolchain.24 A Rust program can be instructed to emit this bitcode directly using the

rustc \--emit=llvm-bc compiler flag.24

This generated bitcode file is then executed by GraalVM's lli launcher. However, lli is not a simple interpreter. It leverages the advanced Graal JIT compiler, the same technology that makes GraalVM a high-performance JDK. Upon execution, the runtime begins by interpreting the LLVM bitcode. Simultaneously, it profiles the running code to identify "hot" paths—functions and loops that are executed frequently. These hot paths are then dynamically compiled Just-in-Time into highly optimized native machine code for the host architecture.25

The key advantage of this JIT-based approach is its ability to perform Profile-Guided Optimizations (PGO) at runtime. An AOT compiler like rustc or a WASM AOT compiler must make conservative optimization choices based on a static analysis of the code. It cannot know which branches will be taken frequently or which function calls are prime candidates for inlining in a real-world workload. The Graal JIT, by contrast, observes the program's actual behavior and can use this profiling information to make much more aggressive and effective optimizations. It can speculatively inline functions, eliminate branches that are rarely taken, and re-optimize code as the application's behavior changes over time.27 This capability gives JIT compilation a theoretically higher peak performance ceiling for long-running, dynamic workloads compared to static AOT compilation.

### **3.2. Primary Value Proposition: High-Performance Polyglot Interoperability**

While the JIT compiler provides a compelling performance story, the primary value proposition of GraalVM is its unparalleled support for high-performance polyglot programming. The GraalVM ecosystem is designed from the ground up to run multiple languages—including Java, JavaScript, Python, Ruby, R, and any language that compiles to LLVM bitcode—within the same process and, crucially, the same memory space.29

This is enabled by the Truffle language implementation framework. When languages are implemented using Truffle, they can interoperate seamlessly. This allows for what GraalVM markets as "zero-cost FFI." Unlike the boundary between JavaScript and WASM, which requires data to be serialized and copied, or traditional FFI which involves marshalling data across different memory management models, GraalVM allows direct object passing. A JavaScript object can be passed to a Rust function (running as LLVM bitcode) and be manipulated as if it were a native C struct, without any explicit conversion or copying of the underlying data.31 This eliminates one of the most significant performance bottlenecks in multi-language systems.

For a concurrent Rust application, this architecture means it can leverage the full power of the host JVM's mature and highly optimized concurrency substrate. Rust threads would map to JVM threads, which have been battle-tested and tuned for performance over decades in demanding enterprise environments. The JVM's sophisticated garbage collectors, schedulers, and memory model would be available to the Rust program, providing a robust foundation for building complex, concurrent, and interoperable applications.

### **3.3. Performance Profile and Trade-offs**

The architectural choice of JIT compilation on a managed runtime imposes a distinct set of performance trade-offs, creating a profile that is almost the inverse of that of WebAssembly.

The most significant trade-off is between startup time and peak throughput. The JIT model is inherently slow to start. The JVM itself must be initialized, application classes must be loaded, and the code must be interpreted for a period while the JIT compiler warms up and gathers profile data before it can produce optimized machine code.34 This makes GraalVM entirely unsuitable for use cases that demand near-instantaneous startup, such as command-line tools or many serverless function platforms. In contrast, AOT-compiled WASM or native binaries excel in these scenarios.

However, for long-running, computationally intensive server applications, the tables turn. Once the Graal JIT is fully warmed up, its ability to use runtime profile data for optimization can allow it to achieve a peak throughput that meets or even exceeds that of statically compiled native code.27 Benchmarks comparing GraalVM's AOT-compiled Native Image with the JIT-powered JVM consistently show that the JVM achieves higher peak performance after a warmup period, although often with less predictable tail latencies due to factors like garbage collection pauses.35

Another critical consideration is memory footprint. A full-fledged JVM is a heavyweight process, consuming a significantly larger amount of memory compared to a lightweight WASM runtime or a self-contained native binary.36 This makes it less suitable for resource-constrained environments like small edge devices or high-density container deployments.

Finally, while the theoretical performance potential for Rust on the LLVM runtime is high, the current reality may be different. Reports on the performance of C extensions for Python running on the GraalVM LLVM runtime have shown them to be significantly slower than their native CPython counterparts, with performance in the range of 0.1x to 0.5x of native.34 This suggests that the LLVM runtime is not yet as mature or as highly optimized as the runtimes for first-party languages like Java or JavaScript. Therefore, while the architecture is promising, achieving near-native performance for Rust code today is not a guarantee and would require careful benchmarking of specific workloads.

### **3.4. Section Insights & Implications**

A strategic comparison of GraalVM and WebAssembly reveals that they represent two fundamentally opposing philosophies on how to achieve platform independence: the "Common Runtime" versus the "Common Binary Format." WebAssembly's approach is to define a common, abstract machine and a portable binary format (.wasm). Any language can compile to this format, and any host can implement a runtime for it. The artifact is portable, and the runtime is, in theory, a commodity. GraalVM's approach is the inverse. It aims to create a single, sophisticated, "universal" runtime—the JVM equipped with the Graal compiler and the Truffle framework. Any language can be implemented as a guest on this runtime, and because they all share the same underlying infrastructure (memory manager, scheduler, JIT), they can interoperate with extreme efficiency. Here, the runtime is the central, high-value piece of technology. This philosophical divergence leads to a clear strategic choice for developers. If the goal is to create and distribute self-contained, secure, and maximally portable components that can run in a wide variety of environments (browsers, edge devices, different cloud providers), WASM and its Component Model are the natural architectural fit. If, however, the goal is to build a complex, multi-language monolithic service where, for instance, high-performance Rust logic needs to share complex, in-memory data structures with Python data science libraries and a Java enterprise framework, GraalVM's architecture is unequivocally superior.

Consequently, the performance benefit of GraalVM for a Rust developer is highly conditional and depends entirely on the specific application workload and its operational context. The term "efficient" is ambiguous; it can mean low startup time, low memory usage, or high peak throughput. The evidence clearly demonstrates that GraalVM's JIT model is deficient in the first two metrics when compared to AOT solutions like native Rust or WASM.35 Its architectural strength lies solely in the third: peak throughput. The JIT's ability to perform profile-guided optimizations at runtime offers a theoretical performance ceiling that static AOT compilation cannot reach.27 While this has been demonstrated for JVM languages, the LLVM runtime's maturity for guest languages like Rust is still an open question.34 This implies that choosing GraalVM for a Rust project is a strategic bet on the future optimization of its LLVM runtime. It is a suitable choice only for a very specific class of application: long-running, server-side, CPU-bound workloads where polyglot interoperability is a core requirement and peak throughput is the single most important performance metric. For the vast majority of other use cases, such as CLI tools, serverless functions, or memory-constrained services, it is the wrong architectural choice.

## **Section 4: A Foundational Contender: RISC-V as a Virtual ISA**

Beyond the evolutionary path of WebAssembly and the alternative paradigm of GraalVM lies a third, more radical contender: using a real-world Instruction Set Architecture (ISA)—RISC-V—as a portable, sandboxed virtual machine target. This approach challenges the very premise that a platform-independent runtime requires a custom-designed, abstract virtual machine. It posits that a simple, clean, and open real-world ISA can serve as a superior compilation target for high-performance systems languages like Rust, particularly when compared to the architectural complexities of a stack-based VM like WebAssembly.

### **4.1. Architectural Showdown: Stack vs. Register Machines**

At the heart of the debate between WebAssembly and a virtualized RISC-V lies a classic computer architecture conflict: the stack machine versus the register machine.

WebAssembly is, by definition, a stack-based virtual machine.38 Its instructions operate implicitly on a stack of values. For example, an

i32.add instruction pops two 32-bit integers from the top of the stack, adds them, and pushes the result back onto the stack. This design has several advantages for a virtual machine: it leads to very compact bytecode, as instructions do not need to encode operand locations, and it simplifies the process of writing a compiler front-end and a simple interpreter.40

However, this architectural elegance comes at a significant performance cost. Modern CPUs are not stack machines; they are register machines. A JIT or AOT compiler for WebAssembly must therefore perform a complex and computationally expensive translation phase. It must analyze the flow of values on the virtual stack and map them onto the finite set of physical registers available on the host CPU. This "register allocation" is a non-trivial task that can result in inefficient machine code, with excessive moving of data between registers and memory (known as "register spilling") if the mapping is suboptimal.41

RISC-V, in contrast, is a modern, load-store, register-based ISA, designed to closely mirror the operation of contemporary hardware.42 Its instructions explicitly name their source and destination registers, for example,

add rd, rs1, rs2, which adds the values in source registers rs1 and rs2 and places the result in destination register rd. Compiling for a register-based virtual ISA is a much more direct process for a modern compiler like LLVM. The compiler's internal register-based representation maps more naturally to the target ISA, simplifying the code generation phase and often resulting in the execution of fewer total VM instructions to perform the same task. This direct mapping is a key reason why register-based VMs consistently outperform stack-based VMs in interpreter and JIT benchmarks.44

### **4.2. RISC-V as a Sandboxed Virtual Machine**

The core concept of this alternative is to decouple the RISC-V ISA from physical hardware. Instead of targeting a physical RISC-V chip, a Rust program is compiled to a standard RISC-V binary (e.g., using the \--target riscv64gc-unknown-none-elf target in rustc). This platform-independent binary is then executed inside a software-based RISC-V emulator or a JIT recompiler, which acts as the sandboxed virtual machine.47

RISC-V is uniquely suited for this role for several reasons:

* **Simplicity and Stability**: The base integer RISC-V ISA (RV32I or RV64I) is remarkably simple, with a minimal set of orthogonal instructions that can be described on a single page.47 This simplicity makes it vastly easier to write a correct, secure, and high-performance interpreter or JIT compiler compared to a complex, evolving specification like WebAssembly. While WASM is constantly growing with new proposals for features like garbage collection, exception handling, and SIMD, the base RISC-V ISA is a stable, frozen target, which is highly desirable for long-term platform stability.41  
* **Open Standard and Mature Toolchain**: As a free and open standard governed by the RISC-V Foundation, it is unencumbered by royalties or proprietary control.49 This has fostered a vibrant ecosystem and, most importantly for Rust developers, has led to its adoption as a first-class target in the LLVM compiler toolchain, which  
  rustc is built upon. This means high-quality, optimizing code generation for RISC-V is already available in the standard Rust compiler.47  
* **Modularity**: The RISC-V ISA is designed to be modular. A virtual machine can choose to implement only the base integer instruction set and specific extensions (like 'M' for multiplication/division) while completely ignoring others (like floating-point or SIMD). This allows the VM to be tailored to the specific needs of the workload, dramatically reducing its complexity and attack surface.47

### **4.3. Case Study: PolkaVM \- A High-Performance RISC-V VM**

A compelling real-world example of this architecture is PolkaVM, a virtual machine developed by Parity Technologies for the Polkadot blockchain ecosystem.51 PolkaVM was created specifically to address the perceived performance and complexity limitations of using WebAssembly as a deterministic execution environment for smart contracts.41

PolkaVM is a JIT recompiler that translates RISC-V bytecode into native machine code at runtime. Its architecture is laser-focused on performance and security, incorporating several key design choices that differentiate it from typical WASM runtimes:

* **Register-Based Foundation**: By using RISC-V, it completely sidesteps the inefficient stack-to-register translation that burdens WASM JITs, allowing for a more direct and efficient code generation pipeline.42  
* **Ahead-of-Time Linker Optimizations**: PolkaVM uses a unique compilation pipeline. Instead of the VM operating directly on the compiler's output, a custom linker post-processes the standard ELF binary produced by the compiler. This linker performs optimizations like macro-op fusion ahead of time, simplifying the work the JIT needs to do at runtime and shifting complexity from the hot path of execution to a one-time offline step.42  
* **Efficient Sandboxing**: It employs an efficient sandboxing technique that uses memory mapping to ensure that the lower portion of the virtual address space is unmapped. This allows the JIT to emit simple, single-instruction memory accesses for guest code, knowing that any out-of-bounds access will immediately cause a hardware page fault. This is significantly more efficient than the multi-instruction guards and bounds checks that some WASM runtimes must inject into the compiled code to ensure memory safety.42

The performance results of this specialized architecture are impressive. For computationally intensive benchmarks, initial reports claim that PolkaVM can outperform WASM-based smart contract runtimes by more than an order of magnitude.53 General-purpose benchmarks show its execution speed reaching up to 45% of native x64 performance, with early prototypes already slightly outperforming the mature Wasmtime runtime on some tests.54 Furthermore, because the RISC-V ISA is so much simpler to parse and validate than the WASM binary format, PolkaVM achieves significantly faster compilation and startup times—a critical metric for its target use case of executing many small, short-lived smart contracts.55

### **4.4. Section Insights & Implications**

The emergence of RISC-V as a virtual ISA represents a modern incarnation of classic computer architecture debates, particularly RISC vs. CISC and Stack vs. Register. In this context, WebAssembly, with its ever-expanding feature set (GC, SIMD, exceptions, tail calls), can be seen as following a CISC-like path. It aims to be a universal target by adding complex features to the ISA itself, ostensibly to make the compiler's job easier and to support a wider range of high-level languages.42 The proponents of the RISC-V approach, such as the creators of PolkaVM, argue that this path leads to a bloated, complex, and unstable specification that is a suboptimal target for high-performance systems programming.41 Their choice of RISC-V reflects a "return to simplicity," a core tenet of the RISC philosophy. The argument is that a simpler, more stable, and hardware-aligned ISA is a better target for a sophisticated compiler like LLVM. It allows for the creation of virtual machines that are themselves simpler, faster, more secure, and easier to formally verify. This suggests a potential future where WASM, in its attempt to be a "one size fits all" solution, becomes an inefficient and overly complex target for performance-critical use cases, for which a leaner, more foundational virtual ISA like RISC-V is a superior choice.

This analysis also highlights a crucial shift in what constitutes the primary bottleneck for high-performance virtualized computation. While modern JIT compilers for both WASM and RISC-V are capable of producing highly optimized code that approaches native execution speeds for long-running computations 54, the performance gap in pure throughput is narrowing. The new frontier of performance is increasingly focused on latency—specifically, the latency of VM instantiation and code loading. Projects like PolkaVM and

libriscv place a heavy emphasis on near-instantaneous compilation times and ultra-low-latency for calls into the VM.55 This focus is driven by the demands of emerging application domains like serverless computing, high-frequency edge functions, and blockchain smart contracts, where workloads are often ephemeral and invoked thousands of times per second. In these scenarios, the "cold start" time—the time taken to create the sandbox and load the code—can completely dominate the total execution time, making fast startup a more critical metric than peak throughput. Here, the fundamental simplicity of the RISC-V ISA gives it a structural advantage. Its simpler format is faster to parse, validate, and compile than the more complex and feature-rich WebAssembly binary format, positioning it as a potentially superior architecture for the next generation of low-latency computing platforms.

## **Section 5: Synthesis and Strategic Recommendations**

The preceding analysis has explored three distinct and compelling technological paths for achieving platform-independent execution of concurrent Rust applications. The first, WebAssembly with WASI, represents the mainstream evolutionary path, undergoing a profound architectural shift from a browser-centric emulation to a sophisticated, server-side component model. The second, the GraalVM LLVM runtime, offers a mature, high-performance polyglot paradigm rooted in the philosophy of a universal, JIT-compiling runtime. The third, using RISC-V as a virtual ISA, presents a foundational challenge to the very concept of an abstract virtual machine, arguing for a return to architectural simplicity for maximum performance and security. Each path offers a unique set of trade-offs. This final section synthesizes these findings into a comparative framework and provides actionable, strategic recommendations for a Rust Open Source Software contributor seeking to navigate this complex and evolving landscape.

### **5.1. Comparative Analysis: A Multi-Axis Framework**

To clarify the strategic choices, the characteristics of each technology are summarized below. The WebAssembly path is evaluated based on its future state with the Component Model and shared-everything-threads, as this represents its true long-term potential.

The following table provides a direct, at-a-glance comparison of the three technological paths across seven key axes relevant to a systems developer. It is designed to distill the report's detailed analysis into a structured, decision-making tool, highlighting the fundamental trade-offs between these competing visions for platform-independent computing.

| Feature Axis | WebAssembly (WASI Component Model) | GraalVM (LLVM Runtime) | RISC-V (Virtual ISA) |
| :---- | :---- | :---- | :---- |
| **Core Architecture** | Stack-based Virtual Machine, AOT-compiled. 39 | JIT-compiled LLVM Bitcode on JVM. 25 | Register-based ISA, typically JIT-recompiled or interpreted. 42 |
| **Concurrency Model** | Evolving to native, composable model: shared-everything-threads & future\<T\>/stream\<T\> ABI. 20 | Mature, high-performance, preemptive threading inherited from the host JVM. | Defined by the specific VM implementation; can be high-performance but lacks standardization. 57 |
| **Performance: Startup** | Very Fast. Designed for AOT compilation and rapid instantiation. 58 | Slow. Requires JVM startup, class loading, and JIT warmup. 35 | Very Fast. Simple ISA allows for extremely fast interpreters or lightweight JITs. 55 |
| **Performance: Peak Throughput** | Good, but can be 1.5x slower than native due to architectural overhead and sandbox checks. 56 | Potentially higher than AOT/native for long-running workloads due to Profile-Guided Optimizations (PGO). 27 | Potentially near-native. Register-based design is a more efficient JIT target than a stack machine. 54 |
| **Security Model** | Excellent. Capability-based, deny-by-default sandbox is a core design principle. 36 | Mature and robust JVM security model, but designed for trusted code; polyglot contexts add isolation. 36 | Excellent. Simple ISA reduces attack surface; sandboxing via process isolation is straightforward and effective. 42 |
| **Interoperability** | Excellent (Language-agnostic). The Component Model provides a formal, contract-based approach via WIT. 14 | Excellent (In-Process Polyglot). Zero-cost FFI between supported languages sharing the same memory space. 31 | Good (Binary-level). Interoperability via standardized syscalls, similar to native processes. 47 |
| **Ecosystem Maturity** | Rapidly maturing. Strong backing from major tech companies (Bytecode Alliance). Standards are still evolving. 14 | Mature. Enterprise-ready, backed by Oracle, with decades of production use. LLVM runtime is less mature than JVM languages. 29 | Nascent but growing exponentially. Strong in embedded and blockchain, but general-purpose VM ecosystem is new. 49 |
| **Ideal Use Case** | Portable, secure, language-agnostic components: web clients, serverless functions, plugins, edge compute. | Long-running, high-throughput, polyglot enterprise services and microservices. | High-performance, low-latency sandboxing: smart contracts, secure plugins, performance-critical serverless. |

This framework reveals a landscape with no single "best" solution, but rather a set of specialized tools optimized for different priorities. WASI and the Component Model are architected for maximum portability and secure, language-agnostic composition. GraalVM is architected for maximum in-process performance and interoperability within a managed, polyglot environment. RISC-V is architected for maximum raw performance and minimal abstraction, offering a target that is as close to the metal as possible while remaining portable.

### **5.2. Strategic Recommendations for the Rust OSS Contributor**

Given this complex landscape, a Rust developer's choice of where to invest their time and contribution efforts should be guided by their specific goals. The following strategies outline four distinct paths, each aligned with a different set of priorities.

#### **The Pragmatic Path: For Immediate, High-Portability Needs**

For developers who need to ship concurrent Rust applications on the web *today*, the only viable option is the existing stack based on wasm-bindgen-rayon. The strategy here is one of clear-eyed acceptance of its limitations.

* **Strategy**: Target the single-threaded wasm32-unknown-unknown profile for libraries whenever possible to maximize stability and portability. For applications that absolutely require concurrency, accept the complexity of the nightly toolchain and the wasm-bindgen-rayon adapter as a temporary cost. Architect the application defensively: use a fixed-size thread pool initialized at startup to avoid the high cost of dynamic thread spawning, and be acutely aware of the memory leaks inherent in the model, designing for graceful restarts or minimal thread churn in long-running applications. A valuable contribution in this area would be to work on stabilizing the \-Z build-std feature in rustc and improving the ergonomics and documentation of the multithreading toolchain to lower the barrier to entry for other developers.

#### **The Visionary Path: For Next-Generation, Server-Side Applications**

For developers building for the future of server-side and edge computing, the most promising and well-supported path is to invest heavily in the WASI Component Model.

* **Strategy**: Begin architecting new applications around the component philosophy. Instead of building monolithic binaries, decompose application logic into smaller, reusable components with their interfaces formally defined in WIT files. Experiment with runtimes that have leading-edge support for WASI Preview 2 and the upcoming Preview 3, such as Wasmtime.14 This path aligns most closely with the future direction of the entire WebAssembly ecosystem and offers the greatest potential for achieving true, portable, and composable "fearless concurrency." The most impactful OSS contributions would be to the development of the Rust SDKs for the Component Model, the implementation of the  
  shared-everything-threads proposal in Rust-based runtimes like Wasmtime, and the creation of high-quality, componentized Rust libraries for the emerging WASI ecosystem.

#### **The Enterprise Path: For Performance-Critical, Polyglot Systems**

For developers working in enterprise environments on long-running services where Rust must tightly integrate with other languages, particularly those on the JVM, the GraalVM LLVM runtime is a powerful, albeit niche, option.

* **Strategy**: Evaluate GraalVM when the primary requirement is not the portability of the binary artifact, but rather the highest possible performance of interoperability between Rust and other languages within a single, complex service. This is not a general-purpose replacement for native Rust, but a specialized tool for a specific problem. The focus should be on rigorous benchmarking of specific, real-world workloads, as performance is not guaranteed to exceed native Rust today and the trade-offs in startup time and memory usage are significant.

#### **The Foundational Path: For the Bleeding Edge of Performance and Security**

For developers interested in pushing the fundamental boundaries of performance and security in virtualized environments, the RISC-V virtual machine ecosystem offers the most fertile ground for innovation.

* **Strategy**: Explore RISC-V as a compilation target for applications where absolute maximum performance and minimal sandbox overhead are non-negotiable. This includes use cases like high-frequency trading systems, secure plugin architectures for performance-critical software, next-generation serverless runtimes, and blockchain execution layers. Engage with and contribute to pioneering projects like PolkaVM 63 and  
  libriscv.57 Impactful contributions could involve improving the Rust-to-RISC-V compilation toolchain, developing a standardized "RISC-V System Interface" analogous to WASI, or creating novel, high-performance RISC-V JIT runtimes written in Rust. This path offers the highest potential for disruptive, foundational innovation in the field of platform-independent computing.

### **5.3. Final Conclusion**

The question of whether a "better" platform than Rust-WASM exists for concurrent, platform-independent applications does not have a simple answer. The analysis indicates that the landscape is not a simple hierarchy but a spectrum of specialized solutions. For the broadest set of use cases, from the browser to the cloud, the future of WebAssembly with the WASI Component Model is not merely an incremental improvement but a transformative leap toward a new paradigm of composable concurrency. It is the most promising and well-supported path for the majority of developers.

However, for specific, demanding workloads, superior alternatives are emerging. GraalVM offers an architecturally elegant solution for high-performance polyglot integration in long-running services, trading portability and startup time for unparalleled interoperability. Most compellingly, RISC-V as a virtual ISA presents a fundamental architectural challenge to WebAssembly, offering a simpler, more stable, and potentially more performant foundation for secure, sandboxed execution. The "best" platform is therefore a function of a strategic choice, weighing the trade-offs between portability, performance profile, interoperability model, and ecosystem maturity. For a Rust OSS contributor looking to make the most significant impact, contributing to the maturation of the WASI Component Model and its shared-everything-threads implementation is likely the most strategic investment, as this represents the burgeoning mainstream of platform-independent systems programming.

#### **Works cited**

1. Multithreading Rust and Wasm, accessed on August 10, 2025, [https://rustwasm.github.io/2018/10/24/multithreading-rust-and-wasm.html](https://rustwasm.github.io/2018/10/24/multithreading-rust-and-wasm.html)  
2. Multithreading with WASM on the Browser. Is it possible yet? : r/rust \- Reddit, accessed on August 10, 2025, [https://www.reddit.com/r/rust/comments/1cwmyaw/multithreading\_with\_wasm\_on\_the\_browser\_is\_it/](https://www.reddit.com/r/rust/comments/1cwmyaw/multithreading_with_wasm_on_the_browser_is_it/)  
3. Announcing wasi-threads \- Bytecode Alliance, accessed on August 10, 2025, [https://bytecodealliance.org/articles/wasi-threads](https://bytecodealliance.org/articles/wasi-threads)  
4. Rayon \- Lib.rs, accessed on August 10, 2025, [https://lib.rs/crates/rayon](https://lib.rs/crates/rayon)  
5. wasm\_bindgen\_rayon \- Rust \- Docs.rs, accessed on August 10, 2025, [https://docs.rs/wasm-bindgen-rayon](https://docs.rs/wasm-bindgen-rayon)  
6. wasm-bindgen-rayon \- crates.io: Rust Package Registry, accessed on August 10, 2025, [https://crates.io/crates/wasm-bindgen-rayon/dependencies](https://crates.io/crates/wasm-bindgen-rayon/dependencies)  
7. RReverser/wasm-bindgen-rayon: An adapter for enabling Rayon-based concurrency on the Web with WebAssembly. \- GitHub, accessed on August 10, 2025, [https://github.com/RReverser/wasm-bindgen-rayon](https://github.com/RReverser/wasm-bindgen-rayon)  
8. Now I am become the Destroyer of Threads \- StackBlitz Blog, accessed on August 10, 2025, [https://blog.stackblitz.com/posts/thread-destroyer/](https://blog.stackblitz.com/posts/thread-destroyer/)  
9. WebAssembly/wasi-threads \- GitHub, accessed on August 10, 2025, [https://github.com/WebAssembly/wasi-threads](https://github.com/WebAssembly/wasi-threads)  
10. I was understanding WASM all wrong\! | by Yuji Isobe \- Medium, accessed on August 10, 2025, [https://medium.com/@yujiisobe/i-was-understanding-wasm-all-wrong-e4bcab8d077c](https://medium.com/@yujiisobe/i-was-understanding-wasm-all-wrong-e4bcab8d077c)  
11. Wasm-bindgen performance \- The Rust Programming Language Forum, accessed on August 10, 2025, [https://users.rust-lang.org/t/wasm-bindgen-performance/55726](https://users.rust-lang.org/t/wasm-bindgen-performance/55726)  
12. “Near-Native Performance”: Wasm is often described as having “near-native perf... | Hacker News, accessed on August 10, 2025, [https://news.ycombinator.com/item?id=30156437](https://news.ycombinator.com/item?id=30156437)  
13. Wasm-bindgen overhead for a canvas game? \- Stack Overflow, accessed on August 10, 2025, [https://stackoverflow.com/questions/69867859/wasm-bindgen-overhead-for-a-canvas-game](https://stackoverflow.com/questions/69867859/wasm-bindgen-overhead-for-a-canvas-game)  
14. WASI and the WebAssembly Component Model: Current Status \- eunomia, accessed on August 10, 2025, [https://eunomia.dev/blog/2025/02/16/wasi-and-the-webassembly-component-model-current-status/](https://eunomia.dev/blog/2025/02/16/wasi-and-the-webassembly-component-model-current-status/)  
15. The WebAssembly Component Model: Introduction, accessed on August 10, 2025, [https://component-model.bytecodealliance.org/](https://component-model.bytecodealliance.org/)  
16. Announcing wasi-threads : r/rust \- Reddit, accessed on August 10, 2025, [https://www.reddit.com/r/rust/comments/1198bbe/announcing\_wasithreads/](https://www.reddit.com/r/rust/comments/1198bbe/announcing_wasithreads/)  
17. Multithreading \- Swift and WebAssembly, accessed on August 10, 2025, [https://book.swiftwasm.org/getting-started/multithreading.html](https://book.swiftwasm.org/getting-started/multithreading.html)  
18. Hypercharge Through Components: Why WASI 0.3 and Composable Concurrency Are a Game Changer | by Enrico Piovesan | WebAssembly — WASM Radar | Jun, 2025 | Medium, accessed on August 10, 2025, [https://medium.com/wasm-radar/hypercharge-through-components-why-wasi-0-3-and-composable-concurrency-are-a-game-changer-0852e673830a](https://medium.com/wasm-radar/hypercharge-through-components-why-wasi-0-3-and-composable-concurrency-are-a-game-changer-0852e673830a)  
19. Components | wasmCloud, accessed on August 10, 2025, [https://wasmcloud.com/docs/concepts/components/](https://wasmcloud.com/docs/concepts/components/)  
20. WebAssembly/shared-everything-threads · GitHub, accessed on August 10, 2025, [https://github.com/WebAssembly/shared-everything-threads/blob/main/proposals/shared-everything-threads/Overview.md](https://github.com/WebAssembly/shared-everything-threads/blob/main/proposals/shared-everything-threads/Overview.md)  
21. Roadmap · WASI.dev, accessed on August 10, 2025, [https://wasi.dev/roadmap](https://wasi.dev/roadmap)  
22. Repository for design and specification of the Component Model \- GitHub, accessed on August 10, 2025, [https://github.com/WebAssembly/component-model](https://github.com/WebAssembly/component-model)  
23. Designing an Async Runtime for WASI 0.2 \- Yoshua Wuyts, accessed on August 10, 2025, [https://blog.yoshuawuyts.com/building-an-async-runtime-for-wasi/](https://blog.yoshuawuyts.com/building-an-async-runtime-for-wasi/)  
24. Compiling to LLVM Bitcode \- GraalVM \- Oracle Help Center, accessed on August 10, 2025, [https://docs.oracle.com/en/graalvm/enterprise/22/docs/reference-manual/llvm/Compiling/](https://docs.oracle.com/en/graalvm/enterprise/22/docs/reference-manual/llvm/Compiling/)  
25. GraalVM LLVM Runtime \- Oracle Help Center, accessed on August 10, 2025, [https://docs.oracle.com/en/graalvm/jdk/17/docs/reference-manual/llvm/](https://docs.oracle.com/en/graalvm/jdk/17/docs/reference-manual/llvm/)  
26. GraalVM LLVM Runtime, accessed on August 10, 2025, [https://www.graalvm.org/latest/reference-manual/llvm/](https://www.graalvm.org/latest/reference-manual/llvm/)  
27. GraalVM can run Rust via LLVM bitcode. \- Reddit, accessed on August 10, 2025, [https://www.reddit.com/r/rust/comments/8d21yh/graalvm\_can\_run\_rust\_via\_llvm\_bitcode/](https://www.reddit.com/r/rust/comments/8d21yh/graalvm_can_run_rust_via_llvm_bitcode/)  
28. Optimizations and Performance \- GraalVM, accessed on August 10, 2025, [https://www.graalvm.org/latest/reference-manual/native-image/optimizations-and-performance/](https://www.graalvm.org/latest/reference-manual/native-image/optimizations-and-performance/)  
29. Get Started with GraalVM, accessed on August 10, 2025, [https://www.graalvm.org/21.3/docs/getting-started/](https://www.graalvm.org/21.3/docs/getting-started/)  
30. Get Started with GraalVM, accessed on August 10, 2025, [https://www.graalvm.org/22.2/docs/getting-started/](https://www.graalvm.org/22.2/docs/getting-started/)  
31. Polyglot Programming \- GraalVM, accessed on August 10, 2025, [https://www.graalvm.org/22.2/reference-manual/polyglot-programming/](https://www.graalvm.org/22.2/reference-manual/polyglot-programming/)  
32. Interoperability \- GraalVM, accessed on August 10, 2025, [https://www.graalvm.org/latest/reference-manual/llvm/Interoperability/](https://www.graalvm.org/latest/reference-manual/llvm/Interoperability/)  
33. Compiling Native Projects via the GraalVM LLVM Toolchain | by Josef Eisl \- Medium, accessed on August 10, 2025, [https://medium.com/graalvm/compiling-native-projects-via-the-graalvm-llvm-toolchain-f606f995bf](https://medium.com/graalvm/compiling-native-projects-via-the-graalvm-llvm-toolchain-f606f995bf)  
34. Frequently Asked Questions \- GraalVM, accessed on August 10, 2025, [https://www.graalvm.org/21.3/reference-manual/python/FAQ/](https://www.graalvm.org/21.3/reference-manual/python/FAQ/)  
35. Benchmarking Web Services using GraalVM Native Image \- Inner Product, accessed on August 10, 2025, [https://www.inner-product.com/posts/benchmarking-graalvm-native-image/](https://www.inner-product.com/posts/benchmarking-graalvm-native-image/)  
36. What Are the Differences Between WebAssembly and the JVM? | F5, accessed on August 10, 2025, [https://www.f5.com/company/blog/what-are-the-differences-between-webassembly-and-the-jvm](https://www.f5.com/company/blog/what-are-the-differences-between-webassembly-and-the-jvm)  
37. Rust vs. Quarkus Native vs. Spring Native \- ConSol Blog, accessed on August 10, 2025, [https://blog.consol.de/software-engineering/web-application-development/rust-vs-quarkus-native-vs-spring-native/](https://blog.consol.de/software-engineering/web-application-development/rust-vs-quarkus-native-vs-spring-native/)  
38. Ask HN: Why did stack-based CPUs lose out? \- Hacker News, accessed on August 10, 2025, [https://news.ycombinator.com/item?id=34198463](https://news.ycombinator.com/item?id=34198463)  
39. Research on WebAssembly Runtimes: A Survey \- arXiv, accessed on August 10, 2025, [https://arxiv.org/html/2404.12621v1](https://arxiv.org/html/2404.12621v1)  
40. Stack machine \- Wikipedia, accessed on August 10, 2025, [https://en.wikipedia.org/wiki/Stack\_machine](https://en.wikipedia.org/wiki/Stack_machine)  
41. RISC-V vs. Wasm: a game-changer for smart contracts – CryptoTvplus \- The Leading Blockchain Media Firm, accessed on August 10, 2025, [https://cryptotvplus.com/2023/10/risc-v-vs-wasm-a-game-changer-for-smart-contracts/](https://cryptotvplus.com/2023/10/risc-v-vs-wasm-a-game-changer-for-smart-contracts/)  
42. Exploring PolkaVM \- RISC-V based smart contract VM that runs ROOM \- kusamaxi, accessed on August 10, 2025, [https://kusamaxi.com/post/polkavm-runs-doom/](https://kusamaxi.com/post/polkavm-runs-doom/)  
43. RISC-V Architecture: A Comprehensive Guide to the Open-Source ISA \- Wevolver, accessed on August 10, 2025, [https://www.wevolver.com/article/risc-v-architecture](https://www.wevolver.com/article/risc-v-architecture)  
44. virtual machine \- What are the pros and cons of register-based VMs and stack-based VMs?, accessed on August 10, 2025, [https://langdev.stackexchange.com/questions/1450/what-are-the-pros-and-cons-of-register-based-vms-and-stack-based-vms](https://langdev.stackexchange.com/questions/1450/what-are-the-pros-and-cons-of-register-based-vms-and-stack-based-vms)  
45. Virtual Machine Showdown: Stack Versus Registers \- Compilers and ..., accessed on August 10, 2025, [https://www.complang.tuwien.ac.at/andi/tuonly/SkriptVMShowdown.pdf](https://www.complang.tuwien.ac.at/andi/tuonly/SkriptVMShowdown.pdf)  
46. Stack based VM vs Register based VM \- HackingNote, accessed on August 10, 2025, [https://www.hackingnote.com/en/versus/stack-based-vm-vs-register-based-vm/](https://www.hackingnote.com/en/versus/stack-based-vm-vs-register-based-vm/)  
47. Exploring alternatives to WASM for smart contracts \- Tech Talk \- Polkadot Forum, accessed on August 10, 2025, [https://forum.polkadot.network/t/exploring-alternatives-to-wasm-for-smart-contracts/2434](https://forum.polkadot.network/t/exploring-alternatives-to-wasm-for-smart-contracts/2434)  
48. I'm using a RISC-V emulator for my scripting backend instead of WASM. Here's the why and how. : r/gameenginedevs \- Reddit, accessed on August 10, 2025, [https://www.reddit.com/r/gameenginedevs/comments/1m0lmnt/im\_using\_a\_riscv\_emulator\_for\_my\_scripting/](https://www.reddit.com/r/gameenginedevs/comments/1m0lmnt/im_using_a_riscv_emulator_for_my_scripting/)  
49. RISC-V \- Wikipedia, accessed on August 10, 2025, [https://en.wikipedia.org/wiki/RISC-V](https://en.wikipedia.org/wiki/RISC-V)  
50. Nanos on the 64-bit RISC-V Architecture \- NanoVMs, accessed on August 10, 2025, [https://nanovms.com/dev/tutorials/nanos-and-riscv](https://nanovms.com/dev/tutorials/nanos-and-riscv)  
51. Why RISC-V and PolkaVM for Smart Contracts? | Documentation \- ink\!, accessed on August 10, 2025, [https://use.ink/docs/v6/background/why-riscv-and-polkavm-for-smart-contracts/](https://use.ink/docs/v6/background/why-riscv-and-polkavm-for-smart-contracts/)  
52. From EVM to RISC-V VM: Polkadot Has Already Made the Leap | by OneBlock+ \- Medium, accessed on August 10, 2025, [https://medium.com/@OneBlockplus/from-evm-to-risc-v-vm-polkadot-has-already-made-the-leap-1f2a60f8b564](https://medium.com/@OneBlockplus/from-evm-to-risc-v-vm-polkadot-has-already-made-the-leap-1f2a60f8b564)  
53. Contracts Update: Solidity on PolkaVM \- Tech Talk \- Polkadot Forum, accessed on August 10, 2025, [https://forum.polkadot.network/t/contracts-update-solidity-on-polkavm/6949](https://forum.polkadot.network/t/contracts-update-solidity-on-polkavm/6949)  
54. Polkadot Virtual Machine (PVM) is a high-performance, RISC-V-based execution layer for decentralized computing. 45% of native x64 speed. Deterministic, efficient, & metered execution. Supports any language compiling to RISC-V. Built for JAM. Built for scale. \- Reddit, accessed on August 10, 2025, [https://www.reddit.com/r/Polkadot/comments/1ji434c/polkadot\_virtual\_machine\_pvm\_is\_a\_highperformance/](https://www.reddit.com/r/Polkadot/comments/1ji434c/polkadot_virtual_machine_pvm_is_a_highperformance/)  
55. Announcing PolkaVM \- a new RISC-V based VM for smart contracts (and possibly more\!), accessed on August 10, 2025, [https://forum.polkadot.network/t/announcing-polkavm-a-new-risc-v-based-vm-for-smart-contracts-and-possibly-more/3811/46](https://forum.polkadot.network/t/announcing-polkavm-a-new-risc-v-based-vm-for-smart-contracts-and-possibly-more/3811/46)  
56. Not So Fast: Analyzing the Performance of WebAssembly vs. Native Code \- ar5iv \- arXiv, accessed on August 10, 2025, [https://ar5iv.labs.arxiv.org/html/1901.09056](https://ar5iv.labs.arxiv.org/html/1901.09056)  
57. libriscv/libriscv: The fastest RISC-V sandbox \- GitHub, accessed on August 10, 2025, [https://github.com/libriscv/libriscv](https://github.com/libriscv/libriscv)  
58. WebAssembly Performance: How Fast Is WASM? \- Clover Dynamics, accessed on August 10, 2025, [https://www.cloverdynamics.com/blogs/web-assembly-performance-how-fast-is-wasm](https://www.cloverdynamics.com/blogs/web-assembly-performance-how-fast-is-wasm)  
59. WASM vs Native Rust performance \- Reddit, accessed on August 10, 2025, [https://www.reddit.com/r/rust/comments/12jbjsc/wasm\_vs\_native\_rust\_performance/](https://www.reddit.com/r/rust/comments/12jbjsc/wasm_vs_native_rust_performance/)  
60. WebAssembly \- Wikipedia, accessed on August 10, 2025, [https://en.wikipedia.org/wiki/WebAssembly](https://en.wikipedia.org/wiki/WebAssembly)  
61. News \- Bytecode Alliance, accessed on August 10, 2025, [https://bytecodealliance.org/articles/](https://bytecodealliance.org/articles/)  
62. mikeroyal/RISC-V-Guide: RISC-V Guide. Learn all about the RISC-V computer architecture along with the Development Tools and Operating Systems to develop on RISC-V hardware. \- GitHub, accessed on August 10, 2025, [https://github.com/mikeroyal/RISC-V-Guide](https://github.com/mikeroyal/RISC-V-Guide)  
63. paritytech/polkavm: A fast and secure RISC-V based virtual machine \- GitHub, accessed on August 10, 2025, [https://github.com/paritytech/polkavm](https://github.com/paritytech/polkavm)