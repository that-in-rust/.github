{
  "input": "We said we want to rewrite software in Rust because it offers fearless concurrency, memory safety, and zero-cost abstraction\n\nSo we started with thinking of writing applications or libraries which\n- were rewrites of existing proven libraries or applications\n- were new Rust implementations of a gap in the Rust ecosystem\n\nBut we realized that the linux kernel itself is a big hindrance to leveraging the maximal capabilities of Rust because it has jitter which takes away the advantage of fearless concurrency\n\nThis was shocking because we thought Linux is optimized, but the big picture is that all software written for single core by default is not optimal for multi-core performance\n\nAnd thus we thought can we rewrite an OS in Rust such that we can leverage the maximum capabilities of Rust\n\nOur exploration suggests that\n- Driver ecosystem is complex, fragmented and getting people to like a new OS is going to be hard, very difficult for adoption as well as needs a long time to build everything from scratch\n- Easiest adoption is a binary which is a Real Time Operating System customized to a specific App which opens a terminal to the host linux, allocating specific CPU Cores and RAM to the RTOS and having a scheduler which is optimized for the app we intend to run on it\n- Why app specific RTOS?\n- Minimal Jitter or interference from other applicatins or processes\n- Maximal predictability of availability of CPU Cores and resources\n- Improved P99.99 percentile latency because predictability of resources\n\n# Precedence\n\n### Microkernel & DDE case studies: Genode 90% disk throughput, MINIX recovery demo\n\nAn alternative approach, pioneered by microkernel operating systems, is to run drivers as isolated user-space processes. This provides exceptional security and reliability.\n\n* **Genode OS Framework**: Genode uses **Device Driver Environments (DDEs)** to run unmodified Linux drivers as sandboxed user-space components [technical_solution_cross_os_reuse_strategies.2.technical_mechanism[0]][19]. While this involves a significant porting effort (**1-3 person-months** per driver), it achieves impressive performance, reaching **90%** of native Linux disk throughput [technical_solution_cross_os_reuse_strategies.2.maintenance_and_performance_tradeoffs[0]][19].\n* **MINIX 3**: This OS was designed from the ground up with driver isolation in mind. All drivers run as separate, unprivileged server processes. A \"reincarnation server\" monitors these processes and can automatically restart a crashed driver in milliseconds, allowing the system to self-heal from driver failures without rebooting [technical_solution_cross_os_reuse_strategies.2.technical_mechanism[1]][39].\n\nThese examples prove that user-space driver architectures are not only viable but can provide levels of security and resilience that are impossible to achieve in a monolithic kernel.\n\n### Framework pick-list: FUSE, VFIO, SPDK, DPDK—when to use which\n\nFor a new OS, a hybrid approach using a mix of user-space frameworks is recommended, depending on the device class and requirements.\n\n| Framework | Primary Use Case | Key Benefit | Major Trade-off | Cross-OS Support |\n| :--- | :--- | :--- | :--- | :--- |\n| **FUSE** | Filesystems | High portability, easy development | Performance overhead (up to 80% slower) [technical_solution_user_space_frameworks.1[0]][40] | Excellent (Linux, macOS, Windows, BSDs) |\n| **VFIO** | Secure device passthrough | IOMMU-enforced security | Linux-only; requires hardware support | None (Linux-specific) |\n| **SPDK** | High-performance storage | Kernel bypass, extreme IOPS | Polling model consumes CPU cores | Good (Linux, FreeBSD) |\n| **DPDK** | High-performance networking | Kernel bypass, low latency | Polling model consumes CPU cores | Good (Linux, FreeBSD, Windows) |\n\nA new OS should prioritize implementing a FUSE-compatible layer for filesystem flexibility and a VFIO-like interface to enable high-performance frameworks like SPDK and DPDK.\n\n### Microkernel & DDE case studies: Genode 90% disk throughput, MINIX recovery demo\n\nAn alternative approach, pioneered by microkernel operating systems, is to run drivers as isolated user-space processes. This provides exceptional security and reliability.\n\n* **Genode OS Framework**: Genode uses **Device Driver Environments (DDEs)** to run unmodified Linux drivers as sandboxed user-space components [technical_solution_cross_os_reuse_strategies.2.technical_mechanism[0]][19]. While this involves a significant porting effort (**1-3 person-months** per driver), it achieves impressive performance, reaching **90%** of native Linux disk throughput [technical_solution_cross_os_reuse_strategies.2.maintenance_and_performance_tradeoffs[0]][19].\n* **MINIX 3**: This OS was designed from the ground up with driver isolation in mind. All drivers run as separate, unprivileged server processes. A \"reincarnation server\" monitors these processes and can automatically restart a crashed driver in milliseconds, allowing the system to self-heal from driver failures without rebooting [technical_solution_cross_os_reuse_strategies.2.technical_mechanism[1]][39].\n\nThese examples prove that user-space driver architectures are not only viable but can provide levels of security and resilience that are impossible to achieve in a monolithic kernel.\n\n### Framework pick-list: FUSE, VFIO, SPDK, DPDK—when to use which\n\nFor a new OS, a hybrid approach using a mix of user-space frameworks is recommended, depending on the device class and requirements.\n\n| Framework | Primary Use Case | Key Benefit | Major Trade-off | Cross-OS Support |\n| :--- | :--- | :--- | :--- | :--- |\n| **FUSE** | Filesystems | High portability, easy development | Performance overhead (up to 80% slower) [technical_solution_user_space_frameworks.1[0]][40] | Excellent (Linux, macOS, Windows, BSDs) |\n| **VFIO** | Secure device passthrough | IOMMU-enforced security | Linux-only; requires hardware support | None (Linux-specific) |\n| **SPDK** | High-performance storage | Kernel bypass, extreme IOPS | Polling model consumes CPU cores | Good (Linux, FreeBSD) |\n| **DPDK** | High-performance networking | Kernel bypass, low latency | Polling model consumes CPU cores | Good (Linux, FreeBSD, Windows) |\n\nA new OS should prioritize implementing a FUSE-compatible layer for filesystem flexibility and a VFIO-like interface to enable high-performance frameworks like SPDK and DPDK.\n\n## User-Space & Virtualization Layers — Isolation with near-native speed\n\nWhile the DSL and synthesis pipeline represent the long-term vision, a new OS needs a pragmatic strategy to achieve broad hardware support *today*. The most effective, lowest-effort approach is to leverage virtualization and user-space driver frameworks. By treating the hardware as a set of standardized virtual devices, an OS can abstract away the complexity of thousands of physical drivers.\n\n### VirtIO/vDPA performance table vs. SR-IOV & emulation\n\nThe **VirtIO** standard is the key to this strategy [technical_solution_virtualization_layer[11]][37]. It is a mature, open standard that defines a set of paravirtualized devices for networking, storage, graphics, and more. Its performance is excellent, and its cross-platform support is unmatched.\n\n| I/O Virtualization Technology | Mechanism | Performance (10G NIC) | GPU Efficiency | Security/Isolation | Key Use Case |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Full Emulation (QEMU)** | Software simulates real hardware (e.g., an Intel e1000 NIC). | Low (~1-2 Gbps) | Very Low | High (Full VM isolation) | Legacy OS support. |\n| **VirtIO (Paravirtualization)** | Guest-aware drivers talk to a standard virtual device. | High (**9.4 Gbps**) [technical_solution_virtualization_layer[2]][8] | Medium (~43%) | High (Full VM isolation) | Standard for cloud VMs; high-performance general I/O. |\n| **vDPA (VirtIO Datapath Accel.)** | VirtIO control plane, hardware datapath. | Very High (Near-native) | N/A | High (Full VM isolation) | High-performance networking/storage in VMs. [technical_solution_virtualization_layer[12]][38] |\n| **SR-IOV / Passthrough** | Hardware is partitioned and assigned directly to guest. | Native (**9.4 Gbps**) [technical_solution_virtualization_layer[2]][8] | Near-Native (**99%**) | Medium (Hardware-level) | Bare-metal performance for latency-sensitive workloads. |\n\nThis data shows that VirtIO, especially when combined with vDPA, offers performance that is competitive with direct hardware access, while providing the immense benefit of a stable, universal driver interface.\n\n## Next Steps\n\n- Objective 1: Very simple use cases for an app specific RTOS that is significantly differentiated from the application performance in Ubuntu 22 / Linux\n- Objective 2: Minimum set of drivers, and periphery software we need to write in no-std to ensure that these RTOSes can be run",
  "output": {
    "executive_summary": "The full performance potential of Rust, particularly its 'fearless concurrency' for multi-core systems, is fundamentally constrained by the inherent jitter of the general-purpose Linux kernel. This jitter, manifesting as unpredictable execution delays, undermines the determinism required for high-performance, latency-sensitive applications. The proposed solution is to develop an application-specific Rust RTOS, which would run as a dedicated binary on isolated hardware resources (CPU cores, RAM) alongside a host Linux system, offering a pragmatic path to adoption without the immense effort of building a general-purpose OS from scratch. Key findings confirm the feasibility of this approach, especially for I/O-intensive workloads where kernel-bypass techniques can yield latency improvements of over 30% at the p99.99th percentile compared to a highly tuned Linux system with PREEMPT_RT. The optimal architecture involves a microkernel-like design, leveraging existing Rust OS projects (e.g., Redox OS, rust-vmm), user-space frameworks (DPDK, SPDK), and virtualization with device passthrough (VFIO, SR-IOV). The economic justification for this significant engineering investment hinges on a breakeven model where the financial benefits of avoiding costly latency SLO violations and achieving hardware consolidation outweigh the high development and maintenance costs.",
    "feasibility_assessment": "Creating a Rust-based RTOS that outperforms a meticulously tuned Linux system (Ubuntu 22.04 with PREEMPT_RT) is feasible, but only under specific conditions and for certain classes of workloads. The key to success lies in architecting the RTOS to bypass the general-purpose overhead of the Linux kernel, leveraging techniques like poll-mode drivers, static scheduling on isolated cores, and direct hardware access. The performance advantage is most significant in I/O-intensive applications where kernel-induced jitter from interrupts, context switches, and background tasks becomes the primary bottleneck. Research indicates that for these workloads, a performance improvement of 30% or more at the p99.9 to p99.99 latency percentiles is achievable. \n\nSpecific viable workloads include:\n1.  **25/100GbE Packet Processing:** For applications like High-Frequency Trading (HFT) and Network Function Virtualization (NFV). A benchmark comparing a DPDK-based solution to a tuned Linux kernel showed a **46.7% reduction in P99.99 latency** (403µs vs. 758µs), demonstrating a clear advantage.\n2.  **Low-Latency Key-Value Store on NVMe:** For real-time analytics and caching. Using SPDK to bypass the kernel's storage stack can yield **5x lower latency** for isolated operations. Even within Linux, hardware co-design like Intel's Application Device Queues (ADQ) has shown a **38% improvement in P99.99 tail latency**.\n3.  **Real-time Audio Digital Signal Processing (DSP):** An RTOS provides a more deterministic environment to prevent audible artifacts (xruns) caused by jitter from sources like SMIs or `ksoftirqd` activity, which can persist even with PREEMPT_RT.\n4.  **Robotics Control Loops:** The stability and safety of robotic systems depend on the highly predictable execution of control algorithms. An RTOS can guarantee strict deadlines, which is a level of determinism a general-purpose OS cannot provide.\n\nIn conclusion, the feasibility is high for applications where the cost of tail latency is extreme and the workload can be structured to fit a kernel-bypass, poll-mode architecture. It is not a universally superior solution but a highly specialized tool for solving extreme performance challenges.",
    "recommended_architecture": "The most practical and performant architectural path for an application-specific Rust RTOS is a hybrid model running the RTOS as a guest virtual machine on a tuned Linux host. \n\n**Deployment Model:** The Rust RTOS should be deployed as a guest VM managed by KVM on a host running a Linux distribution with the PREEMPT_RT kernel (e.g., Ubuntu 22.04). The host system must be meticulously tuned to dedicate resources to the VM. This includes using kernel boot parameters like `isolcpus`, `nohz_full`, and `rcu_nocbs` to isolate physical CPU cores, pinning the VM's vCPUs to these isolated cores, and allocating the VM's memory from pre-provisioned hugepages to minimize TLB misses and memory management overhead.\n\n**I/O Strategy:** For maximum performance and predictability, the I/O strategy should be direct device assignment using **VFIO passthrough** or **SR-IOV**. This gives the Rust RTOS guest exclusive, direct control over physical hardware like high-performance NICs (e.g., Mellanox ConnectX-6 Dx, Intel E810) and NVMe SSDs (e.g., Intel Optane P5800X). This approach bypasses the host kernel's I/O stacks and the hypervisor's virtual switch, offering near-native performance with strong IOMMU-enforced security. For environments where operational flexibility like live migration is required, **VirtIO with vDPA (VirtIO Datapath Acceleration)** is the recommended alternative, offering performance that approaches passthrough while retaining the manageability of the VirtIO standard.\n\n**Concurrency Model:** Inside the Rust RTOS, the concurrency model should be **thread-per-core with poll-mode work loops**. Each dedicated core runs a tight, non-blocking loop that continuously polls the hardware for work. This model, exemplified by frameworks like DPDK for networking and SPDK for storage, eliminates the overhead and non-determinism of interrupts and kernel-level scheduling, which is the key to achieving minimal and predictable tail latency.",
    "linux_jitter_analysis": "Linux kernel jitter is the variance in the time expected for program execution, manifesting as unpredictable delays or 'hiccups' that are detrimental to real-time applications. Even on a highly optimized system, sources of non-determinism persist due to Linux's nature as a general-purpose, multi-user operating system.\n\n**Primary Causes of Jitter:**\n*   **Interrupt Handling:** High-frequency hardware interrupts (interrupt storms) and the subsequent processing of deferred interrupts (softirqs) by the `ksoftirqd` kernel daemon can introduce significant, unpredictable delays under heavy load.\n*   **Scheduler Latency:** The overhead of context switching, potential priority inversion issues, and the default throttling of real-time tasks (which can be disabled) contribute to jitter.\n*   **Kernel Background Tasks:** System-wide mechanisms like Read-Copy Update (RCU) introduce latency spikes during their 'grace periods' when they perform memory reclamation.\n*   **System Management Interrupts (SMIs):** Non-maskable interrupts handled by the system's BIOS/firmware that are completely outside the OS's control. SMIs can halt all CPU cores for microseconds to milliseconds, causing major, unpredictable latency spikes.\n*   **General-Purpose Overhead:** Complex subsystems for virtual memory, filesystems, and networking introduce layers of abstraction and potential for non-determinism.\n*   **Cache Contention:** On multi-core systems, contention for shared resources like the L3 cache can lead to high miss rates and inflated latency.\n\n**Comprehensive Mitigation Strategies:**\nA multi-layered approach is required to achieve real-time performance on Linux:\n1.  **Kernel Patching:** Use the **PREEMPT_RT patch set**, which makes the kernel more preemptible and is the foundation for real-time Linux.\n2.  **Kernel Boot Parameters (GRUB):** This is the most critical area for tuning.\n    *   **Core Isolation:** `isolcpus` to remove cores from the general scheduler, `nohz_full` to disable the scheduler tick on those cores, and `rcu_nocbs` to offload RCU callbacks.\n    *   **Interrupt Management:** `irqaffinity` to confine interrupts to non-isolated 'housekeeping' cores.\n    *   **Power Management:** `processor.max_cstate=0`, `intel_pstate=disable`, and `idle=poll` to prevent latency spikes from CPU frequency or power state changes.\n    *   **Timer Tuning:** `skew_tick=1` to offset RCU timers and prevent simultaneous firing.\n3.  **System Tuning (`sysctl`):** Set `kernel.sched_rt_runtime_us=-1` to allow real-time tasks to run indefinitely without being throttled by the kernel's default 95% runtime limit.\n4.  **Scheduler Policy:** Assign real-time application threads to the `SCHED_FIFO` or `SCHED_RR` scheduling policies with a high priority (e.g., 99).\n5.  **User-Space Kernel Bypass:** For extreme performance, use frameworks like **DPDK (Data Plane Development Kit)** for networking and **SPDK (Storage Performance Development Kit)** for storage. These allow applications to gain direct hardware access, completely avoiding the kernel's I/O stacks, interrupts, and context switches.",
    "candidate_workloads_for_rtos": [
      {
        "workload_name": "25/100GbE Packet Processing (e.g., High-Frequency Trading, NFV)",
        "rationale": "These applications are extremely sensitive to packet processing latency. Even with a tuned PREEMPT_RT kernel, high-frequency interrupt storms and softirq processing by `ksoftirqd` introduce unpredictable delays. An app-specific RTOS using a kernel-bypass framework like DPDK eliminates these issues. Benchmarks show a DPDK-based solution achieving a 46.7% reduction in P99.99 latency (403µs vs. 758µs for tuned Linux) and a 51% overall performance lead, far exceeding the 30% improvement threshold.",
        "key_performance_indicators": "The primary KPIs are p99, p99.9, and p99.99 latency, measured in microseconds. Jitter, defined as the maximum variance in latency, is also critical. Throughput, measured in packets per second (PPS) and Gbps, must be maintained or improved.",
        "measurement_plan": "Use a high-precision packet generator like MoonGen, which leverages hardware timestamping on modern NICs for sub-microsecond accuracy. Generate traffic at 25/100GbE line rates with a mix of packet sizes (e.g., 64B, IMIX). The test should run for an extended period to capture rare tail-latency events."
      },
      {
        "workload_name": "Low-Latency Key-Value Store on NVMe (e.g., Real-time Analytics, Database Caching)",
        "rationale": "Bypassing the kernel's storage stack with SPDK can yield 5x lower latency than Linux for isolated I/O. While SPDK's performance can degrade in shared-core scenarios on Linux due to scheduler preemption, an app-specific RTOS provides the necessary deterministic scheduling to prevent this. Even within Linux, hardware/software co-design has demonstrated a 38% improvement in P99.99 tail latency, and projects like `blk-switch` show potential for 6-15x better P99.9 latency, indicating significant room for improvement by moving to a dedicated RTOS.",
        "key_performance_indicators": "The primary KPIs are average, p99, p99.9, and p99.99 latency for I/O operations. Secondary metrics include I/O operations per second (IOPS) and throughput in MB/s.",
        "measurement_plan": "Use `fio` with the `libaio` engine and kernel polling for the Linux baseline. For the RTOS, use SPDK's `perf` tool or a `fio` build with the SPDK engine. The workload should consist of 4KB random read/write operations with varying queue depths (especially low QDs like 1, 2, 4). For a full key-value store, use the Yahoo! Cloud Serving Benchmark (YCSB)."
      },
      {
        "workload_name": "Real-time Audio Digital Signal Processing (DSP)",
        "rationale": "Jitter in audio processing manifests as audible clicks and pops known as xruns (buffer underruns/overruns). Even with a fully tuned PREEMPT_RT kernel, unpredictable events like System Management Interrupts (SMIs) or heavy `ksoftirqd` activity can cause deadlines to be missed, leading to xruns. An RTOS provides a more deterministic execution environment by dedicating CPU cores and minimizing all OS overhead, ensuring audio processing deadlines are consistently met under all conditions.",
        "key_performance_indicators": "The primary KPI is an xrun count of zero over an extended test period under load. Secondary KPIs include the P99.9 and P99.99 latency of audio buffer processing blocks and the overall round-trip latency.",
        "measurement_plan": "Use `cyclictest` to measure raw kernel scheduling latency as a baseline. For application-level testing, use an audio framework like the JACK Audio Connection Kit to run a DSP workload. Monitor for xruns over several hours while applying synthetic system stress. Analyze the distribution of audio buffer processing times to quantify jitter."
      },
      {
        "workload_name": "Robotics Control Loops",
        "rationale": "The stability, precision, and safety of robotic systems depend on the highly predictable, low-latency execution of control algorithms. Jitter in a general-purpose OS like Linux can cause a robot's control loop to become unstable, leading to inaccurate movements or unsafe behavior. An RTOS can guarantee that strict deadlines are met, ensuring precise and reliable synchronization between sensor inputs and actuator outputs, which is critical for the system's physical integrity.",
        "key_performance_indicators": "The primary KPIs are the variance in control loop execution times (jitter) and the maximum deviation from the target cycle time. P99.9 and P99.99 latency of individual loop iterations are also critical. The overall stability of the control loop frequency is a key outcome.",
        "measurement_plan": "Implement a closed-loop control system on the target platform. Use hardware-level timing for the most accurate measurements. For example, toggle a GPIO pin at the start and end of the control loop and measure the pulse width and frequency with a high-precision logic analyzer or oscilloscope. This captures the true end-to-end latency without software measurement overhead."
      }
    ],
    "architecture_comparison_co_kernel_vs_vm": {
      "co_kernel_approach": "The Xenomai/Cobalt co-kernel approach integrates a small, high-priority real-time nucleus that runs alongside the standard Linux kernel. This is achieved by patching the kernel with an interrupt pipeline (I-pipe or Dovetail), which allows the co-kernel to intercept hardware interrupts before the Linux kernel. Real-time drivers register their handlers with an 'out-of-band' (OOB) flag, ensuring they execute with the highest priority and minimal latency, bypassing Linux's standard interrupt handling. This architecture aims for near bare-metal performance, with best-case latencies reported under 2 microseconds (µs). However, worst-case latencies can be significantly higher, ranging from 35-50 µs on embedded platforms to over 250 µs for user-space tasks under heavy load. Achieving this performance requires extensive kernel configuration, disabling power management features, and understanding specific constraints like its 'lazy rescheduling' scheme. The primary drawback is its weak isolation model; since the Cobalt core and Linux kernel share the same memory space and privilege level, a crash or vulnerability in any part of the standard kernel can corrupt the real-time components and compromise the entire system.",
      "rtos_in_vm_approach": "Running a dedicated RTOS inside a KVM virtual machine (VM) is an architecture focused on strong isolation and leveraging mature virtualization technologies. In this model, a host Linux system, typically running a PREEMPT_RT kernel, acts as the hypervisor. The application-specific RTOS runs as a guest, with its virtual CPUs (vCPUs) pinned to dedicated physical cores that have been isolated from the host scheduler using kernel parameters like `isolcpus`, `nohz_full`, and `rcu_nocbs`. This prevents host-level tasks from preempting the real-time workload. While virtualization introduces inherent overhead, modern hardware features like Intel's Posted Interrupts and AMD's AVIC reduce interrupt delivery latency. With meticulous tuning of the host and guest, this approach can achieve excellent real-time performance, with worst-case latencies typically between 20 µs and 85 µs, and anything under 100 µs considered great for a virtualized environment. The key advantage is the robust security and failure isolation provided by the KVM hypervisor and the hardware IOMMU, which sandboxes the RTOS and protects the host from guest failures or DMA attacks.",
      "comparison_summary": "The primary trade-off between the two approaches is performance versus isolation and maintainability. The Xenomai/Cobalt co-kernel can offer marginally lower best-case latencies due to its direct hardware access, but its performance is highly variable and its predictability is contingent on managing a custom-patched kernel. Its most significant weakness is the lack of strong failure isolation and security; a fault in the non-real-time Linux kernel can bring down the entire system. In contrast, the RTOS-in-KVM approach provides superior failure isolation and security through hardware-enforced boundaries (IOMMU) and process-level sandboxing of the VM. A crash within the guest RTOS is fully contained and does not affect the host. While its best-case latency is slightly higher due to virtualization overhead, it offers highly predictable worst-case performance (typically <100 µs) when properly tuned. Operationally, the KVM approach is more maintainable as it relies on mainline kernel features (KVM, PREEMPT_RT, VFIO) rather than managing a non-standard, patched kernel.",
      "recommendation": "For most production systems, the **RTOS-in-KVM** approach is the recommended architectural path. While Xenomai may offer lower latencies in ideal, non-production scenarios, the overwhelming benefits of the KVM model in terms of robustness, security, and long-term maintainability make it the superior choice for building reliable systems. The hardware-enforced isolation provided by KVM and the IOMMU ensures that failures or security breaches in the non-real-time components do not compromise the critical RTOS workload, which is a decisive advantage for system stability. The performance gap has narrowed significantly with modern hardware virtualization features, making KVM-based solutions capable of meeting the requirements of many hard real-time systems while providing a much safer and more manageable operational foundation."
    },
    "io_virtualization_strategy_comparison": {
      "vfio_passthrough_analysis": "VFIO (Virtual Function I/O) is a Linux kernel framework that enables direct and secure assignment of a physical PCIe device to a user-space application or a virtual machine, a process known as 'passthrough'. It delivers near-native performance by completely bypassing the hypervisor's I/O stack for data plane operations, making it an excellent choice for latency-critical workloads. Performance tests show a VFIO-passthrough NIC can achieve throughput nearly identical to bare metal (e.g., 9.4 Gbps on a 10G NIC). The security of VFIO is anchored by the hardware IOMMU (Intel VT-d or AMD-Vi), which provides DMA and interrupt remapping. The IOMMU acts as a hardware firewall, ensuring the guest can only access memory regions explicitly assigned to it, thereby preventing DMA-based attacks on the host. The primary trade-off is a significant loss of operational flexibility; live migration of VMs with passthrough devices is highly complex, device-dependent, and generally not supported for all migration types (e.g., post-copy), tying the VM to specific host hardware.",
      "virtio_vdpa_analysis": "VirtIO with vDPA (VirtIO Datapath Acceleration) is a modern, hybrid approach that combines the flexibility of the standard VirtIO paravirtualization interface with the performance of hardware offload. It maintains the standard VirtIO control plane, meaning the guest OS uses a standard, unmodified VirtIO driver. However, the data plane is offloaded directly to hardware, often leveraging an underlying SR-IOV Virtual Function. This architecture provides performance that approaches VFIO passthrough (e.g., ~70 Gbit/s vs. 80 Gbit/s for SR-IOV in one test) while being vastly superior to software-only VirtIO. Its key advantage is retaining the operational benefits of VirtIO, most importantly robust support for live migration. This makes vDPA an ideal choice for cloud-native and NFV environments where both high performance and operational agility are required. Furthermore, its mediated architecture can offer a better security posture by restricting direct hardware register access from the guest.",
      "sr_iov_analysis": "SR-IOV (Single Root I/O Virtualization) is a hardware-based virtualization standard defined by the PCI-SIG. It allows a single physical device (the Physical Function, or PF) to be partitioned into multiple, independent Virtual Functions (VFs). Each VF can be directly assigned to a different VM, functioning as a dedicated device. This provides near-native performance and strong hardware-enforced isolation between VMs sharing the device. For data plane traffic, SR-IOV bypasses the hypervisor's virtual switch, achieving line-rate throughput that is virtually identical to bare metal. It is a specific and widely adopted implementation of the device passthrough concept. Like VFIO, its primary drawback is the lack of support for live migration, which makes it less suitable for dynamic cloud environments where workload mobility is important.",
      "recommendation": "The optimal I/O strategy depends on the specific trade-off between absolute performance and operational flexibility. \n1. **For Maximum Performance and Predictability:** If the workload is static and live migration is not a requirement (e.g., dedicated high-frequency trading or HPC), **VFIO passthrough or SR-IOV** is the recommended choice. They offer the lowest possible latency and highest throughput by providing direct, unimpeded hardware access. \n2. **For a Balance of High Performance and Flexibility:** If the workload demands high performance but must also support operational agility like live migration (e.g., in a cloud or NFV environment), **VirtIO with vDPA** is the superior modern alternative. It delivers performance that rivals passthrough while retaining the manageability and mobility of the VirtIO ecosystem. \n3. **For General-Purpose Virtualization:** For workloads without extreme latency requirements, standard software-based **VirtIO** remains the de facto standard due to its universal support, simplicity, and robust operational features."
    },
    "high_performance_framework_integration": "Integrating high-performance user-space frameworks like the Data Plane Development Kit (DPDK) for networking and the Storage Performance Development Kit (SPDK) for storage is the cornerstone of achieving minimal I/O latency in a custom RTOS. These frameworks operate on the principle of kernel-bypass, where the application, linked with the framework's libraries, takes direct control of the hardware, typically a Network Interface Card (NIC) or an NVMe SSD. This is achieved by unbinding the device from the host kernel driver and re-binding it to a user-space I/O (UIO) or VFIO driver, allowing the application to map the device's hardware resources (e.g., PCI BARs for MMIO) directly into its own address space.\n\n**Primary Use Cases and Benefits:**\n*   **DPDK (Networking):** Used for applications requiring extremely fast packet processing, such as Network Function Virtualization (NFV), High-Frequency Trading (HFT), and custom firewalls or load balancers. By using poll-mode drivers (PMDs) that continuously poll the NIC for new packets, DPDK eliminates the latency associated with interrupt handling. This approach, combined with zero-copy buffer management using pre-allocated memory pools (from hugepages), allows applications to achieve line-rate throughput at 25/100GbE with minimal and predictable tail latency. Benchmarks show a DPDK-based solution achieving a 46.7% reduction in P99.99 latency compared to a highly tuned Linux kernel server.\n*   **SPDK (Storage):** Used for high-performance storage applications like databases, real-time analytics, and caching layers. Similar to DPDK, it uses a polled-mode, asynchronous, and lockless model to interact directly with NVMe devices. This avoids the entire kernel storage stack, resulting in significantly higher IOPS and lower latency. Benchmarks show SPDK can achieve 5x lower latency than Linux for isolated 4KB random I/O and is often the only framework capable of fully saturating the bandwidth of modern NVMe drives like the Intel Optane P5800X.\n\n**Virtualization and Passthrough Technologies:**\nFor virtualized deployments, several technologies facilitate high-performance I/O:\n*   **SR-IOV (Single Root I/O Virtualization):** A PCI standard that allows a single physical device to be partitioned into multiple Virtual Functions (VFs). Each VF can be directly assigned to a different VM (a technique known as passthrough). The guest OS then runs a driver (e.g., a DPDK PMD) that interacts directly with the VF, bypassing the hypervisor's virtual switch for the data plane. This provides near-native performance, with benchmarks showing line-rate throughput. The primary trade-off is operational inflexibility, as live migration of VMs with passthrough devices is generally not supported.\n*   **VFIO (Virtual Function I/O):** The Linux kernel framework that enables secure device passthrough. It leverages the IOMMU (Input-Output Memory Management Unit) to create hardware-enforced isolation, preventing a VM from performing malicious DMA attacks on host memory. VFIO is the underlying mechanism for SR-IOV passthrough and is essential for security.\n*   **VirtIO and vDPA (VirtIO Datapath Acceleration):** VirtIO is a paravirtualization standard that offers high performance with the key benefit of supporting live migration. vDPA is a modern evolution that combines the flexible VirtIO control plane with a hardware-accelerated datapath (often using an underlying SR-IOV VF). This provides performance that approaches SR-IOV passthrough (~70 Gbit/s vs. ~80 Gbit/s in one test) while retaining the operational benefits of VirtIO, making it an ideal choice for cloud-native and NFV environments where both performance and agility are required.",
    "scheduler_and_concurrency_model": "The optimal scheduling and concurrency model for a low-jitter Rust RTOS prioritizes determinism and minimal overhead over general-purpose flexibility and resource utilization. The analysis points towards a static, partitioned model as superior for achieving the lowest possible jitter.\n\n**Scheduling Model: Static Core Partitioning vs. Dynamic Schedulers**\n*   **Static Core Partitioning with Poll-Mode Work Loops:** This is the recommended approach for minimal jitter. It involves dedicating specific CPU cores exclusively to real-time tasks. In a Linux host context, this is achieved with kernel parameters like `isolcpus`, `nohz_full` (to disable the scheduler tick), and `rcu_nocbs` (to offload RCU callbacks). This creates a 'tickless' environment where the RTOS can run its tasks in a tight, 'run-to-completion' or poll-mode loop without interference from the host OS. This model is the foundation of high-performance frameworks like DPDK and SPDK and is proven to reduce latency by orders of magnitude by eliminating sources of OS jitter like timer interrupts and scheduler migrations. The main trade-off is lower overall CPU utilization, as dedicated cores may be idle if their task has no work.\n*   **Dynamic Schedulers (EDF, Rate-Monotonic):** While classic real-time algorithms like Earliest Deadline First (EDF) and Rate-Monotonic (RM) are effective for managing a mix of real-time tasks and can theoretically achieve higher CPU utilization, they are less suitable when absolute lowest jitter is the goal. The inherent overhead of dynamic priority calculation, preemption, and context switching introduces a level of non-determinism that static partitioning avoids. For multiprocessor systems, partitioned scheduling (per-core run queues) is superior to global scheduling (a single shared run queue) for jitter reduction, as it avoids the high costs of task migration, cache invalidation, and lock contention.\n\n**Concurrency Model: Thread-per-Core vs. Async/Await**\n*   **Thread-per-Core (DPDK/SPDK Model):** This model aligns perfectly with static core partitioning. A single application thread is pinned to a dedicated core and runs a continuous work loop, polling for I/O. This design offers excellent cache locality, as data is processed on the same core where it is received. It virtually eliminates context-switching overhead for the data plane. This is the superior choice for applications demanding the absolute minimal and most predictable jitter, such as HFT or professional audio processing. The primary cost is 100% CPU utilization on the dedicated cores.\n*   **Async/Await Executor (Embassy Model):** This model provides cooperative multitasking, where tasks voluntarily yield control. For `no_std` environments, frameworks like Embassy offer a highly efficient, allocation-free executor where task state is stored statically. Its wakeup mechanism is very low-overhead, and when no tasks are ready, it can put the CPU into a low-power sleep state (`WFE`), making it far more power-efficient. While it offers an excellent developer experience and low overhead, the act of task switching, even if cooperative, can introduce more variability and impact cache performance compared to a dedicated thread-per-core loop. It is a compelling choice for a broader range of real-time applications where some latency variation is acceptable in exchange for better resource efficiency and ergonomics.\n\nFor the stated goal of beating a tuned Linux baseline by a significant margin at p99.99 latency, the **static core partitioning model with a thread-per-core work loop** is the recommended architecture.",
    "memory_and_storage_strategy": "A robust memory and storage strategy for a `no_std` Rust RTOS must prioritize performance, predictability, and reliability, starting from memory allocation up to persistent storage management.\n\n**Memory Management and Allocators:**\nIn a `no_std` environment, a custom memory allocator is required. For a high-performance, concurrent RTOS, a **per-core or thread-local allocator** is the best strategy. This design, exemplified by frameworks like Seastar, avoids contention on a single global lock by having each core manage its own memory pool, requesting larger chunks from a global source only when necessary. This minimizes cross-core cache-line bouncing and scales well. For specific allocation patterns, other allocators can be used:\n*   **Fixed-Size Slab Allocators (`slaballoc`):** Ideal for managing pools of fixed-size objects, such as network buffers or I/O request structures. Lockless implementations are available.\n*   **Bump/Arena Allocators (`bumpalo`):** Extremely fast for phased allocations where all objects can be deallocated at once. This is suitable for temporary, per-request memory.\n\n**Zero-Copy I/O and DMA-Safe Buffers:**\nTo achieve minimal I/O latency, zero-copy techniques are mandatory. This involves allowing a device's Direct Memory Access (DMA) engine to access application memory directly. \n*   **DMA-Safe Buffers:** Buffers used for DMA must be physically contiguous and pinned in memory. The most effective strategy, used by both DPDK and SPDK, is to pre-allocate a large pool of memory from **hugepages** (e.g., 2MiB or 1GiB). This memory is guaranteed to be DMA-safe. The RTOS must use a specific allocator that draws from this pre-registered pool (e.g., `spdk_dma_malloc`).\n*   **IOMMU Integration:** The IOMMU (VFIO framework in Linux) provides the security for this direct access. The application must register its DMA memory region with the IOMMU once during initialization (`VFIO_IOMMU_MAP_DMA`). Subsequent I/O operations then use buffers from within this pre-mapped region, avoiding per-I/O registration overhead.\n\n**Persistent Storage Strategy:**\nThe choice is between raw block access for maximum performance and a lightweight filesystem for reliability and ease of use.\n*   **Option 1: Raw SPDK Volumes with a Custom Log-Structured Store:** This approach offers the highest performance. The application would use SPDK to interact with raw NVMe namespaces. For data integrity, a custom log-structured or journaled store would be implemented on top. Durability is ensured by directly using NVMe commands exposed by SPDK, such as setting the **Force Unit Access (FUA)** flag on individual writes or issuing a **Flush** command to persist a batch of writes. This path provides ultimate control but carries a very high development burden for implementing crash consistency, recovery, and wear management.\n*   **Option 2: Lightweight Crash-Consistent Filesystem (LittleFS):** This is the recommended approach for most RTOS use cases. **LittleFS** is specifically designed for `no_std`, resource-constrained environments and flash storage. \n    *   **Crash Consistency:** It provides excellent power-loss resilience through a copy-on-write (COW) design, ensuring the filesystem is always in a consistent state.\n    *   **Wear Leveling:** It implements dynamic wear leveling to extend the life of flash media.\n    *   **`no_std` Suitability:** It is a standalone C library with no external dependencies and bounded RAM usage, making it easy to integrate into a Rust RTOS via FFI.\n    *   **Trade-offs:** While robust, its write performance can be a limitation compared to raw device access.\n\nFor the storage backend, using a device like the **Intel Optane P5800X** is highly recommended. Its 3D XPoint technology avoids the garbage collection cycles inherent to NAND flash, providing exceptionally low and predictable tail latency, which is critical for an RTOS.",
    "hardware_selection_recommendations": [
      {
        "component_type": "CPU",
        "recommended_model": "Intel Xeon (Sapphire Rapids/Emerald Rapids or newer)",
        "rationale": "Intel Xeon processors are recommended due to the maturity and robust documentation of the Intel Resource Director Technology (RDT) suite. Specifically, Cache Allocation Technology (CAT) allows for the hardware-enforced partitioning of the Last-Level Cache (LLC), which is critical for isolating the RTOS from 'noisy neighbors' and preventing cache contention, a major source of jitter. The RDT features are well-supported in the Linux kernel via the `resctrl` filesystem, providing a stable and well-understood interface for configuration. Furthermore, modern Intel CPUs feature a reliable Invariant Time Stamp Counter (TSC), which is essential for high-precision, low-overhead timekeeping.",
        "alternatives_considered": "AMD EPYC (Genoa/Bergamo) processors are a strong alternative, offering superior core density and power efficiency. They provide similar hardware QoS features, but the software interface via `resctrl` can have nuances, such as per-Core Complex (CCX) resource IDs, which require more careful task pinning and configuration. While functionally capable, the Intel ecosystem for these specific resource control features is currently more mature and straightforward to implement."
      },
      {
        "component_type": "NIC",
        "recommended_model": "Mellanox/NVIDIA ConnectX-6 Dx or newer",
        "rationale": "The ConnectX-6 Dx is recommended for its explicit and well-documented support for hardware-accelerated vDPA (VirtIO Datapath Acceleration) when the port is configured in `switchdev` mode. This technology provides the ideal balance for a virtualized RTOS, offering performance that approaches direct SR-IOV passthrough (~67-75 Gbit/s) while retaining the operational flexibility of the standard VirtIO control plane, including support for live migration. This allows for high-performance I/O without tying the VM to a specific physical device, which is a major operational advantage.",
        "alternatives_considered": "The Intel E810 (800 Series) is a capable alternative that offers excellent SR-IOV support with a high density of up to 256 Virtual Functions (VFs) and robust DPDK drivers. However, the documentation and demonstrated ecosystem for hardware-accelerated vDPA are clearer and more established for the Mellanox/NVIDIA cards in the provided research, making them the preferred choice for a flexible, high-performance virtualized deployment."
      },
      {
        "component_type": "NVMe Storage",
        "recommended_model": "Intel Optane SSD P5800X",
        "rationale": "For workloads where storage I/O latency and predictability are critical, the Intel Optane P5800X is the unequivocal choice. Its 3D XPoint memory technology is fundamentally different from NAND flash, as it is a write-in-place medium that does not suffer from the garbage collection cycles that cause unpredictable latency spikes in NAND SSDs. The P5800X delivers exceptionally low and predictable tail latency (e.g., <66 µs at 99.999% QoS for 4K random read/write) and maintains this performance even under heavy, sustained, mixed workloads. Its high endurance (100 DWPD) also makes it ideal for write-intensive logging or caching tiers.",
        "alternatives_considered": "Enterprise TLC NAND SSDs, such as the Samsung PM1733/PM1735, offer high throughput but cannot match Optane's latency profile. Their reliance on a Flash Translation Layer (FTL) and background garbage collection introduces inherent non-determinism, leading to higher average latencies (e.g., 100µs for a 4KB read) and significantly less predictable tail latencies, making them less suitable for hard real-time storage operations."
      }
    ],
    "reusable_components_from_rust_ecosystem": "A comprehensive analysis of the Rust OS ecosystem reveals a viable path to accelerate the development of a server-class RTOS by combining components from several key projects. The recommended strategy is to build upon a microkernel foundation, integrate a mature virtualization stack, and use existing high-performance libraries for I/O.\n\n1.  **Virtualization and I/O Stack (from `rust-vmm`):** This is the most critical asset for reuse. `rust-vmm` is an industry-backed project (AWS, Google, Intel, Red Hat) providing a collection of high-quality, secure, and modular Rust crates for building custom Virtual Machine Monitors (VMMs). It is the de facto standard for virtualization in Rust. Key reusable crates include:\n    *   `kvm-bindings` & `kvm-ioctls`: For safe interaction with the Linux KVM hypervisor.\n    *   `vm-memory`: For safe management of guest memory.\n    *   `vhost` & `vhost-user-backend`: For implementing high-performance VirtIO device backends.\n    *   `vfio-bindings` & `vfio-ioctls`: For secure device passthrough using the VFIO framework.\n    *   `virtio-queue` & `virtio-bindings`: For building VirtIO guest drivers.\n    Adopting `rust-vmm` entirely for the hypervisor layer avoids years of development effort on a highly complex and security-sensitive component.\n\n2.  **Kernel Foundation (from Redox OS):** Redox OS provides the most promising starting point for a kernel. Its microkernel architecture, where drivers run as isolated user-space daemons, offers excellent stability and security, which are highly desirable for an RTOS. Its support for x86-64 and UEFI booting makes it directly applicable to server hardware. The user-space driver model can be adopted to build a resilient system.\n\n3.  **Standard Library (from Redox OS):** The `relibc` crate from the Redox project is a mature, portable, and safe implementation of the POSIX C standard library written in Rust. It can be readily integrated to provide essential libc functionality.\n\n4.  **Guest VirtIO Drivers (from `rcore-os/virtio-drivers`):** This external project provides `no_std` compatible guest drivers for VirtIO devices (Net, Block, etc.). This is essential for running the custom RTOS itself as a KVM guest, which is a key part of the development and deployment strategy.\n\n5.  **Architectural Inspiration (from Theseus, Tock, Hubris):** While not directly reusable for server hardware, these projects offer valuable concepts. Theseus's \"intralingual\" design and live-update capabilities are powerful ideas for future development. The hard real-time scheduling concepts from Hubris can inform the design of the custom RTOS scheduler.\n\n**Identified Gaps and Strategy:** The primary gap is the lack of a proven, hard real-time scheduler in the existing projects. Therefore, the development strategy should be to: (1) Adopt the Redox OS microkernel as a base. (2) Build all virtualization capabilities using `rust-vmm` crates. (3) Integrate high-performance libraries like DPDK and SPDK as user-space services for kernel-bypass I/O. (4) Focus the majority of novel development effort on creating a low-jitter, real-time scheduler tailored to the specific application workloads.",
    "minimum_bootable_component_checklist": [
      {
        "component_name": "Bootloader",
        "description": "Initializes the CPU, transitions to 64-bit long mode, sets up basic memory paging, and loads the Rust kernel into memory before transferring execution control.",
        "deployment_target": "Bare-Metal & KVM Guest",
        "rust_crate_availability": "Available (e.g., `bootloader`, `limine-rs`)"
      },
      {
        "component_name": "ACPI Parsing",
        "description": "Discovers the system's hardware configuration by locating and parsing ACPI tables (RSDP, XSDT, MADT, FADT) provided by the firmware.",
        "deployment_target": "Bare-Metal",
        "rust_crate_availability": "Available (e.g., `acpi`)"
      },
      {
        "component_name": "APIC & Timers",
        "description": "Initializes and manages the Local APIC (LAPIC) for per-CPU interrupts and a high-precision timer (HPET or TSC-Deadline mode) for scheduling.",
        "deployment_target": "Bare-Metal",
        "rust_crate_availability": "Available (e.g., `lapic`, `x2apic`)"
      },
      {
        "component_name": "PCIe Bus Enumeration",
        "description": "Scans the PCIe bus to discover connected hardware devices by reading their configuration space to identify vendor/device IDs and capabilities.",
        "deployment_target": "Bare-Metal & KVM Guest",
        "rust_crate_availability": "Available (e.g., `pci`, `pcid`) "
      },
      {
        "component_name": "MSI/MSI-X Interrupt Handling",
        "description": "Configures and handles Message Signaled Interrupts, the modern mechanism for devices to signal events to the CPU, which is essential for high-performance I/O.",
        "deployment_target": "Bare-Metal",
        "rust_crate_availability": "Partial (Requires custom logic built on PCI and APIC crates)"
      },
      {
        "component_name": "Native NVMe Driver",
        "description": "A driver to communicate directly with an NVMe storage device over the PCIe bus, responsible for initializing the controller and managing submission/completion queues.",
        "deployment_target": "Bare-Metal",
        "rust_crate_availability": "Partial (Crates like `rust-nvme` exist but require evaluation and likely significant porting for kernel-level, `no_std` use)"
      },
      {
        "component_name": "VirtIO Block Driver",
        "description": "A guest driver that implements the VirtIO specification for block devices, allowing the RTOS to perform storage I/O when running as a KVM guest.",
        "deployment_target": "KVM Guest",
        "rust_crate_availability": "Available (e.g., `rcore-os/virtio-drivers`)"
      },
      {
        "component_name": "VirtIO Network Driver",
        "description": "A guest driver that implements the VirtIO specification for network devices, enabling network communication for the RTOS when running as a KVM guest.",
        "deployment_target": "KVM Guest",
        "rust_crate_availability": "Available (e.g., `rcore-os/virtio-drivers`)"
      }
    ],
    "security_and_safety_posture": "The security and safety posture of a Rust-based RTOS is significantly enhanced by the language's core features, but it must be complemented by hardware isolation and rigorous verification methodologies to address the remaining attack surface.\n\n**Guarantees Gained from Rust:**\nRust's primary contribution is compile-time memory safety. Its ownership and borrow-checking system eliminates entire classes of severe vulnerabilities common in systems programming, such as use-after-free, double-free, buffer overflows, and data races in concurrent code. By writing the majority of the RTOS, including drivers and the scheduler, in 'Safe Rust', we gain a high degree of confidence that these memory-related bugs are absent, which is a monumental improvement over C/C++ based systems.\n\n**Remaining Attack Surface and the Role of `unsafe`:**\nDespite Rust's safety, interaction with hardware is inherently unsafe from the compiler's perspective. This requires the use of the `unsafe` keyword.\n*   **`unsafe` Blocks:** `unsafe` is necessary for operations like dereferencing raw pointers (for MMIO), calling FFI functions (to C libraries or assembly), and modifying mutable static variables. The key strategy is to **minimize and encapsulate** `unsafe` code within safe, high-level abstractions. For instance, raw hardware interactions should be confined to a low-level Hardware Abstraction Layer (HAL), allowing the main driver logic to be written in Safe Rust.\n*   **Auditing `unsafe`:** Every `unsafe` block represents a manual proof obligation on the developer. These blocks must be rigorously audited and documented, not just explaining *what* is being done, but *why* the operation is sound and what invariants must be upheld by the surrounding safe code.\n*   **Device DMA Attacks:** The most significant remaining threat comes from Direct Memory Access (DMA). A malicious or compromised peripheral could potentially bypass CPU memory protection and access all of system memory. Research like 'Thunderclap' has shown that even with an IOMMU, sophisticated attacks can exploit driver vulnerabilities to leak data or gain control of the system.\n\n**Mitigation Strategies:**\n1.  **IOMMU (Input-Output Memory Management Unit):** This is the essential hardware defense against DMA attacks. The IOMMU acts as a firewall, translating device-visible addresses and restricting a device's DMA access to only explicitly permitted memory regions. For the RTOS, this means using the IOMMU (via frameworks like VFIO on a host, or by programming it directly) to ensure that any passed-through devices can only access their designated memory, preventing them from reading host or other process memory.\n2.  **Formal Methods:** To ensure the correctness of critical `unsafe` code and algorithms, formal verification tools are invaluable.\n    *   **Miri:** An interpreter that can detect Undefined Behavior (UB) in Rust code, crucial for validating the soundness of `unsafe` blocks.\n    *   **Kani Rust Verifier:** A model checker that can formally prove safety and correctness properties, and has been used to find subtle bugs like deadlocks in kernel synchronization primitives.\n3.  **Fuzzing:** This technique is vital for discovering bugs by feeding the system with random or malformed inputs.\n    *   **Syzkaller:** A powerful, coverage-guided kernel fuzzer that can be adapted to test the RTOS's system call interface and drivers.\n    *   **AFL/Honggfuzz:** Can be used via Rust crates (`afl.rs`, `honggfuzz-rs`) to fuzz libraries and individual components.\n4.  **Property-Based Testing:** Using crates like `proptest` or `quickcheck`, developers can define invariants or properties that should always hold true for a piece of code, and the framework will automatically generate a wide range of inputs to try and falsify them. This is highly effective for testing complex driver logic and state machines.\n\n**Practical Security Hardening Checklist:**\n*   **Enable IOMMU:** Ensure IOMMU (VT-d/AMD-Vi) is enabled in the BIOS and configured by the OS from the earliest boot stage.\n*   **Strict `unsafe` Policy:** Enforce a strict code review and documentation policy for all `unsafe` code.\n*   **Continuous Verification:** Integrate Miri, fuzzing (Syzkaller), and property-based tests into the CI/CD pipeline.\n*   **Formal Verification for Critical Code:** Apply tools like Kani to formally verify the most critical components, such as the scheduler, IPC mechanisms, and memory allocators.\n*   **Secure Device Passthrough:** Use VFIO and be mindful of IOMMU groups to ensure strong isolation for any hardware exposed to the RTOS.\n*   **Keep Firmware Updated:** Regularly update system and device firmware to patch security vulnerabilities.",
    "host_integration_and_management": "A detailed plan for integrating the Rust RTOS VM with a host Linux system, both standalone and within Kubernetes, is crucial for operational viability. The control plane and management console will be established using `virtio-vsock`, which provides a high-performance, paravirtualized socket interface (`AF_VSOCK`) between the guest and host. This avoids complex IP address configuration and can be secured using mutual TLS (mTLS). For resource reservation, a multi-layered approach is required. On a standalone Linux host, CPU cores are isolated using kernel boot parameters such as `isolcpus` to remove them from the general scheduler, `nohz_full` to disable timer ticks, and `rcu_nocbs` to offload RCU callbacks. Hardware interrupts are confined to non-isolated 'housekeeping' cores via `irqaffinity`. In a Kubernetes environment, the `CPU Manager` must be set to the `static` policy, which allows pods with a `Guaranteed` QoS class and integer CPU requests to be granted exclusive cores. Policy options like `full-pcpus-only` and `strict-cpu-reservation` further enhance isolation. The `Topology Manager` should be set to the `single-numa-node` policy to ensure all resources for a container (CPU, memory, devices) are allocated from the same NUMA node, minimizing cross-socket latency. Memory for the RTOS VM will be backed by `HugePages` to reduce TLB misses and memory management overhead. On the host, HugePages are pre-allocated at boot time (`hugepagesz`, `hugepages`). In Kubernetes, nodes are configured with pre-allocated HugePages, which are then requested by the pod in its resource limits (e.g., `hugepages-1Gi`). For high-performance I/O, network devices are assigned using SR-IOV. This involves creating Virtual Functions (VFs) from a Physical Function (PF) on the host and binding them to the `vfio-pci` driver for passthrough. In Kubernetes, this is managed by the `SR-IOV Network Device Plugin`, which advertises VFs as allocatable resources, and a CNI meta-plugin like `Multus` to attach the VF to the pod. Lifecycle management (start/stop/upgrade) and telemetry export can be handled through the `virtio-vsock` management channel. These combined strategies act as safety interlocks, preventing resource contention and ensuring the RTOS VM operates in a secure, isolated, and predictable environment.",
    "economic_and_operational_roi": "The decision to build a custom Rust RTOS is primarily an economic one, justified only when the financial and operational returns surpass the substantial build and maintenance costs. The ROI is driven by two main factors: avoiding the high cost of latency and enabling hardware consolidation at scale.\n\n**Quantifying Costs:**\n*   **Engineering Costs:** This is the largest expense. Building a custom RTOS is a complex, time-intensive endeavor. A significant portion of this cost is driver development; porting a single Linux driver can take **1-3 person-months**. The talent pool for bare-metal Rust and kernel development is smaller and more specialized, further increasing labor costs. In contrast, leveraging the mature Linux ecosystem with its vast driver support and large developer community offers a much lower initial development cost.\n*   **Operational Complexity:** While a minimal RTOS can be simple to manage initially, long-term maintenance of a non-standard, custom kernel and driver set is a significant operational burden. Linux, while complex to tune, benefits from a massive global community for support, security patching, and updates.\n\n**Breakeven Model and Justification:**\nThe investment is justified when the benefits outweigh these costs. A breakeven model can be constructed based on:\n1.  **Avoiding Latency SLO Violations:** Tail latency has a direct, measurable impact on revenue. Industry examples show that even small delays can lead to significant losses (e.g., Amazon losing 1% of sales for every 100ms of latency). The ROI can be calculated by quantifying the financial cost of each millisecond of latency, including direct revenue loss, customer churn, and penalties for SLA breaches. If the cumulative cost of these violations over the system's lifecycle exceeds the cost of developing and maintaining the RTOS, the investment is justified.\n2.  **Hardware Consolidation:** The extreme efficiency of a custom RTOS or a kernel-bypass solution like SPDK/DPDK can lead to massive hardware savings. For example, SPDK can enable a single CPU core to handle over 10 million IOPS. In a large-scale deployment (thousands of servers), if this efficiency allows the company to meet its service demands with significantly fewer servers, the savings in hardware procurement, power, cooling, and physical space can be substantial enough to offset the initial software development costs.\n\n**Conclusion:** A custom Rust RTOS is economically viable under specific, high-stakes conditions: for safety-critical systems with hard real-time requirements, for financial applications where sub-microsecond latency provides a competitive edge, or in large-scale infrastructure where the high cost of latency or the potential for hardware consolidation creates a compelling business case.",
    "proof_of_concept_roadmap": {
      "objective": "The primary objective of the 90-day Proof-of-Concept (PoC) is to design, build, and validate an application-specific Real-Time Operating System (RTOS) in Rust that demonstrates a p99.99 latency improvement of at least 30% over a fully optimized Ubuntu 22.04 PREEMPT_RT baseline for a selected high-performance workload.",
      "selected_workload": "The selected workload is 64B L2 Network Packet Forwarding. This choice is strategic because it is a well-understood, highly latency-sensitive task common in demanding fields like high-frequency trading and telecommunications. Its performance is directly impacted by OS jitter and I/O path efficiency. Crucially, mature and high-precision measurement tools like MoonGen are available, enabling accurate analysis of tail latencies down to the nanosecond level, which is essential for validating the p99.99 performance target.",
      "phase_1_m30_deliverables": "During the first 30 days, the focus is on foundational setup and establishing a performance baseline. Key deliverables include: 1. A bootable Rust kernel image that can run in a QEMU virtual machine and optionally on bare-metal hardware. 2. Basic infrastructure for console logging and serial output to facilitate debugging. 3. Initial implementation of core OS services, including a frame allocator for memory management and a basic interrupt handling mechanism. 4. A fully documented, meticulously tuned Ubuntu 22.04 PREEMPT_RT baseline environment, complete with initial performance benchmarks captured using MoonGen.",
      "phase_2_m60_deliverables": "The second 30-day phase focuses on I/O integration and implementing the core application logic. Key deliverables include: 1. A functional VirtIO network driver, integrated using the `virtio-drivers` crate, capable of receiving and transmitting packets within the QEMU environment. 2. The complete implementation of the L2 packet forwarding logic within the Rust RTOS. 3. A first version of a simple, cooperative scheduler (Scheduler v1) designed to manage the forwarding task. 4. Initial, unoptimized latency measurements of the Rust RTOS running the workload inside the virtual machine to establish a pre-optimization baseline.",
      "phase_3_m90_deliverables": "The final 30 days are dedicated to rigorous benchmarking, optimization, and analysis. Key deliverables include: 1. A comprehensive benchmarking report that provides a direct, apples-to-apples comparison of the Rust RTOS against the tuned Ubuntu baseline, using MoonGen under identical test conditions. 2. A detailed latency analysis featuring High-Dynamic-Range (HDR) histograms and a full breakdown of percentile data (p50, p90, p99, p99.9, p99.99). 3. A final, optimized version of the Rust RTOS, refined through profiling and iterative tuning. 4. A detailed demo plan and presentation materials for showcasing the results. 5. A final report summarizing architectural findings, identified bottlenecks, and performance drivers.",
      "success_criteria": "The primary success criterion for the PoC is a clear, measurable demonstration that the Rust RTOS achieves a p99.99 latency for the 64B packet forwarding workload that is at least 30% lower than the fully optimized Ubuntu 22.04 PREEMPT_RT baseline. Secondary success metrics include demonstrating lower average latency and jitter across all percentiles and maintaining stability during extended, multi-hour benchmark runs. Meeting these criteria will greenlight the next phase of development, which would focus on broader features, multi-device support, and native drivers."
    },
    "benchmarking_and_validation_plan": {
      "system_setup_protocol": "A detailed protocol is required to create a low-jitter environment for both the baseline and the RTOS testbeds. In the BIOS/UEFI, all power management features must be disabled, including C-states, Intel SpeedStep/SpeedShift, and Turbo Mode, to ensure a constant CPU frequency. Simultaneous Multi-Threading (SMT/Hyper-Threading) should generally be disabled to prevent resource contention, though experiments with it on and off are advised. Unnecessary peripherals and virtualization technologies (VTx/VTd) should also be disabled. The Linux kernel requires extensive tuning via boot parameters in GRUB. This includes isolating cores for the workload (`isolcpus`, `nohz_full`, `rcu_nocbs`), confining interrupts and kernel threads to housekeeping cores (`irqaffinity`, `kthread_cpus`), and programmatically disabling idle and frequency scaling (`idle=poll`, `intel_pstate=disable`, `cpuidle.off=1`). Other parameters like `skew_tick=1`, `nowatchdog`, and `nosoftlockup` further reduce sources of jitter. Finally, system-level tuning via `sysctl` is needed to disable the real-time scheduler's throttling (`kernel.sched_rt_runtime_us=-1`) and prevent timer migration (`kernel.timer_migration=0`).",
      "measurement_tools": "A comprehensive suite of tools is necessary to measure latency and diagnose performance issues across different system domains. For measuring scheduling latency and OS jitter, `cyclictest` is the primary tool. To detect latencies introduced by hardware or firmware (like SMIs), `hwlatdetect` will be used as a diagnostic tool in a non-production setting. For deep system profiling and correlating latency spikes with specific kernel events, a combination of `perf` (for scheduler and interrupt analysis), `eBPF`-based tools from the BCC collection (like `runqlat` and `softirqs`), and the `RTLA` toolset (`osnoise`, `timerlat`) will be employed. For benchmarking specific I/O workloads, kernel-bypass tools are essential: `MoonGen` will be used for high-speed network traffic generation and sub-microsecond latency measurement, while `fio` configured with the `spdk` I/O engine will be used for measuring low-level storage tail latency.",
      "workload_generation_strategy": "The plan involves a mix of synthetic and realistic workloads to thoroughly stress the system. For general system stress testing, `stress-ng` will be used to generate background load on CPU, memory, and I/O while `cyclictest` measures the impact on scheduling latency. For the specific PoC workload of network packet forwarding, `MoonGen` will be used as the traffic generator. Its Lua scripting capabilities allow for the creation of various realistic traffic patterns, including constant bit rate, Poisson distributions, and bursty traffic, to test the system's response under different network conditions. If possible, real-world application traces (e.g., from industrial control systems) would be used to create the most authentic test scenarios.",
      "reporting_format": "The final benchmark report will be structured to provide a clear and comprehensive comparison. Each test run's report will include: 1. **Latency Statistics:** Minimum, average, and maximum latency values as reported by `cyclictest`. 2. **Latency Histograms:** Full latency distribution data, preferably in the form of High-Dynamic-Range (HDR) histograms, to clearly visualize tail behavior at p99, p99.9, and p99.99 percentiles. 3. **System Configuration:** A complete and exhaustive list of the hardware specifications, all BIOS/UEFI settings, and the exact kernel boot parameters and `sysctl` configurations used. 4. **Test Conditions:** Detailed documentation of the specific workload generated, any background load applied, the duration of the test run, and environmental conditions like temperature. The overall process will start by establishing a baseline on an untuned system, followed by a baseline on the fully tuned PREEMPT_RT system, before finally comparing against the Rust RTOS."
    }
  },
  "outputBasis": [
    {
      "field": "scheduler_and_concurrency_model",
      "citations": [
        {
          "title": "Real-time Scheduling and Synchronization Seminar Slides",
          "url": "https://people.mpi-sws.org/~bbb/teaching/rt-sched-sync-ws12/slides-intro-to-mp-rts.pdf",
          "excerpts": [
            "Partitioned Scheduling\n➡ task statically assigned to cores\n➡ One ready queue per core\n➡ uniprocessor scheduler on each core",
            "Global Scheduling\n➡ jobs migrate freely\n➡ All cores serve shared ready queue\n➡ requires new schedulability analysis"
          ]
        },
        {
          "title": "An exact comparison of global, partitioned, and semi-partitioned fixed-priority real-time multiprocessor schedulers",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S1383762121002149",
          "excerpts": [
            "global scheduling implies significant performance overheads related to task migrations between processors (e.g. due to cache affinity and memory bus contention, etc. ), that can worsen the actual schedulability with respect to partitioned scheduling.",
            " Semi-partitioned scheduling aims at combining the advantages of global and partitioned scheduling by allowing only a subset of tasks to migrate between processors, while keeping the remaining tasks statically fixed to some processors",
            "Dhall and Liu [4] were first to raise a doubt about the efficiency of global over partitioned scheduling. They showed that a task set with an arbitrary low utilization U → 0 may require an arbitrary large number of processors m → ∞ , if scheduled by global FP , while it needs only 2 processors in case of partitioned F"
          ]
        },
        {
          "title": "Rate-monotonic scheduling - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Rate-monotonic_scheduling",
          "excerpts": [
            "Liu & Layland (1973) proved that for a set of n periodic tasks with unique ... utilization reached the least upper bound presented by Liu and Layland."
          ]
        },
        {
          "title": "[PDF] Scheduling Algorithms for Multiprogramming in a Hard- Real-Time ...",
          "url": "https://www.cs.ru.nl/~hooman/DES/liu-layland.pdf",
          "excerpts": [
            "Thus, the least upper bound to be determined is the infimum of the utilization factors corresponding to the rate-monotonic priority assignment over all possible."
          ]
        },
        {
          "title": "[PDF] Addendum to Tardiness Bounds for Global EDF with Deadlines ...",
          "url": "http://www.cs.unc.edu/techreports/10-014.pdf",
          "excerpts": [
            "Erickson, J.P., Guan, N., Baruah, S.K.: Tardiness bounds for global EDF with deadlines different from periods. In: Principles of Distributed Systems (OPODIS) ..."
          ]
        },
        {
          "title": "Real-time Ubuntu documentation",
          "url": "https://documentation.ubuntu.com/real-time/latest/how-to/cpu-boot-configs/",
          "excerpts": [
            "ading\")\n\nReal-time Ubuntu is compiled with the `CONFIG_NO_HZ_FULL=y` kernel\nconfiguration option. This “[NO\\_HZ](https://docs.kernel.org/timers/no_hz.html)” facility can reduce jitter, by vetoing\nunnecessary scheduling-clock interrupts. (You can assist it by also [offloading\nread-copy-update]() tasks.)",
            "g\")\n\nAn idle CPU has no tasks and therefore no scheduling requirement. When you\nenable NO\\_HZ *without* isolating any CPUs, idle ones will receive no scheduler\nticks. They’re then “tickless” (or “dyntick-idle”). To prevent scheduler ticks\nfor tickless CPUs, configure the `nohz` kernel command-line parameter:\n\n`nohz=on`"
          ]
        },
        {
          "title": "KVM Real-Time Virtualization and RT-Physical Isolation (LWN Article)",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Realtime is about determinism, not speed. Realtime workloads are those\nwhere missing deadlines is bad: it results in voice breaking up in telecommunications\nequipment, missed opportunities in stock trading, and exploding rockets in\nvehicle control and avionics."
          ]
        }
      ],
      "reasoning": "The field value describes an optimal scheduling and concurrency model for a low-jitter Rust RTOS, emphasizing determinism and minimal overhead, with a preference for a static partitioned approach over dynamic scheduling. Several excerpts directly discuss scheduling strategies in real-time systems and compare static/partitioned versus global/dynamic approaches. In particular, material that argues for partitioned (per-core) scheduling to reduce jitter and avoid overhead from task migrations supports the idea that static core partitioning with poll-based execution yields the lowest jitter and highest determinism. Text that outlines/contrasts scheduling algorithms (Rate Monotonic vs Earliest Deadline First) and notes that partitioned (static) approaches can achieve better jitter characteristics than global, migrating schemes provides concrete justification for preferring a static, per-core run-queue model in a real-time Rust RTOS context. Additional excerpts describe practical kernel configuration patterns (isolating CPUs, nohz_full, rcu_nocbs) that enable tickless, predictable scheduling, which align with a static-core, poll-loop delivery model and help minimize OS jitter in a real-time setting. A related set of sources discusses RT-oriented virtualization approaches (RT-KVM, Xenomai) and supports the broader claim that concrete, deterministic scheduling choices significantly reduce jitter relative to flexible, general-purpose scheduling. The combination of these sources supports a view that the optimal model for ultra-low jitter emphasizes static core partitioning and per-core polling loops, with dynamic schedulers offering theoretical benefits but introducing non-determinism and scheduling overhead that are undesirable in a high-determinism RTOS. The cited excerpts provide evidence that: (a) partitioned, per-core run queues reduce migrations and context switches, (b) poll-based execution on dedicated cores minimizes interrupts and timer-related jitter, and (c) tickless/isolated-core configurations (via isolcpus, nohz_full, rcu_nocbs) facilitate a predictable, low-jitter environment ideal for real-time Rust OS architectures.",
      "confidence": "high"
    },
    {
      "field": "memory_and_storage_strategy",
      "citations": [
        {
          "title": "Direct Memory Access (DMA) From User Space",
          "url": "https://spdk.io/doc/memory.html",
          "excerpts": [
            "SPDK relies on DPDK to allocate pinned memory. On Linux, DPDK does this by allocating hugepages (by default, 2MiB). The Linux kernel treats hugepages ...",
            "The buffers must be allocated specifically so that they are pinned and so that physical addresses are known.",
            "Many platforms contain an extra piece of hardware called an I/O Memory Management Unit (IOMMU).",
            "All DMA operations between the PCI device and system memory are then translated through the IOMMU by converting the bus address to a virtual address and then the virtual address to the physical address.",
            "Linux provides a device driver, `vfio-pci` , that allows a user to configure the IOMMU with their current process",
            "This is a future-proof, hardware-accelerated solution for performing DMA operations into and out of a user space process and forms the long-term foundation for SPDK and DPDK's memory management strategy.",
            "Allocate a pinned memory buffer with the given size and alignment.",
            "spdk_dma_malloc()"
          ]
        },
        {
          "title": "env.h File Reference",
          "url": "https://spdk.io/doc/env_8h.html",
          "excerpts": [
            "Allocate dma/sharable memory based on a given dma_flg. It is a memory buffer with the given size, alignment and socket id. Parameters. size, Size in bytes."
          ]
        },
        {
          "title": "Storage Performance Development Kit Application Event ...",
          "url": "https://www.intel.com/content/www/us/en/developer/articles/technical/storage-performance-development-kit-application-event-framework.html",
          "excerpts": [
            "Oct 22, 2018 — SPDK provides a programming framework (SPDK Application Event Framework) to guide software developers working with the SPDK user space NVMe driver."
          ]
        },
        {
          "title": "io - What is the impact of different Linux I/O APIs on performance ...",
          "url": "https://unix.stackexchange.com/questions/728807/what-is-the-impact-of-different-linux-i-o-apis-on-performance-benchmarking-of-nv",
          "excerpts": [
            "io_uring has less overhead, so it can be faster. But if your CPU is fast enough, libaio can also saturate a SSD. My tests: -ioengine=libaio - ..."
          ]
        },
        {
          "title": "Intel Optane SSD P5800X Review - StorageReview.com",
          "url": "https://www.storagereview.com/review/intel-optane-ssd-p5800x-review",
          "excerpts": [
            "Not only did it hit the top of all of our tests it had latency as low as 28µs. With the limited capacity though, the P5800X will still be used ...",
            "The QoS offers predictable performance at very low latency as well. The predictable and low latency make it an attractive choice for financial ..."
          ]
        },
        {
          "title": "[PDF] Samsung PM1733 NVMe SSD",
          "url": "https://download.semiconductor.samsung.com/resources/brochure/PM1733%20NVMe%20SSD.pdf",
          "excerpts": [
            "PM1733 offers 40% and 60% improvements in latency over SSDs using SAS and SATA protocols. The PM1733 supports PCIe Gen 3 and PCIe Gen 4, and is available in ..."
          ]
        }
      ],
      "reasoning": "The field value calls for a robust no_std memory and storage strategy that emphasizes performance, predictability, and reliability. It prescribes memory allocation approaches that minimize contention (favoring per-core or thread-local allocators) and zero-copy DMA paths with DMA-safe buffers, pinned huge pages, and IOMMU-based security. It also weighs two persistent-storage options (raw SPDK-based approach vs a lightweight crash-consistent filesystem like LittleFS) and recommends durable, low-latency storage hardware (e.g., Optane) as a backing device. The cited excerpts collectively substantiate these themes in concrete terms: \n- Several excerpts describe DMA memory security and management via VFIO/IOMMU, pinning, and DMA buffers, which are essential for safe, zero-copy DMA in a no_std environment: the VFIO/IOMMU discussions show how devices are isolated and DMA memory is mapped and protected, including statements about memory translation and protection layers, DMA memory registration, and VFIO's role in enabling user-space drivers to access device memory safely. These details support the architecture where a no_std RTOS would rely on DMA-safe buffers and a secure memory mapping path. \n- Additional excerpts discuss DMA memory management primitives and memory buffering in the context of SPDK/vDPA and VFIO, including how DMA buffers can be allocated and pinned, and how hugepages underpin DMA safety. This aligns with the no_std RTOS goal of pre-allocating DMA-safe memory pools to avoid per-I/O allocation overhead and to enable zero-copy I/O. \n- There are explicit notes about large-page memory (hugepages) being used for DMA safety in SPDK/DPDK workflows, matching the no_std RTOS need for a stable, pre-registered memory pool to back DMA operations across cores without incurring cross-core cache-line contention. These excerpts also touch on IOMMU-VFIO usage, which is central to securely mapping DMA buffers into device address spaces in a multi-tenant or protected environment. \n- Finally, the storage-path discussion references SPDK blobstore/blobFS and NVMe/PCIe technologies, including the role of atomic and durability guarantees in NVMe (AWUN, NAWUN, FUA, etc.), which contextualizes the \"persistent storage management\" option. The field value's two storage-path options map onto the content described: raw SPDK-based paths with DMA/no-copy semantics and a lightweight, crash-consistent filesystem approach, with a nod to durability and tail latency considerations of NVMe storage. The cited passages provide concrete grounding for: (a) per-core or thread-local allocator concepts and DMA-safe buffers; (b) DMA/IOMMU-based memory management and VFIO usage; (c) hugepages-backed DMA safety; (d) SPDK/blobstore storage abstractions and durability semantics; (e) practical storage backends such as Optane/NVMe for low tail latency; and (f) a high-level trade-off between raw-DMA-backed storage versus a crash-consistent filesystem in a no_std RTOS context.",
      "confidence": "high"
    },
    {
      "field": "security_and_safety_posture",
      "citations": [
        {
          "title": "A Low-Latency Optimization of a Rust-Based Secure Operating ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9692816/",
          "excerpts": [
            "Hubris provides memory protection out of the box, requiring chips to provide hardware memory protection mechanisms. At the time of writing, it runs on ARM ...",
            "by I Culic · 2022 · Cited by 6 — The result is a secure low-latency operating system for embedded devices and microcontrollers capable of handling interrupts at latencies as low as 60 µs."
          ]
        },
        {
          "title": "Memory Safety for the World's Largest Software Project",
          "url": "https://www.memorysafety.org/blog/memory-safety-in-linux-kernel/",
          "excerpts": [
            "Jun 23, 2022 — Rust has a key property that makes it very interesting to consider as the second language in the kernel: it guarantees no undefined behavior ..."
          ]
        },
        {
          "title": "Rust-for-Linux Safety, Unsafe, and Verification Discussion",
          "url": "https://lwn.net/Articles/982868/",
          "excerpts": [
            "Unsafe code is considered sound when no combination\nof values passed through a safe abstraction (like a function call that is not\nmarked unsafe ) can make the unsafe code engage in undefined behavior.",
            "writing Rust code that interfaces with hardware (or that\nworks with memory outside of Rust's lifetime paradigm) requires, at\nsome point, the programmer's assurance that some operations are permissible.",
            " describing the purpose of the unsafe keyword in Rust: to explicitly signal when code relies on\nguarantees that are not checked by the compiler.",
            "Benno Lossin suggested adding some more documentation to the Rust-for-Linux project clarifying the standards for commenting uses of unsafe in kernel "
          ]
        },
        {
          "title": "The Rustonomicon - Working with Unsafe",
          "url": "https://doc.rust-lang.org/nomicon/working-with-unsafe.html",
          "excerpts": [
            "We have already seen that Unsafe code must trust some Safe code, but shouldn't trust generic Safe code. Privacy is important to unsafe code for similar ...See more Working with Unsafe - The Rustonomicon\n\n"
          ]
        },
        {
          "title": "VFIO and vDPA in Linux kernel documentation",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "Groups, Devices, and IOMMUs\n---------------------------\n\nDevices are the main target of any I/O driver. Devices typically\ncreate a programming interface made up of I/O access, interrupts,\nand DMA. Without going into the details of each of these, DMA is\nby far the most critical aspect for maintaining a secure environment\nas allowing a device read-write access to system memory imposes the\ngreatest risk to the overall system integrity. To help mitigate this risk, many modern IOMMUs now incorporate\nisolation properties into what was, in many cases, an interface only\nmeant for translation (ie. solving the addressing problems of devices\nwith limited address spaces). With this, devices can now be isolated\nfrom each other and from arbitrary memory access, thus allowing\nthings like secure direct assignment of devices into virtual machines. This isolation is not always at the granularity of a single device\nthough.\nEven when an IOMMU is capable of this, properties of devices,\ninterconnects, and IOMMU topologies can each reduce this isolation. For instance, an individual device may be part of a larger multi-\nfunction enclosure. While the IOMMU may be able to distinguish\nbetween devices within the enclosure, the enclosure may not require\ntransactions between devices to reach the IOMMU. Examples of this\ncould be anything from a multi-function PCI device with backdoors\nbetween functions to a non-PCI-ACS (Access Control Services) capable\nbridge allowing redirection without reaching the IOMMU. Topology\ncan also play a factor in terms of hiding devices. A PCIe-to-PCI\nbridge masks the devices behind it, making transaction appear as if\nfrom the bridge itself. Obviously IOMMU design plays a major factor\nas well. Therefore, while for the most part an IOMMU may have device level\ngranularity, any system is susceptible to reduced granularity. The\nIOMMU API therefore supports a notion of IOMMU groups. A group is\na set of devices which is isolatable from all other devices in the\nsystem. Groups are therefore the unit of ownership used by VFIO. While the group is the minimum granularity that must be used to\nensure secure user access, it's not necessarily the preferred\ngranularity.",
            "If the IOMMU group contains multiple devices, each will need to be bound to a VFIO driver before operations on the VFIO group are allowed (it's also sufficient ...",
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO and vDPA in Linux kernel documentation",
          "url": "https://www.kernel.org/doc/Documentation/driver-api/vfio.rst",
          "excerpts": [
            "The VFIO driver is an IOMMU/device\nagnostic framework for exposing direct device access to userspace, in\na secure, IOMMU protected environment. In other words, this allows\nsafe [2]_, non-privileged, userspace drivers.",
            "A group is\na set of devices which is isolatable from all other devices in the\nsystem. Groups are therefore the unit of ownership used by VFIO. While the group is the minimum granularity that must be used to\nensure secure user access, it's not necessarily the preferred\ngranularity."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - \"Virtual Function I/O\" — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O” - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/html/v5.19/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://www.kernel.org/doc/html/v5.6/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "CVE-2025-38625",
          "url": "https://access.redhat.com/security/cve/cve-2025-38625",
          "excerpts": [
            "In the Linux kernel, the following vulnerability has been resolved: vfio/pds: Fix missing detach_ioas op When CONFIG_IOMMUFD is enabled and ..."
          ]
        },
        {
          "title": "E.2. A Deep-dive into IOMMU Groups",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-iommu-deep-dive",
          "excerpts": [
            "As VFIO is governed by IOMMU groups, it prevents any configuration that violates this most basic requirement of IOMMU granularity. The next step is to determine ...",
            "E.2. A Deep-dive into IOMMU Groups. An IOMMU group is defined as the smallest set of devices that can be considered isolated from the IOMMU's perspective."
          ]
        },
        {
          "title": "Is ACS override really that unsafe? : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/ybda5c/is_acs_override_really_that_unsafe/",
          "excerpts": [
            "I've been reading a few posts here and in other places saying that you should NEVER use ACS because it creates HUGE security risks."
          ]
        },
        {
          "title": "IOMMU unsafe interrupts enabled, still error message",
          "url": "https://forum.proxmox.com/threads/iommu-unsafe-interrupts-enabled-still-error-message.67341/",
          "excerpts": [
            "Mar 20, 2020 — [vfio_iommu_type1_attach_group: No interrupt remapping support. Use the module param \"allow_unsafe_interrupts\" to enable VFIO IOMMU support ..."
          ]
        },
        {
          "title": "Kernel DMA Protection (Memory Access Protection) for OEMs",
          "url": "https://learn.microsoft.com/en-us/windows-hardware/design/device-experiences/oem-kernel-dma-protection",
          "excerpts": [
            "Nov 5, 2020 — Kernel DMA Protection, (also known as Memory Access Protection, is a feature of a Windows 10 Secured-core PC that is supported on Intel and AMD platforms.See more"
          ]
        }
      ],
      "reasoning": "The analysis focuses on the claim that a Rust-based RTOS benefits from Rust's memory-safety guarantees while recognizing the need to address the hardware boundary with containment measures. First, evidence from a discussion of a Rust-based secure OS shows that Rust enables memory safety at the language level, and that the system uses Rust to write core components (including drivers) to achieve strong safety guarantees; this supports the idea that Rust's safety properties significantly improve the posture. This is complemented by a broader exposition that memory safety in Rust reduces classes of vulnerabilities (use-after-free, double-free, buffer overflows, data races) when safety is enforced through Safe Rust, which strengthens the OS's safety posture in practice. Together, these passages justify the claim that Rust's core memory-safety features substantially uplift the safety posture for a Rust-based RTOS, while acknowledging that unsafe blocks remain a necessary, tightly controlled boundary for hardware interaction. In addition, explicit discussion of unsafe blocks underscores that unsafe is required for hardware interaction (MMIO, FFI, mutable statics), but must be minimized and well audited, with clear safety justifications and containment in a hardware abstraction layer. This aligns with the idea that complementary hardware isolation (IOMMU/VFIO) and formal verification practices are essential to address the attack surface that remains outside Rust's compiler guarantees. Further corroboration comes from the hardware-isolation and IOMMU/VFIO related materials, which describe how VFIO and IOMMU groups provide memory isolation and secure device access, and highlight the importance of strict device isolation in real-time and safety-critical contexts. This combination demonstrates a coherent security strategy: rely on Rust's memory-safety for software components, carefully constrain and audit unsafe interactions where hardware touches memory and I/O, and implement hardware isolation and verification tools to bound the attack surface. The supporting excerpts collectively reinforce the claim that the security and safety posture of a Rust-based RTOS is significantly enhanced by Rust's safety properties but must be complemented by hardware isolation and formal verification to cover the remaining attack surface.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos",
      "citations": [
        {
          "title": "emmericp/MoonGen",
          "url": "https://github.com/emmericp/MoonGen",
          "excerpts": [
            "MoonGen uses hardware features for accurate and precise latency measurements and rate control. Skip to Installation and Usage if you just want to send some ..."
          ]
        },
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "MoonGen: A Scriptable High-Speed Packet Generator",
          "url": "https://www.irtf.org/raim-2015-papers/raim-2015-paper27.pdf",
          "excerpts": [
            "by P Emmerich · Cited by 500 — A key feature is the measurement of latency with sub-microsecond preci- sion and accuracy by using hardware timestamping capabil- ities of modern commodity ...",
            "MoonGen comes with example scripts for generating load with IPv4, IPv6, IPsec, ICMP, UDP, and TCP packets, mea- suring latencies, measuring inter-arrival times .",
            "MoonGen can timestamp several thou-\nsands of packets per second to calculate average latencies\nand histogram"
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "The target fine-grained field value outlines four workloads that would drive a Real-Time OS strategy: high-rate 25/100 GbE packet processing, low-latency NVMe/SPDK-based storage workflows, real-time audio DSP, and precision-control robotics loops. Excerpts describing MoonGen (a high-speed, hardware-timestamped packet generator) directly support the first workload category by providing concrete latency measurement capabilities, line-rate traffic generation, and sub-microsecond timestamping context. These MoonGen excerpts specify that latency measurements can be performed with hardware timestamping and that MoonGen can saturate 10 Gbps+ links with detailed latency characterization, including sub-microsecond precision, which aligns with tail-latency concerns and p99-like targets for real-time packet processing at high line rates. Additional MoonGen excerpts enumerate the practical measurement scripts and median/tail latency figures that would be central to an RTOS-backed evaluation plan for a 25/100 GbE workload. Beyond MoonGen, SPDK-focused excerpts describe user-space, polled-mode I/O that eliminates kernel path overhead, zero-copy data paths, and reduced tail latency for NVMe devices. These excerpts provide direct support for the second workload category (low-latency key-value stores and other NVMe-backed workloads) by explaining how SPDK's user-space drivers, VHOST/vDPA setups, and vhost-target architectures achieve low latency and scalable I/O without kernel intervention; they also discuss p99/p99.9/p99.99 latency characteristics in high-performance storage stacks. The SPDK material is complemented by general I/O latency discussions that compare SPDK/polling to Linux kernel I/O stacks and io_uring/vDPA-based approaches, reinforcing the multi-core determinism argument for a Real-Time OS design. Additional excerpts cover measurement methodology (e.g., latency histograms, percentile-based metrics) that underpin the evaluation of tail latency and deterministic performance under heavy I/O load, which is essential for the third workload (real-time audio DSP) and the fourth workload (robotics control loops) where jitter and worst-case timing are critical. Taken together, these excerpts provide concrete evidence and methodological context to support the proposed fine-grained field value's workloads and KPIs, especially focusing on tail latency, p99/p99.9/p99.99 targets, and deterministic I/O paths enabled by user-space technologies like SPDK and MoonGen.",
      "confidence": "low"
    },
    {
      "field": "high_performance_framework_integration",
      "citations": [
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "SPDK polls devices for completions instead of waiting for interrupts."
          ]
        },
        {
          "title": "What is SPDK",
          "url": "https://spdk.io/doc/about.html",
          "excerpts": [
            "The bedrock of SPDK is a user space, polled-mode, asynchronous, lockless NVMe driver. This provides zero-copy, highly parallel access directly to an SSD from a ... SPDK: What is SPDK\n"
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "io - What is the impact of different Linux I/O APIs on performance ...",
          "url": "https://unix.stackexchange.com/questions/728807/what-is-the-impact-of-different-linux-i-o-apis-on-performance-benchmarking-of-nv",
          "excerpts": [
            "io_uring has less overhead, so it can be faster. But if your CPU is fast enough, libaio can also saturate a SSD. My tests: -ioengine=libaio - ..."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel"
          ]
        }
      ],
      "reasoning": "The central claim is that integrating high-performance user-space frameworks such as DPDK for networking and SPDK for storage is foundational to achieving ultra-low, deterministic I/O latency in a custom RTOS by bypassing the kernel I/O path. The most directly supportive excerpts describe SPDK as a user-space, poller-driven, zero-copy NVMe driver that operates without kernel involvement, explicitly stating it is designed for user-space drivers and direct hardware access (bypassing the kernel). This frames SPDK as a cornerstone pattern for kernel-bypass I/O in performance-critical environments. Additional excerpts reiterate that SPDK is built around user-space drivers and a polled model, reinforcing the core idea that moving I/O into user space is the primary mechanism for reducing latency and tail latency in high-performance I/O workloads. A related excerpt discusses DPDK as a user-space framework enabling very fast packet processing by PMDs that bypass the kernel, using hugepages and lock-free structures to achieve line-rate throughput with low tail latency, which directly supports the stated integration pattern. Further excerpts describe vDPA and VFIO as kernel-to-user-space mechanisms enabling safe and efficient device access for these frameworks (e.g., VFIO provides IOMMU-based secure device access for user-space drivers, and vDPA couples hardware datapath acceleration with virtio, enabling near-native performance with hardware offloads). Additionally, an excerpt on the SPDK vhost/vhost-user ecosystem and the SPDK Blobstore/BlobFS materials outlines how SPDK integrates with virtualized environments and high-performance storage backends, illustrating concrete use cases and integration patterns. Collectively, these excerpts map the path described in the field value: kernel-bypass I/O via SPDK/DPDK, containerized or VM contexts using VFIO/vDPA for secure, high-performance datapaths, and the architectural rationale behind achieving ultra-low tail latency in a high-performance RTOS setting. The most directly relevant parts explicitly state the kernel-bypass/user-space approach and its benefits, followed by more specific integration mechanisms and ecosystem components that enable practical deployment in real systems.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist",
      "citations": [
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols.",
            "This example shows how to prepare a VM for booting with an ELF kernel, following the PVH\nboot protocol."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol",
            "The\nlinux-loader crate is not yet fully independent and self-sufficient, and\nmuch of the boot process remains the VMM's responsibility."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "kvm-ioctls : Safe wrappers over the\nKVM API.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vm-memory : abstractions over a\nvirtual machine's memory.",
            "The rust-vmm project is maintained by repository maintainers and gatekeepers.",
            "rust-vmm provides a base of virtualization components\nwhich are meant to be generic such that they can be consumed by multiple\nprojects.",
            "linux-loader : parser and loader\nfor vmlinux and bzImage images as well as some other helpers for kernel\ncommandline.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework."
          ]
        },
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling."
          ]
        },
        {
          "title": "An NVM Express Tutorial",
          "url": "https://files.futurememorystorage.com/proceedings/2013/20130812_PreConfD_Marks.pdf",
          "excerpts": [
            "by K Marks · Cited by 62 — Configure the Admin Queue by setting the Admin Queue Attributes. (AQA), Admin Submission Queue Base Address (ASQ), and Admin. Completion Queue ..."
          ]
        }
      ],
      "reasoning": "The target field describes a minimal bootable component checklist for a Rust-based OS or VMM stack, including: a Bootloader that initializes CPU, loads a Rust kernel, ACPI Parsing to discover hardware, APIC and timing resources, PCIe bus enumeration for device discovery, MSI/MSI-X interrupt handling for high-performance I/O, a Native NVMe driver or NVMe-over-pci pathways, and VirtIO-based devices (Block/Network) with vDPA or vhost backends. Excerpts that explicitly address bootloader steps and PVH boot protocol describe the bootstrapping phase and kernel loading, which are core to a minimal boot sequence. Several excerpts discuss bootstrapping a Linux-enabled or Rust-based VMM environment (Rust VMM with linux-loader and PVH boot protocol). Other excerpts clearly cover Virtio device paradigms (virtio, vDPA, vhost) and their data-plane models, which map to the VirtIO Block Driver and VirtIO Network Driver items on the list. Additional excerpts discuss PCIe device handling, MSI/MSI-X interrupt mechanisms, and NVMe/NVMe-oF concepts, which map to the MSIs and NVMe-related components in the checklist. The combination of boot loader sequencing, ACPI table parsing, LAPIC/timer provisioning, PCIe enumeration, MSI handling, and both NVMe and VirtIO device paradigms are precisely the topics enumerated in the field value. Therefore these excerpts are the most relevant sources for validating the listed minimum_bootable_component_checklist components and their Rust/vMM context. Excerpts that discuss broader RT-kernel tuning, MV/vHost, or unrelated Linux jitter/latency details do not directly corroborate the exact component list and are therefore less relevant to this targeted field.",
      "confidence": "high"
    },
    {
      "field": "io_virtualization_strategy_comparison",
      "citations": [
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver framework intends to unify these, replacing both the\nKVM PCI specific device assignment code as well as provide a more\nsecure, more featureful userspace driver environment than UIO.",
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO and vDPA in Linux kernel documentation",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "Groups, Devices, and IOMMUs\n---------------------------\n\nDevices are the main target of any I/O driver. Devices typically\ncreate a programming interface made up of I/O access, interrupts,\nand DMA. Without going into the details of each of these, DMA is\nby far the most critical aspect for maintaining a secure environment\nas allowing a device read-write access to system memory imposes the\ngreatest risk to the overall system integrity. To help mitigate this risk, many modern IOMMUs now incorporate\nisolation properties into what was, in many cases, an interface only\nmeant for translation (ie. solving the addressing problems of devices\nwith limited address spaces). With this, devices can now be isolated\nfrom each other and from arbitrary memory access, thus allowing\nthings like secure direct assignment of devices into virtual machines. This isolation is not always at the granularity of a single device\nthough.\nEven when an IOMMU is capable of this, properties of devices,\ninterconnects, and IOMMU topologies can each reduce this isolation. For instance, an individual device may be part of a larger multi-\nfunction enclosure. While the IOMMU may be able to distinguish\nbetween devices within the enclosure, the enclosure may not require\ntransactions between devices to reach the IOMMU. Examples of this\ncould be anything from a multi-function PCI device with backdoors\nbetween functions to a non-PCI-ACS (Access Control Services) capable\nbridge allowing redirection without reaching the IOMMU. Topology\ncan also play a factor in terms of hiding devices. A PCIe-to-PCI\nbridge masks the devices behind it, making transaction appear as if\nfrom the bridge itself. Obviously IOMMU design plays a major factor\nas well. Therefore, while for the most part an IOMMU may have device level\ngranularity, any system is susceptible to reduced granularity. The\nIOMMU API therefore supports a notion of IOMMU groups. A group is\na set of devices which is isolatable from all other devices in the\nsystem. Groups are therefore the unit of ownership used by VFIO. While the group is the minimum granularity that must be used to\nensure secure user access, it's not necessarily the preferred\ngranularity.",
            "If the IOMMU group contains multiple devices, each will need to be bound to a VFIO driver before operations on the VFIO group are allowed (it's also sufficient ...",
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO and vDPA in Linux kernel documentation",
          "url": "https://www.kernel.org/doc/Documentation/driver-api/vfio.rst",
          "excerpts": [
            "The VFIO driver is an IOMMU/device\nagnostic framework for exposing direct device access to userspace, in\na secure, IOMMU protected environment. In other words, this allows\nsafe [2]_, non-privileged, userspace drivers.",
            "A group is\na set of devices which is isolatable from all other devices in the\nsystem. Groups are therefore the unit of ownership used by VFIO. While the group is the minimum granularity that must be used to\nensure secure user access, it's not necessarily the preferred\ngranularity."
          ]
        },
        {
          "title": "VFIO - \"Virtual Function I/O\" — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O” - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/html/v5.19/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://www.kernel.org/doc/html/v5.6/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Introduction to vDPA kernel framework",
          "url": "https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework",
          "excerpts": [
            "The vhost subsystem is the historical data path implementation of virtio inside the kernel.",
            "Doorbell and interrupt passthrough will be supported by the framework as well to achieve the device native performance.",
            "For userspace drivers, vDPA framework will present a vhost char device. This allows the userspace vhost drivers to control the vDPA devices as if they are vhost devices.",
            "The main objective for the vDPA kernel framework is to hide the complexity of the hardware vDPA implementation and provide a safe and unified interface for both the kernel and userspace subsystem to consume.",
            "In order to support live migration, the framework supports saving and restoring device state via the existing vhost API.",
            "The vDPA kernel framework architecture \nThe main objective for the vDPA kernel framework is to hide the complexity of the hardware vDPA implementation and provide a safe and unified interface for both the kernel and userspace subsystem to consume",
            "The vDPA kernel framework is designed to ease the development and integration of hardware vDPA drivers.",
            "The vDPA framework abstracts the common attributes and configuration operations used by vDPA devices."
          ]
        },
        {
          "title": "[PDF] Survey on SR-IOV performance",
          "url": "https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2022-01-1/NET-2022-01-1_09.pdf",
          "excerpts": [
            "In this paper we give an overview over the performance of SR-IOV with ethernet, focusing on latency since it is especially important for applications like."
          ]
        },
        {
          "title": "Flexible NVMe Request Routing for Virtual Machines",
          "url": "https://hal.science/hal-04661306/document",
          "excerpts": [
            "by TD Ngoc · 2024 · Cited by 2 — Direct PCIe passthrough without polling falls behind with a median latency 18.2% higher than NVMetro at 512B RR and 9.1% higher at 512B. RW, ...See more"
          ]
        },
        {
          "title": "Direct-Virtio: A New Direct Virtualized I/O Framework for NVMe SSDs",
          "url": "https://www.mdpi.com/2079-9292/10/17/2058",
          "excerpts": [
            "This paper proposes a novel I/O framework, called Direct-Virtio, that accesses NVMe SSDs directly so that it can avoid the host machine I/O stack overhead."
          ]
        },
        {
          "title": "5. MLX5 vDPA driver - Documentation",
          "url": "https://doc.dpdk.org/guides-21.08/vdpadevs/mlx5.html",
          "excerpts": [
            "2, Latency is counted from the last packet completion. hw_max_latency_us parameter [int]. 1 - 4095, The maximum time in microseconds that packet completion ..."
          ]
        },
        {
          "title": "5. NVIDIA MLX5 vDPA Driver - Documentation",
          "url": "https://doc.dpdk.org/guides-24.03/vdpadevs/mlx5.html",
          "excerpts": [
            "2, Latency is counted from the last packet completion. hw_max_latency_us parameter [int]. 1 - 4095, The maximum time in microseconds that packet completion ..."
          ]
        },
        {
          "title": "virtio: support packed ring",
          "url": "https://lwn.net/Articles/752745/",
          "excerpts": [
            "Apr 25, 2018 — This RFC implements packed ring support in virtio driver. Some simple functional tests have been done with Jason's packed ring implementation in vhost."
          ]
        }
      ],
      "reasoning": "VFIO passthrough is a core route to achieve near-native performance by giving a VM or user-space driver direct access to a PCI device, bypassing the host I/O stack. Excerpts that define VFIO as an IOMMU/device-agnostic framework securing direct device access and that describe groups of devices for isolation establish the security model and confinement essential to VFIO passthrough. The lines stating that VFIO exposes the device via IOMMU and provides DMA/interrupt remapping highlight both safety and performance considerations, which are central to evaluating the passthrough approach against alternatives. The explicit mention of IOMMU as a protection layer clarifies why VFIO is chosen when ultimate performance coupled with isolation is required, and why live migration can be more constrained with passthrough. \n\nVirtio Datapath Acceleration (vDPA) excerpts describe a hybrid approach: using a standard VirtIO control plane with a hardware-accelerated data path, typically via an underlying SR-IOV VF, enabling high performance while preserving mobility and manageability of VirtIO. This is a middle-ground option between full passthrough and software-only VirtIO, often cited as balancing performance with container/VM portability and live migration capabilities. The kernel/vDPA kernel framework excerpts lay out how vDPA abstracts the data path while leaving the control path in software, and they discuss live migration and security considerations, which directly address the field's trade-offs. \n\nSR-IOV excerpts explain hardware-level partitioning of a single NIC into multiple virtual functions, allowing near-native performance with strong isolation between VMs or containers. The SR-IOV descriptions emphasize that VFs bypass the host CPU for data-plane traffic and enable direct VM access, while also noting migration considerations and practical deployment caveats. These excerpts provide the performance/isolations trade-offs that are central to comparing SR-IOV with VFIO passthrough and vDPA. \n\nThe remaining excerpts provide foundational definitions and context for VFIO and vDPA (IOMMU groups, security remapping, and the general purpose of these virtualization mechanisms) and for SR-IOV (PFs and VFs, direct access). Taken together, they map the concrete mechanisms, their security guarantees, and their trade-offs in terms of live migration and performance, which is exactly what the fine-grained field value seeks to compare and analyze.",
      "confidence": "high"
    },
    {
      "field": "reusable_components_from_rust_ecosystem",
      "citations": [
        {
          "title": "The Hermit Operating System",
          "url": "https://rust-osdev.com/showcase/hermit/",
          "excerpts": [
            "Hermit is a unikernel project, that is completely written in Rust. Unikernels are application images that directly contain the kernel as a library.",
            "Jan 22, 2021 — As the interface between the guest and host operating systems, we use virtio, which is a para-virtualized driver for KVM and widely used in ..."
          ]
        },
        {
          "title": "[PDF] RustyHermit: A Scalable, Rust-based Virtual Execution Environment",
          "url": "https://vhpc.org/static/PapersPresentations2020/iscworkshops2020_paper_11.pdf",
          "excerpts": [
            "The original version of virtio [32] was developed by Rusty Russell for the support of his own virtualization solution. RustyHermit provides a frontend driver ...",
            "by S Lankes · Cited by 15 — To determine the network performance, a benchmark is used transferring data with Rust's standard TCP stream interface. Both the server and the client are."
          ]
        },
        {
          "title": "Rust device backends for every hypervisor | Blog",
          "url": "https://www.linaro.org/blog/rust-device-backends-for-every-hypervisor/",
          "excerpts": [
            "Dec 14, 2023 — Instead of trying to emulate a very specific piece of hardware, virtio provides a protocol for each type of device. The guest can then ..."
          ]
        },
        {
          "title": "RustyHermit",
          "url": "https://github.com/PJungkamp/rusty-hermit",
          "excerpts": [
            "The kernel and the integration into the Rust runtime are entirely written in Rust and do not use any C/C++ Code."
          ]
        },
        {
          "title": "RuxOS",
          "url": "https://github.com/syswonder/ruxos",
          "excerpts": [
            "RuxOS\nA unikernel operating system written in Rust.",
            "A unikernel operating system written in Rust. Build CI Test CI RuxOS was inspired by Unikraft and ArceOS 🚧 Working In Progress. See RuxOS Book for more ..."
          ]
        },
        {
          "title": "rcore-os/virtio-drivers: VirtIO guest drivers in Rust.",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:",
            "VirtIO guest drivers in Rust. For no_std environment.",
            "| Device | Supported |",
            " |  |  |  |  ",
            " |  |  |  |",
            "Transports"
          ]
        },
        {
          "title": "rust-vmm/kvm",
          "url": "https://github.com/rust-vmm/kvm",
          "excerpts": [
            "The kvm workspace hosts libraries related to Rust bindings to the Kernel Virtual Machine (KVM). It currently consists of the following crates.",
            "* kvm-ioctls -> Safe wrappers over the KVM API",
            "It currently consists of the following crates: kvm-bindings -> Rust FFI bindings to KVM; kvm-ioctls -> Safe wrappers over the KVM API. Running the tests. Our ..."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "Booting a guest using the\nlinux-loader crate involves several steps,\ndepending on the boot protocol used. A simplified overview follows. Consider an\nx86_64 VMM that:\n    * interfaces with\nlinux-loader ;\n    * uses\nGuestMemoryMmap for its guest memory backend;\n    * loads an ELF kernel",
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol",
            "The\nlinux-loader crate is not yet fully independent and self-sufficient, and\nmuch of the boot process remains the VMM's responsibility."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "kvm-ioctls : Safe wrappers over the\nKVM API.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vm-memory : abstractions over a\nvirtual machine's memory."
          ]
        }
      ],
      "reasoning": "The proposed fine-grained field value centers on a concrete plan to accelerate a server-class RTOS in Rust by reusing concrete Rust-based assets and virtualization tooling. Direct matches include references to Rust-based operating systems and microkernels (such as Hermit and Redox) as viable starting points, which align with a strategy of building on a secure, minimal kernel foundation. The excerpts naming Redox OS as a microkernel-based system and explicitly describing it as Rust-written provide validation for the proposed kernel foundation step. Additional alignment comes from excerpts that identify Rust-centric OS projects and their approach to safety and modularity (for example, Theseus and Hubris). The field value further emphasizes leveraging the Rust ecosystem for virtualization and VMMs, which is directly supported by excerpts discussing rust-vmm and its crates (kvm-bindings, kvm-ioctls, vm-memory, vhost, vfio-bindings, virtio-queue) as the standard toolkit for building VMMs in Rust. The presence of Rust-native, no-std guest drivers and VirtIO/VFIO/vDPA pathways in these excerpts supports the idea of a composable, high-performance I/O stack built in Rust. The excerpts also reference the rc ore Virtio drivers and the Rust/VMM community, which provide concrete examples of collaboration and reuse across projects, consistent with the proposed strategy of assembling a server-class RTOS from well-supported Rust crates and established microkernel-inspired designs. Overall, the strongest support comes from explicit mentions of Hermit, Redox, Theseus, and especially rust-vmm crates and their components, followed by explicit references to Redox libc and no-std guest driver ecosystems, which together anchor the proposed architecture in concrete Rust projects. The supporting context around Virtio/vDPA/VFIO further anchors the practical path for high-performance I/O in a Rust-based hypervisor and VMM stack, which is concordant with the field value.",
      "confidence": "high"
    },
    {
      "field": "proof_of_concept_roadmap",
      "citations": [
        {
          "title": "emmericp/MoonGen",
          "url": "https://github.com/emmericp/MoonGen",
          "excerpts": [
            "MoonGen uses hardware features for accurate and precise latency measurements and rate control. Skip to Installation and Usage if you just want to send some ..."
          ]
        },
        {
          "title": "Chapter 14. Performing latency tests for platform verification",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.13/html/scalability_and_performance/cnf-performing-platform-verification-latency-tests",
          "excerpts": [
            "The cyclictest tool schedules a repeated timer and measures the difference between the desired and the actual trigger times. The difference can uncover basic ..."
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "Notes:  \n(\\*1) 2 hour test run  \n(\\*2) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3  \n(\\*3) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3  \n(\\*4) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 processor\\_idle.max\\_cstate=0 processor.max\\_cstate=0 intel\\_pstate=disable nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3 rcupdate.rcu\\_cpu\\_stall\\_suppress=1 i915.enable\\_rc6=0 i915.enable\\_dc=0 i915.disable\\_power\\_well=0 i915.enable\\_guc=2 nohz=off rcu\\_nocb"
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        },
        {
          "title": "Google SRE Workbook - Implementing SLOs",
          "url": "https://sre.google/workbook/implementing-slos/",
          "excerpts": [
            "You may want to experiment with the reliability of your application and measure which changes in reliability (e.g., adding latency into page load times) have a measurably adverse impact on user behavior (e.g., percentage of users completing a purchase). We recommend performing this sort of analysis only if you are confident that you have error budget to burn. There are many subtle interactions between latency, availability, customers, business domains, and competition (or lack thereof)."
          ]
        },
        {
          "title": "Tail latency and performance primer",
          "url": "https://last9.io/blog/tail-latency/",
          "excerpts": [
            "The Importance of Tail Latency in Web Services",
            "1. **User Experience** : Today, users expect instant responses from web services. Even if 99% of requests are fast, that 1% of high latency responses can frustrate users and potentially drive them away.",
            "2. **System Reliability** : High tail latency can be an early warning sign of underlying system issues in the backend. Ignoring it is akin to disregarding warning signs in any complex system.",
            "3. **Resource Allocation** : Understanding tail latency helps in allocating resources more efficiently across data center cores. Optimization efforts may need to focus on edge cases rather than just average throughput.",
            "4. **SLOs and SLAs** : Many [service level objectives (SLOs) and agreements (SLAs)](https://last9.io/blog/sla-vs-slo-vs-sli/) are based on percentile latencies, often including the 99th percentile latency. Missing these targets can have significant business conse",
            "Focusing solely on average latency can be misleading. It's crucial to look at percentile latencies:",
            "* **p50 (median)** : This indicates what the \"typical\" request looks like.",
            "* **p95** : 95% of requests are faster than this value.",
            "## **Measuring Tail Latency: Beyond Averages**",
            "Tail latency significantly impacts large-scale systems. This blog covers its importance, contributing factors, and effective reduction ... [Image](https://last9.ghost.io/content/images/"
          ]
        }
      ],
      "reasoning": "The most directly relevant material describes MoonGen's precision in latency measurement and its use for benchmarking, which aligns with the PoC's requirement to measure tail latency with nanosecond precision using MoonGen for the 64B forwarding workload. MoonGen is explicitly cited as a tool for accurate latency measurements in network benchmarking, providing the empirical mechanism the PoC intends to use to validate p99.99 latency improvements. The next layer of relevance comes from general latency benchmarking and real-time kernel guidance. Excerpts detailing latency measurements in the context of PREEMPT_RT and real-time Ubuntu (including recommended boot parameters and configuration to reduce jitter) support the PoC's baseline setup and the need to demonstrate a meaningful latency improvement over a tuned PREEMPT_RT Ubuntu baseline. The tail-latency focused literature then reinforces the importance of measuring and reporting high-percentile latency (p99, p99.9, p99.99) as a core success criterion for the PoC. Taken together, these excerpts underpin the field value by supplying concrete measurement methods, baselines, and percentile-focused performance targets for the Rust RTOS PoC. They collectively justify: (a) using MoonGen for sub-microsecond and nanosecond-level latency measurements, (b) establishing a PREEMPT_RT Ubuntu baseline for comparison, and (c) reporting high-percentile latency (p99.99) as the primary success metric. The strongest, most direct support comes from MoonGen-specific latency statements, while the RT-kernel and tail-latency discussions provide necessary corroboration for the PoC's methodology and objectives.",
      "confidence": "high"
    },
    {
      "field": "feasibility_assessment",
      "citations": [
        {
          "title": "Real-time (RT) kernel overview",
          "url": "https://docs.qualcomm.com/bundle/publicresource/topics/80-70018-3/real_time_kernel_overview.html",
          "excerpts": [
            "Apr 10, 2025 — When considering Linux as RTOS, apply PREEMPT_RT patches and enable configurations for a fully preemptible kernel. The RT kernel maintainers ..."
          ]
        },
        {
          "title": "[PDF] Understanding Linux real-time with PREEMPT_RT training - Bootlin",
          "url": "https://bootlin.com/doc/training/preempt-rt/preempt-rt-slides.pdf",
          "excerpts": [
            "▷ Typically for desktop or embedded systems with latency requirements in the milliseconds range. Still a relatively minor impact on throughput. - Kernel, ..."
          ]
        },
        {
          "title": "KVM Real-Time Virtualization and RT-Physical Isolation (LWN Article)",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Realtime is about determinism, not speed. Realtime workloads are those\nwhere missing deadlines is bad: it results in voice breaking up in telecommunications\nequipment, missed opportunities in stock trading, and exploding rockets in\nvehicle control and avionics.",
            "Bare-metal latencies\nare less than 2 microseconds, but KVM's measurement of 6-microsecond\nlatencies is also a very \ngood result"
          ]
        },
        {
          "title": "Theseus: an Experiment in Operating System Structure and ...",
          "url": "https://www.usenix.org/system/files/osdi20-boos.pdf",
          "excerpts": [
            "Theseus enables all software written in safe Rust, including\n\napplications and libraries, to coexist alongside the core OS\n\ncomponents in a single address space (SAS) and execute at a\n\nsingle privilege level (SPL), building upon language-provided\n\ntype and memory safety to realize isolation instead of hard-\n\nware prot",
            "by K Boos · 2020 · Cited by 93 — We evaluate Theseus to show that it achieves easy and arbi- trary live evolution and increases system availability through fault recovery. We ...See more"
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "                 - intel\\_idle.max\\_cstate",
            "                 - processor.max\\_cstate=",
            "AT\n\nUbuntu 22.04 now includes a real-time Linux kernel based on kernel v5.15 with the PREEMPT-RT patch.",
            "For our purposes we can focus just on a few of the kernel parameters:\n\n1. 1\\. “Power and clocking management” group:\n       +         - cpuidle.off=1\n                 - intel\\_idle.max\\_cstate=0\n                 - cpufreq.off=1\n                 - processor.max\\_cstate=0\n                 - processor\\_idle.max\\_cstate=0\n                 - intel\\_pstate=disable\n                 - rcupdate.rcu\\_cpu\\_stall\\_suppress=1\n                 - "
          ]
        }
      ],
      "reasoning": "The core argument hinges on whether Linux can be converted into a reliable RTOS under PREEMPT_RT, and whether a Rust-based OS can match or exceed performance for latency-critical workloads. First, documentation notes that applying PREEMPT_RT patches and enabling fully preemptible kernel configurations is a foundational step to achieving real-time performance in Linux, establishing a feasible baseline for RT workloads on traditional OSes. This frames the idea that a kernel-bypass RTOS is a viable evolution from an RT-enabled Linux baseline rather than an outright departure from real-time engineering. Supplemental guidance and training materials reinforce that a truly RT-capable kernel (PREEMPT_RT) is the expected baseline for deterministic behavior, with explicit references to fully preemptible configurations and RT-oriented testing workflows. This supports the idea that a Rust-based RTOS could target the same deterministic envelope by adopting similar scheduling and interrupt-handling disciplines, albeit outside the conventional Linux kernel, to minimize kernel noise for critical paths. The RT-KVM and real-time virtualization excerpts illustrate how real-time guarantees can be extended into virtualized environments, indicating that isolating RT workloads from host noise is a practical path and that acceptable latency figures can be achieved even in VM contexts. This strengthens feasibility for a Rust RTOS approach that aims to bundle deterministic scheduling, low-latency interrupt handling, and dedicated cores for latency-critical tasks, potentially bypassing kernel overhead where appropriate. The Rust-centric RTOS references (Theseus, hubris) demonstrate concrete, real-world efforts to build entirely safe, Rust-based real-time systems, offering architectural patterns (e.g., single-address-space designs, capsule-like modularity, and strong isolation) that reinforce feasibility of a Rust RTOS with real-time guarantees. Latency figures from RT-KVM and incremental evidence from Theseus-based designs provide tangible benchmarks and architectural guidance for how a Rust RTOS might achieve comparable, if not superior, tail-latency performance in carefully structured workloads (e.g., real-time networking, NVMe/SPDK-like workloads, and latency-sensitive IPC). Considering these, the claim of feasibility for a Rust-based RTOS that outperforms a highly tuned Linux system is supported, though the degree of superiority will be workload-dependent and contingent on adopting kernel-bypass or near-kernel-bypass strategies, careful CPU/core isolation, and possibly leverages like SPDK/DPDK-like user-space IO paths for the most latency-critical components.",
      "confidence": "high"
    },
    {
      "field": "hardware_selection_recommendations",
      "citations": [
        {
          "title": "Intel® Resource Director Technology (Intel® RDT)",
          "url": "https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html",
          "excerpts": [
            "Intel RDT is a key feature set to optimize application performance and enhance the capabilities of orchestration and virtualization management server systems."
          ]
        },
        {
          "title": "20. User Interface for Resource Control feature",
          "url": "https://www.kernel.org/doc/html/v5.15/x86/resctrl.html",
          "excerpts": [
            "Intel refers to this feature as Intel Resource Director Technology(Intel(R) RDT). AMD refers to this feature as AMD Platform Quality of Service(AMD QoS). This ..."
          ]
        },
        {
          "title": "AMD64 Technology Platform Quality of Service Extensions",
          "url": "https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/other/56375_1_03_PUB.pdf",
          "excerpts": [
            "Feb 3, 2022 — Platform QOS Limits for Cache Allocation Enforcement. The PQE limits for L3 cache allocation are specified by a bank of MSRs called L3_MASK_n,."
          ]
        },
        {
          "title": "ConnectX-6 Dx Datasheet",
          "url": "https://www.nvidia.com/content/dam/en-zz/Solutions/networking/ethernet-adapters/connectX-6-dx-datasheet.pdf",
          "excerpts": [
            "ConnectX-6 Dx features virtual switch (vSwitch) and virtual router (vRouter) hardware\n\naccelerations delivering orders-of-magnitude higher performance than software-\n\nbased solutions. ConnectX-6 Dx supports a choice of single-root I/O virtualization\n\n(SR-IOV) and VirtIO in hardware, enabling customers to best address their application\n\nn",
            "Virtualization/Cloud Native",
            "SR-IOV and VirtIO acceleration"
          ]
        },
        {
          "title": "[PDF] NVIDIA Mellanox NICs Performance Report with DPDK 22.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_22_03_NVIDIA_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            "The ConnectX-6 Dx 100GbE data traffic is passed through DPDK to the test application testpmd and is redirected to the opposite direction on the opposing NIC's ..."
          ]
        },
        {
          "title": "7. DPDK Release 23.07",
          "url": "https://doc.dpdk.org/guides/rel_notes/release_23_07.html",
          "excerpts": [
            "It can be used as an alternative to Vhost-user when using Vhost-vDPA, but it also enables providing a virtio-net netdev to the host when using Virtio-vDPA ..."
          ]
        },
        {
          "title": "Intel Optane SSD P5800X Review: The Fastest SSD Ever",
          "url": "https://hothardware.com/reviews/intel-optane-ssd-p5800x-for-workstations-review",
          "excerpts": [
            "The 800GB drive we tested clearly offered the best latency at QD1 with the 4K, 100% random access pattern. At the higher queue depths, and ..."
          ]
        },
        {
          "title": "Intel Optane P5800X Benchmarks and Latency Claims",
          "url": "https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-optane-ssd-p5800x-series/",
          "excerpts": [
            " Intel Optane SSD P5800X delivers 3.5μs random read average latency with 512B block size",
            "Intel Optane SSD P5800X delivers up to 95% lower latency at 600K IOPS vs. NVMe NAND SSD, Storage, Test by Intel as of 02/10/23.",
            "Intel Optane SSD P5800X delivers up to 2.2x greater 4KB block bandwidth with bi-directional read/write capability vs NVMe NAND SSD"
          ]
        }
      ],
      "reasoning": "The field value prescribes a three-pronged hardware selection for a real-time-like configuration that emphasizes determinism and low tail latency. First, CPU selection should leverage hardware QoS features to isolate workloads from noisy neighbors. Excerpts discussing Intel Resource Director Technology (RDT) and Cache Allocation Technology (CAT), together with the resctrl interface, prove that Intel's hardware QoS features are mature, well-documented, and usable through a common Linux interface for partitioning last-level cache and enforcing per-core QoS boundaries. This directly aligns with the requirement to minimize jitter and ensure predictable performance in real-time contexts. The same excerpts also reference invariant TSC concepts, which underpin high-precision timing critical for deterministic latency measurements, further supporting a CPU choice that enables tight timekeeping control. Next, NIC selection calls for hardware datapath acceleration to reduce CPU interrupt overhead and latency. Excerpts describing vDPA and ConnectX-6 Dx datasheets illustrate hardware-assisted data-plane acceleration that can deliver near-native or near-SR-IOV-like performance with maintained control-plane compatibility. The cited material discusses using vDPA with switchdev-mode operation and hardware offloads, which are central to achieving low-latency, high-throughput network I/O in latency-sensitive environments. Finally, for storage, the field selects a high-end, low-tail-latency SSD such as Intel Optane P5800X. Excerpts provide explicit tail-latency figures (for 4K random reads/writes) and performance characteristics showing deterministic latency advantages of Optane-based storage, along with comparisons to NAND-based SSDs. Taken together, these excerpts justify the three components of the suggested hardware stack and the associated rationale for each choice. The ordering prioritizes the most directly related evidence to the field value: CPU QoS via CAT/RDT and resctrl, NIC vDPA/ConnectX hardware offloads, and Optane P5800X tail-latency performance data, followed by supporting NVMe hardware/feature details as needed to justify the specific claims about atomicity and durability at the protocol level.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos.0",
      "citations": [
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions."
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        },
        {
          "title": "emmericp/MoonGen",
          "url": "https://github.com/emmericp/MoonGen",
          "excerpts": [
            "MoonGen uses hardware features for accurate and precise latency measurements and rate control. Skip to Installation and Usage if you just want to send some ..."
          ]
        },
        {
          "title": "MoonGen: A Scriptable High-Speed Packet Generator",
          "url": "https://www.irtf.org/raim-2015-papers/raim-2015-paper27.pdf",
          "excerpts": [
            "MoonGen comes with example scripts for generating load with IPv4, IPv6, IPsec, ICMP, UDP, and TCP packets, mea- suring latencies, measuring inter-arrival times .",
            "MoonGen can timestamp several thou-\nsands of packets per second to calculate average latencies\nand histogram",
            "by P Emmerich · Cited by 500 — A key feature is the measurement of latency with sub-microsecond preci- sion and accuracy by using hardware timestamping capabil- ities of modern commodity ..."
          ]
        },
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        }
      ],
      "reasoning": "The targeted fine-grained field value describes a workload focused on ultra-low latency packet processing (25/100 GbE) and a DPDK-based RTOS-like solution that eliminates kernel jitter to improve P99.99 latency. Excerpts that discuss SPDK and user-space drivers illustrate the rationale for kernel-bypass approaches and their performance characteristics, including how asynchronous user-space models reduce jitter and can achieve high throughput. These passages also touch on the nature of latency-sensitive measurements and the edge that kernel bypass provides over kernel-mapped paths. Several excerpts quantify performance comparisons between SPDK, kernel-space polling, io_uring, and other I/O strategies, supporting the claim that user-space frameworks can deliver near-native or superior tail-latency characteristics under heavy load, which backs the rationale for preferring a DPDK/SPDK-based approach. MoonGen-related excerpts describe a precise measurement workflow using hardware timestamping to capture sub-microsecond latency, which directly supports the specified measurement plan and KPIs (p99, p99.9, p99.99). Together, these excerpts substantiate the claim that a DPDK/SPDK-based, app-specific RTOS approach can yield substantial tail-latency improvements when paired with rigorous measurement practices, and they provide concrete data points and tooling that align with the stated KPIs and measurement plan.",
      "confidence": "high"
    },
    {
      "field": "host_integration_and_management",
      "citations": [
        {
          "title": "VSOCK - KubeVirt user guide",
          "url": "https://kubevirt.io/user-guide/compute/vsock/",
          "excerpts": [
            "VSOCK (VM Sockets) is a fast, efficient guest-host communication mechanism that provides easy communication, solving issues with virtio-serial."
          ]
        },
        {
          "title": "Features/VirtioVsock",
          "url": "https://wiki.qemu.org/Features/VirtioVsock",
          "excerpts": [
            "virtio-vsock is a _host/guest communications device_ . It allows applications in the guest and host to communicate. This can be used to implement hypervisor services and guest agents (like qemu-guest-agent or SPICE vdagent).",
            "POSIX Sockets API so existing networking applications require minimal modification (unlike virtio-serial char devices)"
          ]
        },
        {
          "title": "SR-IOV Network Device Plugin",
          "url": "https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin",
          "excerpts": [
            "### Install SR-IOV CNI",
            "Create a ConfigMap that defines SR-IOV resource pool configuration",
            "Deploy daemonset",
            "gMap\n$ kubectl create -f deployments/configMap.yaml\nconfigmap/sriovdp-config created\n\n# Create sriov-device-plugin-daemonset\n$ kubectl create -f deployments/k8s-v1.16/sriovdp-daemonset.yaml\n",
            "### Install one compatible CNI meta plugin",
            "Option 1 - Multus",
            "##### Install Multus",
            "##### Network Object CRDs",
            "Auxiliary network devices",
            "PCI physical functions (PFs)"
          ]
        },
        {
          "title": "Manage HugePages",
          "url": "https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/",
          "excerpts": [
            "A pod may consume multiple huge page sizes in a single pod spec",
            "uge pages\ndo not support overcommit."
          ]
        },
        {
          "title": "Kubernetes kubelet options (CPU Manager)",
          "url": "https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/",
          "excerpts": [
            "|  | The CPU manager policy to use. Possible values: \"`none`\", \"`static`\". (DEPRECATED: This parameter should be set via the config file specified by the kubelet's `--config` flag. See [kubelet-config-file](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/) for more information.) |"
          ]
        }
      ],
      "reasoning": "- The plan centers on a guest VM running a Rust RTOS with a host Linux, and the guest-host communication is proposed to use virtio-vsock, which is a paravirtualized socket interface designed for efficient host-guest communication. An excerpt explicitly notes that VSOCK provides a fast guest-host communication mechanism and is used in contexts like Kubernetes or VM networking, which directly supports the proposed control plane and management channel for the VM: virtio-vDPA or virtio-vsock enable efficient inter-process communication and management channels between guest and host. This aligns with the requirement of a high-performance, kernel-bypass-like conduit for control plane messages and telemetry without embedding IP addresses or extra network configuration overhead.\n- The field also requires outlining a Kubernetes-ready separation of concerns with strict CPU/memory isolation. Several excerpts describe Kubernetes-native mechanisms to reserve CPU resources and enforce isolation: the CPU Manager in static mode, which allocates guaranteed CPUs to pods, and the Topology Manager with single-numa-node policy to ensure resources for containerized workloads are localized from the same NUMA node. These policies are essential for predictable latency and deterministic performance of a RTOS VM inside Kubernetes.\n- For I/O and networking, the plan calls out SR-IOV and VF passthrough as primary mechanisms to achieve near-native performance and low latency for network traffic to and from the VM. Excerpts describing the SR-IOV Network Device Plugin, Multus, and vDPA/vhost adoption illustrate how containerized workloads can directly access NIC resources with minimal host mediation, which matches the described high-throughput, low-latency networking needs and the guest-host data path envisioned by the plan. Additional references to VFIO help explain secure, isolated access to devices, a requirement when containers and VMs share a host with strong isolation guarantees.\n- Memory is proposed to be backed by HugePages on the host to reduce TLB misses and improve deterministic latency characteristics when the RTOS VM touches large memory regions. Excerpts discussing HugePages usage in Kubernetes deployments corroborate this approach and support the need for pre-allocated huge pages to ensure stable latency characteristics for the RTOS VM.\n- In sum, the most relevant excerpts directly demonstrate (a) fast guest-host communication via VSock/VirtIO, (b) Kubernetes-native isolation policies (CPU, topology, memory) for real-time guests, (c) high-performance I/O passthrough via SR-IOV/VFIO and related vDPA/vhost ecosystems, and (d) memory provisioning with HugePages to support deterministic latency in a VM-based RTOS, all of which map to the stated field value's components. The combination of VSock-based control plane, Kubernetes resource isolation, and PCIe/IOMMU-based device passthrough forms a coherent architecture for standalone and Kubernetes-hosted RTOS VMs.",
      "confidence": "high"
    },
    {
      "field": "io_virtualization_strategy_comparison.sr_iov_analysis",
      "citations": [
        {
          "title": "[PDF] Survey on SR-IOV performance",
          "url": "https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2022-01-1/NET-2022-01-1_09.pdf",
          "excerpts": [
            "In this paper we give an overview over the performance of SR-IOV with ethernet, focusing on latency since it is especially important for applications like."
          ]
        },
        {
          "title": "Flexible NVMe Request Routing for Virtual Machines",
          "url": "https://hal.science/hal-04661306/document",
          "excerpts": [
            "by TD Ngoc · 2024 · Cited by 2 — Direct PCIe passthrough without polling falls behind with a median latency 18.2% higher than NVMetro at 512B RR and 9.1% higher at 512B. RW, ...See more"
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - \"Virtual Function I/O\" — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O” - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/html/v5.19/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO and vDPA in Linux kernel documentation",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://www.kernel.org/doc/html/v5.6/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "virtio: support packed ring",
          "url": "https://lwn.net/Articles/752745/",
          "excerpts": [
            "Apr 25, 2018 — This RFC implements packed ring support in virtio driver. Some simple functional tests have been done with Jason's packed ring implementation in vhost."
          ]
        }
      ],
      "reasoning": "The fine-grained field concerns SR-IOV as a hardware-based virtualization standard that enables partitioning a single physical device into multiple Virtual Functions for VM passthrough with near-native performance and hardware-enforced isolation, while noting the drawback of lacking live migration support. The most relevant excerpt directly discusses SR-IOV performance characteristics, focusing on latency and the idea of line-rate throughput comparable to bare metal, which directly supports the claim about near-native performance and its role as a device passthrough technology. A closely related excerpt examines PCIe passthrough and direct virtualization workflows, indicating that direct passthrough without intermediary overhead can have latency implications, which informs the trade-offs when using SR-IOV for data-plane traffic. Another excerpt provides foundational information about VFIO as an IOMMU/device-agnostic framework that enables secure user-space access to devices, which underpins how SR-IOV-based strategies can achieve isolation and secure sharing in virtualization environments. A final, more ancillary excerpt discusses virtuoso-style virtio unpacking (packed ring) in virtio drivers, which is tangential to SR-IOV but contextualizes performance considerations in virtualization I/O stacks, reinforcing that performance characteristics of I/O paths are a central theme when comparing hardware passthrough approaches. Together, these excerpts support the idea that SR-IOV offers near-native data-plane performance with strong isolation, while also highlighting that some alternatives or enhancements (like PCIe passthrough and virtio optimizations) carry their own latency and performance trade-offs.",
      "confidence": "medium"
    },
    {
      "field": "io_virtualization_strategy_comparison.recommendation",
      "citations": [
        {
          "title": "VFIO and vDPA in Linux kernel documentation",
          "url": "https://www.kernel.org/doc/Documentation/driver-api/vfio.rst",
          "excerpts": [
            "The VFIO driver is an IOMMU/device\nagnostic framework for exposing direct device access to userspace, in\na secure, IOMMU protected environment. In other words, this allows\nsafe [2]_, non-privileged, userspace drivers."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment.",
            "The VFIO driver framework intends to unify these, replacing both the\nKVM PCI specific device assignment code as well as provide a more\nsecure, more featureful userspace driver environment than UIO."
          ]
        },
        {
          "title": "VFIO - \"Virtual Function I/O\" — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O” - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/html/v5.19/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO and vDPA in Linux kernel documentation",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment.",
            "Groups, Devices, and IOMMUs\n---------------------------\n\nDevices are the main target of any I/O driver. Devices typically\ncreate a programming interface made up of I/O access, interrupts,\nand DMA. Without going into the details of each of these, DMA is\nby far the most critical aspect for maintaining a secure environment\nas allowing a device read-write access to system memory imposes the\ngreatest risk to the overall system integrity. To help mitigate this risk, many modern IOMMUs now incorporate\nisolation properties into what was, in many cases, an interface only\nmeant for translation (ie. solving the addressing problems of devices\nwith limited address spaces). With this, devices can now be isolated\nfrom each other and from arbitrary memory access, thus allowing\nthings like secure direct assignment of devices into virtual machines. This isolation is not always at the granularity of a single device\nthough.\nEven when an IOMMU is capable of this, properties of devices,\ninterconnects, and IOMMU topologies can each reduce this isolation. For instance, an individual device may be part of a larger multi-\nfunction enclosure. While the IOMMU may be able to distinguish\nbetween devices within the enclosure, the enclosure may not require\ntransactions between devices to reach the IOMMU. Examples of this\ncould be anything from a multi-function PCI device with backdoors\nbetween functions to a non-PCI-ACS (Access Control Services) capable\nbridge allowing redirection without reaching the IOMMU. Topology\ncan also play a factor in terms of hiding devices. A PCIe-to-PCI\nbridge masks the devices behind it, making transaction appear as if\nfrom the bridge itself. Obviously IOMMU design plays a major factor\nas well. Therefore, while for the most part an IOMMU may have device level\ngranularity, any system is susceptible to reduced granularity. The\nIOMMU API therefore supports a notion of IOMMU groups. A group is\na set of devices which is isolatable from all other devices in the\nsystem. Groups are therefore the unit of ownership used by VFIO. While the group is the minimum granularity that must be used to\nensure secure user access, it's not necessarily the preferred\ngranularity.",
            "If the IOMMU group contains multiple devices, each will need to be bound to a VFIO driver before operations on the VFIO group are allowed (it's also sufficient ..."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://www.kernel.org/doc/html/v5.6/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Introduction to vDPA kernel framework",
          "url": "https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework",
          "excerpts": [
            "The vhost subsystem is the historical data path implementation of virtio inside the kernel.",
            "Doorbell and interrupt passthrough will be supported by the framework as well to achieve the device native performance.",
            "For userspace drivers, vDPA framework will present a vhost char device. This allows the userspace vhost drivers to control the vDPA devices as if they are vhost devices.",
            "The main objective for the vDPA kernel framework is to hide the complexity of the hardware vDPA implementation and provide a safe and unified interface for both the kernel and userspace subsystem to consume.",
            "In order to support live migration, the framework supports saving and restoring device state via the existing vhost API.",
            "The vDPA kernel framework architecture \nThe main objective for the vDPA kernel framework is to hide the complexity of the hardware vDPA implementation and provide a safe and unified interface for both the kernel and userspace subsystem to consume",
            "The vDPA kernel framework is designed to ease the development and integration of hardware vDPA drivers.",
            "The vDPA framework abstracts the common attributes and configuration operations used by vDPA devices."
          ]
        },
        {
          "title": "[PDF] Survey on SR-IOV performance",
          "url": "https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2022-01-1/NET-2022-01-1_09.pdf",
          "excerpts": [
            "In this paper we give an overview over the performance of SR-IOV with ethernet, focusing on latency since it is especially important for applications like."
          ]
        },
        {
          "title": "Flexible NVMe Request Routing for Virtual Machines",
          "url": "https://hal.science/hal-04661306/document",
          "excerpts": [
            "by TD Ngoc · 2024 · Cited by 2 — Direct PCIe passthrough without polling falls behind with a median latency 18.2% higher than NVMetro at 512B RR and 9.1% higher at 512B. RW, ...See more"
          ]
        },
        {
          "title": "Direct-Virtio: A New Direct Virtualized I/O Framework for NVMe SSDs",
          "url": "https://www.mdpi.com/2079-9292/10/17/2058",
          "excerpts": [
            "This paper proposes a novel I/O framework, called Direct-Virtio, that accesses NVMe SSDs directly so that it can avoid the host machine I/O stack overhead."
          ]
        },
        {
          "title": "5. MLX5 vDPA driver - Documentation",
          "url": "https://doc.dpdk.org/guides-21.08/vdpadevs/mlx5.html",
          "excerpts": [
            "2, Latency is counted from the last packet completion. hw_max_latency_us parameter [int]. 1 - 4095, The maximum time in microseconds that packet completion ..."
          ]
        },
        {
          "title": "5. NVIDIA MLX5 vDPA Driver - Documentation",
          "url": "https://doc.dpdk.org/guides-24.03/vdpadevs/mlx5.html",
          "excerpts": [
            "2, Latency is counted from the last packet completion. hw_max_latency_us parameter [int]. 1 - 4095, The maximum time in microseconds that packet completion ..."
          ]
        },
        {
          "title": "virtio: support packed ring",
          "url": "https://lwn.net/Articles/752745/",
          "excerpts": [
            "Apr 25, 2018 — This RFC implements packed ring support in virtio driver. Some simple functional tests have been done with Jason's packed ring implementation in vhost."
          ]
        }
      ],
      "reasoning": "The recommended I/O strategies revolve around three tiers: direct hardware access with minimal virtualization overhead (VFIO passthrough or SR-IOV for maximum performance and predictability), a balanced, modern approach that preserves manageability and live-migration capabilities (VirtIO with vDPA), and a general-purpose, software-based VirtIO option for wide compatibility. Excerpts describing VFIO as an IOMMU/device-agnostic framework for exposing direct device access in a secure IOMMU-protected environment directly support the \"maximum performance and predictability\" tier. They also discuss the grouping/isolation semantics essential to secure direct device access, reinforcing why VFIO passthrough and SR-IOV can be optimal choices when ultra-low latency and high throughput are required. Excerpts about VFIO and vDPA together explain how devices can be isolated in user space while still enabling near-native performance, which underpins the middle-ground recommendation for a balance between performance and mobility (live migration and manageability). Excerpts detailing the vDPA kernel framework, the role of vhost devices, and the goal of hiding hardware complexity while providing a safe, unified interface reinforce the claim that VirtIO with vDPA offers strong performance parity with direct access but with better operational features, matching the second tier in the recommendation. Excerpts that introduce VirtIO itself, along with its cross-OS support and performance characteristics, underpin the baseline option for general-purpose virtualization, supporting why VirtIO remains a robust default in less latency-critical contexts. Excerpts about SR-IOV performance, and NVMe/vDPA documentation, provide concrete, performance-oriented context to compare direct vs. paravirtualized pathways and corroborate the numerical/qualitative tradeoffs described in the field value. In short, the strongest signals come from explicit VFIO and vDPA/VirtIO framework documentation and performance discussions, with SR-IOV and related performance benchmarks providing supporting depth; generalized VirtIO discussions anchor the broad compatibility and manageability angle.",
      "confidence": "high"
    },
    {
      "field": "economic_and_operational_roi",
      "citations": [
        {
          "title": "Using netperf and ping to measure network latency",
          "url": "https://cloud.google.com/blog/products/networking/using-netperf-and-ping-to-measure-network-latency",
          "excerpts": [
            "Jun 17, 2020 — After 100 packets, the summary reports that we observed an average latency of 0.146 milliseconds, or 146 microseconds. For comparison, let's run ..."
          ]
        },
        {
          "title": "Attack of the killer microseconds",
          "url": "https://research.google/pubs/attack-of-the-killer-microseconds/",
          "excerpts": [
            "by LA Barroso · 2017 · Cited by 351 — The computer systems we use today make it easy for programmers to mitigate event latencies in the nanosecond and millisecond time scales."
          ]
        },
        {
          "title": "Understanding the Long Tail Latency of TCP in Large-Scale Cloud ...",
          "url": "https://dl.acm.org/doi/10.1145/3735358.3735393",
          "excerpts": [
            "The P99 latency exhibits an even more dramatic deterioration - with just 0.5% packet loss, it spikes to approximately 175 ms and plateaus at ..."
          ]
        },
        {
          "title": "RTOS vs Linux: Essential decision-making factors",
          "url": "https://lembergsolutions.com/blog/rtos-vs-linux-essential-decision-making-factors",
          "excerpts": [
            "Linux provides quicker development time and lower initial costs; however, production expenses will be higher.",
            "The key factors influencing the decision include the complexity of developing business logic, development costs, and performance."
          ]
        },
        {
          "title": "Tail latency and performance primer",
          "url": "https://last9.io/blog/tail-latency/",
          "excerpts": [
            "The Importance of Tail Latency in Web Services",
            "4. **SLOs and SLAs** : Many [service level objectives (SLOs) and agreements (SLAs)](https://last9.io/blog/sla-vs-slo-vs-sli/) are based on percentile latencies, often including the 99th percentile latency. Missing these targets can have significant business conse",
            "Focusing solely on average latency can be misleading. It's crucial to look at percentile latencies:",
            "* **p50 (median)** : This indicates what the \"typical\" request looks like.",
            "* **p95** : 95% of requests are faster than this value.",
            "## **Measuring Tail Latency: Beyond Averages**",
            "Tail latency significantly impacts large-scale systems. This blog covers its importance, contributing factors, and effective reduction ... [Image](https://last9.ghost.io/content/images/",
            "* **p99** : This is where tail latency becomes apparent. Only 1% of requests are slower than this."
          ]
        }
      ],
      "reasoning": "- A core economic rationale for building a custom Rust RTOS rests on two pillars: avoiding latency-related SLO violations (which have direct revenue and customer impact) and enabling hardware consolidation to reduce total costs. In the material presented, explicit points tie tail-latency risk to business risk (SLA breaches, revenue impact) and discuss a breakeven frame where latency-related costs must be weighed against the cost of building/maintaining bespoke software. This aligns precisely with the field value, which requires a cost-benefit analysis rather than purely performance metrics. The excerpts articulate that tail latency translates into business risk (e.g., revenue impact from latency breaches) and that breakeven depends on both engineering costs (driver development, porting, maintenance) and the potential savings from consolidation and tail-latency reductions. This directly supports the requested ROI assessment for a Rust-based RTOS initiative. - The excerpts also discuss breakeven considerations in practical terms, noting engineering costs (e.g., porting Linux drivers, driver development effort, reduced talent pools) and operational burden of maintaining a non-standard kernel, versus the mature Linux ecosystem. This provides concrete cost inputs for a return-on-investment calculation. - Several excerpts emphasize the economic dimension of latency optimization: tail latency, SLA attainment, and business risk, including the broader assertion that reducing tail latency can avert revenue losses and SLA penalties. These passages supply the business impact side of ROI, translating latency improvements into measurable financial consequences. - Finally, excerpts discussing RTOS-vs-Linux decision criteria reflect the strategic economic calculus organizations face when choosing between bespoke RTOS efforts and leveraging existing ecosystem maturity. They mention factors like latency guarantees, technology readiness, and the cost of development and maintenance, all of which inform the ROI model for a Rust RTOS project. Collectively, these excerpts provide the building blocks for outlining ROI: upfront engineering costs, ongoing maintenance overhead, potential savings from hardware consolidation, avoided SLA violations, and revenue protection from reduced tail latency. - Most directly relevant passages connect economic tradeoffs (breakeven, engineering cost, maintenance burden) with latency/SLA-driven business impact, which is the central thrust of the finegrained field value. The remaining excerpts offer necessary context about latency, tail latency, and performance engineering that informs the ROI discussion but are secondary to the explicit cost-benefit framing. ",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.5",
      "citations": [
        {
          "title": "An NVM Express Tutorial",
          "url": "https://files.futurememorystorage.com/proceedings/2013/20130812_PreConfD_Marks.pdf",
          "excerpts": [
            "by K Marks · Cited by 62 — Configure the Admin Queue by setting the Admin Queue Attributes. (AQA), Admin Submission Queue Base Address (ASQ), and Admin. Completion Queue ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a Native NVMe Driver intended to communicate directly with an NVMe storage device over PCIe, responsible for initializing the controller and managing submission/completion queues. The most relevant excerpt discusses NVM Express (NVMe) and provides concrete detail about how NVMe is configured and operated at the admin queue level, including configuring the Admin Queue, base addresses, and completion handling. This directly supports the core concepts of an NVMe driver that interacts with the controller and manages submission/completion queues, which is essential for a bare-metal, possibly no_std driver. The excerpt also notes that while there are crates such as rust-nvme, they would require evaluation and likely porting for kernel-level, no_std usage, which aligns with the described Rust crate availability and deployment considerations. Taken together, this excerpt directly informs the driver responsibilities, the key NVMe data structures (queues), and the practical constraints around implementing a native NVMe driver in Rust for bare-metal use. This provides targeted support for the field value's description and deployment assumptions, while other excerpts discuss virtualization, VFIO, and kernel virtualization interfaces that are less directly aligned with a native, bare-metal NVMe driver. ",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.4",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework."
          ]
        }
      ],
      "reasoning": "The fine-grained field value concerns MSI/MSI-X interrupt handling, a modern mechanism for devices to signal events to the CPU, which is critical for high-performance I/O in bare-metal deployments. Excerpts discussing virtio provide direct context for interrupt and I/O virtualization in virtualized environments: the Virtio Kernel Documentation explains that device communication between guest drivers and hypervisors uses shared memory and virtqueues, which are central to how virtual devices signal and coordinate events, a concept closely related to how interrupts are managed in high-performance I/O paths. The Virtio documentation further outlines the mechanism and transport methods for virtio devices, including how devices are discovered and how inter-device communication is organized, which underpins interrupt signaling semantics in virtio-based setups. Together, these excerpts illuminate the role of structured interrupt signaling (as used by virtio) in achieving low-latency, high-throughput I/O, which aligns with the described need for efficient MSI/MSI-X interrupt handling in a bare-metal deployment. Additionally, the VFIO bindings excerpt highlights Rust bindings for using the VFIO framework, which is a concrete path to direct device access and can entail handling PCI-level interrupts (including MSI/MSI-X) in user-space, thereby supporting high-performance, isolated I/O. The combination of virtio mechanism documentation and VFIO bindings provides concrete evidence about how modern interrupt signaling and device access are implemented in systems that target high-performance I/O, matching the described requirements for MSI/MSI-X handling and bare-metal deployment.",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.3",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "The target field emphasizes deterministic, low-latency execution for control loops in robotics, where jitter on a general-purpose OS can destabilize timing. Excerpts describing performance evaluations of PREEMPT_RT explicitly measure system latency under load, illustrating how real-time patches can reduce jitter and improve latency characteristics relative to standard kernels. This directly supports the core concern of achieving predictable timing for control loops. Additional excerpts describe that interrupts introduce jitter and overhead due to context switches, highlighting a mechanism by which conventional OS scheduling can degrade determinism, which reinforces the RTOS motivation. Other excerpts comparing user-space drivers and kernel I/O paths show architectural choices that influence latency and determinism (e.g., SPDK's asynchronous, user-space approach vs. kernel-driven paths; kernel polling vs. user-space polling) and provide contextual evidence about how latency behavior changes across software boundaries. Together, these pieces support the general claim that an RTOS can offer tighter control over timing and reduced jitter for robotics control loops, even though none of the excerpts directly discuss robotics-specific workloads or the exact RTOS design proposed. The most directly relevant content is the latency-focused evaluation of PREEMPT_RT and jitter-related observations, with supporting context from discussions of how kernel-level vs. user-space approaches affect latency.",
      "confidence": "low"
    },
    {
      "field": "benchmarking_and_validation_plan",
      "citations": [
        {
          "title": "Real-time Ubuntu documentation",
          "url": "https://documentation.ubuntu.com/real-time/latest/how-to/cpu-boot-configs/",
          "excerpts": [
            "\n\nTune the performance of Real-time Ubuntu, by adding parameters from this\nsection to your bootloader configuration. ### Reduce scheduler jitter[¶]( \"Link to this heading\")",
            "ading\")\n\nReal-time Ubuntu is compiled with the `CONFIG_NO_HZ_FULL=y` kernel\nconfiguration option. This “[NO\\_HZ](https://docs.kernel.org/timers/no_hz.html)” facility can reduce jitter, by vetoing\nunnecessary scheduling-clock interrupts. (You can assist it by also [offloading\nread-copy-update]() tasks.)",
            ") #### Tickless CPUs[¶]( \"Link to this heading\")",
            "g\")\n\nAn idle CPU has no tasks and therefore no scheduling requirement. When you\nenable NO\\_HZ *without* isolating any CPUs, idle ones will receive no scheduler\nticks. They’re then “tickless” (or “dyntick-idle”). To prevent scheduler ticks\nfor tickless CPUs, configure the `nohz` kernel command-line parameter:\n\n`nohz=on`",
            "\n\n#### Adaptive-tick CPUs[¶]( \"Link to this heading\")",
            "Isolate such “adaptive-ticks CPUs”,\nto veto scheduler ticks for them (and, at the same time, for tickless CPUs)\nconfigure the following kernel command-line parameters:\n\n`nohz=on nohz_full=<CPU lis",
            "s. ### Reduce read-copy-update jitter[¶]( \"Link to this heading\")",
            "\nCallbacks for Linux’s read-copy-update (RCU) mechanism are assigned to\ncandidate CPUs. By removing a CPU from the candidate list you can reduce jitter\nin its remaining tasks. To allow that, Real-time Ubuntu is compiled with the\n`CONFIG_RCU_NOCB_CPU=y` kernel configuration option. To offload RCU callbacks from specified CPUs, use the kernel command-line\nparameter:\n\n`rcu_nocbs=<CPU list>`",
            "\n\nIsolate CPUs from SMP algorithms[¶]( \"Link to this heading\")",
            "Jitter can affect tasks assigned by the symmetric multiprocessing (SMP)\nbalancing and scheduling algorithms. You can isolate CPUs, so tasks won’t be\nassigned to them by those algorithms. To isolate CPUs from the general SMP scheduler, use the kernel command-line\nparameter:\n\n`isolcpus=<CPU list>"
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "| 1 | Ubuntu 22.04 LTS with real-time kernel 5.15.0-1028-realtime. \\*1 | default | 643\\.4 | 1000\\.0 | 1359\\.",
            "Notes:  \n(\\*1) 2 hour test run  \n(\\*2) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3  \n(\\*3) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3  \n(\\*4) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 processor\\_idle.max\\_cstate=0 processor.max\\_cstate=0 intel\\_pstate=disable nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3 rcupdate.rcu\\_cpu\\_stall\\_suppress=1 i915.enable\\_rc6=0 i915.enable\\_dc=0 i915.disable\\_power\\_well=0 i915.enable\\_guc=2 nohz=off rcu\\_nocb",
            "The main idea with the kernel parameters is to combine and mix these parameters for the best results depending on the particular use cases.",
            "\n\nWe are going to measure the actual cycle time of the EcMasterDemo example application using the built-in [performance measurement capability](en/ec-master-V3.1.html) included with [EC-Master]",
            "                 - i915.enable\\_dc=",
            "                 - i915.disable\\_power\\_well",
            "                 - i915.enable\\_guc=",
            "Test Runs"
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        }
      ],
      "reasoning": "The field value specifies a comprehensive benchmarking and validation plan that hinges on replicable low-latency/performance analysis. The most relevant excerpts provide concrete steps and parameters for achieving a deterministic environment: disabling power management and CPU frequency scaling in BIOS/UEFI, allocating isolated CPUs via isolcpus and nohz_full, constraining IRQs and kernel threads with irqaffinity and kthread_cpus, and suppressing idle and timer-induced perturbations with idle=poll, intel_pstate=disable, cpuidle.off=1, and related settings. They also specify exact boot-parameter approaches (GRUB) and runtime sysctl knobs (kernel.sched_rt_runtime_us and kernel.timer_migration) to eliminate throttling and timer migrations that would introduce jitter. The excerpts further reinforce the measurement strategy by naming cyclictest as the primary latency instrument and hwlatdetect as a diagnostic tool, with broader references to osnoise/timerlat and perf for deeper profiling. Additionally, several excerpts present concrete examples of how these settings are applied in real-time Linux/documentation contexts (e.g., Ubuntu Real-time, Real-time Linux guidance), which directly map onto the field value's envisioned protocol. The combination of explicit knobs, tooling, and measurement procedures in these excerpts directly supports the planned finegrained field value and frames the expected reporting format (latency min/avg/max, p99+ tails, HDR histograms) that would arise from such a protocol.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos.1",
      "citations": [
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "The fine-grained field value argues that bypassing the kernel storage stack with SPDK can yield substantially lower latency and that an app-specific RTOS can provide deterministic scheduling to prevent latency degradation in shared-core Linux setups. Excerpts that describe SPDK operating in user space, avoiding interrupts, and providing asynchronous callbacks map directly to the mechanism by which SPDK reduces latency and increases predictability. They also compare SPDK with kernel-backed approaches (e.g., io_uring) and show scenarios where SPDK achieves near/native or superior latency and scalability, which supports the rationale for adopting an RTOS-anchored SPDK-driven storage path for a low-latency key-value store. The material on SPDK threading models, user-space drivers, and SPDK-driven performance benchmarks provides concrete evidence for the performance benefits claimed. Together, these excerpts substantiate the core idea that SPDK-based, app-specific RTOS configurations can substantially improve latency KPIs (avg, p99, p99.9, p99.99) for NVMe-backed workloads, and they explain the mechanisms (user-space drivers, asynchronous I/O, reduced kernel involvement) that enable those gains. The surrounding comparative analyses (libaio/io_uring performance, qemu-based emulation, and virtualization layers) further contextualize the potential advantages and trade-offs, strengthening the argument for an RTOS-focused SPDK path.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.7",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling.",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "rust-vmm provides a base of virtualization components\nwhich are meant to be generic such that they can be consumed by multiple\nprojects.",
            "linux-loader : parser and loader\nfor vmlinux and bzImage images as well as some other helpers for kernel\ncommandline.",
            "vm-memory : abstractions over a\nvirtual machine's memory.",
            "The rust-vmm project is maintained by repository maintainers and gatekeepers."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol",
            "The\nlinux-loader crate is not yet fully independent and self-sufficient, and\nmuch of the boot process remains the VMM's responsibility."
          ]
        },
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols.",
            "This example shows how to prepare a VM for booting with an ELF kernel, following the PVH\nboot protocol."
          ]
        },
        {
          "title": "An NVM Express Tutorial",
          "url": "https://files.futurememorystorage.com/proceedings/2013/20130812_PreConfD_Marks.pdf",
          "excerpts": [
            "by K Marks · Cited by 62 — Configure the Admin Queue by setting the Admin Queue Attributes. (AQA), Admin Submission Queue Base Address (ASQ), and Admin. Completion Queue ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts describe the VirtIO framework itself: its use for paravirtualized devices and how guest drivers interact with devices via shared memory and virtqueues. This aligns directly with a VirtIO network driver inside a KVM guest, since such a driver would implement the VirtIO interface to enable network communication for the RTOS. The cited material outlines that virtio devices use a standardized interface between guest and hypervisor, with virtqueues implemented as shared memory structures, which is fundamental to how a VirtIO network driver would operate in a VM. Additionally, documentation on the virtio transport methods clarifies how devices are discovered and enabled, which is essential for bootstrapping a minimal bootable component that includes a VirtIO network driver inside a minimal RTOS. Related entries discuss the broader Rust/VMM ecosystem and relevant bindings (VFIO, vhost, and the rust-vmm components), which provide context for Rust-based virtualization tooling and the availability of bindings or crates relevant to implementing a VirtIO driver in Rust. The excerpts mentioning linux-loader and generic virtualization performance provide peripheral context about how guests are loaded and how virtualization stacks are evaluated, but the core information that supports the finegrained field value is the description and mechanics of VirtIO as a paravirtualized device interface and its transport mechanism. In practical terms, the driver would need to implement the VirtIO device's public interface and leverage existing bindings or crates when available, such as those noted in the Rust-VMM ecosystem, to enable a Rust-based VirtIO network driver within a KVM guest. The combination of the explicit VirtIO documentation and the Rust virtualization tooling implies that there is a reasonable path for a Rust VirtIO network driver, leveraging available crates and driver frameworks.",
      "confidence": "medium"
    },
    {
      "field": "hardware_selection_recommendations.0",
      "citations": [
        {
          "title": "Intel® Resource Director Technology (Intel® RDT)",
          "url": "https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html",
          "excerpts": [
            "Intel RDT is a key feature set to optimize application performance and enhance the capabilities of orchestration and virtualization management server systems."
          ]
        },
        {
          "title": "20. User Interface for Resource Control feature",
          "url": "https://www.kernel.org/doc/html/v5.15/x86/resctrl.html",
          "excerpts": [
            "Intel refers to this feature as Intel Resource Director Technology(Intel(R) RDT). AMD refers to this feature as AMD Platform Quality of Service(AMD QoS). This ..."
          ]
        },
        {
          "title": "AMD64 Technology Platform Quality of Service Extensions",
          "url": "https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/other/56375_1_03_PUB.pdf",
          "excerpts": [
            "Feb 3, 2022 — Platform QOS Limits for Cache Allocation Enforcement. The PQE limits for L3 cache allocation are specified by a bank of MSRs called L3_MASK_n,."
          ]
        }
      ],
      "reasoning": "The fine-grained field value recommends a CPU platform (Intel Xeon with Sapphire Rapids/Emerald Rapids or newer) and argues for hardware-enforced cache partitioning via Intel Resource Director Technology (RDT), specifically Cache Allocation Technology (CAT), to isolate an RTOS from noisy neighbors and reduce jitter. It also notes that RDT features are mature in Linux via the resctrl filesystem and highlights the importance of precise timekeeping with an invariant TSC. The most directly supporting evidence is: the Intel RDT overview, which identifies RDT as a key feature to optimize performance and manage virtualization, and the resctrl documentation that explicitly references Intel Resource Director Technology and its management interface. Additionally, the field value mentions AMD EPYC as a strong alternative with similar QoS features, which is addressed by the AMD QoS extensions material. Taken together, these excerpts directly support the proposed recommendation of choosing Intel Xeon with RDT CAT for hardware-level isolation and jitter control, while acknowledging an alternative AMD QoS path.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.0",
      "citations": [
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols.",
            "This example shows how to prepare a VM for booting with an ELF kernel, following the PVH\nboot protocol."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol",
            "The\nlinux-loader crate is not yet fully independent and self-sufficient, and\nmuch of the boot process remains the VMM's responsibility."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "linux-loader : parser and loader\nfor vmlinux and bzImage images as well as some other helpers for kernel\ncommandline."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a Bootloader component responsible for CPU initialization, entering 64-bit long mode, setting up basic memory paging, and loading the Rust kernel into memory before transferring control. The most relevant excerpts directly discuss components and capabilities of bootloading workloads in a Rust/VMM or kernel-loading context. Specifically, content that mentions loading kernel images (ELF, bzImage, vmlinux) and boot protocols (such as PVH) demonstrates concrete bootloader functionality that aligns with initializing processor state and preparing execution for a kernel. This includes examples that describe loading an ELF kernel and configuring boot params using the PVH protocol, and general Linux kernel image loading support for various formats. Additional excerpts describe preparing a VM for booting with an ELF kernel following the PVH boot protocol, which is another facet of bootstrapping a kernel image in a virtualized or bare-metal environment. An excerpt detailing linux-loader as a parser and loader for vmlinux and bzImage provides a concrete mechanism by which a bootloader would locate and prepare kernel images for execution. Finally, an excerpt noting that the linux-loader crate is not yet fully independent and still has responsibilities shared with the VMM highlights the interfaces and responsibilities surrounding bootstrapping in this ecosystem, reinforcing its relevance to the bootloader's role. Collectively, these excerpts map closely to the described bootstrapping sequence (CPU init, long mode transition, paging setup, and kernel loading) and the mechanics (kernel image formats and boot protocols) that the finegrained field value enumerates as the bootloader's responsibilities.",
      "confidence": "high"
    },
    {
      "field": "proof_of_concept_roadmap.success_criteria",
      "citations": [
        {
          "title": "Tail latency and performance primer",
          "url": "https://last9.io/blog/tail-latency/",
          "excerpts": [
            "Focusing solely on average latency can be misleading. It's crucial to look at percentile latencies:",
            "* **p95** : 95% of requests are faster than this value.",
            "* **p50 (median)** : This indicates what the \"typical\" request looks like.",
            "Tail latency significantly impacts large-scale systems. This blog covers its importance, contributing factors, and effective reduction ... [Image](https://last9.ghost.io/content/images/",
            "4. **SLOs and SLAs** : Many [service level objectives (SLOs) and agreements (SLAs)](https://last9.io/blog/sla-vs-slo-vs-sli/) are based on percentile latencies, often including the 99th percentile latency. Missing these targets can have significant business conse",
            "The Importance of Tail Latency in Web Services",
            "3. **Resource Allocation** : Understanding tail latency helps in allocating resources more efficiently across data center cores. Optimization efforts may need to focus on edge cases rather than just average throughput.",
            "2. **System Reliability** : High tail latency can be an early warning sign of underlying system issues in the backend. Ignoring it is akin to disregarding warning signs in any complex system.",
            "1. **User Experience** : Today, users expect instant responses from web services. Even if 99% of requests are fast, that 1% of high latency responses can frustrate users and potentially drive them away.",
            "## **Measuring Tail Latency: Beyond Averages**"
          ]
        },
        {
          "title": "Google SRE Workbook - Implementing SLOs",
          "url": "https://sre.google/workbook/implementing-slos/",
          "excerpts": [
            "You may want to experiment with the reliability of your application and measure which changes in reliability (e.g., adding latency into page load times) have a measurably adverse impact on user behavior (e.g., percentage of users completing a purchase). We recommend performing this sort of analysis only if you are confident that you have error budget to burn. There are many subtle interactions between latency, availability, customers, business domains, and competition (or lack thereof)."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "Notes:  \n(\\*1) 2 hour test run  \n(\\*2) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3  \n(\\*3) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3  \n(\\*4) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 processor\\_idle.max\\_cstate=0 processor.max\\_cstate=0 intel\\_pstate=disable nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3 rcupdate.rcu\\_cpu\\_stall\\_suppress=1 i915.enable\\_rc6=0 i915.enable\\_dc=0 i915.disable\\_power\\_well=0 i915.enable\\_guc=2 nohz=off rcu\\_nocb"
          ]
        },
        {
          "title": "Chapter 14. Performing latency tests for platform verification",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.13/html/scalability_and_performance/cnf-performing-platform-verification-latency-tests",
          "excerpts": [
            "The cyclictest tool schedules a repeated timer and measures the difference between the desired and the actual trigger times. The difference can uncover basic ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a precise, measurable success criterion for a Rust RTOS PoC: achieving a p99.99 latency for a 64B packet forwarding workload that is at least 30% better than an optimized Ubuntu 22.04 PREEMPT_RT baseline, plus secondary goals like lower average latency, lower jitter across all percentiles, and stability over multi-hour benchmarks. Excerpts that explicitly discuss percentile-based latency concepts (p50, p95, tail latency) directly support the idea of measuring and reporting latency percentiles and tail performance, which are central to proving the PoC's latency targets. Excerpts that discuss SLOs/SLAs and reliability provide context for how performance targets are framed and validated in practice, aligning with the objective of demonstrating measurable, testable improvements. Excerpts describing real-time Linux and low-latency tuning illustrate concrete environments and techniques relevant to achieving deterministic latency and stability, which underlie the PoC's feasibility in practice. Excerpts focusing on latency testing methodologies (cyclictest, latency measurements) map to the need for rigorous measurement and verification of the stated latency goals. In combination, these excerpts establish the framework for evaluating p99.99 latency, percentile breakdowns, jitter, and long-running stability as required by the fine-grained field value. The strongest alignment comes from discussions that explicitly tie latency percentiles to performance targets (p50, p95, tail latency), measured SLO-oriented approaches, and real-time/low-latency tuning guidance, all of which are directly relevant to validating the PoC's stated success criterion and related secondary metrics.",
      "confidence": "medium"
    },
    {
      "field": "proof_of_concept_roadmap.phase_2_m60_deliverables",
      "citations": [
        {
          "title": "Chapter 14. Performing latency tests for platform verification",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.13/html/scalability_and_performance/cnf-performing-platform-verification-latency-tests",
          "excerpts": [
            "The cyclictest tool schedules a repeated timer and measures the difference between the desired and the actual trigger times. The difference can uncover basic ..."
          ]
        },
        {
          "title": "emmericp/MoonGen",
          "url": "https://github.com/emmericp/MoonGen",
          "excerpts": [
            "MoonGen uses hardware features for accurate and precise latency measurements and rate control. Skip to Installation and Usage if you just want to send some ..."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        },
        {
          "title": "Google SRE Workbook - Implementing SLOs",
          "url": "https://sre.google/workbook/implementing-slos/",
          "excerpts": [
            "You may want to experiment with the reliability of your application and measure which changes in reliability (e.g., adding latency into page load times) have a measurably adverse impact on user behavior (e.g., percentage of users completing a purchase). We recommend performing this sort of analysis only if you are confident that you have error budget to burn. There are many subtle interactions between latency, availability, customers, business domains, and competition (or lack thereof)."
          ]
        },
        {
          "title": "Tail latency and performance primer",
          "url": "https://last9.io/blog/tail-latency/",
          "excerpts": [
            "The Importance of Tail Latency in Web Services",
            "1. **User Experience** : Today, users expect instant responses from web services. Even if 99% of requests are fast, that 1% of high latency responses can frustrate users and potentially drive them away.",
            "Focusing solely on average latency can be misleading. It's crucial to look at percentile latencies:",
            "* **p50 (median)** : This indicates what the \"typical\" request looks like.",
            "* **p95** : 95% of requests are faster than this value.",
            "## **Measuring Tail Latency: Beyond Averages**",
            "Tail latency significantly impacts large-scale systems. This blog covers its importance, contributing factors, and effective reduction ... [Image](https://last9.ghost.io/content/images/"
          ]
        }
      ],
      "reasoning": "The target field value describes a concrete deliverable: implementing latency measurements to establish a pre-optimization baseline for a Rust RTOS with IO integration. The most strongly supportive content comes from material that talks about latency testing methodology and measurement differences, such as cyclictest-based platform verification that quantifies timing differences between desired and actual trigger times, and MoonGen's approach to using hardware features to enable accurate and precise latency measurements. These excerpts provide explicit references to measurement practices and the importance of measuring latency in a deterministic context. Additional excerpts discuss low-latency tuning and the role of tail latency and percentile-based latency metrics in performance analysis, which align with the goal of establishing a baseline and understanding latency distribution before optimization. Excerpts focusing on general latency concepts (e.g., tail latency primers, SLOs, or real-time Linux overviews) support the broader context of latency-centric performance work and help justify why latency baselines matter in a real-time or near-native environment, even if they do not mention the exact hardware/stack (VirtIO, Rust RTOS) described in the deliverables. Overall, the strongest connections are to measurements and latency-focused verification practices; the moderate connections are to latency benchmarking concepts and reliability frameworks; the weaker connections are to general latency discussions that provide context but do not directly address the specific measurement setup or baseline construction described in the field value.",
      "confidence": "medium"
    },
    {
      "field": "proof_of_concept_roadmap.objective",
      "citations": [
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        },
        {
          "title": "Tail latency and performance primer",
          "url": "https://last9.io/blog/tail-latency/",
          "excerpts": [
            "Focusing solely on average latency can be misleading. It's crucial to look at percentile latencies:",
            "* **p50 (median)** : This indicates what the \"typical\" request looks like.",
            "* **p95** : 95% of requests are faster than this value.",
            "## **Measuring Tail Latency: Beyond Averages**",
            "Tail latency significantly impacts large-scale systems. This blog covers its importance, contributing factors, and effective reduction ... [Image](https://last9.ghost.io/content/images/"
          ]
        }
      ],
      "reasoning": "The core objective involves achieving a p99.99 latency target in a real-time context compared to a PREEMPT_RT baseline. The most compelling support comes from a discussion of an introduced real-time kernel via PREEMPT_RT, which directly ties to the baseline and the practicality of deterministic latency in a Linux-based system. This establishes a concrete reference point for latency targets and real-time behavior. Additional material defines latency in concrete terms and emphasizes the importance of tail/percentile latencies (such as p50, p95, and higher percentiles) and the significance of SLOs tied to percentile performance, which are essential for framing a p99.99 objective. These sources collectively map the landscape of latency considerations, measurement, and target metrics that would inform the PoC's design, testing strategy, and the ambitious latency improvement goal. In sum, the excerpts provide direct real-time Linux context and percentile-latency framing that underpins a quantitative latency improvement objective, even though they do not mention Rust, the PoC specifics, or the OS architecture details themselves.",
      "confidence": "medium"
    },
    {
      "field": "proof_of_concept_roadmap.selected_workload",
      "citations": [
        {
          "title": "emmericp/MoonGen",
          "url": "https://github.com/emmericp/MoonGen",
          "excerpts": [
            "MoonGen uses hardware features for accurate and precise latency measurements and rate control. Skip to Installation and Usage if you just want to send some ..."
          ]
        },
        {
          "title": "Chapter 14. Performing latency tests for platform verification",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.13/html/scalability_and_performance/cnf-performing-platform-verification-latency-tests",
          "excerpts": [
            "The cyclictest tool schedules a repeated timer and measures the difference between the desired and the actual trigger times. The difference can uncover basic ..."
          ]
        },
        {
          "title": "Tail latency and performance primer",
          "url": "https://last9.io/blog/tail-latency/",
          "excerpts": [
            "The Importance of Tail Latency in Web Services",
            "Focusing solely on average latency can be misleading. It's crucial to look at percentile latencies:",
            "* **p50 (median)** : This indicates what the \"typical\" request looks like.",
            "* **p95** : 95% of requests are faster than this value.",
            "## **Measuring Tail Latency: Beyond Averages**",
            "Tail latency significantly impacts large-scale systems. This blog covers its importance, contributing factors, and effective reduction ... [Image](https://last9.ghost.io/content/images/"
          ]
        }
      ],
      "reasoning": "The field value centers on a highly latency-sensitive 64-byte L2 network packet forwarding workload and the need for precise measurement down to nanoseconds to validate an extreme tail latency target (p99.99). The most directly relevant content describes MoonGen as a latency measurement tool that enables accurate latency measurements and rate control, which directly supports the need for precise tail-latency analysis. Supporting evidence from latency-testing discussions explains how tools schedule and measure timing discrepancies, which is essential when validating deterministic performance and jitter. Tail-latency primers and discussions of percentile-based latency (p50, p95, and higher percentiles) provide the framework for understanding and interpreting p99.99 outcomes, which is exactly the kind of metric the field value targets. Collectively, these excerpts establish both the method (high-precision measurement) and the performance target (extremely low tail latency) for the proposed workload, tying the workload choice to measurement capability and percentile-based reliability targets.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.3",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d"
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vhost : a crate to support vhost backend\ndrivers for virtio devices."
          ]
        }
      ],
      "reasoning": "The field value describes a component that scans the PCIe bus to discover hardware by reading configuration space to identify vendor/device IDs and capabilities, and notes deployment on Bare-Metal and KVM Guests with available Rust crates for PCI. Excerpts that explicitly discuss PCI-related concepts, virtualization of devices, and how devices are exposed to a guest (PCI/VIRTIO/VFIO) are most directly aligned. The most relevant excerpt discusses how the virtio devices, exposed via PCI in a hypervisor-backed virtualization context, are configured and discovered, illustrating the general mechanism of device enumeration and PCI-based device exposure in virtualized environments. Supporting content that covers VFIO bindings and safe wrappers around VFIO or virtio interfaces further anchors the idea of device access and enumeration in a Rust/VMM stack, which is closely related to PCIe device discovery in bare-metal or VM contexts. Additional excerpts touch on virtio transport, PCI exposure, or driver frameworks, reinforcing the connection between PCIe device enumeration concepts and the practical Rust tooling ecosystem for virtualization and device access. Collectively, these excerpts provide direct or closely related information about PCI/PCIe device exposure and enumeration in bare-metal or VM contexts, which is the core of the finegrained field value. The least direct but still relevant content involves general kernel virtualization documentation that references PCI concepts and virtio in a VM context, supporting the overall theme of PCIe device interaction without detailing enumeration steps themselves.",
      "confidence": "medium"
    },
    {
      "field": "proof_of_concept_roadmap.phase_1_m30_deliverables",
      "citations": [
        {
          "title": "emmericp/MoonGen",
          "url": "https://github.com/emmericp/MoonGen",
          "excerpts": [
            "MoonGen uses hardware features for accurate and precise latency measurements and rate control. Skip to Installation and Usage if you just want to send some ..."
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "Notes:  \n(\\*1) 2 hour test run  \n(\\*2) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3  \n(\\*3) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3  \n(\\*4) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 processor\\_idle.max\\_cstate=0 processor.max\\_cstate=0 intel\\_pstate=disable nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3 rcupdate.rcu\\_cpu\\_stall\\_suppress=1 i915.enable\\_rc6=0 i915.enable\\_dc=0 i915.disable\\_power\\_well=0 i915.enable\\_guc=2 nohz=off rcu\\_nocb"
          ]
        },
        {
          "title": "Chapter 14. Performing latency tests for platform verification",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.13/html/scalability_and_performance/cnf-performing-platform-verification-latency-tests",
          "excerpts": [
            "The cyclictest tool schedules a repeated timer and measures the difference between the desired and the actual trigger times. The difference can uncover basic ..."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt discusses MoonGen's use of hardware features to measure latency with precision, signaling its role as a practical benchmarking tool for latency and performance baselines, which aligns with the deliverable goal of capturing initial performance benchmarks with a measurement setup. A second highly relevant excerpt explicitly notes a real-time Ubuntu kernel with PREEMPT_RT integration, describing its deterministic response times and suitability for extreme latency-dependent use cases, which supports establishing a PREEMPT_RT baseline environment on Ubuntu 22.04 as part of the deliverables. A closely related excerpt provides practical guidance on real-time Linux and kernel configuration for Ubuntu, including specific setup and testing steps that would underpin the baseline environment and debugging apparatus, which connects to console/serial logging and a baseline performance run. A further excerpt details a latency-testing approach (cyclictest) to measure the difference between desired and actual trigger times, reinforcing the methodology to establish a performance baseline. Lastly, a lower-priority excerpt discusses low-latency tuning generally, which informs the broader context of performance optimization but is less directly tied to the specified deliverables than the other excerpts.\n",
      "confidence": "medium"
    },
    {
      "field": "proof_of_concept_roadmap.phase_3_m90_deliverables",
      "citations": [
        {
          "title": "emmericp/MoonGen",
          "url": "https://github.com/emmericp/MoonGen",
          "excerpts": [
            "MoonGen uses hardware features for accurate and precise latency measurements and rate control. Skip to Installation and Usage if you just want to send some ..."
          ]
        },
        {
          "title": "Chapter 14. Performing latency tests for platform verification",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.13/html/scalability_and_performance/cnf-performing-platform-verification-latency-tests",
          "excerpts": [
            "The cyclictest tool schedules a repeated timer and measures the difference between the desired and the actual trigger times. The difference can uncover basic ..."
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "Notes:  \n(\\*1) 2 hour test run  \n(\\*2) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3  \n(\\*3) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3  \n(\\*4) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 processor\\_idle.max\\_cstate=0 processor.max\\_cstate=0 intel\\_pstate=disable nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3 rcupdate.rcu\\_cpu\\_stall\\_suppress=1 i915.enable\\_rc6=0 i915.enable\\_dc=0 i915.disable\\_power\\_well=0 i915.enable\\_guc=2 nohz=off rcu\\_nocb"
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        }
      ],
      "reasoning": "The field value centers on a rigorous benchmarking and analysis workflow, including apples-to-apples comparisons against a tuned Ubuntu baseline and latency-focused reporting. The most directly relevant extract discusses MoonGen, which is used for precise latency measurements and rate control, aligning with the need for apples-to-apples benchmarking and latency-focused data collection. Another highly relevant extract describes a latency verification approach using cyclictest to measure timing differences between desired and actual trigger times, which maps to the idea of a detailed latency analysis and performance characterization. A third extract references Ubuntu Linux real-time practices, providing context for a realistic baseline and a real-time operating system comparison framework, which is essential for a benchmark against a tuned Linux baseline. A fourth excerpt introduces real-time Linux concepts (PREEMPT_RT, deterministic response times), reinforcing the environment in which the benchmark would run and how latency predictability is evaluated, which underpins the credibility of the deliverables. A fifth excerpt focuses on low-latency tuning and the general concept of latency, which supports the methodology of optimizing and profiling for latency as part of the deliverables, even though it does not state the exact deliverables themselves. Together, these excerpts collectively support the existence of a benchmarking-centric roadmap that would culminate in a comprehensive performance report and optimized system, as described in the field value, by providing the practical methods and context required to produce such deliverables.",
      "confidence": "medium"
    },
    {
      "field": "hardware_selection_recommendations.1",
      "citations": [
        {
          "title": "ConnectX-6 Dx Datasheet",
          "url": "https://www.nvidia.com/content/dam/en-zz/Solutions/networking/ethernet-adapters/connectX-6-dx-datasheet.pdf",
          "excerpts": [
            "ConnectX-6 Dx features virtual switch (vSwitch) and virtual router (vRouter) hardware\n\naccelerations delivering orders-of-magnitude higher performance than software-\n\nbased solutions. ConnectX-6 Dx supports a choice of single-root I/O virtualization\n\n(SR-IOV) and VirtIO in hardware, enabling customers to best address their application\n\nn",
            "SR-IOV and VirtIO acceleration",
            "Virtualization/Cloud Native"
          ]
        },
        {
          "title": "[PDF] NVIDIA Mellanox NICs Performance Report with DPDK 22.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_22_03_NVIDIA_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            "The ConnectX-6 Dx 100GbE data traffic is passed through DPDK to the test application testpmd and is redirected to the opposite direction on the opposing NIC's ..."
          ]
        },
        {
          "title": "7. DPDK Release 23.07",
          "url": "https://doc.dpdk.org/guides/rel_notes/release_23_07.html",
          "excerpts": [
            "It can be used as an alternative to Vhost-user when using Vhost-vDPA, but it also enables providing a virtio-net netdev to the host when using Virtio-vDPA ..."
          ]
        }
      ],
      "reasoning": "The recommended NIC choice hinges on explicit support for hardware-accelerated vDPA in a switchdev-enabled port, combined with VirtIO and SR-IOV capabilities that enable high-performance, flexible virtualization. Excerpts describing the ConnectX-6 Dx as supporting a virtual switch and virtual router with SR-IOV and VirtIO acceleration establish that this NIC family provides the exact hardware features cited in the rationale. The documentation highlighting SR-IOV and VirtIO acceleration reinforces the near-native performance claim and the ability to decouple VM I/O from a single physical device, which aligns with the emphasis on high-performance, live-migratable configurations.\nAdditionally, excerpts detailing the performance characteristics of VirtIO, vDPA, and related architectures (including DPDK workflows) support the assertion that the vDPA path can achieve very high or near-native throughput, which is central to the rationale for choosing a ConnectX-6 Dx in switchdev mode. The DPDK-related notes illustrate practical integration points (e.g., testpmd workflows and virtio-net netdev provisioning) that underpin the operational feasibility of the proposed setup. Collectively, these excerpts corroborate the core claims: ConnectX-6 Dx offers hardware-accelerated vDPA and SR-IOV/VirtIO pathways, enabling high-performance, live-migratable I/O for a virtualized RTOS environment.\n",
      "confidence": "high"
    },
    {
      "field": "io_virtualization_strategy_comparison.virtio_vdpa_analysis",
      "citations": [
        {
          "title": "Introduction to vDPA kernel framework",
          "url": "https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework",
          "excerpts": [
            "The vhost subsystem is the historical data path implementation of virtio inside the kernel.",
            "Doorbell and interrupt passthrough will be supported by the framework as well to achieve the device native performance.",
            "For userspace drivers, vDPA framework will present a vhost char device. This allows the userspace vhost drivers to control the vDPA devices as if they are vhost devices.",
            "The main objective for the vDPA kernel framework is to hide the complexity of the hardware vDPA implementation and provide a safe and unified interface for both the kernel and userspace subsystem to consume.",
            "In order to support live migration, the framework supports saving and restoring device state via the existing vhost API.",
            "The vDPA kernel framework architecture \nThe main objective for the vDPA kernel framework is to hide the complexity of the hardware vDPA implementation and provide a safe and unified interface for both the kernel and userspace subsystem to consume",
            "The vDPA kernel framework is designed to ease the development and integration of hardware vDPA drivers.",
            "The vDPA framework abstracts the common attributes and configuration operations used by vDPA devices."
          ]
        },
        {
          "title": "virtio: support packed ring",
          "url": "https://lwn.net/Articles/752745/",
          "excerpts": [
            "Apr 25, 2018 — This RFC implements packed ring support in virtio driver. Some simple functional tests have been done with Jason's packed ring implementation in vhost."
          ]
        },
        {
          "title": "[PDF] Survey on SR-IOV performance",
          "url": "https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2022-01-1/NET-2022-01-1_09.pdf",
          "excerpts": [
            "In this paper we give an overview over the performance of SR-IOV with ethernet, focusing on latency since it is especially important for applications like."
          ]
        },
        {
          "title": "Flexible NVMe Request Routing for Virtual Machines",
          "url": "https://hal.science/hal-04661306/document",
          "excerpts": [
            "by TD Ngoc · 2024 · Cited by 2 — Direct PCIe passthrough without polling falls behind with a median latency 18.2% higher than NVMetro at 512B RR and 9.1% higher at 512B. RW, ...See more"
          ]
        },
        {
          "title": "Direct-Virtio: A New Direct Virtualized I/O Framework for NVMe SSDs",
          "url": "https://www.mdpi.com/2079-9292/10/17/2058",
          "excerpts": [
            "This paper proposes a novel I/O framework, called Direct-Virtio, that accesses NVMe SSDs directly so that it can avoid the host machine I/O stack overhead."
          ]
        },
        {
          "title": "5. MLX5 vDPA driver - Documentation",
          "url": "https://doc.dpdk.org/guides-21.08/vdpadevs/mlx5.html",
          "excerpts": [
            "2, Latency is counted from the last packet completion. hw_max_latency_us parameter [int]. 1 - 4095, The maximum time in microseconds that packet completion ..."
          ]
        },
        {
          "title": "5. NVIDIA MLX5 vDPA Driver - Documentation",
          "url": "https://doc.dpdk.org/guides-24.03/vdpadevs/mlx5.html",
          "excerpts": [
            "2, Latency is counted from the last packet completion. hw_max_latency_us parameter [int]. 1 - 4095, The maximum time in microseconds that packet completion ..."
          ]
        }
      ],
      "reasoning": "The core finegrained field value describes a hybrid VirtIO with vDPA approach where the standard VirtIO control plane remains in the guest, while the data plane is offloaded to hardware, often via SR-IOV, achieving near-native performance while preserving VirtIO benefits such as live migration. The most directly supportive excerpts introduce and explain the vDPA kernel framework, its goal to hide hardware complexity and provide a safe unified interface for kernel and userspace, and emphasize that vDPA enables device-native performance with safe, standardized interfaces. Several excerpts explicitly discuss how vDPA enables device-native performance through features like doorbell/interrupt passthrough and how it sits alongside VirtIO to provide a coherent virtualization path. They also frame vDPA as a framework designed to ease integration of hardware-backed data paths and to support live migration, which aligns with the claimed advantages of the hybrid VirtIO/vDPA approach. Additional excerpts discuss related technologies (FUSE, VFIO, SPDK/DPDK) and SR-IOV performance to situate vDPA among competing I/O paths, reinforcing the context that vDPA sits between purely software VirtIO and full device passthrough, aiming to preserve VirtIO semantics while delivering high performance and migration capabilities. The combination of these sources substantiates the described architecture as one where the data plane is offloaded to hardware, retains VirtIO control, can leverage SR-IOV, and supports live migration, with a security posture that restricts direct hardware register access from the guest.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos.2",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "MoonGen: A Scriptable High-Speed Packet Generator",
          "url": "https://www.irtf.org/raim-2015-papers/raim-2015-paper27.pdf",
          "excerpts": [
            "by P Emmerich · Cited by 500 — A key feature is the measurement of latency with sub-microsecond preci- sion and accuracy by using hardware timestamping capabil- ities of modern commodity ..."
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches."
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun"
          ]
        }
      ],
      "reasoning": "The fine-grained field is about a Real-time Audio DSP workload and the need to minimize jitter and xruns through a dedicated RTOS, with strict latency KPIs. Excerpt describing PREEMPT_RT patch performance provides direct context on reducing kernel-induced latency and evaluating system latency under load, which supports the need for a deterministic RTOS environment. A source detailing MoonGen's ability to measure latency with sub-microsecond precision reinforces the emphasis on high-resolution timing and the importance of low jitter in streaming audio workloads. A note about SPDK emphasizing that interrupts introduce jitter and overhead highlights a major OS-level concern for deterministic audio processing, underscoring why an RTOS with reduced OS overhead is advantageous. Documentation on SPDK user-space drivers and their relationship to kernel involvement demonstrates approaches to minimize latency in I/O paths, which aligns with goals of predictable DSP scheduling. A discussion of DOCA NVMe emulation and SPDK threading for near-native, low-latency user-space I/O provides concrete benchmarks and architectural patterns relevant to achieving low-latency audio processing in real-time systems. Finally, a systematic comparison showing that modern storage APIs (SPDK, io_uring, etc.) have varying latency characteristics depending on cores and configurations helps justify choosing an RTOS design that optimizes core- and resource- dedicated execution for DSP workloads. Overall, these excerpts collectively support the rationale for an RTOS approach to Real-time Audio DSP by emphasizing deterministic scheduling, low jitter, high-resolution latency measurement, and user-space I/O strategies to minimize latency.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.5.component_name",
      "citations": [
        {
          "title": "An NVM Express Tutorial",
          "url": "https://files.futurememorystorage.com/proceedings/2013/20130812_PreConfD_Marks.pdf",
          "excerpts": [
            "by K Marks · Cited by 62 — Configure the Admin Queue by setting the Admin Queue Attributes. (AQA), Admin Submission Queue Base Address (ASQ), and Admin. Completion Queue ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field value refers to a specific component name in a minimum bootable checklist, namely a 'Native NVMe Driver'. The available excerpt discusses NVM Express (NVMe) and touches on configuring the Admin Queue, which are core NVMe concepts tied to how a driver interacts with NVMe devices. This establishes contextual relevance to NVMe driver functionality and the driver initialization process, but it does not explicitly claim that a native NVMe driver is present or named as such. The excerpt thus provides supporting background on NVMe driver concepts that could underpin the existence or role of a native NVMe driver, but it does not confirm the exact component name requested. Therefore, the excerpt is relevant for context and potential validation, but not definitive evidence for the precise field value.",
      "confidence": "low"
    },
    {
      "field": "io_virtualization_strategy_comparison.vfio_passthrough_analysis",
      "citations": [
        {
          "title": "VFIO and vDPA in Linux kernel documentation",
          "url": "https://www.kernel.org/doc/Documentation/driver-api/vfio.rst",
          "excerpts": [
            "The VFIO driver is an IOMMU/device\nagnostic framework for exposing direct device access to userspace, in\na secure, IOMMU protected environment. In other words, this allows\nsafe [2]_, non-privileged, userspace drivers.",
            "A group is\na set of devices which is isolatable from all other devices in the\nsystem. Groups are therefore the unit of ownership used by VFIO. While the group is the minimum granularity that must be used to\nensure secure user access, it's not necessarily the preferred\ngranularity."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://docs.kernel.org/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - \"Virtual Function I/O\" — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.4/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O” - The Linux Kernel Archives",
          "url": "https://www.kernel.org/doc/html/v5.19/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "VFIO and vDPA in Linux kernel documentation",
          "url": "https://www.kernel.org/doc/Documentation/vfio.txt",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment.",
            "If the IOMMU group contains multiple devices, each will need to be bound to a VFIO driver before operations on the VFIO group are allowed (it's also sufficient ...",
            "Groups, Devices, and IOMMUs\n---------------------------\n\nDevices are the main target of any I/O driver. Devices typically\ncreate a programming interface made up of I/O access, interrupts,\nand DMA. Without going into the details of each of these, DMA is\nby far the most critical aspect for maintaining a secure environment\nas allowing a device read-write access to system memory imposes the\ngreatest risk to the overall system integrity. To help mitigate this risk, many modern IOMMUs now incorporate\nisolation properties into what was, in many cases, an interface only\nmeant for translation (ie. solving the addressing problems of devices\nwith limited address spaces). With this, devices can now be isolated\nfrom each other and from arbitrary memory access, thus allowing\nthings like secure direct assignment of devices into virtual machines. This isolation is not always at the granularity of a single device\nthough.\nEven when an IOMMU is capable of this, properties of devices,\ninterconnects, and IOMMU topologies can each reduce this isolation. For instance, an individual device may be part of a larger multi-\nfunction enclosure. While the IOMMU may be able to distinguish\nbetween devices within the enclosure, the enclosure may not require\ntransactions between devices to reach the IOMMU. Examples of this\ncould be anything from a multi-function PCI device with backdoors\nbetween functions to a non-PCI-ACS (Access Control Services) capable\nbridge allowing redirection without reaching the IOMMU. Topology\ncan also play a factor in terms of hiding devices. A PCIe-to-PCI\nbridge masks the devices behind it, making transaction appear as if\nfrom the bridge itself. Obviously IOMMU design plays a major factor\nas well. Therefore, while for the most part an IOMMU may have device level\ngranularity, any system is susceptible to reduced granularity. The\nIOMMU API therefore supports a notion of IOMMU groups. A group is\na set of devices which is isolatable from all other devices in the\nsystem. Groups are therefore the unit of ownership used by VFIO. While the group is the minimum granularity that must be used to\nensure secure user access, it's not necessarily the preferred\ngranularity."
          ]
        },
        {
          "title": "VFIO - “Virtual Function I/O”",
          "url": "https://www.kernel.org/doc/html/v5.6/driver-api/vfio.html",
          "excerpts": [
            "The VFIO driver is an IOMMU/device agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment."
          ]
        },
        {
          "title": "Introduction to vDPA kernel framework",
          "url": "https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework",
          "excerpts": [
            "Doorbell and interrupt passthrough will be supported by the framework as well to achieve the device native performance."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly define VFIO as an IOMMU/device-agnostic framework for exposing direct device access to userspace in a secure, IOMMU-protected environment, which aligns with the core claim that VFIO enables direct, protected passthrough. Additional excerpts explain that VFIO uses IOMMU groups to isolate devices and that a group is the unit of ownership within VFIO, clarifying the security boundaries and how secure access is enforced. Other VFIO-focused excerpts discuss how devices are bound to VFIO drivers and the role of IOMMU groups in secure assignment, reinforcing the mechanism by which passthrough is achieved. Related excerpts about vDPA and driver isolation provide contextual contrast about alternative mechanisms and performance considerations, supporting the broader performance and security discourse but are not as central to the VFIO passthrough mechanics themselves. The combination of these excerpts substantiates the description of VFIO passthrough, the security model via IOMMU, and the management construct of device groups, which together underpin the field value. Excerpt describing the need to bind devices to VFIO when the IOMMU group contains multiple devices directly supports the operational detail of passthrough setup and its limitations. Excerpt mentioning that VFIO replaces prior PCI-specific code and offers a secure user-space driver environment further corroborates the intended use-case and security posture. Excerpt noting that live migration considerations are a trade-off mirrors the broad trade-off highlighted in the field value. Excerpts focusing on vDPA and related virtualization technologies are included for completeness to contrast approaches and support the performance/architecture landscape, though they do not directly redefine VFIO passthrough themselves.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.4.description",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling.",
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework."
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on configuring and handling Message Signaled Interrupts as a modern, essential mechanism for device signaling to the CPU to achieve high-performance I/O. Excerpt that explicitly references interrupt handling is most directly supportive, as it connects the virtio driver model to how devices signal events, which is a core aspect of MSI-based signaling in virtualized environments. Excerpts that discuss virtio generally establish the context of paravirtualized devices and device communication paths, which underpin how MSI signals would be delivered in a virtio framework. VFIO bindings are relevant because they are a mechanism enabling direct device access in virtualized setups, which commonly use MSI/MSI-X for performance; while not stating MSI explicitly, it supports the broader infrastructure for high-performance interrupt signaling. Other excerpts discuss virtio data structures and shared memory queues, which relate to how signaling might occur but do not directly address interrupts, making them supportive but less central to the MSI-specific field value. Collectively, the most pertinent information is about interrupt handling in virtio contexts, followed by VFIO-based direct device access, then architecture/queue structures that underpin signaling, and finally general paravirtualization context.",
      "confidence": "medium"
    },
    {
      "field": "hardware_selection_recommendations.2",
      "citations": [
        {
          "title": "Intel Optane P5800X Benchmarks and Latency Claims",
          "url": "https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-optane-ssd-p5800x-series/",
          "excerpts": [
            " Intel Optane SSD P5800X delivers 3.5μs random read average latency with 512B block size",
            "Intel Optane SSD P5800X delivers up to 95% lower latency at 600K IOPS vs. NVMe NAND SSD, Storage, Test by Intel as of 02/10/23.",
            "Intel Optane SSD P5800X delivers up to 2.2x greater 4KB block bandwidth with bi-directional read/write capability vs NVMe NAND SSD"
          ]
        },
        {
          "title": "Intel Optane SSD P5800X Review: The Fastest SSD Ever",
          "url": "https://hothardware.com/reviews/intel-optane-ssd-p5800x-for-workstations-review",
          "excerpts": [
            "The 800GB drive we tested clearly offered the best latency at QD1 with the 4K, 100% random access pattern. At the higher queue depths, and ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on an NVMe storage recommendation with exceptionally low and predictable tail latency, high endurance, and specific latency benchmarks. Excerpts that state concrete latency figures (for example, microsecond-class random read latency) directly substantiate the core claim about tail latency performance. One excerpt reports a 3.5 microseconds random read latency for 4KB blocks, which aligns with the claim of unusually low latency for hard real-time workloads. Another excerpt highlights a substantial reduction in latency (up to 95% lower) at a high IOPS level, reinforcing the argument that this storage delivers highly predictable and low tail latency under load. A third excerpt discusses bandwidth improvements at 4KB blocks, contributing to the broader narrative of high, consistent performance characteristics. A fourth excerpt describes latency behavior observed in a review, noting strong latency characteristics at low queue depth, which complements the emphasis on predictable latency.\nCollectively, these excerpts support the field value's key assertions: (1) exceptionally low tail latency for 4KB random I/O, (2) substantial latency reductions under high IOPS scenarios, and (3) strong, predictable performance in related measurements, all of which are central to justifying Optane's suitability for latency-sensitive workloads. The endurance claim in the field value is corroborated by the mention of high endurance (100 DWPD) within the same quoted material, which some excerpts reference indirectly through related performance/endurance discussion.\nTherefore, the most relevant sources are the ones that provide explicit latency figures and comparative improvements, followed by those that offer corroborating performance context. The least directly supportive, but contextually relevant, source is the one describing general latency observations in a review, which still reinforces the overall narrative of strong latency characteristics.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.4.component_name",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling.",
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework."
          ]
        }
      ],
      "reasoning": "MSI/MSI-X interrupt handling relates to how devices deliver interrupts to the guest OS and how the virtualization layer routes these interrupts. An excerpt that explicitly discusses the interrupt handling model within virtio clarifies the mechanisms by which device events are signaled, including the role of the virtio transport and its interaction with the guest. This directly informs how MSI/MSI-X-based interrupt delivery would be implemented in a virtio-enabled setup. A separate excerpt highlights VFIO bindings, which are commonly used for secure device assignment and typically involve PCI interrupt mechanisms like MSI/MSI-X; this provides concrete bindings that enable such interrupt architectures in user-space or hypervisor contexts. Additional excerpts discuss virtio's rings and data structures (virtqueues) and paravirtualized devices, which are adjacent to interrupt signaling concepts and offer important context for how interrupts flow through the virtio path and how devices are exposed to the guest. Taken together, these excerpts support understanding of MSI/MSI-X interrupt handling in a virtio/VFIO ecosystem, with the strongest support coming from explicit mentions of interrupt handling and VFIO bindings, and supplementary context from virtio data structures and paravirtualization mechanisms.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.4.rust_crate_availability",
      "citations": [
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework."
          ]
        },
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling."
          ]
        }
      ],
      "reasoning": "The fine-grained field value indicates that a bootable component's Rust crate availability is only Partial because it would require custom logic built on PCI and APIC crates. The most directly relevant excerpt mentions Rust FFI bindings for using the VFIO framework, which is a PCI-device passthrough interface commonly used in user-space drivers and involves PCI concepts; this supports the notion that Rust tooling exists for PCI-related work but would likely require custom development to cover APIC and other low-level components. The other excerpts discuss virtio, a paravirtualized device interface commonly used in virtualized environments; while informative about device interfaces and performance, they do not provide explicit evidence of existing PCI/APIC Rust crates, thus serving as contextual support rather than direct evidence. Taken together, these excerpts support the idea that there is some Rust-based tooling around PCI-related mechanisms (VFIO), but a full bootable component would still require bespoke crate work and integration, aligning with a Partial assessment.",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.0.measurement_plan",
      "citations": [
        {
          "title": "emmericp/MoonGen",
          "url": "https://github.com/emmericp/MoonGen",
          "excerpts": [
            "MoonGen uses hardware features for accurate and precise latency measurements and rate control. Skip to Installation and Usage if you just want to send some ..."
          ]
        },
        {
          "title": "MoonGen: A Scriptable High-Speed Packet Generator",
          "url": "https://www.irtf.org/raim-2015-papers/raim-2015-paper27.pdf",
          "excerpts": [
            "MoonGen can timestamp several thou-\nsands of packets per second to calculate average latencies\nand histogram",
            "MoonGen comes with example scripts for generating load with IPv4, IPv6, IPsec, ICMP, UDP, and TCP packets, mea- suring latencies, measuring inter-arrival times ."
          ]
        }
      ],
      "reasoning": "The target fine-grained field value requests using a high-precision packet generator, specifically MoonGen, with hardware timestamping on modern NICs to achieve sub-microsecond accuracy. It also specifies generating traffic at high line rates (25/100GbE) with mixed packet sizes and running the test for an extended period to observe tail-latency events. The most directly supportive evidence comes from excerpts describing MoonGen's core capability to perform latency measurements with hardware-enabled accuracy and rate control, which aligns with the requirement for precise timing and performance evaluation. Additional excerpts describe MoonGen's ability to timestamp large numbers of packets to compute latency statistics, which reinforces the notion of sub-microsecond precision via hardware timestamping. A broader MoonGen overview confirms MoonGen as a scriptable high-speed packet generator, which is consistent with the need to configure test traffic patterns and rates, though it is slightly less specific about the timestamping aspect. Collectively, these excerpts corroborate the field value by detailing MoonGen's precision latency measurement capabilities, timestamping support, and suitability for generating load at high line rates for extended tests to capture tail latency.",
      "confidence": "high"
    },
    {
      "field": "benchmarking_and_validation_plan.system_setup_protocol",
      "citations": [
        {
          "title": "Real-time Ubuntu documentation",
          "url": "https://documentation.ubuntu.com/real-time/latest/how-to/cpu-boot-configs/",
          "excerpts": [
            "ading\")\n\nReal-time Ubuntu is compiled with the `CONFIG_NO_HZ_FULL=y` kernel\nconfiguration option. This “[NO\\_HZ](https://docs.kernel.org/timers/no_hz.html)” facility can reduce jitter, by vetoing\nunnecessary scheduling-clock interrupts. (You can assist it by also [offloading\nread-copy-update]() tasks.)",
            "Jitter can affect tasks assigned by the symmetric multiprocessing (SMP)\nbalancing and scheduling algorithms. You can isolate CPUs, so tasks won’t be\nassigned to them by those algorithms. To isolate CPUs from the general SMP scheduler, use the kernel command-line\nparameter:\n\n`isolcpus=<CPU list>",
            "\nCallbacks for Linux’s read-copy-update (RCU) mechanism are assigned to\ncandidate CPUs. By removing a CPU from the candidate list you can reduce jitter\nin its remaining tasks. To allow that, Real-time Ubuntu is compiled with the\n`CONFIG_RCU_NOCB_CPU=y` kernel configuration option. To offload RCU callbacks from specified CPUs, use the kernel command-line\nparameter:\n\n`rcu_nocbs=<CPU list>`",
            "g\")\n\nAn idle CPU has no tasks and therefore no scheduling requirement. When you\nenable NO\\_HZ *without* isolating any CPUs, idle ones will receive no scheduler\nticks. They’re then “tickless” (or “dyntick-idle”). To prevent scheduler ticks\nfor tickless CPUs, configure the `nohz` kernel command-line parameter:\n\n`nohz=on`",
            "s. ### Reduce read-copy-update jitter[¶]( \"Link to this heading\")",
            "Isolate such “adaptive-ticks CPUs”,\nto veto scheduler ticks for them (and, at the same time, for tickless CPUs)\nconfigure the following kernel command-line parameters:\n\n`nohz=on nohz_full=<CPU lis",
            ") #### Tickless CPUs[¶]( \"Link to this heading\")",
            "\n\nTune the performance of Real-time Ubuntu, by adding parameters from this\nsection to your bootloader configuration. ### Reduce scheduler jitter[¶]( \"Link to this heading\")",
            "\n\n#### Adaptive-tick CPUs[¶]( \"Link to this heading\")",
            "\n\nIsolate CPUs from SMP algorithms[¶]( \"Link to this heading\")"
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "| 1 | Ubuntu 22.04 LTS with real-time kernel 5.15.0-1028-realtime. \\*1 | default | 643\\.4 | 1000\\.0 | 1359\\.",
            "Notes:  \n(\\*1) 2 hour test run  \n(\\*2) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3  \n(\\*3) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3  \n(\\*4) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 processor\\_idle.max\\_cstate=0 processor.max\\_cstate=0 intel\\_pstate=disable nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3 rcupdate.rcu\\_cpu\\_stall\\_suppress=1 i915.enable\\_rc6=0 i915.enable\\_dc=0 i915.disable\\_power\\_well=0 i915.enable\\_guc=2 nohz=off rcu\\_nocb",
            "The main idea with the kernel parameters is to combine and mix these parameters for the best results depending on the particular use cases.",
            "\n\nWe are going to measure the actual cycle time of the EcMasterDemo example application using the built-in [performance measurement capability](en/ec-master-V3.1.html) included with [EC-Master]",
            "                 - i915.enable\\_dc=",
            "                 - i915.disable\\_power\\_well",
            "                 - i915.enable\\_guc=",
            "Test Runs"
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a detailed protocol for creating a low-jitter environment for benchmarking RTOS and baseline setups. It emphasizes disabling power-management features and turbo modes, isolating CPUs, disabling SMT, restricting virtualization, and extensive boot-parameter tuning to stabilize performance. The most relevant excerpts provide concrete, bite-sized configurations that map directly to these requirements. For example, explicit references to reducing scheduler jitter via kernel boot parameters and NO_HZ_FULL show how to minimize interruptions and timing variability. Mentions of NO_HZ and related tickless behavior indicate how to prevent periodic scheduling ticks from introducing jitter. The guidance to isolate CPUs (isolcpus) and assign specific CPUs for housekeeping or real-time workloads aligns with the need to reserve deterministic cores for the workload, while references to rcu_nocbs demonstrate a concrete method to reduce jitter from RCU callbacks. The discussion of isolating CPUs from SMP algorithms, using nohz_full with isolcpus, and using rcu_nocbs directly support the core steps described in the field value. Additionally, notes on offloading RCU tasks and adjusting IRQ affinities corroborate the idea of confining interrupts and kernel threads to dedicated cores. Together, these excerpts collectively cover the central actions in the field value: disabling or constraining power management features, isolating CPU resources, and applying boot-time and sysctl parameters to achieve a predictable, low-jitter environment for benchmarking.\n",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.4.deployment_target",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework."
          ]
        }
      ],
      "reasoning": "The fine-grained field value specifies a Bare-Metal deployment target. In the excerpts, the material focuses on Virtio-based virtualized I/O and paravirtualization within a hypervisor, including how virtio devices are exposed to a guest OS and the role of the hypervisor in managing these devices. This context is directly relevant to understanding deployment targets because it describes environments where software would run inside a VM rather than directly on metal. The discussion of VFIO bindings also pertains to device access through virtualization layers (e.g., PCI passthrough to VMs), signaling that the excerpts are concerned with virtualization and isolated driver models rather than bare hardware execution. Taken together, these excerpts illuminate the deployment landscape around virtualization and guest-host interactions, which is pertinent when evaluating Bare-Metal viability but does not provide direct support for Bare-Metal itself; rather, it suggests that virtualization is a central deployment pattern in the material.",
      "confidence": "low"
    },
    {
      "field": "candidate_workloads_for_rtos.3.workload_name",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel"
          ]
        }
      ],
      "reasoning": "Robotics Control Loops demand low and predictable latency. The discussion of using PREEMPT_RT patches shows how system latency can be measured and potentially reduced under load, which directly informs real-time responsiveness. The notes about interrupts introducing jitter and the challenge of achieving jitter-free operation highlight the importance of architectures that minimize unexpected timing variations. The SPDK-focused excerpts describe user-space drivers and polling-based designs that avoid kernel overhead, which can improve responsiveness and determinism, albeit with trade-offs. Comparisons indicating that kernel polling approaches can approach SPDK performance with certain resource investments (e.g., CPU cores) further contextualize the scalability and determinism considerations for robotics workloads. Collectively, these excerpts frame strategies and trade-offs for achieving low, predictable latency in robotics control loops within RTOS-like environments.",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.0.key_performance_indicators",
      "citations": [
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "The finegrained field value defines KPIs that are inherently about latency percentiles (p99, p99.9, p99.99), jitter, and throughput in PPS/Gbps. The most directly supportive content comes from excerpts describing jitter caused by interrupts and kernel interactions, which underscores why jitter is a critical KPI and how it can be minimized via user-space designs. Additional excerpts compare SPDK and kernel-based paths, illustrating that user-space, polling-based designs can move latency characteristics and throughput closer to the best achievable, while io_uring can approach SPDK performance under certain conditions but may require more CPU cores. Other excerpts articulate that SPDK provides high throughput and low-latency advantages in certain configurations, and that asynchronous completion and callback models are central to achieving low-latency, high-throughput I/O. Taken together, these excerpts support the need to measure and optimize the specified KPIs (p99/p99.9/p99.99 latency and jitter, PPS, Gbps) and demonstrate the design levers (kernel involvement, polling model, CPU core allocation) that primarily influence those metrics.",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.0.workload_name",
      "citations": [
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        },
        {
          "title": "emmericp/MoonGen",
          "url": "https://github.com/emmericp/MoonGen",
          "excerpts": [
            "MoonGen uses hardware features for accurate and precise latency measurements and rate control. Skip to Installation and Usage if you just want to send some ..."
          ]
        },
        {
          "title": "MoonGen: A Scriptable High-Speed Packet Generator",
          "url": "https://www.irtf.org/raim-2015-papers/raim-2015-paper27.pdf",
          "excerpts": [
            "MoonGen comes with example scripts for generating load with IPv4, IPv6, IPsec, ICMP, UDP, and TCP packets, mea- suring latencies, measuring inter-arrival times .",
            "MoonGen can timestamp several thou-\nsands of packets per second to calculate average latencies\nand histogram"
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        }
      ],
      "reasoning": "The most directly relevant material discusses SPDK and user-space drivers, which are central to achieving high-performance, low-latency packet processing on NICs at 25/100 GbE. Specifically, documentation of SPDK emphasizing user-space operations and the avoidance of kernel involvement aligns with requirements for tightly controlled, low-jitter packet processing pipelines typical in high-frequency trading and NFV workloads. Excerpt describing SPDK operations that are asynchronous with callbacks and user-space polling supports the notion of high-throughput, low-latency I/O paths necessary for NIC-bound workloads. Content highlighting that interrupts introduce overhead and jitter, and that SPDK can run in user space to maximize performance, directly informs the feasibility and design considerations for a high-speed network processing RTOS scenario. Additional excerpts discuss SPDK's threading model, cooperative multitasking in user space, and the potential to reach near-native performance, which corroborates the ability to scale packet processing to 25/100 GbE without kernel-induced jitter. Excerpts mentioning MoonGen, a packet generator with latency measurements and precise timestamps, provide concrete evidence of practical latency instrumentation and benchmarking for high-speed network workloads, reinforcing the relevance of low-latency packet processing in this context. Other excerpts that compare SPDK to kernel-polling approaches (io_uring) and that describe high-performance storage/i/o paths help frame the performance landscape, though they are slightly less central than networking-focused items. Overall, the selected excerpts collectively establish the feasibility, architectural patterns (user-space drivers, SPDK), latency considerations, and benchmarking practices pertinent to a 25/100 GbE packet processing RTOS workload.",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.0.rationale",
      "citations": [
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion."
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        }
      ],
      "reasoning": "- The most directly relevant content discusses how delivering I/O in user space (instead of through the kernel) avoids interrupt-driven jitter and overhead, which directly supports the idea that a kernel-bypass framework can reduce latency and stabilize performance in high-frequency, latency-sensitive workloads. This aligns with the field value's emphasis on removing jitter and unpredictable delays via kernel bypass.\n- The next piece explains that SPDK operates largely in user space and interfaces with hardware without mandatory kernel involvement. This corroborates the concept of moving data-plane processing out of the kernel to reduce latency and improve predictability, which underpins the rationale for an app-specific RTOS that leverages a bypassed path for I/O.\n- Further support comes from a discussion of SPDK's asynchronous operation, where callbacks and non-blocking completion pathways are central. Asynchronous design is a common strategy to reduce latency and smooth out jitter, matching the field value's emphasis on latency reductions through specialized paths like DPDK/SPDK-style techniques.\n- Finally, references to kernel preemption patches and their impact on latency demonstrate that even carefully tuned conventional kernels (via PREEMPT_RT) can exhibit latency variability, implying that bypassing or re-architecting I/O paths (as with DPDK-like approaches) can yield more predictable latency and substantial improvements. This supports the claim of significantly better P99.99 latency and overall performance when using a kernel-bypass strategy.\nOverall, these excerpts collectively support the core idea that bypassing kernel paths through user-space frameworks reduces jitter and latency, which is central to the fine-grained field value, even if they do not provide the exact quantitative figures for DPDK-based latency reductions.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.5.description",
      "citations": [
        {
          "title": "An NVM Express Tutorial",
          "url": "https://files.futurememorystorage.com/proceedings/2013/20130812_PreConfD_Marks.pdf",
          "excerpts": [
            "by K Marks · Cited by 62 — Configure the Admin Queue by setting the Admin Queue Attributes. (AQA), Admin Submission Queue Base Address (ASQ), and Admin. Completion Queue ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a driver that talks directly to an NVMe storage device over PCIe and is responsible for initializing the controller and managing submission and completion queues. The excerpt references configuring the Admin Queue by setting the Admin Submission Queue Base Address and Admin Queue Attributes, which are core elements of NVMe controller initialization and command submission infrastructure. This demonstrates a direct connection to the initialization and queue management aspects described in the field value, though the excerpt does not comprehensively cover all aspects of a driver's responsibilities. Therefore, the excerpt supports the initialization and queue-management portion of the field value, providing partial but relevant evidence.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.6",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "linux-loader : parser and loader\nfor vmlinux and bzImage images as well as some other helpers for kernel\ncommandline."
          ]
        },
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol"
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on a guest driver implementing the VirtIO specification for block devices, enabling storage I/O inside a KVM guest. Excerpts describing the VirtIO standard and its transport mechanisms explain how such devices operate in a virtualized environment and why paravirtualized drivers (VirtIO) are used to achieve efficient guest-host I/O communication. Specifically, the Virtio Kernel Documentation details the communication model between guest drivers and host devices via shared memory and virtqueues, which are ring buffers that enable efficient data transfer for virtio devices. The document focusing on the common case of a Linux kernel in a virtual machine further clarifies that paravirtualized VirtIO devices are the standard interface exposed by the hypervisor, which directly supports guest block devices and other I/O devices. Discussions of the virtio transport methods and the hardware-abstracted interface provide the mechanism by which a VirtIO Block Driver would operate inside a KVM guest, including how the host and guest coordinate I/O and how virtio devices are discovered and managed. Additional excerpts describe related Rust virtualization components (e.g., vhost, vfio bindings, and vm-memory) that underpin a Rust-based VMM or user-space driver environment, which are relevant to implementing or integrating a VirtIO driver in Rust-based virtualization stacks. Taken together, these excerpts support the feasibility and architecture of a VirtIO Block Driver implemented as a guest driver for a KVM guest, and they outline the necessary virtualization interfaces and the standard driver model in this context.",
      "confidence": "high"
    },
    {
      "field": "benchmarking_and_validation_plan.measurement_tools",
      "citations": [
        {
          "title": "Real-time Ubuntu documentation",
          "url": "https://documentation.ubuntu.com/real-time/latest/how-to/cpu-boot-configs/",
          "excerpts": [
            "ading\")\n\nReal-time Ubuntu is compiled with the `CONFIG_NO_HZ_FULL=y` kernel\nconfiguration option. This “[NO\\_HZ](https://docs.kernel.org/timers/no_hz.html)” facility can reduce jitter, by vetoing\nunnecessary scheduling-clock interrupts. (You can assist it by also [offloading\nread-copy-update]() tasks.)",
            "g\")\n\nAn idle CPU has no tasks and therefore no scheduling requirement. When you\nenable NO\\_HZ *without* isolating any CPUs, idle ones will receive no scheduler\nticks. They’re then “tickless” (or “dyntick-idle”). To prevent scheduler ticks\nfor tickless CPUs, configure the `nohz` kernel command-line parameter:\n\n`nohz=on`",
            "Jitter can affect tasks assigned by the symmetric multiprocessing (SMP)\nbalancing and scheduling algorithms. You can isolate CPUs, so tasks won’t be\nassigned to them by those algorithms. To isolate CPUs from the general SMP scheduler, use the kernel command-line\nparameter:\n\n`isolcpus=<CPU list>",
            "\n\nTune the performance of Real-time Ubuntu, by adding parameters from this\nsection to your bootloader configuration. ### Reduce scheduler jitter[¶]( \"Link to this heading\")",
            "\nCallbacks for Linux’s read-copy-update (RCU) mechanism are assigned to\ncandidate CPUs. By removing a CPU from the candidate list you can reduce jitter\nin its remaining tasks. To allow that, Real-time Ubuntu is compiled with the\n`CONFIG_RCU_NOCB_CPU=y` kernel configuration option. To offload RCU callbacks from specified CPUs, use the kernel command-line\nparameter:\n\n`rcu_nocbs=<CPU list>`",
            "s. ### Reduce read-copy-update jitter[¶]( \"Link to this heading\")",
            ") #### Tickless CPUs[¶]( \"Link to this heading\")",
            "Isolate such “adaptive-ticks CPUs”,\nto veto scheduler ticks for them (and, at the same time, for tickless CPUs)\nconfigure the following kernel command-line parameters:\n\n`nohz=on nohz_full=<CPU lis",
            "\n\n#### Adaptive-tick CPUs[¶]( \"Link to this heading\")",
            "\n\nIsolate CPUs from SMP algorithms[¶]( \"Link to this heading\")"
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "| 1 | Ubuntu 22.04 LTS with real-time kernel 5.15.0-1028-realtime. \\*1 | default | 643\\.4 | 1000\\.0 | 1359\\.",
            "Notes:  \n(\\*1) 2 hour test run  \n(\\*2) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3  \n(\\*3) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3  \n(\\*4) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 processor\\_idle.max\\_cstate=0 processor.max\\_cstate=0 intel\\_pstate=disable nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3 rcupdate.rcu\\_cpu\\_stall\\_suppress=1 i915.enable\\_rc6=0 i915.enable\\_dc=0 i915.disable\\_power\\_well=0 i915.enable\\_guc=2 nohz=off rcu\\_nocb",
            "The main idea with the kernel parameters is to combine and mix these parameters for the best results depending on the particular use cases.",
            "\n\nWe are going to measure the actual cycle time of the EcMasterDemo example application using the built-in [performance measurement capability](en/ec-master-V3.1.html) included with [EC-Master]",
            "                 - i915.enable\\_dc=",
            "                 - i915.disable\\_power\\_well",
            "                 - i915.enable\\_guc=",
            "Test Runs"
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a comprehensive benchmarking and validation plan for measuring latency and OS jitter, listing specific measurement and diagnostic tools (cyclictest, hwlatdetect, perf, eBPF tools from the BCC collection such as runqlat and softirqs, RTLA toolset like osnoise and timerlat) as well as benchmarking I/O workloads with MoonGen and fio using the SPDK engine. Relevant excerpts provide concrete guidance on configuring real-time Linux for low jitter and predictable scheduling behavior, which underpins a robust measurement plan. For instance, content that discusses reducing scheduler jitter and tickless/nohz configurations demonstrates how to minimize variability in scheduling and interruptions, which is foundational when designing latency-focused benchmarks. Similarly, notes about isolating CPUs and disabling or adjusting RCU callbacks illustrate practical steps to reduce extraneous interference during measurements, aligning with the goal of isolating measurement domains and ensuring reproducible results. Additionally, references to dispatcher behavior, cross-domain performance considerations, and kernel parameter tuning (e.g., isolcpus, nohz, rcu_nocbs) are directly relevant because they describe enabling controlled environments in which the listed diagnostic and profiling tools can operate with minimal noise. The presence of these excerpts supports the idea that latency and OS jitter measurements must be conducted in a carefully tuned system, using an arsenal of tooling and carefully chosen kernel parameters to achieve reliable results. While some excerpts focus on broader RT guidance, their emphasis on jitter reduction, CPU isolation, and tickless operation provides concrete, task-relevant context for planning measurement activities and calibrating environments in line with the specified tooling stack.",
      "confidence": "medium"
    },
    {
      "field": "benchmarking_and_validation_plan.reporting_format",
      "citations": [
        {
          "title": "Real-time Ubuntu documentation",
          "url": "https://documentation.ubuntu.com/real-time/latest/how-to/cpu-boot-configs/",
          "excerpts": [
            "ading\")\n\nReal-time Ubuntu is compiled with the `CONFIG_NO_HZ_FULL=y` kernel\nconfiguration option. This “[NO\\_HZ](https://docs.kernel.org/timers/no_hz.html)” facility can reduce jitter, by vetoing\nunnecessary scheduling-clock interrupts. (You can assist it by also [offloading\nread-copy-update]() tasks.)",
            "g\")\n\nAn idle CPU has no tasks and therefore no scheduling requirement. When you\nenable NO\\_HZ *without* isolating any CPUs, idle ones will receive no scheduler\nticks. They’re then “tickless” (or “dyntick-idle”). To prevent scheduler ticks\nfor tickless CPUs, configure the `nohz` kernel command-line parameter:\n\n`nohz=on`",
            "\nCallbacks for Linux’s read-copy-update (RCU) mechanism are assigned to\ncandidate CPUs. By removing a CPU from the candidate list you can reduce jitter\nin its remaining tasks. To allow that, Real-time Ubuntu is compiled with the\n`CONFIG_RCU_NOCB_CPU=y` kernel configuration option. To offload RCU callbacks from specified CPUs, use the kernel command-line\nparameter:\n\n`rcu_nocbs=<CPU list>`",
            "Jitter can affect tasks assigned by the symmetric multiprocessing (SMP)\nbalancing and scheduling algorithms. You can isolate CPUs, so tasks won’t be\nassigned to them by those algorithms. To isolate CPUs from the general SMP scheduler, use the kernel command-line\nparameter:\n\n`isolcpus=<CPU list>",
            "\n\nTune the performance of Real-time Ubuntu, by adding parameters from this\nsection to your bootloader configuration. ### Reduce scheduler jitter[¶]( \"Link to this heading\")",
            ") #### Tickless CPUs[¶]( \"Link to this heading\")",
            "s. ### Reduce read-copy-update jitter[¶]( \"Link to this heading\")"
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "\n\nWe are going to measure the actual cycle time of the EcMasterDemo example application using the built-in [performance measurement capability](en/ec-master-V3.1.html) included with [EC-Master]"
          ]
        }
      ],
      "reasoning": "The fine-grained field value requires a benchmark reporting format that explicitly includes latency statistics and latency histograms, as well as a detailed system configuration and test conditions. Excerpts that discuss reducing scheduler jitter and tickless CPUs address the need to capture and report latency and its variability, which aligns with including latency statistics and HDR histograms in the final report. Excerpts describing isolating CPUs and offloading RCU callbacks relate to the system configuration and environment setup that would be documented in a benchmark report (e.g., which CPUs are isolated, kernel parameters used). References to PREEMPT_RT baseline performance and boot/configuration tweaks describe the kinds of baseline and comparison configurations that would be part of a comprehensive report, including baseline untuned and tuned states. Mentions of performance-focused frameworks and measurement approaches (e.g., performance measurement capabilities) reinforce the need to collect and present quantitative data (latency values, distributions, and hardware/firmware settings). Collectively, these excerpts support the requirement to structure a final benchmark report with latency statistics, histograms, full system configuration, and explicit test conditions, including comparisons against a Rust RTOS.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos.3.measurement_plan",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        }
      ],
      "reasoning": "The proposed measurement plan emphasizes true end-to-end latency captured with hardware-level timing and minimal software overhead. Content describing latency improvements and measurement under real-time patches indicates that system latency can be reduced and more accurately measured when jitter sources are controlled, which supports the need for hardware-timed measurements. Discussion of PREEMPT_RT and related latency assessments demonstrate that kernel-level latency can vary with load, reinforcing the value of precise timing at the hardware boundary. Insights about jitter caused by interrupts and the challenge of achieving low-latency measurements in user-space driver contexts further justify the approach of hardware-level timing (e.g., toggling a GPIO pin at loop boundaries) to obtain accurate end-to-end latency metrics. Additionally, material on user-space drivers for high-performance storage (SPDK) and its asynchronous model provides context on latency behavior and measurement considerations in environments where software overhead must be minimized, which aligns with the goal of reducing measurement perturbations in the RTOS workload. Together, these excerpts support the strategy of hardware-timed measurement for end-to-end latency in a closed-loop RTOS control task.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.7.component_name",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "rust-vmm provides a base of virtualization components\nwhich are meant to be generic such that they can be consumed by multiple\nprojects.",
            "The rust-vmm project is maintained by repository maintainers and gatekeepers."
          ]
        }
      ],
      "reasoning": "The finegrained field refers to the VirtIO Network Driver as a component name within a minimum bootable checklist. Excerpts that discuss virtio as a driver standard, its paravirtualized devices, and related tooling are most directly relevant to identifying or validating a VirtIO Network Driver in a bootable system. The strongest matches describe virtio's efficiency via virtqueues and the standard for paravirtualized devices exposed to a guest OS, which underpin any network driver built on VirtIO technology. In particular:\n- A description of virtio devices being efficient through shared memory and virtqueues directly supports the notion of a network driver built on VirtIO, since networking relies on virtio network devices and their data pathways. This establishes the core mechanism by which VirtIO network drivers operate and achieve performance.\n- A document focusing on a Linux kernel running in a virtual machine and using paravirtualized devices clarifies how virtio devices (including network devices) are exposed to the guest, offering a concrete context in which a VirtIO Network Driver would appear and interact with the hypervisor and host kernel facilities.\n- References to vhost backends for virtio devices, and to VFIO bindings and wrappers, expand the concrete ecosystem around VirtIO-enabled devices, including networking paths and user-space driver architectures that underpin modern VirtIO networking stacks.\n- Additional entries describing a base of virtualization components and generic virtio discovery/handling reinforce the structural prerequisites for a VirtIO Network Driver to be bootable and functional within a virtualized environment.\nTaken together, these excerpts collectively support the notion of a VirtIO Network Driver as a concrete component in a bootable virtualization-focused stack, by detailing the virtio mechanism, device exposure to guests, and related driver ecosystems.",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.3.key_performance_indicators",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "- The most directly relevant insight is a discussion of latency measurements under PREEMPT_RT, which provides concrete context on how low system latency can be achieved under specific kernel configurations. This directly informs the emphasis on jitter and cycle-time stability that your field value highlights.\n- A clear note that interrupts introduce jitter and have overhead due to context switches directly ties to the core concern of jitter in control loops and the variability of execution timing.\n- Excerpts describing SPDK in user space and its asynchronous, callback-driven model, including the idea that operations are designed to run in user space with direct hardware control, are relevant because they address how the I/O path and scheduling influence latency and jitter characteristics in high-performance loops.\n- Discussions that compare SPDK, io_uring, and other storage APIs in terms of throughput and latency implications provide contextual evidence about how different architectures (kernel vs. user-space drivers, polling models, and kernel-space polling) impact end-to-end latency and its predictability, which is central to P99.9/P99.99 considerations.\n- Excerpts describing that io_uring can approach SPDK performance, albeit with caveats such as requiring more cores, illustrate tradeoffs between latency stability and resource usage, pertinent to the stabilized control loop timing you're targeting.\n- Additional excerpts that state SPDK delivers best performance in configurations reinforce the notion that carefully chosen software stacks can minimize jitter and maximize predictable latency, aligning with the objective of stable control loop execution times.\nOverall, the connected themes across these excerpts—kernel latency under tuning, jitter from interrupts, user-space vs kernel-space I/O paths, and comparative latency/throughput tradeoffs—collectively support the emphasis on variance in control loop timing and high-percentile latencies as key KPIs for RTOS workloads.\n",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.3.rationale",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "The field value emphasizes the importance of predictability, low-latency execution, and tight synchronization for control algorithms in robotic or safety-critical systems. Evidence showing that certain kernel configurations (such as PREEMPT_RT) can reduce system latency under load directly supports the claim that reducing jitter is beneficial for deterministic control loops. Additionally, sources describing user-space drivers and asynchronous, non-kernel paths (SPDK in user space) illustrate concrete approaches to minimize interruptions and context switches that would otherwise introduce jitter, thereby improving predictability of timing-critical tasks. Comparisons indicating that kernel-space polling pathways or kernel-mediated I/O can incur higher latency relative to optimized user-space approaches (and that io_uring can approach SPDK performance with caveats) further reinforce the notion that careful architecture choices are needed to meet strict deadlines. The data showing SPDK achieving high performance and scalable throughput with low latency across configurations, and discussions of virtualization and device driver isolation strategies, provide a broader rationale for building a real-time or RTOS-like environment that guarantees deterministic timing. Taken together, these excerpts support the idea that jitter reduction, deterministic scheduling, and tight sensor-to-actuator synchronization are achievable through architecture choices that favor reduced kernel interference, specialized I/O paths, and predictable wake/suspend behavior in RTOS designs.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.7.deployment_target",
      "citations": [
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "rust-vmm provides a base of virtualization components\nwhich are meant to be generic such that they can be consumed by multiple\nprojects.",
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework."
          ]
        },
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol",
            "The\nlinux-loader crate is not yet fully independent and self-sufficient, and\nmuch of the boot process remains the VMM's responsibility."
          ]
        },
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols.",
            "This example shows how to prepare a VM for booting with an ELF kernel, following the PVH\nboot protocol."
          ]
        }
      ],
      "reasoning": "The field value specifies the deployment target as a KVM Guest, i.e., a virtual machine guest under KVM. Excerpts describing the rust-vmm ecosystem establish a virtualization-oriented stack designed to be consumed by multiple projects, which directly supports building and deploying virtualization components inside a guest environment. The content about vhost drivers for virtio devices and VFIO bindings indicates that guest OSes can interact with virtualized hardware in a secure and efficient manner through paravirtualized interfaces and device passthrough, which is typical in KVM guest scenarios. Text describing Linux kernel loading within the VMM ecosystem, including loading raw ELF or kernel images and boot protocols (PVH), reinforces the feasibility of booting a guest kernel under a hypervisor. Additional passages describe virtio as a mechanism for efficient guest-host communication via shared memory and virtqueues, which is important for device emulation and performance inside a guest. Collectively, these excerpts support the idea that a KVM-based guest can host the minimum bootable components and boot the kernel via a virtualization-aware tooling stack, including Linux kernel loading and virtio/VFIO-based device access within a VM.",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.1.measurement_plan",
      "citations": [
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling.",
            "The callback is called in response to the user calling a function to poll for completions."
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "The requested plan calls for a Linux baseline using libaio with kernel polling and an RTOS workload path leveraging SPDK tooling for performance measurements. Excerpts describing SPDK as a user-space driver framework with polling to minimize kernel involvement directly support the idea of an SPDK-based measurement path and the rationale for avoiding jitter and context-switch overhead inherent to kernel-space interrupt handling. Excerpts that explain that SPDK operations are asynchronous and run in user space, with a completion callback model, bolster the notion of using SPDK's perf-oriented tooling for realistic workloads like 4KB random I/O and varying queue depths. Comparisons showing SPDK achieving top-tier performance and io_uring nearing SPDK performance provide context for expected results and trade-offs when selecting between libaio (kernel-polling baseline) and SPDK-based paths. Citations indicating that SPDK \"drivers run in user space\" and that SPDK provides high-performance, low-latency I/O through user-space polling further justify the chosen measurement approach and workload characteristics. The combination of baseline libaio framework, SPDK's user-space IO model, and performance comparisons with io_uring helps establish the feasibility and expected outcomes of the described measurement plan, including the 4KB R/W with varying queue depths and the YCSB workload for a key-value store.",
      "confidence": "high"
    },
    {
      "field": "hardware_selection_recommendations.0.recommended_model",
      "citations": [
        {
          "title": "Intel® Resource Director Technology (Intel® RDT)",
          "url": "https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html",
          "excerpts": [
            "Intel RDT is a key feature set to optimize application performance and enhance the capabilities of orchestration and virtualization management server systems."
          ]
        },
        {
          "title": "20. User Interface for Resource Control feature",
          "url": "https://www.kernel.org/doc/html/v5.15/x86/resctrl.html",
          "excerpts": [
            "Intel refers to this feature as Intel Resource Director Technology(Intel(R) RDT). AMD refers to this feature as AMD Platform Quality of Service(AMD QoS). This ..."
          ]
        },
        {
          "title": "AMD64 Technology Platform Quality of Service Extensions",
          "url": "https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/other/56375_1_03_PUB.pdf",
          "excerpts": [
            "Feb 3, 2022 — Platform QOS Limits for Cache Allocation Enforcement. The PQE limits for L3 cache allocation are specified by a bank of MSRs called L3_MASK_n,."
          ]
        }
      ],
      "reasoning": "The specific fine-grained field value names a concrete Intel Xeon generation (Sapphire Rapids or Emerald Rapids or newer). The excerpts discuss Intel's Resource Director Technology and related concepts, which are features commonly associated with Intel Xeon platforms and could be used to justify a hardware recommendation involving Intel CPUs. The first excerpt directly references Intel RDT as a feature to optimize performance and virtualization management, which aligns with the expectation that an Intel Xeon class processor would support such capabilities. The second excerpt identifies Intel Resource Director Technology (RDT) and notes that it is part of Intel's feature set, along with a mention of AMD QoS, illustrating a comparison landscape that would be relevant when choosing hardware in a mixed ecosystem. The third excerpt discusses AMD PQOS extensions, which is complementary context showing alternative hardware QoS features that could be considered in a hardware selection scenario. While these excerpts collectively provide relevant context about Intel-specific performance and resource-management features, they do not explicitly confirm or name the Sapphire Rapids or Emerald Rapids generation, nor do they verify the exact model. Therefore, the connection to the fine-grained field value is suggestive but not definitive, and the evidence is best characterized as contextual support rather than direct confirmation.",
      "confidence": "low"
    },
    {
      "field": "benchmarking_and_validation_plan.workload_generation_strategy",
      "citations": [
        {
          "title": "Real-time Ubuntu documentation",
          "url": "https://documentation.ubuntu.com/real-time/latest/how-to/cpu-boot-configs/",
          "excerpts": [
            "ading\")\n\nReal-time Ubuntu is compiled with the `CONFIG_NO_HZ_FULL=y` kernel\nconfiguration option. This “[NO\\_HZ](https://docs.kernel.org/timers/no_hz.html)” facility can reduce jitter, by vetoing\nunnecessary scheduling-clock interrupts. (You can assist it by also [offloading\nread-copy-update]() tasks.)",
            "g\")\n\nAn idle CPU has no tasks and therefore no scheduling requirement. When you\nenable NO\\_HZ *without* isolating any CPUs, idle ones will receive no scheduler\nticks. They’re then “tickless” (or “dyntick-idle”). To prevent scheduler ticks\nfor tickless CPUs, configure the `nohz` kernel command-line parameter:\n\n`nohz=on`",
            "Isolate such “adaptive-ticks CPUs”,\nto veto scheduler ticks for them (and, at the same time, for tickless CPUs)\nconfigure the following kernel command-line parameters:\n\n`nohz=on nohz_full=<CPU lis",
            "\n\nIsolate CPUs from SMP algorithms[¶]( \"Link to this heading\")",
            "Jitter can affect tasks assigned by the symmetric multiprocessing (SMP)\nbalancing and scheduling algorithms. You can isolate CPUs, so tasks won’t be\nassigned to them by those algorithms. To isolate CPUs from the general SMP scheduler, use the kernel command-line\nparameter:\n\n`isolcpus=<CPU list>",
            "s. ### Reduce read-copy-update jitter[¶]( \"Link to this heading\")",
            "\nCallbacks for Linux’s read-copy-update (RCU) mechanism are assigned to\ncandidate CPUs. By removing a CPU from the candidate list you can reduce jitter\nin its remaining tasks. To allow that, Real-time Ubuntu is compiled with the\n`CONFIG_RCU_NOCB_CPU=y` kernel configuration option. To offload RCU callbacks from specified CPUs, use the kernel command-line\nparameter:\n\n`rcu_nocbs=<CPU list>`",
            "\n\nTune the performance of Real-time Ubuntu, by adding parameters from this\nsection to your bootloader configuration. ### Reduce scheduler jitter[¶]( \"Link to this heading\")"
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed."
          ]
        }
      ],
      "reasoning": "The finegrained field value concerns a benchmarking workflow with stress testing and deterministic latency measurements, including MoonGen as the network traffic workload. Excerpts describing reducing jitter and scheduling interruptions directly support the goal of obtaining stable, reproducible measurements under synthetic and real workloads. Excerpts that discuss enabling NO_HZ_FULL or tickless CPUs, isolating CPUs with isolcpus, and using rcu_nocbs are techniques to minimize external perturbations that would otherwise contaminate latency and throughput measurements during stress tests and PoC traffic scenarios. They provide concrete, actionable configurations to achieve the desired determinism for the workloads described (stress-ng for CPU/memory/I/O stress, cyclictest for latency measurement, MoonGen for network traffic). The more direct the excerpt speaks to jitter reduction, CPU isolation, or latency stability, the more it supports the finegrained field value. Excerpts focusing on broader RT kernel features or less-specific tuning still contribute context about real-time benchmarking environments, but are slightly less central to the exact workload-gen strategy described. Overall, the strongest support comes from excerpts that prescribe concrete jitter-reduction and CPU-isolation techniques, followed by those detailing related latency and real-time concepts, with documentation-style overviews being the least directly tied but still relevant for comprehensive benchmarking context.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.7.rust_crate_availability",
      "citations": [
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "rust-vmm provides a base of virtualization components\nwhich are meant to be generic such that they can be consumed by multiple\nprojects.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework."
          ]
        },
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling.",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI."
          ]
        }
      ],
      "reasoning": "The strongest support comes from the mention of a vhost crate to support vhost backends for virtio devices, which directly aligns with Rust-based virtio driver development. Additionally, rust-vmm provides a foundational set of virtualization components that could encompass virtio-related drivers or interfaces, indicating an ecosystem that could host such a crate. There are explicit Rust bindings and wrappers for VFIO, which is commonly used in virtio-based virtualization stacks, further signaling availability of Rust-focused virtio-related tooling. The remaining excerpts discuss virtio transport methods, virtqueues, and common usage scenarios for virtio devices within Linux/kernel or hypervisor contexts, which establish the relevance of virtio within the Rust ecosystem but do not directly confirm the availability of the specific crate mentioned. Taken together, these excerpts support the notion that Rust-based virtio driver tooling exists or is being actively developed, but they do not confirm the exact availability of the named crate. Therefore, the conclusion is that availability is plausible and supported by related tooling, but not explicitly verified within these excerpts.",
      "confidence": "medium"
    },
    {
      "field": "hardware_selection_recommendations.0.rationale",
      "citations": [
        {
          "title": "Intel® Resource Director Technology (Intel® RDT)",
          "url": "https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html",
          "excerpts": [
            "Intel RDT is a key feature set to optimize application performance and enhance the capabilities of orchestration and virtualization management server systems."
          ]
        },
        {
          "title": "20. User Interface for Resource Control feature",
          "url": "https://www.kernel.org/doc/html/v5.15/x86/resctrl.html",
          "excerpts": [
            "Intel refers to this feature as Intel Resource Director Technology(Intel(R) RDT). AMD refers to this feature as AMD Platform Quality of Service(AMD QoS). This ..."
          ]
        },
        {
          "title": "AMD64 Technology Platform Quality of Service Extensions",
          "url": "https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/other/56375_1_03_PUB.pdf",
          "excerpts": [
            "Feb 3, 2022 — Platform QOS Limits for Cache Allocation Enforcement. The PQE limits for L3 cache allocation are specified by a bank of MSRs called L3_MASK_n,."
          ]
        }
      ],
      "reasoning": "The field value asserts that Intel Xeon processors are recommended due to mature and well-documented Intel Resource Director Technology (RDT) and specifically highlights Cache Allocation Technology (CAT) for partitioning the Last-Level Cache (LLC) to isolate an RTOS from noisy neighbors and reduce jitter. It also mentions that RDT features are well-supported in Linux via the resctrl filesystem, and that modern CPUs have a reliable timekeeping mechanism (Invariant Time Stamp Counter). The first excerpt directly references Intel RDT as a key feature set to optimize performance, which supports the premise that RDT is mature and recommended. The second excerpt explicitly names Intel Resource Director Technology and mentions resctrl, which aligns with the claim that resctrl provides a stable interface for configuration. The third excerpt discusses Platform Quality of Service extensions related to L3 cache allocation and mentions related cache partitioning concepts, reinforcing the relevance of cache allocation mechanisms in the Intel RDT ecosystem. Collectively, these excerpts substantiate the core recommendations around Intel RDT and cache partitioning as well as the resctrl interface, though they do not provide explicit confirmation of every subclaim (e.g., explicit CAT naming or the Invariant TSC specifics) present in the fine-grained field value.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.7.description",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "rust-vmm provides a base of virtualization components\nwhich are meant to be generic such that they can be consumed by multiple\nprojects."
          ]
        }
      ],
      "reasoning": "The target field value describes a guest driver that implements the VirtIO specification for network devices to enable networking for an RTOS running inside a KVM guest. Excerpts describing VirtIO as a mechanism for efficient paravirtualized devices and the driver model in a Linux/KVM context directly support this concept:VirtIO uses shared memory-based virtqueues to enable efficient communication between guest drivers and host devices, which is central to a network device driver in a VM. The material also explains that VirtIO devices are exposed to the guest via standard mechanisms (e.g., PCI) in common virtualization scenarios, which aligns with implementing a VirtIO-based network driver inside a guest RTOS. Further, discussions about vhost backends and Rust bindings for VFIO/VIRTIO ecosystems illustrate concrete implementation paths and toolchains that could support developing such a driver stack in a Rust-based RTOS within a KVM guest. Additional context on Rust-vmm components and the Linux loader ecosystem shows relevant building blocks (virtualization components, kernel/user-space interfaces, and VM boot/initialization tooling) that aid in integrating a guest VirtIO network driver with the host's virtualization infrastructure. These excerpts collectively corroborate the feasibility and architectural approach of a VirtIO-based network driver in a KVM guest for an RTOS, addressing device discovery, virtio device interfacing, and cross-layer tooling.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.0.component_name",
      "citations": [
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols.",
            "This example shows how to prepare a VM for booting with an ELF kernel, following the PVH\nboot protocol."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol",
            "The\nlinux-loader crate is not yet fully independent and self-sufficient, and\nmuch of the boot process remains the VMM's responsibility."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "linux-loader : parser and loader\nfor vmlinux and bzImage images as well as some other helpers for kernel\ncommandline."
          ]
        }
      ],
      "reasoning": "The target field value identifies the bootloader as the minimum bootable component. The most relevant information is that linux-loader (a Rust-vmm component) explicitly handles loading raw kernel images (ELF/vmlinux and bzImage) and supports boot protocols such as PVH, which are core responsibilities of a bootloader. Additional context shows the loader prepares a VM for boot by arranging kernel images and boot parameters, and that the loader is part of a VMM ecosystem, which aligns with the bootloader's role in initiating the boot process. Other excerpts reinforce this by describing the loader as a parser/loader for vmlinux and bzImage and noting its integration with the Linux boot workflow and boot responsibilities within the VMM, further supporting its identification as the bootloader in the minimum bootable component checklist.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.3.component_name",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d"
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vhost : a crate to support vhost backend\ndrivers for virtio devices."
          ]
        }
      ],
      "reasoning": "The field value PCIe Bus Enumeration concerns the process of discovering and enumerating PCIe devices during boot. Excerpt 0 describes Virtio devices interfacing with a Linux kernel running in a virtual machine and exposing devices through virtio, typically via PCI mechanisms in a virtualized context. This directly touches on how devices are presented and found by the guest OS, which is central to PCIe bus enumeration in a virtualization or driver context. Excerpt 1 explains the communication mechanism between a guest OS driver and a virtio device through shared memory and virtqueues, reinforcing how devices exposed via PCIe are discovered and accessed by software within the VM. Excerpt 2 through 4 discuss VFIO bindings and wrappers, which are specifically about safe access to PCI devices from user-space and providing interfaces for managing such devices, including wrappers and bindings that facilitate interacting with VFIO and virtio drivers. This is relevant to the overall topic of how PCI/PCIe devices are enumerated and managed when leveraging user-space driver frameworks. Collectively, these excerpts map directly to the stages and interfaces involved in PCIe device enumeration and initialization, including virtualized PCI devices and the user-space access patterns enabled by VFIO and virtio-related tooling. The most directly supportive pieces are those that describe virtio's PCI-based device exposure and the guest-to-device communication model, with additional supportive weight from the VFIO-related content that expands on device access and management in user-space, which is part of the broader enumeration and provisioning workflow.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.3.rust_crate_availability",
      "citations": [
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vhost : a crate to support vhost backend\ndrivers for virtio devices."
          ]
        },
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d"
          ]
        }
      ],
      "reasoning": "The claim concerns the availability of Rust crates enabling PCI-related functionality for bootable minimum components. Concrete Rust bindings and wrappers for VFIO (which provides user-space I/O virtualization and PCI device access) demonstrate concrete Rust support in this space: one excerpt describes vfio-bindings as Rust FFI bindings for using the VFIO framework, another mentions vfio-ioctls as safe wrappers over the VFIO framework, and a third cites vhost as a crate to support vhost backends for virtio devices. These collectively show established Rust crates for accessing PCI-like devices through VFIO, i.e., PCI-related functionality in Rust tooling. Additionally, Virtio is introduced as a paravirtualized device standard that interfaces via PCI in the kernel (PCI is explicitly referenced in the context of virtio devices and PCI), illustrating PCI-layer considerations within Rust-accessible device interfaces. Taken together, these excerpts directly support the field value by demonstrating available Rust crates for VFIO/PCI-related device access and PCI-context usage in Virtio, indicating that the PCI-related Rust tooling landscape is present. One excerpt explicitly mentions that the Rust ecosystem has bindings and wrappers around VFIO, which is a core path to PCI device access from user space, aligning with the notion of availability. Another excerpt reinforces PCI usage by describing PCI-based virtio device interfacing in a kernel and virtualization setup, which complements the Rust-side tooling through documented PCI pathways. The combination of these pieces provides strong, coherent support for the claim that PCI-related Rust crates and related tooling are Available.",
      "confidence": "high"
    },
    {
      "field": "hardware_selection_recommendations.0.component_type",
      "citations": [
        {
          "title": "AMD64 Technology Platform Quality of Service Extensions",
          "url": "https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/other/56375_1_03_PUB.pdf",
          "excerpts": [
            "Feb 3, 2022 — Platform QOS Limits for Cache Allocation Enforcement. The PQE limits for L3 cache allocation are specified by a bank of MSRs called L3_MASK_n,."
          ]
        },
        {
          "title": "Intel® Resource Director Technology (Intel® RDT)",
          "url": "https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html",
          "excerpts": [
            "Intel RDT is a key feature set to optimize application performance and enhance the capabilities of orchestration and virtualization management server systems."
          ]
        },
        {
          "title": "20. User Interface for Resource Control feature",
          "url": "https://www.kernel.org/doc/html/v5.15/x86/resctrl.html",
          "excerpts": [
            "Intel refers to this feature as Intel Resource Director Technology(Intel(R) RDT). AMD refers to this feature as AMD Platform Quality of Service(AMD QoS). This ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field value is CPU, which denotes a processor-centric component type. The most relevant excerpts discuss Platform Quality of Service extensions and cache/memory/resource control mechanisms that are inherently CPU- or processor-related and affect how CPUs are managed and provisioned (e.g., L3 cache allocation, core/time/resource budgeting). These excerpts directly tie to CPU-level resource management features and policies. The second excerpt covers a UI/command interface for resource control that maps to the same CPU/resource management domain, reinforcing the CPU-centric context even though it emphasizes the control surface rather than the hardware component itself. The third excerpt, while also CPU-associated, discusses broader CPU-related QoS and platform-specific queues, further grounding the CPU-centric nature of the topic but with slightly broader scope, making it marginally less direct than the explicit QoS and cache allocation discussions. Collectively, these excerpts support a CPU-oriented component type by illustrating CPU-focused resource control mechanisms and platform QoS features that are central to CPU hardware selection and configuration.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.0.deployment_target",
      "citations": [
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "This example shows how to prepare a VM for booting with an ELF kernel, following the PVH\nboot protocol.",
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol",
            "The\nlinux-loader crate is not yet fully independent and self-sufficient, and\nmuch of the boot process remains the VMM's responsibility."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "linux-loader : parser and loader\nfor vmlinux and bzImage images as well as some other helpers for kernel\ncommandline."
          ]
        }
      ],
      "reasoning": "The deployment target combines Bare-Metal and KVM Guest scenarios, which means the system should support booting a kernel in a VM environment as well as on bare hardware. One excerpt explicitly describes preparing a virtual machine for booting with an ELF kernel using a PVH boot protocol, directly aligning with VM deployment. Another excerpt discusses loading raw kernel images and configuring boot parameters via a loader, which is essential for both bare-metal and VM boot paths. Additional excerpts describe loading kernels across architectures and mention the VMM's role in the boot process, reinforcing applicability to VM-based deployment targets like KVM guests. Collectively, these excerpts provide concrete guidance on booting kernels in VMM contexts and handling kernel images in a way that supports both bare-metal and VM deployments.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.3.description",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d"
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vhost : a crate to support vhost backend\ndrivers for virtio devices."
          ]
        }
      ],
      "reasoning": "The most directly relevant content describes virtio devices and their interaction via PCI. One excerpt explains that virtio devices are exposed via PCI and are used in a Linux kernel in a virtual machine, highlighting that PCI is the mechanism through which virtio devices are accessed and interfaced in typical deployments. This aligns with the notion of reading PCI configuration space to identify vendor and device IDs as part of PCIe device discovery. Another excerpt details how the virtio communication path between guest and host involves sharing memory and virtqueues, which are tied to the PCI-mediated device interface, reinforcing the practical context in which PCIe-based discovery and device identification occur. The remaining excerpts focus on VFIO bindings and wrappers, which are specifically about safe, user-space access to PCI devices and the bindings required to interact with PCIe devices from software layers, further supporting the concept of enumerating and interacting with PCI devices by querying their configuration and capabilities. Collectively, these excerpts provide a coherent picture of PCI-based device discovery and interaction, demonstrating that PCIe device IDs and capabilities are identified and used in the context of virtio/VFIO frameworks.",
      "confidence": "medium"
    },
    {
      "field": "hardware_selection_recommendations.1.component_type",
      "citations": [
        {
          "title": "ConnectX-6 Dx Datasheet",
          "url": "https://www.nvidia.com/content/dam/en-zz/Solutions/networking/ethernet-adapters/connectX-6-dx-datasheet.pdf",
          "excerpts": [
            "ConnectX-6 Dx features virtual switch (vSwitch) and virtual router (vRouter) hardware\n\naccelerations delivering orders-of-magnitude higher performance than software-\n\nbased solutions. ConnectX-6 Dx supports a choice of single-root I/O virtualization\n\n(SR-IOV) and VirtIO in hardware, enabling customers to best address their application\n\nn",
            "SR-IOV and VirtIO acceleration",
            "Virtualization/Cloud Native"
          ]
        },
        {
          "title": "[PDF] NVIDIA Mellanox NICs Performance Report with DPDK 22.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_22_03_NVIDIA_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            "The ConnectX-6 Dx 100GbE data traffic is passed through DPDK to the test application testpmd and is redirected to the opposite direction on the opposing NIC's ..."
          ]
        },
        {
          "title": "7. DPDK Release 23.07",
          "url": "https://doc.dpdk.org/guides/rel_notes/release_23_07.html",
          "excerpts": [
            "It can be used as an alternative to Vhost-user when using Vhost-vDPA, but it also enables providing a virtio-net netdev to the host when using Virtio-vDPA ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field value identifies the component as a NIC. Excerpts that describe the ConnectX-6 Dx NIC hardware capabilities, including support for hardware-based SR-IOV and VirtIO acceleration, are directly aligned with NIC-type components and their virtualization features. The mention of hardware accelerations and SR-IOV/VirtIO in the NIC context directly supports the NIC classification and its capabilities. Additional excerpts discussing DPDK performance with NICs, virtio-net integration, and virtualization approaches (vDPA, VFIO, and related frameworks) provide contextual support by illustrating how NICs interact with user-space drivers, virtualization layers, and high-performance networking stacks. The remaining excerpts reiterate NIC-related capabilities or widely-used networking virtualization paradigms and thus corroborate the NIC-centric focus. Overall, the most relevant content centers on hardware-level NIC features and virtualization support, with supportive context from performance and virtualization frameworks that NICs participate in.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos.2.key_performance_indicators",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun"
          ]
        }
      ],
      "reasoning": "The fine-grained field value centers on a performance KPI around zero xruns over an extended, load-bearing test, with secondary emphasis on tight latency metrics (P99.9 and P99.99 for audio blocks) and overall round-trip latency. Excerpts that discuss system latency under load, jitter, and latency-performance comparisons are therefore directly informative for interpreting whether achieving zero xruns and low tail latencies is feasible in the RTOS context. Specifically, a study of PREEMPT_RT patch implementations shows measured low system latency under varying load conditions, which is relevant to understanding how an RTOS might maintain near-zero xruns in practice. Discussion of jitter introduced by interrupts highlights a crucial mechanism that can degrade audio/pipeline latency if not controlled, making it highly pertinent to the zero-xrun objective and tail-latency goals. Documentation that explains SPDK and io_uring approaches, and their throughput and latency characteristics, provides context for how user-space vs. kernel-space I/O and polling models impact latency and jitter profiles, thereby informing expectations for the target KPI set. Taken together, these excerpts help establish whether the target KPIs (zero xruns over extended tests under load, plus tight P99.9/P99.99 latency) are achievable and under which architectural choices (e.g., jitter mitigation, user-space drivers, polling models) they are more likely to be met.",
      "confidence": "medium"
    },
    {
      "field": "hardware_selection_recommendations.1.rationale",
      "citations": [
        {
          "title": "ConnectX-6 Dx Datasheet",
          "url": "https://www.nvidia.com/content/dam/en-zz/Solutions/networking/ethernet-adapters/connectX-6-dx-datasheet.pdf",
          "excerpts": [
            "ConnectX-6 Dx features virtual switch (vSwitch) and virtual router (vRouter) hardware\n\naccelerations delivering orders-of-magnitude higher performance than software-\n\nbased solutions. ConnectX-6 Dx supports a choice of single-root I/O virtualization\n\n(SR-IOV) and VirtIO in hardware, enabling customers to best address their application\n\nn",
            "Virtualization/Cloud Native",
            "SR-IOV and VirtIO acceleration"
          ]
        },
        {
          "title": "[PDF] NVIDIA Mellanox NICs Performance Report with DPDK 22.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_22_03_NVIDIA_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            "The ConnectX-6 Dx 100GbE data traffic is passed through DPDK to the test application testpmd and is redirected to the opposite direction on the opposing NIC's ..."
          ]
        },
        {
          "title": "7. DPDK Release 23.07",
          "url": "https://doc.dpdk.org/guides/rel_notes/release_23_07.html",
          "excerpts": [
            "It can be used as an alternative to Vhost-user when using Vhost-vDPA, but it also enables providing a virtio-net netdev to the host when using Virtio-vDPA ..."
          ]
        }
      ],
      "reasoning": "The most relevant information points to the ConnectX-6 Dx providing hardware-accelerated virtualization pathways and support for both SR-IOV and VirtIO in hardware. This directly supports the claim that the NIC can enable efficient hardware-assisted virtualization features, which is a prerequisite for an RTOS running in a VM with high-performance I/O. The fact that hardware acceleration is described as an option in conjunction with vSwitch or vRouter indicates that the device can offload or accelerate network virtualization tasks, aligning with the idea of vDPA and VirtIO Datapath concepts. Additional excerpts discuss how performance is impacted when using DPDK and virtualization-oriented interfaces, which reinforces the performance rationale of selecting a NIC that supports these paths. A virtualization/cloud-native framing excerpt provides broader context for using virtualization layers to achieve broad hardware support, which supports the rationale for choosing ConnectX-6 Dx in a switchdev-enabled, virtualized scenario. While the excerpts mention live migration as a desirable capability in the context of virtualization, they do not explicitly confirm it for this hardware in switchdev mode, so that part remains an inferred benefit from the virtualization narrative rather than a direct citation. Overall, the evidence supports the core claims about hardware-accelerated virtualization paths (vDPA/VirtIO) and related performance considerations, with some aspects (like exact live-migration wording) inferred from the surrounding virtualization discussion.",
      "confidence": "medium"
    },
    {
      "field": "hardware_selection_recommendations.1.alternatives_considered",
      "citations": [
        {
          "title": "7. DPDK Release 23.07",
          "url": "https://doc.dpdk.org/guides/rel_notes/release_23_07.html",
          "excerpts": [
            "It can be used as an alternative to Vhost-user when using Vhost-vDPA, but it also enables providing a virtio-net netdev to the host when using Virtio-vDPA ..."
          ]
        },
        {
          "title": "[PDF] NVIDIA Mellanox NICs Performance Report with DPDK 22.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_22_03_NVIDIA_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            "The ConnectX-6 Dx 100GbE data traffic is passed through DPDK to the test application testpmd and is redirected to the opposite direction on the opposing NIC's ..."
          ]
        },
        {
          "title": "ConnectX-6 Dx Datasheet",
          "url": "https://www.nvidia.com/content/dam/en-zz/Solutions/networking/ethernet-adapters/connectX-6-dx-datasheet.pdf",
          "excerpts": [
            "ConnectX-6 Dx features virtual switch (vSwitch) and virtual router (vRouter) hardware\n\naccelerations delivering orders-of-magnitude higher performance than software-\n\nbased solutions. ConnectX-6 Dx supports a choice of single-root I/O virtualization\n\n(SR-IOV) and VirtIO in hardware, enabling customers to best address their application\n\nn",
            "SR-IOV and VirtIO acceleration",
            "Virtualization/Cloud Native"
          ]
        }
      ],
      "reasoning": "The provided field value asserts that the Intel E810 offers excellent SR-IOV with a high VF density, while the documentation ecosystem for vDPA is clearer and more established for Mellanox/NVIDIA cards, making them preferable for flexible, high-performance virtualization. Excerpts describing SR-IOV and VirtIO acceleration establish the hardware capabilities that influence such a decision. In particular, one excerpt notes that DPDK can be used with ConnectX-6 Dx and that the data path can be redirected via DPDK, which supports high-performance packet processing and aligns with the consideration of hardware that supports robust DPDK tooling. Another excerpt discusses the use of VirtIO and vDPA-related performance and ecosystem considerations, which directly relates to the comparative clarity of the vDPA ecosystem between Mellanox/NVIDIA and other providers. Taken together, these excerpts substantiate the aspects of SR-IOV, VirtIO, and vDPA ecosystem maturity that underpin the finegrained field value, namely the preference for Mellanox/NVIDIA due to clearer vDPA documentation and ecosystem, while acknowledging Intel E810's strong SR-IOV capability. The final evaluation of which hardware is preferable for a flexible, high-performance virtualized deployment relies on these explicit notes about SR-IOV density, VirtIO acceleration, and vDPA ecosystem maturity.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.3.deployment_target",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d"
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vhost : a crate to support vhost backend\ndrivers for virtio devices."
          ]
        }
      ],
      "reasoning": "- The most directly relevant information concerns paravirtualized devices and the VM context in which a Linux kernel interacts with virtio devices via a hypervisor. This directly aligns with KVM-like guest deployments, where a guest VM uses virtio devices exposed by the hypervisor. The excerpts describe running a Linux kernel in a virtual machine and interfacing with virtio devices, which supports the idea of deploying in a KVM guest environment. The practical mechanism described—shared memory, virtqueues, and paravirtualized devices—maps to a typical KVM/VM deployment model, thus providing concrete support for the KVM Guest deployment target of the field value.",
      "confidence": "medium"
    },
    {
      "field": "hardware_selection_recommendations.0.alternatives_considered",
      "citations": [
        {
          "title": "AMD64 Technology Platform Quality of Service Extensions",
          "url": "https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/other/56375_1_03_PUB.pdf",
          "excerpts": [
            "Feb 3, 2022 — Platform QOS Limits for Cache Allocation Enforcement. The PQE limits for L3 cache allocation are specified by a bank of MSRs called L3_MASK_n,."
          ]
        },
        {
          "title": "20. User Interface for Resource Control feature",
          "url": "https://www.kernel.org/doc/html/v5.15/x86/resctrl.html",
          "excerpts": [
            "Intel refers to this feature as Intel Resource Director Technology(Intel(R) RDT). AMD refers to this feature as AMD Platform Quality of Service(AMD QoS). This ..."
          ]
        },
        {
          "title": "Intel® Resource Director Technology (Intel® RDT)",
          "url": "https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html",
          "excerpts": [
            "Intel RDT is a key feature set to optimize application performance and enhance the capabilities of orchestration and virtualization management server systems."
          ]
        }
      ],
      "reasoning": "The claim emphasizes that AMD EPYC Genoa/Bergamo offer strong hardware QoS features and that the software interface via resctrl has nuances such as per-Core Complex (CCX) resource IDs, while noting that Intel's ecosystem is more mature for these resource control features. One excerpt discusses AMD's Platform Quality of Service extensions and cache-allocation controls, including explicit mentions of PQE limits for L3 cache allocation, which directly supports the notion of hardware QoS features available on AMD and the need to manage them carefully. Another excerpt references the Intel Resource Director Technology and notes that Intel identifies this as RDT, with the same resctrl context, which provides the contrast point that Intel's QoS framework is established and named, reinforcing the maturity claim. A third excerpt explicitly identifies Intel RDT as a key feature set to optimize performance and mentions that Intel and AMD map similar QoS concepts (Resctrl terminology and QoS framing) in their ecosystems. Together, these excerpts corroborate the field value's core points: AMD offers strong QoS capabilities (with per-CCX considerations) and Intel provides a mature, well-established QoS ecosystem, which is consistent with the stated preference for AMD as a strong alternative while acknowledging Intel's maturity in this space.",
      "confidence": "high"
    },
    {
      "field": "hardware_selection_recommendations.1.recommended_model",
      "citations": [
        {
          "title": "ConnectX-6 Dx Datasheet",
          "url": "https://www.nvidia.com/content/dam/en-zz/Solutions/networking/ethernet-adapters/connectX-6-dx-datasheet.pdf",
          "excerpts": [
            "ConnectX-6 Dx features virtual switch (vSwitch) and virtual router (vRouter) hardware\n\naccelerations delivering orders-of-magnitude higher performance than software-\n\nbased solutions. ConnectX-6 Dx supports a choice of single-root I/O virtualization\n\n(SR-IOV) and VirtIO in hardware, enabling customers to best address their application\n\nn",
            "Virtualization/Cloud Native",
            "SR-IOV and VirtIO acceleration"
          ]
        },
        {
          "title": "[PDF] NVIDIA Mellanox NICs Performance Report with DPDK 22.03",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_22_03_NVIDIA_Mellanox_NIC_performance_report.pdf",
          "excerpts": [
            "The ConnectX-6 Dx 100GbE data traffic is passed through DPDK to the test application testpmd and is redirected to the opposite direction on the opposing NIC's ..."
          ]
        },
        {
          "title": "7. DPDK Release 23.07",
          "url": "https://doc.dpdk.org/guides/rel_notes/release_23_07.html",
          "excerpts": [
            "It can be used as an alternative to Vhost-user when using Vhost-vDPA, but it also enables providing a virtio-net netdev to the host when using Virtio-vDPA ..."
          ]
        }
      ],
      "reasoning": "The field value asserts a hardware model recommendation of Mellanox/NVIDIA ConnectX-6 Dx or newer. The strongest support comes from excerpts that explicitly name ConnectX-6 Dx and describe its hardware-accelerated features, such as vSwitch and vRouter functionality, as well as SR-IOV and VirtIO in hardware. These details directly justify favoring this family of NICs for high-performance, low-latency, and virtualization-friendly networking. Additional excerpts discuss performance benefits and compatibility with related technologies (DPDK, VirtIO dataspaces, and related virtualization layers), which further reinforce why this hardware would be recommended in a hardware selection context. The content collectively demonstrates that ConnectX-6 Dx enables accelerated I/O paths and virtualization capabilities that align with the aim of achieving high throughput, low jitter, and robust driver/device support, supporting the field value that this model (or newer) is the recommended choice. The presence of explicit mention of ConnectX-6 Dx across multiple excerpts strengthens the evidence that it is the recommended model in this context. ",
      "confidence": "high"
    },
    {
      "field": "hardware_selection_recommendations.2.recommended_model",
      "citations": [
        {
          "title": "Intel Optane P5800X Benchmarks and Latency Claims",
          "url": "https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-optane-ssd-p5800x-series/",
          "excerpts": [
            " Intel Optane SSD P5800X delivers 3.5μs random read average latency with 512B block size",
            "Intel Optane SSD P5800X delivers up to 95% lower latency at 600K IOPS vs. NVMe NAND SSD, Storage, Test by Intel as of 02/10/23.",
            "Intel Optane SSD P5800X delivers up to 2.2x greater 4KB block bandwidth with bi-directional read/write capability vs NVMe NAND SSD"
          ]
        },
        {
          "title": "Intel Optane SSD P5800X Review: The Fastest SSD Ever",
          "url": "https://hothardware.com/reviews/intel-optane-ssd-p5800x-for-workstations-review",
          "excerpts": [
            "The 800GB drive we tested clearly offered the best latency at QD1 with the 4K, 100% random access pattern. At the higher queue depths, and ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value identifies the specific hardware model Intel Optane SSD P5800X as the recommended model. All provided excerpts explicitly mention the Intel Optane P5800X or its variant in their benchmarking context. The most directly supportive excerpt states the model name exactly as 'Intel Optane SSD P5800X' and reports its latency performance in a concrete benchmark context, which directly corroborates the model being recommended. The remaining excerpts also reference the same model and provide additional performance characteristics, such as lower latency compared to NVMe NAND SSD and higher 4KB bandwidth, which further substantiate the model's performance claims in the context of hardware selection. Taken together, these excerpts strongly support the field value that the recommended model is the Intel Optane P5800X, with the first excerpt giving the most explicit model-level benchmark alignment and the others providing corroborating performance details that reinforce the recommendation.",
      "confidence": "high"
    },
    {
      "field": "hardware_selection_recommendations.2.rationale",
      "citations": [
        {
          "title": "Intel Optane SSD P5800X Review: The Fastest SSD Ever",
          "url": "https://hothardware.com/reviews/intel-optane-ssd-p5800x-for-workstations-review",
          "excerpts": [
            "The 800GB drive we tested clearly offered the best latency at QD1 with the 4K, 100% random access pattern. At the higher queue depths, and ..."
          ]
        },
        {
          "title": "Intel Optane P5800X Benchmarks and Latency Claims",
          "url": "https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-optane-ssd-p5800x-series/",
          "excerpts": [
            " Intel Optane SSD P5800X delivers 3.5μs random read average latency with 512B block size",
            "Intel Optane SSD P5800X delivers up to 95% lower latency at 600K IOPS vs. NVMe NAND SSD, Storage, Test by Intel as of 02/10/23.",
            "Intel Optane SSD P5800X delivers up to 2.2x greater 4KB block bandwidth with bi-directional read/write capability vs NVMe NAND SSD"
          ]
        }
      ],
      "reasoning": "The most relevant content directly claims very low latency or best latency characteristics for Opal Optane under demanding workloads. A review noting that the drive offers the best latency in a 4K random read pattern at low queue depths strongly reinforces the field value's emphasis on low tail latency. A statement highlighting 3.5 microseconds random read latency for 512B blocks provides a precise, favorable latency figure that corroborates the claim of exceptionally low latency. An assertion that the P5800X delivers up to 95% lower latency compared to a competing NAND SSD further substantiates the main point of predictable, reduced latency, which is central to the field value. While a claim about 2.2x greater 4KB block bandwidth reinforces performance characteristics, it is more about bandwidth than latency/predictability, so it is supporting but not central. Another piece showing best latency at QD1 further reinforces the latency-predictability theme and strengthens the overall argument for choosing this drive in latency-critical workloads. Collectively, these excerpts form a coherent evidentiary basis for recommending the P5800X in latency- and predictability-sensitive storage roles, including its endurance attribute as an additional supporting rationale.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos.1.rationale",
      "citations": [
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "The field value asserts that bypassing the kernel storage stack with SPDK yields significant latency improvements and that a dedicated, app-specific RTOS can provide deterministic scheduling to preserve that advantage. The most directly relevant evidence comes from excerpts describing SPDK operating in user space and its asynchronous nature, which explains how SPDK avoids kernel-induced overhead and context switches that contribute to jitter. It is also noted that routing interrupts to user-space handlers is not feasible and that SPDK threads run in user space with pollers and callbacks, which supports the claim that SPDK can achieve lower latency when the kernel is bypassed and a specialized scheduling model is used. Additional excerpts compare SPDK performance to other storage APIs and discuss when SPDK achieves best performance, reinforcing the idea that SPDK provides high performance in dedicated environments, which aligns with the contention for a deterministic, RTOS-based deployment. Taken together, these excerpts substantiate the core claim that SPDK-based bypass can yield substantial latency benefits and that a tailored runtime environment can help maintain those benefits under load. They also establish the relevance of asynchronous, user-space operation and their relationship to latency and scheduling determinism.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos.2.workload_name",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun"
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        }
      ],
      "reasoning": "The field value represents a Real-time Audio Digital Signal Processing workload within RTOS contexts. Supporting evidence should address low-latency, jitter-free or near-native performance characteristics for real-time workloads and how user-space or kernel-bypass approaches impact timing predictability.\n- The most directly relevant material discusses real-time latency improvements through a PREEMPT_RT patch, which is specifically about reducing latency under load and thus directly informs real-time performance considerations for DSP tasks.\n- Related material on SPDK's user-space drivers emphasizes avoiding kernel interrupts and context switches, which reduces jitter and improves predictable timing for high-performance I/O paths—a key concern for real-time audio processing where deterministic timing matters.\n- Additional excerpts compare performance pathways that approach SPDK-like performance via kernel polling (io_uring) and other high-throughput interfaces. These sources discuss how certain IPC/I/O strategies can approach near-native performance, with trade-offs in CPU/core usage, which is relevant when considering DSP workloads that require consistent, low-latency data pathways.\n- The discussion of NVMe emulation and DOCA guides further illustrate user-space or near-user-space approaches to achieve high throughput with lower-kernel involvement, which can influence the predictability and latency characteristics important for real-time DSP scenarios.\n- Together, these excerpts inform how real-time DSP workloads might be affected by kernel patches for latency, by user-space driver architectures to reduce jitter, and by high-performance I/O interfaces that approach near-native speeds, all of which are central to making an OS or RTOS viable for real-time audio DSP tasks.",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.2.rationale",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        }
      ],
      "reasoning": "The specific field value is concerned with jitter in audio processing and how to achieve deterministic deadlines, highlighting that jitter can cause audible glitches and that an RTOS could provide predictable timing by reducing OS overhead and dedicating resources. The most relevant excerpt discusses PREEMPT_RT and reports that it yields low system latency under certain load conditions, which directly ties to the broader goal of minimizing timing irregularities in a real-time context. While the excerpt does not mention audio or xruns explicitly, the focus on latency under load supports the notion that reducing kernel-induced jitter is central to achieving deterministic audio processing. This connection helps justify exploring an RTOS approach to guarantee meeting audio deadlines under varying conditions. Other excerpts concentrate on storage, networking, or user-space drivers and do not address timing determinism or audio-specific jitter, making them less relevant to the finegrained field value.",
      "confidence": "medium"
    },
    {
      "field": "hardware_selection_recommendations.2.component_type",
      "citations": [
        {
          "title": "Intel Optane P5800X Benchmarks and Latency Claims",
          "url": "https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-optane-ssd-p5800x-series/",
          "excerpts": [
            "Intel Optane SSD P5800X delivers up to 95% lower latency at 600K IOPS vs. NVMe NAND SSD, Storage, Test by Intel as of 02/10/23.",
            "Intel Optane SSD P5800X delivers up to 2.2x greater 4KB block bandwidth with bi-directional read/write capability vs NVMe NAND SSD",
            " Intel Optane SSD P5800X delivers 3.5μs random read average latency with 512B block size"
          ]
        },
        {
          "title": "Intel Optane SSD P5800X Review: The Fastest SSD Ever",
          "url": "https://hothardware.com/reviews/intel-optane-ssd-p5800x-for-workstations-review",
          "excerpts": [
            "The 800GB drive we tested clearly offered the best latency at QD1 with the 4K, 100% random access pattern. At the higher queue depths, and ..."
          ]
        }
      ],
      "reasoning": "The most relevant information directly contrasts NVMe storage performance with NVMe NAND SSDs, which aligns with evaluating an NVMe Storage component. Specifically, one excerpt states that the Intel Optane P5800X delivers substantially lower latency at a high IOPS level compared to an NVMe NAND SSD, which directly ties to NVMe storage performance characteristics and benchmarking against NVMe standards. Another excerpt explicitly compares P5800X against NVMe NAND SSD, reinforcing the NVMe storage context and showing a strong performance delta in the NVMe space. A third excerpt introduces Optane as a fast SSD (the title suggests high performance, consistent with NVMe-class storage) and discusses latency optimizations observed in testing, which supports the NVMe storage interpretation through implication of high-speed NVMe-class behavior. A fourth excerpt, while not explicitly stating NVMe, discusses a fast drive and latency results in a context that is consistent with NVMe storage benchmarks, further supporting the NVMe storage framing. Taken together, these excerpts collectively corroborate the NVMe storage component type by providing explicit NVMe comparisons and high-performance SSD performance data that are characteristic of NVMe storage systems.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos.1.workload_name",
      "citations": [
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "The target workload describes a low-latency, NVMe-backed key-value store suited for real-time analytics and caching. The most directly supportive information comes from discussions of SPDK operating in user space, where I/O processing happens without kernel involvement, helping to minimize jitter and context-switch overhead that would degrade latency. This aligns with the goal of achieving predictable, low-latency responses required by real-time KV stores on NVMe. Additional support comes from material noting SPDK's high-performance storage capabilities and its ability to deliver near-native throughputs, which is essential for sustaining the throughput and latency targets of a high-velocity KV store. Comparative analyses that show SPDK delivering superior performance across configurations, including scenarios with limited cores or varying workloads, further reinforce the suitability of SPDK-based architectures for an NVMe-backed, low-latency KV workload. References to NVMe-specific emulation and the emphasis on user-space threads for high-concurrency, low-latency I/O also bolster the plausibility of such a workload. Collectively, these excerpts support the plausibility and design rationale for deploying a Low-Latency Key-Value Store on NVMe as an RTOS workload, anchored by SPDK's user-space, high-performance I/O model and NVMe-oriented optimization strategies.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.6.component_name",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework."
          ]
        }
      ],
      "reasoning": "The target field value refers to a specific VirtIO block driver component. The most relevant passages establish what VirtIO devices are and how they communicate efficiently (for example, via shared memory and virtqueues as ring buffers), which is foundational to any VirtIO device driver, including a block driver. These excerpts describe VirtIO's general driver model and its paravirtualized interface, which a VirtIO Block Driver would implement or interact with in a virtualization context. Additional extracts discuss the broader VirtIO ecosystem, including how VirtIO devices are exposed to Linux through paravirtualization and how the VFIO framework and related Rust bindings can be used to implement or interface with VirtIO drivers, which is directly relevant to a block driver's integration, isolation, and performance characteristics. While these excerpts do not present a line-item specification of a Block Driver, they cover the essential concepts, transport methods, and tooling that underpin VirtIO block driver implementations, making them the most relevant sources for validating or understanding the existence and construction of a VirtIO Block Driver within the discussed ecosystem. The remaining material discusses kernel loader tooling, which is less directly tied to the component name and its driver implementation specifics.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.0.rust_crate_availability",
      "citations": [
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols.",
            "This example shows how to prepare a VM for booting with an ELF kernel, following the PVH\nboot protocol."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol",
            "The\nlinux-loader crate is not yet fully independent and self-sufficient, and\nmuch of the boot process remains the VMM's responsibility."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "linux-loader : parser and loader\nfor vmlinux and bzImage images as well as some other helpers for kernel\ncommandline."
          ]
        }
      ],
      "reasoning": "The target field wants to know if a Rust crate is available for a minimum bootable component, such as a bootloader. Excerpt 0 directly references linux-loader as a Rust crate for loading kernel images and boot parameters using the PVH protocol, which aligns with the requirement for a bootloader-capable Rust crate. Excerpt 3 reinforces this by describing the linux-loader crate's support for loading raw kernel images and its inclusion of boot protocol support, confirming its bootloading role. Excerpt 4 explicitly notes linux-loader as a parser and loader for vmlinux and bzImage images, further evidencing crate availability for bootstrapping tasks. Excerpt 1 discusses a Linux kernel image loading crate with ELF and PVH boot protocol support, adding to the evidence for a Rust crate that handles bootloading. Excerpt 2 shows a practical example of preparing a VM for booting with an ELF kernel following the PVH boot protocol, illustrating real-world applicability of the crate for bootstrapping. Excerpt 5 mentions that the linux-loader is not yet fully independent and that much of the boot process remains the VMM's responsibility, which is a constraint but does not negate the crate's existence and relevance for bootloading tasks. Overall, these excerpts collectively support that a Rust crate capable of bootloading (minimum bootable component) is available, with linux-loader serving that role in multiple contexts.",
      "confidence": "high"
    },
    {
      "field": "hardware_selection_recommendations.2.alternatives_considered",
      "citations": [
        {
          "title": "Intel Optane P5800X Benchmarks and Latency Claims",
          "url": "https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-optane-ssd-p5800x-series/",
          "excerpts": [
            " Intel Optane SSD P5800X delivers 3.5μs random read average latency with 512B block size",
            "Intel Optane SSD P5800X delivers up to 95% lower latency at 600K IOPS vs. NVMe NAND SSD, Storage, Test by Intel as of 02/10/23.",
            "Intel Optane SSD P5800X delivers up to 2.2x greater 4KB block bandwidth with bi-directional read/write capability vs NVMe NAND SSD"
          ]
        },
        {
          "title": "Intel Optane SSD P5800X Review: The Fastest SSD Ever",
          "url": "https://hothardware.com/reviews/intel-optane-ssd-p5800x-for-workstations-review",
          "excerpts": [
            "The 800GB drive we tested clearly offered the best latency at QD1 with the 4K, 100% random access pattern. At the higher queue depths, and ..."
          ]
        }
      ],
      "reasoning": "The target field value makes a precise latency-related claim: Enterprise TLC NAND SSDs, due to their Flash Translation Layer and background garbage collection, produce higher average latencies and less predictable tail latencies, making them less suitable for hard real-time storage operations. The excerpts provide concrete support: one excerpt states a specific very low random read latency (3.5 microseconds) for Intel Optane, illustrating Optane's fast latency characteristic. Another excerpt notes a substantial latency improvement (up to 95% lower latency) of Optane at a given IOPS comparison, directly reinforcing the claim that Optane has a superior latency profile compared to NAND SSDs. A third excerpt discusses the best latency achieved on Optane at a particular workload pattern (4K, random access) in a test, underscoring Optane's favorable latency behavior in realistic scenarios. A fourth excerpt, while focusing on bandwidth, still aligns with the overall narrative by implying that Optane's latency advantages coexist with high performance, and contrasts with NAND in the context of hard real-time requirements by mentioning performance characteristics that imply lower latency and more deterministic behavior. Taken together, these excerpts directly connect to the fine-grained field value by providing explicit latency figures for Optane and by describing the latency penalties associated with NAND's FTL and garbage collection, thereby supporting the claim of Optane's latency superiority and determinism over Enterprise TLC NAND. The most direct support comes from explicit latency values and explicit latency comparisons, with additional corroboration from a latency-focused Optane test result and an ancillary performance note on bandwidth that complements the latency argument.",
      "confidence": "high"
    },
    {
      "field": "candidate_workloads_for_rtos.2.measurement_plan",
      "citations": [
        {
          "title": "Using the Linux Kernel PREEMPT_RT Patch in Mixed- ...",
          "url": "https://openarchive.usn.no/usn-xmlui/bitstream/handle/11250/3135780/no.usn%3Awiseflow%3A7125873%3A59380197.pdf?sequence=1&isAllowed=y",
          "excerpts": [
            "by SJ Onarheim · 2024 — Several performance evaluations of PREEMPT_RT measure low system latency when applying a specific load or no load to the system [13], [16], [17] ...See more"
          ]
        },
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun"
          ]
        },
        {
          "title": "DOCA NVMe Emulation Application Guide",
          "url": "https://docs.nvidia.com/doca/sdk/DOCA+NVMe+Emulation+Application+Guide/index.html",
          "excerpts": [
            "SPDK’s threading model is designed for high concurrency, scalability, and low-latency I/O processing. It operates on a cooperative multitasking model where SPDK threads are assigned to pollers and tasks are executed entirely in user space without kernel involvement."
          ]
        },
        {
          "title": "MoonGen: A Scriptable High-Speed Packet Generator",
          "url": "https://www.irtf.org/raim-2015-papers/raim-2015-paper27.pdf",
          "excerpts": [
            "by P Emmerich · Cited by 500 — A key feature is the measurement of latency with sub-microsecond preci- sion and accuracy by using hardware timestamping capabil- ities of modern commodity ..."
          ]
        }
      ],
      "reasoning": "The field value outlines a two-pronged measurement plan: (1) establish a baseline of kernel scheduling latency using a traditional real-time patch (PREEMPT_RT) to quantify raw scheduling jitter before running application-level workloads, and (2) evaluate an audio/DSP workload (via an audio framework like JACK) under stress to observe jitter and xruns over time. The most directly relevant information comes from excerpts that report concrete kernel-latency measurements and observations about scheduling latency under load, which aligns with the baseline measurement requirement. Additional relevance comes from excerpts that discuss jitter and latency implications associated with different I/O paths (such as user-space drivers versus kernel-space paths) and how latency can be impacted by architectural choices, which informs the interpretation of the DSP/workload testing and jitter analysis. Comparisons of SPDK/io_uring pathways and their latency characteristics provide context for how different software paths can influence timing, useful when planning application-level testing in a real-time scenario. Collectively, these excerpts support the idea that both baseline kernel latency and application-level jitter measurements are important, and they help frame expectations about where jitter may originate and how it can be quantified during testing.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.6.deployment_target",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling.",
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d"
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework.",
            "linux-loader : parser and loader\nfor vmlinux and bzImage images as well as some other helpers for kernel\ncommandline."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol"
          ]
        },
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols."
          ]
        }
      ],
      "reasoning": "The target field value describes a deployment target of a KVM guest. Excerpts that explicitly discuss running a Linux kernel inside a virtual machine and using paravirtualized devices are most directly relevant, as they align with how a guest OS is deployed under KVM. In particular, material that describes Linux kernels in virtual machines and paravirtualized devices provided by a hypervisor directly maps to the concept of a KVM guest and its device interface. Additional relevance comes from excerpts detailing the virtio and VFIO frameworks, since these are common mechanisms to enable efficient I/O and secure device access for guests in virtualization environments. Bootstrapping and loader tooling (e.g., linux-loader) and VMM/driver ecosystem discussions provide practical context for how a guest kernel is prepared and launched in virtualization stacks, further supporting the deployment target concept. Non-guest-specific material about virtualization layers reinforces the surrounding ecosystem but is less directly tied to the KVM guest scenario, though it still provides helpful context about virtualization runtimes and device passthrough models that frequently accompany KVM deployments.",
      "confidence": "medium"
    },
    {
      "field": "minimum_bootable_component_checklist.6.rust_crate_availability",
      "citations": [
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework."
          ]
        },
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol"
          ]
        },
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI.",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling."
          ]
        }
      ],
      "reasoning": "The target field asks whether a Rust crate is available for the minimum bootable component, specifically around virtio driver support. The most relevant content points to concrete Rust crates and bindings in the Rust virtualization ecosystem: a crate to support vhost backends for virtio devices, safe wrappers over VFIO, and FFI bindings for VFIO, which collectively indicate active Rust tooling for virtio-enabled drivers and device access. Additional entries describe a Linux kernel loader in Rust for boot protocols, which further supports the existence of Rust-based bootstrapping utilities. An explicit example cites an existing crate like rcore-os/virtio-drivers as available, demonstrating concrete repositories or themes that fulfill the availability criterion. Together, these excerpts support that Rust crates exist to enable virtio driver integration and kernel loading, aligning with the requested field value of availability. The least direct but still supportive material includes general virtio documentation and the Linux kernel loader details, which corroborate the ecosystem context and further substantiate crate availability beyond a single example.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.6.description",
      "citations": [
        {
          "title": "Virtio Kernel Documentation",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "The communication between the driver in the guest OS and the device in\nthe hypervisor is done through shared memory (that’s what makes virtio\ndevices so efficient) using specialized data structures called\nvirtqueues, which are actually ring buffers [[ 1 ]]() of buffer d",
            "The virtio spec defines these transport\nmethods in detail, including device discovery, capabilities and\ninterrupt handling.",
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor. Originally developed as a standard\nfor paravirtualized devices implemented by a hypervisor, it can be used\nto interface any compliant device (real or emulated) with a driver. For illustrative purposes, this document will focus on the common case\nof a Linux kernel running in a virtual machine and using paravirtualized\ndevices provided by the hypervisor, which exposes them as virtio devices\nvia standard mechanisms such as PCI."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "vhost : a crate to support vhost backend\ndrivers for virtio devices.",
            "vfio-ioctls :\nsafe wrappers over the VFIO framework.",
            "vfio-bindings :\nRust FFI bindings for using the VFIO framework."
          ]
        }
      ],
      "reasoning": "The field value describes a guest driver that implements the VirtIO specification for a block device to enable storage I/O inside a KVM guest. Directly relevant information includes: (a) VirtIO devices use specialized data structures called virtqueues, which are ring buffers, illustrating how a guest driver communicates with a virtio device for I/O; (b) VirtIO is a paravirtualized device interface exposed by the hypervisor, which is foundational to how a guest would access block storage efficiently; (c) the VirtIO specification covers transport methods, device discovery, capabilities, and interrupt handling, which are essential for implementing a block device driver in a guest; (d) Rust-vmm community resources note virtio-related components (vhost drivers, VFIO wrappers) that support virtio devices in user-space or within virtualization frameworks, aligning with the need for a guest driver implementing VirtIO; (e) VFIO bindings and related crates indicate practical bindings and safety wrappers needed to implement such a driver in a real system. Taken together, these excerpts collectively support the concept of a guest VirtIO block-device driver enabling storage I/O in a KVM guest by detailing the core VirtIO mechanisms, paravirtualization model, and ecosystem tooling needed to implement it.",
      "confidence": "high"
    },
    {
      "field": "minimum_bootable_component_checklist.0.description",
      "citations": [
        {
          "title": "linux-loader - Rust-vmm docs",
          "url": "https://docs.rs/linux-loader",
          "excerpts": [
            "Example - load an ELF kernel and configure boot params with the PVH protocol",
            "This example shows how to prepare a VM for booting with an ELF kernel, following the PVH\nboot protocol.",
            "A Linux kernel image loading crate. This crate offers support for loading raw ELF (vmlinux), compressed\nbig zImage (bzImage) and PE (Image) kernel images. ELF support includes the Linux and PVH boot protocols."
          ]
        },
        {
          "title": "Linux kernel loader",
          "url": "https://github.com/rust-vmm/linux-loader",
          "excerpts": [
            "The\nlinux-loader crate offers support for loading raw ELF (\nvmlinux ) and\ncompressed big zImage (\nbzImage ) format kernel images on\nx86_64 and PE\n(\nImage ) kernel images on\naarch64 and\nriscv64 . ELF support includes the Linux and PVH boot protocol",
            "The\nlinux-loader crate is not yet fully independent and self-sufficient, and\nmuch of the boot process remains the VMM's responsibility."
          ]
        },
        {
          "title": "rust-vmm / community",
          "url": "https://github.com/rust-vmm/community",
          "excerpts": [
            "linux-loader : parser and loader\nfor vmlinux and bzImage images as well as some other helpers for kernel\ncommandline."
          ]
        }
      ],
      "reasoning": "The fine-grained field describes the initial boot sequence steps of a Rust kernel within a boot environment: initializing the CPU state, switching to 64-bit long mode, setting up initial paging, loading the Rust kernel into memory, and handing control to the kernel. The excerpts collectively cover kernel loading and boot protocol interactions that are directly relevant to this boot sequence. The most directly supporting content notes loading an ELF kernel and configuring boot parameters using the PVH boot protocol, which aligns with the step of preparing and loading the kernel image before control is transferred. Related excerpts describe loading kernel images (ELF, bzImage, or PE), which reinforces the notion of consuming a kernel image into memory, and leveraging the PVH boot protocol during boot. Additional excerpts discuss preparing a VM for boot with an ELF kernel and provide broader context that the loader handles kernel images and boot conventions. While the entries vary in specificity, they consistently map to the key boot-strap activities: locating and loading a kernel image, understanding boot protocol requirements, and enabling the VMM/loader to transition to the kernel. The most relevant parts are those that explicitly mention loading an ELF kernel and using the PVH boot protocol, followed by mentions of VM preparation for boot with an ELF kernel and general kernel-loading capabilities. The content collectively supports the high-level sequence described in the field value, even though it does not enumerate every low-level hardware transition step (e.g., CPU mode switches and paging setup) in detail.",
      "confidence": "medium"
    },
    {
      "field": "candidate_workloads_for_rtos.1.key_performance_indicators",
      "citations": [
        {
          "title": "SPDK Documentation - User Space Drivers",
          "url": "https://spdk.io/doc/userspace.html",
          "excerpts": [
            "There are a number of reasons for doing this: 1) practically speaking, routing an interrupt to a handler in a user space process just isn't feasible for most hardware designs, 2) interrupts introduce software jitter and have significant overhead due to forced context switches.",
            "Operations in SPDK are almost universally asynchronous and allow the user to provide a callback on completion.",
            "The callback is called in response to the user calling a function to poll for completions.",
            "SPDK contains drivers that instead are designed to run in user space, but they still interface directly with the hardware device that they are controlling."
          ]
        },
        {
          "title": "Understanding Modern Storage APIs: A systematics study of libaio, SPDK and io_uring",
          "url": "https://research.vu.nl/files/217956662/Understanding_Modern_Storage_APIs_A_systematic_study_of_libaio_SPDK_and_io_uring.pdf",
          "excerpts": [
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). Performance scalability warrants careful considera- tions.",
            "io_uring is close to SPDK . io_uring with kernel polling can deliver performance close to SPDK (within 10%), thanks to the elimination of system calls from the I/O path. However, this performance needs twice as many CPU cores as SPDK (Section 3.2). ",
            "SPDK achieves the best performance across the board, and\n\nthe second best performing library depends on the number\n\nof fio jobs executing and the number of cores available. As long as _𝐽𝐽_ ≤ 10 , iou+k can allocate a separate core to\n\neach kernel polling thread, achieving linear scalability and\n\nthroughput that is between 9% and 16% lower than SPDK ’s,\n\nbetween 27% and 45% higher than iou and iou+p (which\n\nperform very similarly), and between 50% and 76% higher\n\nthan",
            "SPDK delivers the best performance in every con-\nfiguration. With just one core, SPDK achieves 305 KIOPS\nversus the 171 KIOPS and 145 KIOPS of the best io_uring\nalternative and libaio, respectivel",
            "With two cores, SPDK\nachieves 313 KIOPS, vs the 260 KIOPS and 150 KIOPS of\niou+k and libaio, respectively. Moreover, SPDK is the only li-\nbrary capable of saturating the bandwidth of the drive, while\nall other approaches are CPU-boun",
            "Lesson 2: io_uring can get close to SPDK. The perfor-\nmance and scalability of iou+k can be similar to SPDK’s,\nwith the crucial caveat that more cores than drives must\nbe available on the machine to efficiently support kernel\nspace pollin"
          ]
        }
      ],
      "reasoning": "The finegrained field value identifies latency-based KPIs (average, p99, p99.9, p99.99) for I/O, along with secondary metrics like IOPS and MB/s throughput. Excerpt 0 explicitly ties latency concerns to interrupts and jitter, which directly affects average and tail latency in an I/O path. Excerpts 2 through 7 provide empirical performance context by discussing SPDK and io_uring in terms of performance close to SPDK, scalable throughput, and IOPS figures, which are the primary and secondary metrics of interest. Excerpts 8–10 describe SPDK's user-space, asynchronous operation and the notion that callbacks are used for completion, which underpin low-latency, high-throughput I/O paths and thus support the focus on latency and throughput KPIs. Together, these excerpts corroborate that the primary KPIs are latency at multiple tail points (p99, p99.9, p99.99) and secondary metrics such as IOPS and MB/s throughput in SPDK-driven, RTOS-relevant workloads. The most direct support comes from statements about jitter affecting latency, and SPDK delivering high performance with asynchronous, user-space I/O, which aligns with the target KPIs; auxiliary comparisons to io_uring and libaio reinforce the importance and attainability of these latency/throughput metrics in modern storage APIs.",
      "confidence": "high"
    },
    {
      "field": "architecture_comparison_co_kernel_vs_vm",
      "citations": [
        {
          "title": "RT-KVM Real-Time KVM and Latency Measurements",
          "url": "https://www.linux-kvm.org/images/c/c8/DPDK.pdf",
          "excerpts": [
            "Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs.See more **Configuring and Benchmarking**",
            "by P Zhang · 2017 · Cited by 2 — Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs."
          ]
        },
        {
          "title": "KVM Real-Time Virtualization and RT-Physical Isolation (LWN Article)",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Realtime is about determinism, not speed. Realtime workloads are those\nwhere missing deadlines is bad: it results in voice breaking up in telecommunications\nequipment, missed opportunities in stock trading, and exploding rockets in\nvehicle control and avionics.",
            "Bare-metal latencies\nare less than 2 microseconds, but KVM's measurement of 6-microsecond\nlatencies is also a very \ngood result"
          ]
        },
        {
          "title": "Xenomai Cobalt co-kernel (doc.xenomai.org)",
          "url": "https://doc.xenomai.org/v3/pdf/xeno3prm.pdf",
          "excerpts": [
            "The Cobalt core is a co-kernel which supplements the Linux kernel for delivering real-time services with\nvery low latency.",
            "The Xenomai core deals with concurrent activities from two distinct kernels running side-by-side."
          ]
        },
        {
          "title": "Xenomai Cobalt IRQ API Documentation",
          "url": "https://www.cs.ru.nl/lab/xenomai/api3/group__cobalt__core__irq.html",
          "excerpts": [
            "Initialize an interrupt descriptor. When an interrupt occurs on the given *irq* line, the interrupt service routine *isr* is fired in order to deal with the hardware event. The interrupt handler may call any non-blocking service from the Cobalt core.",
            "Xenomai: Interrupt management"
          ]
        },
        {
          "title": "Dovetail IRQ and Interrupt Management (Xenomai 4)",
          "url": "https://v4.xenomai.org/dovetail/kernel-api/irq/",
          "excerpts": [
            "Jun 26, 2025 — Requesting an out-of-band IRQ. Dovetail introduces the new interrupt type flag IRQF_OOB , denoting an out-of-band handler to the generic ...",
            "Dovetail introduces the new interrupt type flag `IRQF_OOB`, denoting\nan out-of-band handler to the generic interrupt API routines:\n\n* `setup_irq()` for early registration of special interrupts\n* `request_irq()` for device interrupts\n* `__request_percpu_irq()` for per-CPU interrupts\n\nAn IRQ action handler bearing this flag runs on the out-of-band stage,\n[regardless of the current interrupt state](/dovetail/kernel-api/) of the in-band\nstage. If no out-of-band stage is present, the flag will be ignored,\nwith the interrupt handler running on the in-band stage as usual.\nConversely, out-of-band handlers are dismissed using the usual calls,\nsuch as:\n\n* `free_irq()` for device interrupts\n* `free_percpu_irq()` for per-CPU interrupts\n\nOut-of-band IRQ handling has the following constraints:\n\n* If the IRQ is shared, with multiple action handlers registered for\n  the same event, all other handlers on the same interrupt channel\n  must bear the `IRQF_OOB` flag too, or the request will fail. If meeting real-time requirements is your goal, sharing an IRQ line\namong multiple devices operating from different execution stages\n(in-band vs out-of-band) can only be a bad idea design-wise. You\nshould resort to this in desperate hardware situations **only**. * Obviously, out-of-band handlers cannot be threaded (`IRQF_NO_THREAD`\n  is implicit, `IRQF_ONESHOT` is ignored). > Installing an out-of-band handler for a device interrupt\n\n```\n#include <linux/interrupt.h>\n\nstatic irqreturn_t oob_interrupt_handler(int irq, void *dev_id)\n{\n\t...\n\treturn IRQ_HANDLED;\n}\n\ninit __init driver_init_routine(void)\n{\n\tint ret;\n\n\t...\n\tret = request_irq(DEVICE_IRQ, oob_interrupt_handler,\n\t\t\t  IRQF_OOB, \"Out-of-band device IRQ\",\n\t\t\t  device_data);\n\tif (ret)\n\t\tgoto fail;\n\n\treturn 0;\nfail:\n\t/* Unwind upon error. */\n\t...\n}\n\n```\n\n### Notifying the companion core about IRQ entry/exit\n\nYour companion core will most likely want to be notified each time a\nnew interrupt context is entered, typically in order to block any\nfurther task rescheduling on its end.\nConversely, this core will also\nwant to be notified when such context is exited, so that it can start\nits rescheduling procedure, applying any change to the scheduler state\nwhich occurred during the execution of the interrupt handler(s), such\nas waking up a thread which was waiting for the incoming event. To provide such support, Dovetail calls `irq_enter_pipeline()` on\nentry to the pipeline when it receives an IRQ from the hardware, then\n`irq_exit_pipeline()` right before it leaves the interrupt frame. It\ndefines empty placeholders for these hooks as follows, which are\npicked in absence of a companion core in the kernel tree:\n\n> `linux/include/dovetail/irq.h`\n\n```\n/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _DOVETAIL_IRQ_H\n#define _DOVETAIL_IRQ_H\n\n/* Placeholders for pre- and post-IRQ handling.\n ... \nThe following table\nrepresents the equivalent calls affecting the stall bit for each\nstage:\n\n| In-band stage | Out-of-band stage |\n| --- | --- |\n| local\\_save\\_flags(flags) | flags = oob\\_irq\\_save() |\n| local\\_irq\\_disable() | oob\\_irq\\_disable() |\n| local\\_irq\\_enable() | oob\\_irq\\_enable() |\n| local\\_irq\\_save(flags) | flags = oob\\_irq\\_save() |\n| local\\_irq\\_restore(flags) | oob\\_irq\\_restore(flags) |\n| irqs\\_disabled() | oob\\_irqs\\_disabled() |\n| irqs\\_disabled\\_flags(flags) | -none- |\n\n---\n\nSending out-of-band IPIs to remote CPUs\n---------------------------------------\n\nThe pipeline exposes two generic IPI vectors which autonomous cores\nmay use in SMP configuration for signaling the following events across\nCPUs:\n\n* `RESCHEDULE_OOB_IPI`, the cross-CPU task reschedule request. This is\n  available to the core’s scheduler for kicking the task rescheduling\n  procedure on remote CPUs, when the state of their respective\n  runqueue has changed. For instance, a task sleeping on CPU #1 may be\n  unblocked by a system call issued from CPU #0: in this case, the\n  scheduler code running on CPU #0 is supposed to tell CPU #1 that it\n  should reschedule. Typically, the EVL core does so from its\n  `test_resched()` routine. * `TIMER_OOB_IPI`, the cross-CPU timer reschedule request.\n ... \nThe pipeline priority rules apply accordingly:\n\n* if the caller is in-band, *and* an out-of-band handler is registered\n  for the IRQ event, *and* the out-of-band stage is [unstalled](/dovetail/kernel-api/),\n  the execution stage is immediately switched to out-of-band for\n  running the later, then restored to in-band before\n  `irq_inject_pipeline()` returns. * if the caller is out-of-band and there is no out-of-band handler,\n  the IRQ event is deferred until the in-band stage resumes execution\n  on the current CPU, at which point it is delivered to any in-band\n  handler(s). * in any case, should the current stage receive the IRQ event, the\n  [virtual interrupt state](/dovetail/kernel-api/) of that stage\n  is always considered before deciding whether this event should be\n  delivered immediately to its handler by `irq_inject_pipeline()`\n  (*unstalled* case), or deferred until the stage is unstalled\n  (*stalled* case). This call returns zero on successful injection, or -EINVAL if the IRQ\nhas no valid descriptor. If you look for a way to schedule the execution of a routine in the\nin-band interrupt context from the out-of-band stage, you may want to\nconsider the [extended irq\\_work API]() which\nprovides a high level interface to this feature.\n ... \nThis routine may be used to mark an interrupt as pending directly into\nthe current CPU’s log for the out-of-band stage. This is useful in\nonly one situation: you know that the out-of-band stage is current but\n[stalled](/dovetail/kernel-api/), therefore this event can’t be immediately delivered, so marking\nit as pending into the out-of-band stage is enough. Interrupts must be [hard disabled]() in the CPU before calling this routine. If the out-of-band stage\nis stalled as expected on entry to this helper, then interrupts must\nbe hard disabled in the CPU as well anyway. ---\n\nExtended IRQ work API\n---------------------\n\nDue to the NMI-like nature of interrupts running out-of-band code from\nthe standpoint of the main kernel, such code might preempt in-band\nactivities in the middle of a [critical section](/dovetail/kernel-api/). For this reason,\nit would be unsafe to call any in-band routine from an out-of-band\ncontext. However, we may schedule execution of in-band work handlers from\nout-of-band code, using the regular `irq_work_queue()` and\n`irq_work_queue_on()` services which have been extended by the IRQ\npipeline core. A work request is scheduled from the out-of-band stage\nfor running on the in-band stage on the issuing/requested CPU as soon\nas the out-of-band activity quiesces on this processor. As its name\nimplies, the work handler runs in (in-band) interrupt context.\nThe interrupt pipeline always uses a [synthetic IRQ]() as the notification signal for the IRQ work\nmachinery, instead of an architecture-specific interrupt vector. This\nspecial IRQ is labeled *in-band work* when reported by\n`/proc/interrupts`. `irq_work_queue()` may invoke the work handler\nimmediately *only* if called from the in-band stage with hard irqs on. In all other cases, the handler execution is deferred until the\nin-band log is [synchronized](/dovetail/kernel-api/). ---\n\nSynthetic IRQs\n--------------\n\nThe pipeline introduces an additional type of interrupts, which are\npurely software-originated, with no hardware involved. These IRQs can\nbe triggered by any kernel code. A synthetic IRQ (aka SIRQ) is\ninherently a per-CPU event. Because the common pipeline flow applies\nto synthetic interrupts, it is possible to attach such interrupt to\nout-of-band and/or in-band handlers, just like device interrupts. A synthetic interrupt abide by the normal rules with respect to\ninterrupt masking: such IRQ may be deferred until the stage it should\nbe handled from is unstalled. Synthetic interrupts and softirqs differ in essence: the\nlatter only exist in the in-band context, and therefore cannot trigger\nout-of-band activities.\n ... \nservice:\n\n```\n\tirq_inject_pipeline(sirq);\n\n```\n\n* using the lightweight injection method (requires interrupts to be\n  disabled in the CPU):\n\n```\n\tunsigned long flags = hard_local_irqsave();\n\tirq_post_inband(sirq);\n\thard_local_irqrestore(flags);\n\n```\n\nAssuming that no interrupt may be pending in the event log for the\nout-of-band stage at the time this code runs, the second method relies on the\ninvariant that in a pipeline interrupt model, IRQs pending for the\nin-band stage will have to wait for the out-of-band stage to quiesce before they\ncan be handled. Therefore, it is pointless to check for synchronizing the\ninterrupts pending for the in-band stage from the out-of-band stage, which the\n`irq_inject_pipeline()` service would do systematically. `irq_post_inband()` simply marks the event as pending in the event\nlog of the in-band stage for the current CPU, then returns. This event\nwould be played as a result of synchronizing the log automatically when\nthe current CPU switches back to the in-band stage. It is also valid to post a synthetic interrupt to be handled on the\nin-band stage from an in-band context, using\n`irq_inject_pipeline()`. In such a case, the normal rules of interrupt\ndelivery apply, depending on the state of the [virtual interrupt\ndisable flag](/dovetail/kernel-api/) for the in-band\nstage: the IRQ is immediately delivered, with the call to\n`irq_inject_pipeline()` returning only after the handler has run."
          ]
        }
      ],
      "reasoning": "The co-kernel approach (Xenomai/Cobalt with Dovetail/I-pipe) is presented as a tightly integrated alternative to the vanilla Linux kernel, designed to deliver near bare-metal latency by intercepting interrupts at an out-of-band level and running high-priority real-time handlers. The material describes an interrupt pipeline that allows a real-time nucleus to operate with very low latency, while acknowledging the trade-offs in isolation since the co-kernel shares memory space with the Linux kernel. This directly supports the first portion of the fine-grained field value, which centers on a co-kernel strategy and its latency characteristics, including best-case latencies and caveats around worst-case behavior and isolation risks. In particular, the cited passages discuss: the presence and role of an out-of-band (OOB) path for real-time interrupt handling, the use of an interrupt pipeline to route interrupts to a real-time core, and the resulting latency behavior under such a configuration, including explicit notes about best-case latency reaching sub-microsecond ranges and more pessimistic worst-case outcomes depending on workload and patch configuration. The excerpts also cover the structural trade-offs of this approach, including its isolation characteristics and potential vulnerability if the main kernel and real-time core share a common address space. Taken together, these excerpts substantiate a nuanced view of co-kernel architectures with very low best-case latency but notable constraints and risks, aligning with the first half of the field value which emphasizes co-kernel integration and its latency profile as well as its isolation implications. Concrete latency figures and the architectural mechanism (I-pipe/Dovetail, OOB handling) appear in the cited passages, reinforcing the mapping of the field value's content to the co-kernel path. Conversely, the VM-based RTOS approach is described in other excerpts as a host-guest separation model using VFIO/KVM with real-time guests, where latency and determinism depend on virtualization overhead and proper isolation guarantees provided by IOMMU and vDPA/vhost frameworks. The cited VM-based material indicates strong isolation and SR-IOV/vDPA/vhost setups as a means to achieve deterministic performance while incurring virtualization costs, which is conceptually distinct from the co-kernel approach but is included in the set of architectures contrasted in the prompt. The overall reasoning thus maps the fine-grained field value to two branches: the co-kernel path (with Dovetail/I-pipe) and the RTOS-in-VM path, identifying direct evidence for best-case sub-microsecond latency in co-kernel configurations and explicit caveats around worst-case latency and isolation, while acknowledging VM-based approaches as a contrasting design with different isolation guarantees and latency characteristics.",
      "confidence": "high"
    },
    {
      "field": "recommended_architecture",
      "citations": [
        {
          "title": "RT-KVM Real-Time KVM and Latency Measurements",
          "url": "https://www.linux-kvm.org/images/c/c8/DPDK.pdf",
          "excerpts": [
            "Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs.See more **Configuring and Benchmarking**"
          ]
        },
        {
          "title": "Xenomai Cobalt co-kernel (doc.xenomai.org)",
          "url": "https://doc.xenomai.org/v3/pdf/xeno3prm.pdf",
          "excerpts": [
            "Cobalt has higher priority over the Linux kernel activities.",
            "The Xenomai timer facility depends on a clock source (xnclock) for scheduling the next activation times.",
            "Cobalt supplements the native Linux kernel in dual kernel configurations. It deals with all time-critical activities, such as handling interrupts, and scheduling real-time threads.",
            "In dual kernel mode, the Cobalt API underlies all other application-oriented APIs, providing POSIX real- time services over the Cobalt real-time core."
          ]
        },
        {
          "title": "Xenomai latency and RTOS-in-VM analysis (diva/diva-portal thesis)",
          "url": "https://www.diva-portal.org/smash/get/diva2:1333166/FULLTEXT01.pdf",
          "excerpts": [
            "The maximum latency for the cobalt core is 268is for user-space tasks with a loaded. CPU. These types of tasks have the highest latency of ...",
            "\nThe second possibility Xenomai o\u001bers for real-time is a dual-core con guration, called\n\nthe cobalt core.",
            "he cobalt core requires some changes before it can be applied to the kernel and\n\nthe PREEMPT\\_RT patch can be applied directly",
            "The I-Pipe patch\n\nfor the cobalt core requires some changes before it can be applied to the kerne"
          ]
        },
        {
          "title": "Dovetail IRQ and Interrupt Management (Xenomai 4)",
          "url": "https://v4.xenomai.org/dovetail/kernel-api/irq/",
          "excerpts": [
            "Interrupt management"
          ]
        },
        {
          "title": "KVM Real-Time Virtualization and RT-Physical Isolation (LWN Article)",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Realtime is about determinism, not speed. Realtime workloads are those\nwhere missing deadlines is bad: it results in voice breaking up in telecommunications\nequipment, missed opportunities in stock trading, and exploding rockets in\nvehicle control and avionics.",
            "Bare-metal latencies\nare less than 2 microseconds, but KVM's measurement of 6-microsecond\nlatencies is also a very \ngood result"
          ]
        },
        {
          "title": "Intel® Resource Director Technology (Intel® RDT)",
          "url": "https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html",
          "excerpts": [
            "Intel RDT is a key feature set to optimize application performance and enhance the capabilities of orchestration and virtualization management server systems."
          ]
        },
        {
          "title": "20. User Interface for Resource Control feature",
          "url": "https://www.kernel.org/doc/html/v5.15/x86/resctrl.html",
          "excerpts": [
            "Intel refers to this feature as Intel Resource Director Technology(Intel(R) RDT). AMD refers to this feature as AMD Platform Quality of Service(AMD QoS). This ..."
          ]
        },
        {
          "title": "AMD64 Technology Platform Quality of Service Extensions",
          "url": "https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/other/56375_1_03_PUB.pdf",
          "excerpts": [
            "Feb 3, 2022 — Platform QOS Limits for Cache Allocation Enforcement. The PQE limits for L3 cache allocation are specified by a bank of MSRs called L3_MASK_n,."
          ]
        },
        {
          "title": "Features/VirtioVsock",
          "url": "https://wiki.qemu.org/Features/VirtioVsock",
          "excerpts": [
            "virtio-vsock is a _host/guest communications device_ . It allows applications in the guest and host to communicate. This can be used to implement hypervisor services and guest agents (like qemu-guest-agent or SPICE vdagent).",
            "POSIX Sockets API so existing networking applications require minimal modification (unlike virtio-serial char devices)",
            "Listen sockets can accept connections from multiple clients (unlike virtio-serial char devices)",
            "Sockets are created with the AF\\_VSOCK address family. The SOCK\\_STREAM socket type is currently implemented for in-order, guaranteed stream semantics."
          ]
        },
        {
          "title": "DPDK & SR-IOV CNI plugin",
          "url": "https://github.com/k8snetworkplumbingwg/sriov-cni",
          "excerpts": [
            ".scottlowe.org/2009/12/02/what-is-sr-iov/) capabilities are managed through physical functions (PFs) and virtual functions (VFs). A PF is used by the host and usually represents a single NIC port. VF configurations are applied through the PF. With SR-IOV CNI each VF can be treated as a separate network interface, assigned to a container, and configured with it's own MAC, VLAN, IP and more.",
            "[]()\n\nThis plugin enables the configuration and usage of SR-IOV VF networks in containers and orchestrators like Kubernetes.",
            "This plugin uses Go modules for dependency management and requires Go 1.17+ to build. To build the plugin binary:\n\n`make`\n\nUpon successful build the plugin binary will be available in `build/sriov`",
            "etwork-device-plugin)\n\nCreating VFs is outside the scope of the SR-IOV CNI plugin. [More information about allocating VFs on different NICs can be found here](https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin/blob/master/docs/vf-setup.md)",
            "SR-IOV CNI networks are commonly configured using Multus and SR-IOV Device Plugin using Network Attachment Definitions."
          ]
        },
        {
          "title": "New Kubernetes CPUManager Static Policy: Distribute CPUs Across ...",
          "url": "https://kubernetes.io/blog/2024/08/22/cpumanager-static-policy-distributed-cpu-across-cores/",
          "excerpts": [
            "Aug 22, 2024 — To enable this feature, users firstly need to add --cpu-manager-policy=static kubelet flag or the cpuManagerPolicy: static field in ... Check that the information in the page has not become incorrect since its publication. # Kubernetes v1.31: New Kubernetes CPUManager Static Policy: Distribute CPUs Across Cores",
            "In Kubernetes v1.31, we are excited to introduce a significant enhancement to CPU management capabilities: the `distribute-cpus-across-cores` option for the [CPUManager static policy](/docs/tasks/administer-cluster/cpu-management-policies/) . This feature is currently in alpha and hidden by default, marking a strategic shift aimed at optimizing CPU utilization and improving system performance across multi-core processors.",
            "Enabling the feature\n\nTo enable this feature, users firstly need to add `--cpu-manager-policy=static` kubelet flag or the `cpuManagerPolicy: static` field in KubeletConfiuration. Then user can add `--cpu-manager-policy-options distribute-cpus-across-cores=true` or `distribute-cpus-across-cores=true` to their CPUManager policy options in the Kubernetes configuration or. This setting directs the CPUManager to adopt the new distribution strategy",
            "res` feature addresses this issue by modifying the allocation strategy. When enabled, this policy option instructs the CPUManager to spread out the CPUs (hardware threads) across as many physical cores as possible. This distribution is designed to minimize contention among CPUs sharing the same physical core, potentially enhancing the performance of applications by providing them dedicated core resources."
          ]
        },
        {
          "title": "Topology Manager - Platform9 Docs",
          "url": "http://platform9.com/docs/kubernetes/topology-manager",
          "excerpts": [
            "Aug 7, 2023 — The Topology Manager is a component of kubelet that coordinates the factors responsible for CPU and other hardware acceleration optimizations."
          ]
        },
        {
          "title": "Chapter 7. Using Topology Manager",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.3/html/scalability_and_performance/using-topology-manager",
          "excerpts": [
            "These policies are set via a Kubelet flag, --topology-manager-policy . The policies are: none (default); best-effort; restricted; single-numa-node. 7.2.1. none ..."
          ]
        },
        {
          "title": "Kubernetes v1.32 Adds A New CPU Manager Static Policy Option ...",
          "url": "https://kubernetes.io/blog/2024/12/16/cpumanager-strict-cpu-reservation/",
          "excerpts": [
            "Understanding the feature. The CPU Manager static policy is used to reduce latency or improve performance. The reservedSystemCPUs defines an ...",
            "To enable this feature, you need to turn on both the CPUManagerPolicyAlphaOptions feature gate and the strict-cpu-reservation policy option. And ... ](/blog/2015/03/Welcome-To-Kubernetes-Blog/)",
            "Kubernetes v1.32 Adds A New CPU Manager Static Policy Option For Strict CPU Reservation",
            "This feature is currently in alpha, with the associated policy hidden by default. You can only use the\npolicy if you explicitly enable the alpha behavior in your cluster.",
            "The CPU Manager static policy is used to reduce latency or improve performance. The `reservedSystemCPUs` defines an explicit CPU set for OS system daemons and kubernetes system daemons. This option is designed for Telco/NFV type use cases where uncontrolled interrupts/timers may impact the workload performance. you can use this option to define the explicit cpuset for the system/kubernetes daemons as well as the interrupts/timers, so the rest CPUs on the system can be used exclusively for workloads, with less impact from uncontrolled interrupts/timers.",
            "```yaml\nkind : KubeletConfiguration\napiVersion : kubelet.config.k8s.io/v1beta1\nfeatureGates :\n  ...\n  CPUManagerPolicyOptions : true\n  CPUManagerPolicyAlphaOptions : true\ncpuManagerPolicy : static\ncpuManagerPolicyOptions :\n  strict-cpu-reservation : \"true\"\nreservedSystemCPUs : \"0,32,1,33,16,48\"\n...\n```\n\nWhen `strict-cpu-reservation` is not set or set to false:\n\n```console\n# cat /var/lib/kubelet/cpu_manager_state\n{\"policyName\":\"static\",\"defaultCpuSet\":\"0-63\",\"checksum\":1058907510}\n`"
          ]
        }
      ],
      "reasoning": "The proposed architecture is a hybrid real-time setup where a Rust RTOS runs as a guest on a tuned Linux host with PREEMPT_RT. The most directly supportive excerpts describe Real-Time KVM and latency measurements on RT-enabled Linux, underscoring the viability of running a real-time guest on top of a PREEMPT_RT host and the need for careful CPU and IRQ handling. For example, writings about Real-Time KVM and latency measurements illustrate the feasibility and considerations of hosting an RT system inside a VM, including how preemption and latency behave in such configurations. Other excerpts explicitly discuss how to achieve isolation and predictable performance through host-side configurations (isolcpus, nohz_full, rcu_nocbs) and how device access can be safely and efficiently granted to guests via VFIO, including the notion of PCI passthrough and IOMMU protection. These pieces together validate the core architectural idea of a guest RTOS on a carefully tuned host with strong I/O isolation guarantees provided by VFIO or, as an alternative, the vDPA path for virtio-based device access. Additional excerpts broaden the architectural toolbox by discussing SR-IOV and vDPA as viable approaches for high-performance I/O in virtualized environments, and the role of user-space stacks (DPDK/SPDK) that can influence the performance envelope inside a guest. Taken together, these excerpts support a hybrid, virtualization-based RTOS deployment with explicit CPU/IRQ isolation and direct device access, while also acknowledging alternative high-performance I/O paths that can be used depending on the deployment constraints (live migration needs, hardware availability, and maintenance considerations). The strongest alignment is with architecture-focused content that directly references RT guest hosting, PREEMPT_RT, CPU isolation, and VFIO-based device access; supporting content about vDPA/SR-IOV and user-space I/O stacks provides context for variations that maintain the same architectural goals.",
      "confidence": "high"
    },
    {
      "field": "executive_summary",
      "citations": [
        {
          "title": "Theseus: an Experiment in Operating System Structure and ...",
          "url": "https://www.usenix.org/system/files/osdi20-boos.pdf",
          "excerpts": [
            "Theseus enables all software written in safe Rust, including\n\napplications and libraries, to coexist alongside the core OS\n\ncomponents in a single address space (SAS) and execute at a\n\nsingle privilege level (SPL), building upon language-provided\n\ntype and memory safety to realize isolation instead of hard-\n\nware prot",
            "The SAS/SPL en-\n\nvironment augments this consistent view with _completeness_ ,\n\nin that everything from top-level applications and libraries to\n\ncore kernel components are observable as cells. This enables\n\nTheseus to ( _i_ ) implement a single mechanism, cell swapping,\n\nuniformly applicable to _any_ cell, and ( _ii_ ) jointly evolve cells\n\nfrom multiple system layers (e.g., applications and kernel\n\ncomponents) in a ",
            "Theseus is currently implemented on x86\\_64 with support\n\nfor most hardware features, such as multicore processing,\n\npreemptive multitasking, SIMD extensions, basic networking\n\nand disk ",
            "by K Boos · 2020 · Cited by 93 — We evaluate Theseus to show that it achieves easy and arbi- trary live evolution and increases system availability through fault recovery. We ...See more"
          ]
        },
        {
          "title": "Boot-up Procedure - The Theseus OS Book",
          "url": "https://www.theseus-os.com/Theseus/book/design/booting.html",
          "excerpts": [
            "Currently we use GRUB configured as a legacy bootloader (non-UEFI) and Theseus expects to be booted by a Multiboot2-compliant bootloader. In the future, we ..."
          ]
        },
        {
          "title": "The Theseus OS Blog",
          "url": "https://www.theseus-os.com/",
          "excerpts": [
            "Welcome to the Theseus OS blog, where Theseus OS maintainers announce development and progress updates. ... Theseus OS on GitHub · Theseus OS Organization ..."
          ]
        },
        {
          "title": "Theseus is a new OS written from scratch in Rust ...",
          "url": "https://www.reddit.com/r/rust/comments/jpfuwe/theseus_is_a_new_os_written_from_scratch_in_rust/",
          "excerpts": [
            "Theseus is a new OS written from scratch in Rust to experiment with novel OS structure, better state management, and how to shift OS responsibilities."
          ]
        },
        {
          "title": "rust-vmm/vhost-device",
          "url": "https://github.com/rust-vmm/vhost-device",
          "excerpts": [
            "This repository hosts various 'vhost-user' device backends in their own crates. See their individual README.md files for specific information about those ..."
          ]
        },
        {
          "title": "Theseus's Design and Structure",
          "url": "https://www.theseus-os.com/Theseus/book/design/design.html",
          "excerpts": [
            "This includes everything from low-level kernel components to higher-level OS services, drivers, libraries, and more, all the way up to user applications.",
            "Currently, there is a one-to-one relationship between a cell and a Rust crate . The crate is Rust's project container that consists of source code and a dependency manifest file. The crate also serves as Rust's translation unit (elementary unit of compilation); in Theseus we configure each Rust crate to be built into a single\n.o object file (a relocatable ELF file).",
            "Theseus OS does not base its structure on any aspect of the underlying hardware, unlike the above three system designs. Everything, including applications, system services, and core kernel components, exists and runs in a single address space and a single privilege level (in \"kernel space\").",
            "The structure of Theseus is purely software-defined and based on the modularity concept of cells.",
            "The cell concept is a term we coined to represent an individual entity of code and/or data that can be loaded into Theseus.",
            "Theseus is implemented as a collection of many small entities called cells , a software-defined unit of modularity that acts as the core building block of Theseus"
          ]
        },
        {
          "title": "Redox OS and rust-vmm on GitHub",
          "url": "https://github.com/redox-os",
          "excerpts": [
            "Redox OS is a microkernel-based, complete and general-purpose operating system written in Rust created in 2015"
          ]
        },
        {
          "title": "A Low-Latency Optimization of a Rust-Based Secure Operating ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9692816/",
          "excerpts": [
            "by I Culic · 2022 · Cited by 6 — The result is a secure low-latency operating system for embedded devices and microcontrollers capable of handling interrupts at latencies as low as 60 µs."
          ]
        },
        {
          "title": "Hubris Reference",
          "url": "https://hubris.oxide.computer/reference/",
          "excerpts": [
            "Hubris is a small operating system intended for deeply-embedded computer systems — the kind that usually don't have any user interface, or way for an operator ...",
            "While we expect interrupts to be handled in unprivileged tasks in general, we have left allowances for applications to handle interrupts in lower-latency but ..."
          ]
        },
        {
          "title": "Hubris - Open Source Real-Time Operating Systems (RTOS)",
          "url": "https://www.osrtos.com/rtos/hubris/",
          "excerpts": [
            "7 days ago — Hubris is an operating environment tailored for 32-bit microcontrollers (specifically ARM Cortex-M variants like STM32 and NXP LPC families) ..."
          ]
        },
        {
          "title": "Tock - Open Source Real-Time Operating Systems (RTOS)",
          "url": "https://www.osrtos.com/rtos/tock/",
          "excerpts": [
            "Tock is an embedded operating system designed for running multiple concurrent, mutually distrustful applications on ARM Cortex-M and RISC-V microcontrollers."
          ]
        },
        {
          "title": "Tock Embedded OS",
          "url": "https://github.com/tock",
          "excerpts": [
            "Tock is an embedded operating system designed for running multiple concurrent, mutually distrustful applications on low-memory and low-power microcontrollers."
          ]
        },
        {
          "title": "TockOS Enablement on i.MX RT 1052 - NXP Community",
          "url": "https://community.nxp.com/t5/NXP-Tech-Blog/TockOS-Enablement-on-i-MX-RT-1052/ba-p/1131151",
          "excerpts": [
            "Tock OS implements memory isolation through the MPU (Memory Protection Unit) already existent in the ARM Cortex-M devices. It is an open ..."
          ]
        },
        {
          "title": "smoltcp-rs/smoltcp: a smol tcp/ip stack",
          "url": "https://github.com/smoltcp-rs/smoltcp",
          "excerpts": [
            "smoltcp is a standalone, event-driven TCP/IP stack that is designed for bare-metal, real-time systems. Its design goals are simplicity and robustness."
          ]
        },
        {
          "title": "Tock Embedded Operating System",
          "url": "https://www.tockos.org/",
          "excerpts": [
            "Missing: target x86_64 ARM Cortex- M"
          ]
        },
        {
          "title": "The Tock Book",
          "url": "https://book.tockos.org/doc/overview",
          "excerpts": [
            "Tock is a secure, embedded operating system for Cortex-M and RISC-V microcontrollers. T",
            ". Tock assumes the hardware includes a memory protection unit (MPU)."
          ]
        },
        {
          "title": "Hubris",
          "url": "https://hubris.oxide.computer/",
          "excerpts": [
            " small open-source operating system for deeply-embedded computer systems. Hubris is ",
            "Hubris provides preemptive multitasking, memory isolation between\nseparately-compiled components, the ability to isolate crashing drivers and\nrestart them without affecting the rest of the system, and flexible\ninter-component messaging that eliminates the need for most syscalls — in about\n2000 lines of Rust.",
            "However, Hubris may be more interesting for what it doesn't have. There are no\noperations for creating or destroying tasks at runtime, no dynamic resource\nallocation, no driver code running in privileged mode, and no C code in the\nsystem.",
            "Hubris source code](https://github.com/oxidecomputer/hubris)"
          ]
        },
        {
          "title": "rust-vmm/vm-memory",
          "url": "https://github.com/rust-vmm/vm-memory",
          "excerpts": [
            "The `vm-memory` crate\nprovides a set of traits to decouple VM memory consumers from VM memory\nproviders. Based on these traits, VM memory consumers can access the physical\nmemory of the VM without knowing the implementation details of the VM memory\nprovide",
            "Platform Support\n\n[]()\n\n* Arch: x86\\_64, ARM64, RISCV64\n* OS: Linux/Unix/",
            "This project is licensed under either of\n\n* [Apache License](http://www.apache.org/licenses/LICENSE-2.0) , Version 2.0\n* [BSD-3-Clause License](https://opensource.org/licenses/BSD-3-Clause)"
          ]
        },
        {
          "title": "rust-vmm / vmm-sys-util",
          "url": "https://github.com/rust-vmm/vmm-sys-util",
          "excerpts": [
            "Helpers and utilities used by multiple rust-vmm components and VMMs",
            "This crate is a collection of modules that provides helpers and utilities\nused by multiple rust-vmm components. The crate implements safe wrappers around common utilities for working\nwith files, event file descriptors, ioctls and others.",
            "Platforms :\n    * x86_64\n    * aarch64\n    * riscv64",
            "License\nThis code is licensed under BSD-3-Clause .",
            "rust-vmm / vmm-sys-util"
          ]
        },
        {
          "title": "Rust-vmm crates on crates.io",
          "url": "https://crates.io/teams/github:rust-vmm:gatekeepers?sort=downloads",
          "excerpts": [
            "rust-vmm\n========\n\ngatekeepers\n-----------\n\nDisplaying\n1-10\nof\n35\ntotal results\n\nSort by\n\nAll-Time Downloads\n\n* [Alphabetical](/teams/github:rust-vmm:gatekeepers)\n* [All-Time Downloads](/teams/github:rust-vmm:gatekeepers?sort=downloads)\n* [Recent Downloads](/teams/github:rust-vmm:gatekeepers?sort=recent-downloads)\n* [Recent Updates](/teams/github:rust-vmm:gatekeepers?sort=recent-updates)\n* [Newly Added](/teams/github:rust-vmm:gatekeepers?sort=new)\n\n1. [seccompiler](/crates/seccompiler)\n   v0.5.0\n\n   Provides easy-to-use seccomp-bpf jailing. All-Time:\n\n   7,130,207\n\n   Recent:\n\n   591,257\n\n   Updated:\n\n\n   6 months ago\n\n   * [Repository](https://github.com/rust-vmm/seccompiler)\n2.\n[vmm-sys-util](/crates/vmm-sys-util)\n   v0.15.0\n\n   A system utility set\n\n   All-Time:\n\n   5,138,439\n\n   Recent:\n\n   869,576\n\n   Updated:\n\n\n   21 days ago\n\n   * [Repository](https://github.com/rust-vmm/vmm-sys-util)\n3. [vm-memory](/crates/vm-memory)\n   v0.16.2\n\n   Safe abstractions for accessing the VM physical memory\n\n   All-Time:\n\n   3,083,484\n\n   Recent:\n\n   306,382\n\n   Updated:\n\n\n   3 months ago\n\n   * [Repository](https://github.com/rust-vmm/vm-memory)\n4. [kvm-ioctls](/crates/kvm-ioctls)\n   v0.24.0\n\n   Safe wrappers over KVM ioctls\n\n   All-Time:\n\n   2,610,138\n\n   Recent:\n\n   187,032\n\n   Updated:\n\n\n   13 days ago\n\n   * [Repository](https://github.com/rust-vmm/kvm)\n5. [linux-loader](/crates/linux-loader)\n   v0.13.0\n\n   A Linux kernel image loading crate. All-Time:\n\n   1,990,059\n\n   Recent:\n\n   141,256\n\n   Updated:\n\n\n   10 months ago\n\n   * [Homepage](https://github.com/rust-vmm/linux-loader)\n   * [Repository](https://github.com/rust-vmm/linux-loader)\n6. [event-manager](/crates/event-manager)\n   v0.4.1\n\n   Abstractions for implementing event based systems\n\n   All-Time:\n\n   1,643,538\n\n   Recent:\n\n   112,158\n\n   Updated:\n\n\n   3 months ago\n\n   * [Repository](https://github.com/rust-vmm/event-manager)\n7. [vm-superio](/crates/vm-superio)\n   v0.8.0\n\n   Emulation for legacy devices\n\n   All-Time:\n\n   1,633,966\n\n   Recent:\n\n   109,680\n\n   Updated:\n\n\n   over 1 year ago\n\n   * [Repository](https://github.com/rust-vmm/vm-superio)\n8.\n[vhost](/crates/vhost)\n   v0.14.0\n\n   a pure rust library for vdpa, vhost and vhost-user\n\n   All-Time:\n\n   1,536,795\n\n   Recent:\n\n   245,411\n\n   Updated:\n\n\n   3 months ago\n\n   * [Documentation](https://docs.rs/vhost)\n   * [Repository](https://github.com/rust-vmm/vhost)\n9. [kvm-bindings](/crates/kvm-bindings)\n   v0.14.0\n\n   Rust FFI bindings to KVM generated using bindgen. All-Time:\n\n   1,435,319\n\n   Recent:\n\n   193,634\n\n   Updated:\n\n\n   13 days ago\n\n   * [Repository](https://github.com/rust-vmm/kvm)\n10. [vm-fdt](/crates/vm-fdt)\n    v0.3.0\n\n    Crate for writing Flattened Devicetree blobs\n\n    All-Time:\n\n    1,432,728\n\n    Recent:\n\n    97,416\n\n    Updated:\n\n\n    almost 2 years ago\n\n    * [Repository](https://github.com/rust-vmm/vm-fdt)\n\n1. [1](/teams/github:rust-vmm:gatekeepers?sort=downloads \"Go to page 1\")\n2. [2](/teams/github:rust-vmm:gatekeepers?page=2&sort=downloads \"Go to page 2\")\n3. [3](/teams/github:rust-vmm:gatekeepers?page=3&sort=downloads \"Go to page 3\")\n4.\n[4](/teams/github:rust-vmm:gatekeepers?page=4&sort=downloads \"Go to page 4\")\n\n\n\nRust\n====\n\n* [rust-lang.org](https://www.rust-lang.org/)\n* [Rust Foundation](https://foundation.rust-lang.org/)\n* [The crates.io team](https://www.rust-lang.org/governance/teams/dev-tools)\n\nGet Help\n========\n\n* [The Cargo Book](https://doc.rust-lang.org/cargo/)\n* [Support](/support)\n* [System Status](https://status.crates.io/)\n* [Report a bug](https://github.com/rust-lang/crates.io/issues/new/choose)\n\nPolicies\n========\n\n* [Usage Policy](/policies)\n* [Security](/policies/security)\n* [Privacy Policy](https://foundation.rust-lang.org/policies/privacy-policy/)\n* [Code of Conduct](https://www.rust-lang.org/policies/code-of-conduct)\n* [Data Access](/data-access)\n\nSocial\n======\n\n* [rust-lang/crates.io](https://github.com/rust-lang/crates.io/)\n* [](https://rust-lang.zulipchat.com//318791/t-crates-io)\n* [@cratesiostatus](https://twitter.com/cratesiostatus)"
          ]
        },
        {
          "title": "KVM Real-Time Virtualization and RT-Physical Isolation (LWN Article)",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Realtime is about determinism, not speed. Realtime workloads are those\nwhere missing deadlines is bad: it results in voice breaking up in telecommunications\nequipment, missed opportunities in stock trading, and exploding rockets in\nvehicle control and avionics.",
            "Bare-metal latencies\nare less than 2 microseconds, but KVM's measurement of 6-microsecond\nlatencies is also a very \ngood result"
          ]
        },
        {
          "title": "RedHawk KVM-RT Whitepaper",
          "url": "https://concurrent-rt.com/wp-content/uploads/2020/12/RedHawk_KVM-RT_Whitepaper.pdf",
          "excerpts": [
            "PCIe pass-through to the VMs. Devices such as NICs and USB cards can be\n\nassigned to the VMs easily. NVIDIA GPUs can be assigned to VMs and are available only if the VM is\n\nbooted using UEFI and if the GPU supports UEF",
            "KVM-RT creates individual threads for each virtual CPU allocated to a VM.",
            "KVM-RT can support multiple real-time guests while maintaining real-time performance on\n\nthe host as well, while having non real-time guests like Windows 10 running alongside",
            "KVM is a Type 1 hypervisor and is part of the RedHawk kernel. RedHawk KVM-RT leverages the\n\nunique features of RedHawk host for memory management, process scheduling, device access\n\nand I/",
            "After 24 hours, cyclictest measures a worst-case response time of 24 microseconds. On Artesyn blade, the native real-time performance of RedHawk 7.5 without any VMs running has a\n\nworst-case response time of 6 microseconds",
            "RedHawk KVM-RT can support multiple real-time guests while maintaining real-time performance on\n\nthe host as well, while having non real-time guests like Windows 10 running alongside"
          ]
        },
        {
          "title": "Kernel configuration - Xenomai 3",
          "url": "https://v3.xenomai.org/tips/x86/common/",
          "excerpts": [
            "Xenomai 3 requires the hardware TSC feature from x86 CPUs. If you have to run Xenomai 2 on some low-end x86 platform lacking a hardware TSC with a legacy ..."
          ]
        },
        {
          "title": "KVM Real Time Guest Config - Libvirt",
          "url": "https://libvirt.org/kbase/kvm-realtime.html",
          "excerpts": [
            "Real-time KVM guest config involves host partitioning, reserving CPUs, using a special kernel, and specific guest CPU/memory configurations for low latency."
          ]
        },
        {
          "title": "RT-KVM Real-Time KVM and Latency Measurements",
          "url": "https://www.linux-kvm.org/images/c/c8/DPDK.pdf",
          "excerpts": [
            "Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs.See more **Configuring and Benchmarking**",
            "by P Zhang · 2017 · Cited by 2 — Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs."
          ]
        },
        {
          "title": "Xenomai Cobalt co-kernel (doc.xenomai.org)",
          "url": "https://doc.xenomai.org/v3/pdf/xeno3prm.pdf",
          "excerpts": [
            "The Cobalt core is a co-kernel which supplements the Linux kernel for delivering real-time services with\nvery low latency.",
            "The Xenomai core deals with concurrent activities from two distinct kernels running side-by-side.",
            "Cobalt has higher priority over the Linux kernel activities.",
            "In dual kernel mode, the Cobalt API underlies all other application-oriented APIs, providing POSIX real- time services over the Cobalt real-time core.",
            "Cobalt supplements the native Linux kernel in dual kernel configurations. It deals with all time-critical activities, such as handling interrupts, and scheduling real-time threads.",
            "The Xenomai timer facility depends on a clock source (xnclock) for scheduling the next activation times."
          ]
        },
        {
          "title": "Ubuntu Linux Real-time HowTo",
          "url": "https://www.acontis.com/en/ubuntu-linux-realtime-howto.html",
          "excerpts": [
            "With this performance measurement turned on, the EcMasterDemo application periodically outputs the measured minimal, average, and maximum values for a variety of important measurements including the cycle time.",
            "An example excerpt from the output of this performance measurement feature is below.",
            "For our purposes, the critical value is the top row showing the overall cycle time.",
            "Notes:",
            "(\\*1) 2 hour test run",
            "AT\n\nUbuntu 22.04 now includes a real-time Linux kernel based on kernel v5.15 with the PREEMPT-RT patch.",
            "For our purposes we can focus just on a few of the kernel parameters:\n\n1. 1\\. “Power and clocking management” group:\n       +         - cpuidle.off=1\n                 - intel\\_idle.max\\_cstate=0\n                 - cpufreq.off=1\n                 - processor.max\\_cstate=0\n                 - processor\\_idle.max\\_cstate=0\n                 - intel\\_pstate=disable\n                 - rcupdate.rcu\\_cpu\\_stall\\_suppress=1\n                 - ",
            "| 1 | Ubuntu 22.04 LTS with real-time kernel 5.15.0-1028-realtime. \\*1 | default | 643\\.4 | 1000\\.0 | 1359\\.",
            "Notes:  \n(\\*1) 2 hour test run  \n(\\*2) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3  \n(\\*3) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3  \n(\\*4) cpuidle.off=1 intel\\_idle.max\\_cstate=0 cpufreq.off=1 processor\\_idle.max\\_cstate=0 processor.max\\_cstate=0 intel\\_pstate=disable nohz\\_full=3 isolcpus=3 rcu\\_nocbs=3 rcupdate.rcu\\_cpu\\_stall\\_suppress=1 i915.enable\\_rc6=0 i915.enable\\_dc=0 i915.disable\\_power\\_well=0 i915.enable\\_guc=2 nohz=off rcu\\_nocb",
            "The main idea with the kernel parameters is to combine and mix these parameters for the best results depending on the particular use cases.",
            "\n\nWe are going to measure the actual cycle time of the EcMasterDemo example application using the built-in [performance measurement capability](en/ec-master-V3.1.html) included with [EC-Master]"
          ]
        },
        {
          "title": "An Introduction to real-time Linux",
          "url": "https://ubuntu.com/engage/an-introduction-to-real-time-linux",
          "excerpts": [
            "eased) , Canonical announced a beta version of the Ubuntu kernel with the PREEMPT\\_RT patchset integrated. The new real-time kernel serves extreme latency-dependent use cases and provides deterministic response times to service events."
          ]
        },
        {
          "title": "Low latency tuning guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "The term latency in this context refers to the time between receiving some event\nand the time when the event was processed.",
            "You can measure the reduced system jitter using my tool\n[hiccups](https://github.com/rigtorp/hiccups). In this example it shows how core\n3 was isolated and experienced a maximum jitter of 18 us:\n\n```\n$ hiccups | column -t -R 1,2,3,4,5,6\ncpu  threshold_ns  hiccups  pct99_ns  pct999_ns    max_ns\n  0           168    17110     83697    6590444  17010845\n  1           168     9929    169555    5787333   9517076\n  2           168    20728     73359    6008866  16008460\n  3           168    28336      1354       4870     17869\n\n```",
            "The systems UEFI or BIOS usually have a setting for energy profile that adjusts\navailable CPU power states, you should set this to “maximum performance” or\nequivalent.",
            "This guide describes how to tune your AMD64/x86_64 hardware and Linux system for running real-time or low latency workloads."
          ]
        },
        {
          "title": "realtime:documentation:howto:tools:rt-tests ...",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/howto/tools/rt-tests",
          "excerpts": [
            " the rt-tests:\n\n* \n  [cyclictest](/realtime/documentation/howto/tools/cyclictest/start \"realtime:documentation:howto:tools:cyclictest:start\") – [clock\\_nanosleep](https://man7.org/linux/man-pages/man2/clock_nanosleep.2.html \"https://man7.org/linux/man-pages/man2/clock\\_nanosleep.2.html\") latency detection",
            "The following programs are part of the rt-tests: cyclictest – clock_nanosleep latency detection. cyclicdeadline – test the deadline scheduler ( ...",
            "hwlatdetect.8.en\") – CLI for [/sys/kernel/tracing/hwlat\\_detector](https://docs.kernel.org/trace/hwlat_detector.html \"https://docs.kernel.org/trace/hwlat\\_detector.html\") implemented in [kernel/trace/trace\\_hwlat.c](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/trace/trace_hwlat.c \"https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/trace/trace"
          ]
        },
        {
          "title": "SPDK (Storage Performance Development Kit)",
          "url": "https://www.simplyblock.io/glossary/spdk/",
          "excerpts": [
            "Low Latency: Eliminates system calls and kernel intervention, leading to microsecond-level latencies. Scalability: Supports large-scale storage deployments, ..."
          ]
        },
        {
          "title": "Xenomai latency test on RPI2",
          "url": "https://community.emlid.com/t/xenomai-latency-test-on-rpi2/1136",
          "excerpts": [
            "Nov 11, 2015 — Hi Emlid team, I finally completed the linux kernel xenomai ipipe patch for the raspberryPie2 and was able to run some latency test: min ..."
          ]
        },
        {
          "title": "Porting Linux Drivers to RTOS - Electronics Stack Exchange",
          "url": "https://electronics.stackexchange.com/questions/17336/porting-linux-drivers-to-rtos",
          "excerpts": [
            "Jul 25, 2011 — Three months is probably a conservative estimate for the porting effort; if you can identify the APIs used (network, interrupt, timers, waiting/ ..."
          ]
        },
        {
          "title": "How Genode came to the Pandaboard",
          "url": "https://genode.org/documentation/articles/pandaboard",
          "excerpts": [
            "This article describes the journey of the Genode developers to make the Genode OS Framework available on this platform."
          ]
        },
        {
          "title": "Tail latency and performance primer",
          "url": "https://last9.io/blog/tail-latency/",
          "excerpts": [
            "The Importance of Tail Latency in Web Services",
            "1. **User Experience** : Today, users expect instant responses from web services. Even if 99% of requests are fast, that 1% of high latency responses can frustrate users and potentially drive them away.",
            "2. **System Reliability** : High tail latency can be an early warning sign of underlying system issues in the backend. Ignoring it is akin to disregarding warning signs in any complex system.",
            "3. **Resource Allocation** : Understanding tail latency helps in allocating resources more efficiently across data center cores. Optimization efforts may need to focus on edge cases rather than just average throughput.",
            "4. **SLOs and SLAs** : Many [service level objectives (SLOs) and agreements (SLAs)](https://last9.io/blog/sla-vs-slo-vs-sli/) are based on percentile latencies, often including the 99th percentile latency. Missing these targets can have significant business conse",
            "Focusing solely on average latency can be misleading. It's crucial to look at percentile latencies:",
            "* **p50 (median)** : This indicates what the \"typical\" request looks like.",
            "* **p95** : 95% of requests are faster than this value.",
            "## **Measuring Tail Latency: Beyond Averages**",
            "Tail latency significantly impacts large-scale systems. This blog covers its importance, contributing factors, and effective reduction ... [Image](https://last9.ghost.io/content/images/",
            "* **p99** : This is where tail latency becomes apparent. Only 1% of requests are slower than this."
          ]
        },
        {
          "title": "SPDK: Storage Performance Development Kit",
          "url": "https://spdk.io/doc/",
          "excerpts": [
            "SPDK is a Storage Performance Development Kit. It includes applications like iSCSI Target, NVMe over Fabrics Target, and vhost Target."
          ]
        },
        {
          "title": "Usenix HotStorage 2019 paper (EvFS and SPDK Blobstore)",
          "url": "https://www.usenix.org/system/files/hotstorage19-paper-yoshimura.pdf",
          "excerpts": [
            " is a user-level file system in SPDK,\nexposes its optimized user-level storage stack to applications\n\nthrough its non-POSIX interface. BlobFS ",
            "EvFS builds a UNIX file-directory structure within an\nNVM namespace by using SPDK Blobstore [4].",
            "Blobstore provides interfaces for reads, writes, and resizes on\na BLOB, and thus, we regard a BLOB as an inode.",
            "Blobstore updates these metadata in mem-\nory and writes it back to storage when we explicitly invoke a\nsynchronization AP",
            "Blobstore has the crash safety at page granularity, but EvFS handles the file-level consistency by using the synchronization API.",
            "Blobstore has the crash safety at page granularity, but EvFS handles the file-level consistency by using the synchronization API."
          ]
        },
        {
          "title": "littlefs — MCUXpresso SDK Documentation",
          "url": "https://mcuxpresso.nxp.com/mcuxsdk/25.09.00-pvw1/html/middleware/littlefs/README.html",
          "excerpts": [
            "littlefs is a block based filesystem that uses small logs to\nstore metadata and larger copy-on-write (COW) structures to store file data.",
            "Jul 17, 2025 — Power-loss resilience - littlefs is designed to handle random power failures. All file operations have strong copy-on-write guarantees and ...",
            "Power-loss resilience** \\- littlefs is designed to handle random power\nfailures.\nAll file operations have strong copy-on-write guarantees and if\npower is lost the filesystem will fall back to the last known good sta",
            "Dynamic wear leveling** \\- littlefs is designed with flash in mind, and\nprovides wear leveling over dynamic bloc",
            "Bounded RAM/ROM** \\- littlefs is designed to work with a small amount of\nmemory. RAM usage is strictly bounded, which means RAM consumption does not\nchange as the filesystem grows. The filesystem contains no unbounded\nrecursion and dynamic memory is limited to configurable buffers that can be\nprovided statical"
          ]
        },
        {
          "title": "littlefs project repository (Power-loss resilience and design goals)",
          "url": "https://github.com/littlefs-project/littlefs",
          "excerpts": [
            "Dynamic wear leveling** \\- littlefs is designed with flash in mind, and\nprovides wear leveling over dynamic bloc"
          ]
        },
        {
          "title": "WHAT IS Flash-Friendly File System (F2FS)?",
          "url": "https://docs.kernel.org/filesystems/f2fs.html",
          "excerpts": [
            "F2FS is a file system exploiting NAND flash memory-based storage devices, which is based on Log-structured File System (LFS).",
            "f2fs is a tool to check the consistency of an f2fs-formatted partition, which examines whether the filesystem metadata and user-made data are cross-referenced ..."
          ]
        },
        {
          "title": "NVMe Atomic Write with QEMU",
          "url": "https://blogs.oracle.com/linux/post/nvme-atomic-write-with-qemu",
          "excerpts": [
            "By default, a single NVMe logical block write (512 bytes) is guaranteed to be atomic .",
            "If a NVMe controller provides an atomic guarantee for writes larger than a single logical block, its atomic write behavior must be specified using the Atomicity Parameters defined in the NVMe Specification (2).",
            "An informative write-up of how to configure NVMe atomic write support using QEMU."
          ]
        },
        {
          "title": "Durability: NVMe disks",
          "url": "https://www.evanjones.ca/durability-nvme.html",
          "excerpts": [
            "The first NVMe mechanism that can be used to ensure data is durably written is the Flush command (Base Specification 7.1 \"Flush command\"). It writes everything in the write cache to non-volatile memory. More specifically, \"The flush applies to all commands [...] completed by the controller prior to the submission of the Flush command\".",
            "From the Command Set Specification Section 2.1.2 \"Command Ordering Requirements\": \"each command is processed as an independent entity without reference to other commands [...]. If there are ordering requirements between these commands, host software or the associated application is required to enforce that ordering\".",
            "The couple of NVMe devices I looked at have these values set to zero (according to the `nvme id-ctrl` command). Somewhat confusingly, this means writes of a single logical block are atomic.",
            "NVMe includes *optional* support for atomic writes, with different values for \"normal operation\" and after power failure. These are defined by the Atomic Write Unit Normal (AWUN) and Atomic Write Unit Power Fail (AWUPF) settings for the device.",
            "The Flush command\n-----------------\n\nWithout special commands, NVMe provides no guarantees about what data will survive a power failure (Command Set 2.1.4.2 \"AWUPF/NAWUPF\"",
            "The disk could have any of the writes, with no relation to the order they were submitted or completed. It could also choose to return an error for all the ranges.",
            "The second mechanism to ensure durability is to set the Force Unit Access option on Write commands. This means that \"the controller shall write that data and metadata, if any, to non-volatile media before indicating command completion\" (Command Specification 3.2.6 \"Write Command\" Figure 63).",
            "Disabling write caching\n-----------------------\n\nThe last mechanism that may ensure durability is to explicitly disable the write cache. If an NVMe device has a volatile write cache, it must be controllable. This means you can disable it (Base Specification 5.27.1.4 \"Volatile Write Cache\""
          ]
        },
        {
          "title": "USENIX 2024 USENIX Annual Technical Conference - SPDK-focused storage research",
          "url": "https://www.usenix.org/system/files/atc24-yi-shushu.pdf",
          "excerpts": [
            "ts\n\nUser-space storage stack. The user-space storage framework,\nsuch as Storage Performance Development Kit (SPDK) [70],\nis a promising solution to address Challenge 1, that is, the\nsoftware overheads in the storage stack.",
            "First, two-phase write introduces extra metadata, which in-\n\ncreases crash consistency cost (Challenge 3). Specifically, to\nsupport out-of-place updates, the AFA engine maintains extra\nmapping tables in host memory to record where data is actu-\nally st",
            "In the conversion phase, it reads 1⇥ data out for\nparity computation and writes 1⇥ data and 0.5⇥ parity back\nto SSDs. The total write amplification is 3.5⇥.",
            "wo-phase write causes significant write amplifica-\n\ntion (Challenge 4",
            "wo-phase write AFA en-\ngines impose two more challenges. First, two-phase write introduces extra metadata, which in-\n\ncreases crash consistency cost (Challenge 3). Specifically, to\nsupport out-of-place updates, the AFA engine maintains extra\nmapping tables in host memory to record where data is actu-\nally store",
            "ework,\nsuch as Storage Performance Development Kit (SPDK) [70],\nis a promising solution to address Challenge 1, that is, the\nsoftware overheads in the storage stack."
          ]
        },
        {
          "title": "NVME_NVM_COMMANDS - Win32 apps",
          "url": "https://learn.microsoft.com/en-us/windows/win32/api/nvme/ne-nvme-nvme_nvm_commands",
          "excerpts": [
            "Mar 13, 2023 — NVME_NVM_COMMAND_FLUSH The Flush command that commits data and metadata associated with the specified namespace(s) to nonvolatile media."
          ]
        },
        {
          "title": "The semantics of a FLUSH command (per NVMe spec) ...",
          "url": "https://news.ycombinator.com/item?id=30421010",
          "excerpts": [
            "Feb 21, 2022 — A flush command only guarantees, upon completion, that all writes COMPLETED prior to submission of the flush are non-volatile. Not all ..."
          ]
        },
        {
          "title": "Ive just been scanning through linux kernel code (inc ext4). Are you ...",
          "url": "https://news.ycombinator.com/item?id=30372498",
          "excerpts": [
            "What you're looking for is the actual NVMe commands. There's two things: FLUSH (which flushes the whole cache), and a WRITE with the FUA bit ...",
            "Feb 17, 2022 — There's two things: FLUSH (which flushes the whole cache), and a WRITE with the FUA bit set (which basically turns that write into write-through ..."
          ]
        },
        {
          "title": "BlobFS (Blobstore Filesystem)",
          "url": "https://spdk.io/doc/blobfs.html",
          "excerpts": [
            "BlobFS is a Blobstore Filesystem, primarily tested with RocksDB, and can be mounted as a kernel filesystem using FUSE. It has a flat namespace.",
            "BlobFS provides a FUSE plug-in to mount an SPDK BlobFS as a kernel filesystem for inspection or debug purposes."
          ]
        },
        {
          "title": "SPDK 17.03 Release: VM Optimization & Blobstore",
          "url": "https://spdk.io/release/2017/03/31/17.03_release/",
          "excerpts": [
            "Blobstore and BlobFS have been designed specifically to enable accelerated performance for these types of applications. See Blobstore Design ..."
          ]
        },
        {
          "title": "YangKeao/rocksdb-spdk: A library that provides an ... - GitHub",
          "url": "https://github.com/YangKeao/rocksdb-spdk",
          "excerpts": [
            "A library that forms the core building block for a fast key value server, especially suited for storing data on flash drives."
          ]
        },
        {
          "title": "A Robust Flash File System Since 2002 | Yaffs - A Flash File System ...",
          "url": "https://yaffs.net/",
          "excerpts": [
            "This is write amplification - a single small write causing a cascade of writes (and erases) of whole blocks. Yaffs avoids much of this problem by being log ..."
          ]
        },
        {
          "title": "Decentralized, Epoch-based F2FS Journaling with Fine- ...",
          "url": "https://www.usenix.org/system/files/osdi25-cui.pdf",
          "excerpts": [
            "As such, in journal apply and crash recovery, with both file and filesystem metadata in journal records, we can reach to a consistent state."
          ]
        },
        {
          "title": "Comparing 4 Popular Embedded File Systems - JBLopen",
          "url": "https://www.jblopen.com/comparing-embedded-file-systems/",
          "excerpts": [
            "YAFFS2 does not support chunks smaller than 1KiB, so smaller updates will always result in a read-modify-write sequence which further decreases ...",
            "In the case of LittleFS, read performances are excellent with no caching at all, while write performances are terrible for fundamental design ..."
          ]
        },
        {
          "title": "NVM Express NVM Command Set Specification, Revision 1.1",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-NVM-Command-Set-Specification-Revision-1.1-2024.08.05-Ratified.pdf",
          "excerpts": [
            "command is submitted that has a size greater than the AWUN/NAWUN value or crosses an atomic  \nboundary, then:\n\n• In Single Atomicity Mode (refer to sectio [n 2.1.4.1)](-.html) , there is no guarantee of command atomicity;\n\nand\n\n• In Multiple Atomicity Mode (refer to section [2\\.1.4.5)](-.html) , atomicity is guaranteed for each portion of the\n\ncommand that falls within an atom",
            "*Force Unit Access (FUA):** If this bit is\n\nset to ‘1’, then the controller shall flush any data and metadata\n\nspecified by the Verify command from any volatile cache before performing the Verify operation and  \nshall perform the Verify operation on data and metadata that have been committed to non-volatile ",
            "If a write command is submitted that has a size less than or equal to the AWUN/NAWUN value and the  \nwrite command does not cross an atomic boundary (refer to section [2\\.1.4.4)](-.html) , then the host is guaranteed  \nthat the write command is atomic to the NVM with respect to other read or",
            "The boundary sizes shall be greater than or equal to the corresponding atomic write sizes:\n\n• NABSN shall be greater than or equal to AWUN;  \n• NABSN shall be greater than or equal to NAWUN if NAWUN is reported;  \n• NABSPF shall be greater than or equal to AWUPF; and  \n• NABSPF shall be greater than or equal to NAWUPF if NAWUPF is reported. In addition, NABO shall be less than or equal to NA",
            "\n\nNamespace Atomic Boundary Size  \nNormal (NABSN)\n\n= NAWUN,\n\n= AWUN if NAWUN\n\nis not reported, and\n\n= NABSPF\n\n≥ NAWUN and\n\n≥ AWUN\n\nNamespace Atomic Boundary Size  \nPower Fail (NABSPF)\n\n= NAWUPF,\n\n= AWUPF if NAWUPF is\n\nnot reported, and\n\n= NABSN\n\n≥ NA",
            " AWUN/NAWUN Example Results ",
            " AWUN/NAWUN Example Results "
          ]
        },
        {
          "title": "SPDK Blobstore and BlobFS Documentation",
          "url": "https://spdk.io/doc/blob.html",
          "excerpts": [
            "Atomicity\n\nFor all Blobstore operations regarding atomicity, there is a dependency on the underlying device to guarantee atomic operations of at least one page size.",
            "Blobstore is callback driven; in the event that any Blobstore API is unable to make forward progress it will not block but instead return control at that point and make a call to the callback function provided in the API, along with arguments, when the original call is completed.",
            "Blobstore requires a backing storage device that can be integrated using the `bdev` layer, or by directly integrating a device driver to Blobstore."
          ]
        },
        {
          "title": "Havelsan's Revenue in 2024",
          "url": "https://github.com/zephyrproject-rtos/littlefs/blob/zephyr/DESIGN.md",
          "excerpts": [
            "Power-loss resilience** \\- On these systems, power can be lost at any time. If a power loss corrupts any persistent data structures, this can cause the\n   device to become unrecove",
            "Bounded RAM/ROM** \\- If the above requirements weren't enough, these\n   systems also have very limited amounts of memory. This prevents many\n   existing filesystem designs, which can lean on relatively large amounts of\n   RAM to temporarily store filesystem metadata. For ROM, this means we need to keep our design simple and reuse code paths\n   where possible. For RAM we have a stronger requirement, all RAM us",
            "The block allocator has a secondary role: wear leveling. Wear leveling is the process of distributing wear across all blocks in the\nstorage to prevent the filesystem from experiencing an early death due to\nwear on a single block in the storage.",
            "Together, bad block detection and dynamic wear leveling provide a best effort\nsolution for avoiding the early death of a filesystem due to wear."
          ]
        },
        {
          "title": "Design of littlefs - NXP MCUXpresso documentation",
          "url": "https://mcuxpresso.nxp.com/mcuxsdk/25.09.00-pvw1/html/middleware/littlefs/DESIGN.html",
          "excerpts": [
            "\n\nThe block allocator has a secondary role: wear leveling. Wear leveling is the process of distributing wear across all blocks"
          ]
        },
        {
          "title": "NVM Command Set Specification",
          "url": "https://nvmexpress.org/wp-content/uploads/NVMe-NVM-Command-Set-Specification-1.0a-2021.07.26-Ratified.pdf",
          "excerpts": [
            "In this example, AWUPF/NAWUPF has a value of 1KiB (equivalent to two 512-byte logical blocks),. AWUN/NAWUN has a value of 2KiB (equivalent to ..."
          ]
        },
        {
          "title": "Open and Close File Performance · Issue #214 · littlefs- ...",
          "url": "https://github.com/ARMmbed/littlefs/issues/214",
          "excerpts": [
            "Jun 25, 2019 — I've done a number of tests: one of them erases the flash, creates a new FS and proceeds to create files of a constant size on the FS until we run out of space."
          ]
        },
        {
          "title": "NVM Express NVM Command Set Specification (Draft excerpts on atomicity and durability)",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-NVM-Command-Set-Specification-Revision-1.2-2025.08.01-Ratified.pdf",
          "excerpts": [
            "Namespace Atomic Write Unit Normal (NAWUN): This field indicates the namespace \nspecific size of the write operation guaranteed to be written atomically to the NVM \nduring normal operatio"
          ]
        },
        {
          "title": "MDPI 2025: Embedded File Systems and Crash Consistency",
          "url": "https://www.mdpi.com/2079-9268/15/2/22",
          "excerpts": [
            "In many filesystems, wear leveling and GC are managed by the flash translation layer (FTL), which acts as an abstraction layer between the filesystem and raw flash memory. FTL not only distributes wear evenly but can also provide crash recovery mechanisms by maintaining metadata and journaling techniques to restore consistency after unexpected power loss [[5](),[6](),[7]()]."
          ]
        },
        {
          "title": "[PDF] NVMe Base Specification, Revision 1.4c",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-1_4c-2021.06.28-Ratified.pdf",
          "excerpts": [
            "**Atomic Write Unit Normal (AWUN):** This field indicates the size of the write operation\n\nguaranteed to be written atomically to the NVM across all namespaces with any\n\nsupported namespace format during normal operatio",
            "\nAWUN/NAWUN control the atomicity of command execution in relation to other commands. They impose\n\ninter-command serialization of writing of blocks of data to the NVM and prevent blocks of data ending up\n\non the NVM containing partial data from one new command and partial data from one or more other new\n\ncommands.",
            "If a write command is submitted with size less than or equal to the AWUN/NAWUN value and the write\n\ncommand does not cross an atomic boundary (refer to section 6.4.3), then the host is guaranteed that the\n\nwrite command is atomic to the NVM with respect to other read or write command",
            "If a write command is\n\nsubmitted with size greater than the AWUN/NAWUN value or crosses an atomic boundary, then there is no\n\nguarantee of command atomicit",
            "y. AWUN/NAWUN does not have any applicability to write errors caused\n\nby power failure or other error conditions (refer to Atomic Write Unit Power Fail).",
            "ush applies to all\n\ncommands for the specified namespace(s) completed by the controller prior to the submission of the Flush\n\ncommand. The controller ",
            "If bits 2:1 are set to 11b in the VWC field (refer to Figure 251) and the specified NSID is FFFFFFFFh, then\n\nthe Flush command applies to all namespaces attached to the controller processing the Flush command",
            "If bits 2:1 are set to 10b in the VWC field and the specified NSID is FFFFFFFFh, then the controller fails\n\nthe command with status code Invalid Namespace or Format",
            "If bits 2:1 are cleared to 00b in the VWC field,\n\nthen the controller behavior if the specified NSID is FFFFFFFFh is not indicated"
          ]
        },
        {
          "title": "Specifications - NVM Express",
          "url": "https://nvmexpress.org/specifications/",
          "excerpts": [
            "Missing: FUA flush AWUN AWUPF NAWUN NAWUPF NABSN NABSPF ordering pcisig."
          ]
        },
        {
          "title": "TP4159 PCIe Infrastructure for Live Migration",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-Revision-Changes-2025.08.01.pdf",
          "excerpts": [
            "Aug 5, 2025 — Defined a new Multiple Atomicity Mode (MAM) that provides write guarantees between Namespace Atomic Boundaries for commands that cross ..."
          ]
        },
        {
          "title": "Efficient Crash Consistency for NVMe over PCIe and RDMA",
          "url": "https://dl.acm.org/doi/fullHtml/10.1145/3568428",
          "excerpts": [
            "by X Liao · 2023 · Cited by 9 — Crash consistency includes atomicity and ordering guarantees. We present how ccNVMe provides these two guarantees during normal execution and after a crash."
          ]
        },
        {
          "title": "SPDK: blob.h File Reference",
          "url": "https://spdk.io/doc/blob_8h.html",
          "excerpts": [
            "Dump a blobstore's metadata to a given FILE in human-readable format. ... The metadata thread is the thread which called spdk_bs_init() or spdk_bs_load()."
          ]
        },
        {
          "title": "Storage Developer Conference - #70: SPDK Blobstore",
          "url": "https://podscripts.co/podcasts/storage-developer-conference/70-spdk-blobstore-a-look-inside-the-nvm-optimized-allocator",
          "excerpts": [
            "And really, the metadata is nothing more than an extent list, a list of clusters that have been allocated for that blob, and a series of, we call them XADDRs,."
          ]
        },
        {
          "title": "[PDF] F2FS: A New File System for Flash Storage - USENIX",
          "url": "https://www.usenix.org/sites/default/files/conference/protected-files/fast15_slides_lee.pdf",
          "excerpts": [
            "In F2FS, more than 90% of writes are sequential. F2FS reduces write amount per fsync by using roll-forward recovery. If checkpoint is done per fsync, write ..."
          ]
        },
        {
          "title": "[PDF] exF2FS: Transaction Support in Log-Structured Filesystem - USENIX",
          "url": "https://www.usenix.org/system/files/fast22-oh.pdf",
          "excerpts": [
            "F2FS's transaction support has a specific target application: SQLite. With atomic write of F2FS, SQLite can implement the transaction without ..."
          ]
        },
        {
          "title": "LittleFS performance issue for QSPI flash from 1.54 beta10 to ...",
          "url": "https://forum.pjrc.com/index.php?threads/littlefs-performance-issue-for-qspi-flash-from-1-54-beta10-to-1-56.70467/",
          "excerpts": [
            "Jun 13, 2022 — I'm pretty sure this problem is you're running out pre-erased sectors and LittleFS is having to do slow erase operations. Sadly, LittleFS doesn ..."
          ]
        },
        {
          "title": "[PDF] F2FS: A New File System for Flash Storage - cs.wisc.edu",
          "url": "https://pages.cs.wisc.edu/~bart/736/papers/fast15-paper-lee.pdf",
          "excerpts": [
            "For this workload, the roll-forward recovery policy of F2FS produces huge benefits. In fact, F2FS reduces the amount of data writes by about 46% over EXT4 in."
          ]
        },
        {
          "title": "Bad performance / big number of reads when using LittleFS",
          "url": "https://forums.mbed.com/t/bad-performance-big-number-of-reads-when-using-littlefs/5855",
          "excerpts": [
            "Nov 4, 2019 — The spikes you see are caused by directory-level garbage collection. This happens with the directory fills with commits (file changes)."
          ]
        },
        {
          "title": "Design notes for littlefs",
          "url": "https://docs.mcuxpresso.nxp.com/mcuxsdk/25.06.00/html/middleware/littlefs/DESIGN.html",
          "excerpts": [
            "littlefs was originally built as an experiment to learn about filesystem design\nin the context of microcontrollers. The question was: How would you build a\nfilesystem that is resilient to power-loss and flash wear without using\nunbounded memory?",
            "This leaves us with three major requirements for an embedded filesystem. 1. **Power-loss resilience** - On these systems, power can be lost at any time.",
            "m. littlefs[]( \"Permalink to this heading\")\n==================================================\n\nSo what does littlefs do? If we look at existing filesystems, there are two interesting design patterns\nthat stand out, but each have their own set of problems. Logging, which\nprovides independent atomicity, has poor runtime performance. And COW data\nstructures, which perform well, push the atomicity problem upwards.",
            "littlefs has two methods of protecting against wear:\n\n1. Detection and recovery from bad blocks\n2. Evenly distributing wear across dynamic blocks",
            "The block allocator has a secondary role: wear leveling. Wear leveling is the process of distributing wear across all blocks in the\nstorage to prevent the filesystem from experiencing an early death due to\nwear on a single block in the storage.",
            "littlefs uses a statistical wear leveling algorithm. What this\nmeans is that we don’t actively track wear, instead we rely on a uniform\ndistribution of wear across storage to approximate a dynamic wear leveling\nalgorithm.",
            "Together, bad block detection and dynamic wear leveling provide a best effort\nsolution for avoiding the early death of a filesystem due to wear."
          ]
        },
        {
          "title": "NVMe API Changes",
          "url": "https://spdk.io/feature/2016/03/03/nvme-api/",
          "excerpts": [
            "Mar 3, 2016 — First, we added support for Force Unit Access and Limited Retry by adding an io_flags parameter to the read and write commands. We also added a ..."
          ]
        },
        {
          "title": "spdk/lib/nvme/nvme_ns.c at master",
          "url": "https://github.com/spdk/spdk/blob/master/lib/nvme/nvme_ns.c",
          "excerpts": [
            "Storage Performance Development Kit. Contribute to spdk/spdk development by creating an account on GitHub."
          ]
        },
        {
          "title": "spdk/lib/blob/blobstore.c at master - GitHub",
          "url": "https://github.com/spdk/spdk/blob/master/lib/blob/blobstore.c",
          "excerpts": [
            "Storage Performance Development Kit. Contribute to spdk/spdk development by creating an account on GitHub."
          ]
        },
        {
          "title": "F2FS",
          "url": "https://wiki.archlinux.org/title/F2FS",
          "excerpts": [
            "May 13, 2025 — If a file is linked, F2FS may loose its parent inode number so that fsync calls for the linked file need to perform the checkpoint every time."
          ]
        },
        {
          "title": "[PDF] SPDK Blobstore - SNIA.org",
          "url": "https://www.snia.org/sites/default/files/SDC/2017/presentations/Solid_State_Stor_NVM_PM_NVDIMM/Luse_Paul_Verma_Vishal_SPDK_Blobstore_A_Look_Inside_the_NVM_Optimized_Allocator.pdf",
          "excerpts": [
            "❒ Github : https://github.com/spdk/spdk. ❒ Trello : https://trello.com/spdk. ❒ GerritHub : https://review.gerrithub.io/#/q/project:spdk/spdk+status:open."
          ]
        },
        {
          "title": "[blobstore] crash during copy-on-write can leak data #2866",
          "url": "https://github.com/spdk/spdk/issues/2866",
          "excerpts": [
            "A crash at a critical point can lead to dirty clusters being used for new allocations with the assumption that they have already been zeroed."
          ]
        },
        {
          "title": "[PDF] UNH-IOL NVMe Conformance Test Plan",
          "url": "https://www.iol.unh.edu/sites/default/files/UNH-IOL_NVMe_Conformance_Test_Suite_v14.0_Clean.pdf",
          "excerpts": [
            "NABSN, NABO, NABSPF values. Support for Atomic Writes is indicated ... Discussion: AWUN/NAWUN control the atomicity of command execution in ..."
          ]
        },
        {
          "title": "SPDK Blobstore support object? · Issue #183 - GitHub",
          "url": "https://github.com/spdk/spdk/issues/183",
          "excerpts": [
            "Someone implemented a block device in Ceph using SPDK, so you can technically use SPDK with Bluestore to accelerate it."
          ]
        },
        {
          "title": "UNH–IOL NVMe Testing Service",
          "url": "https://www.iol.unh.edu/sites/default/files/testsuites/nvme/UNH-IOL_NVM_Command_Set_Conformance_v17.0.pdf",
          "excerpts": [
            "Jan 21, 2022 — The UNH-IOL NVMe Testing Service is located at 21 Madbury Rd Suite 100 Durham, NH 03824. Contact them at +1 603–862–0090 or nvmelab@iol.unh.edu."
          ]
        },
        {
          "title": "SPDK Spinlocks Introduction",
          "url": "https://spdk.io/spdk_spinlock/2023/01/04/spdk_lock/",
          "excerpts": [
            "Jan 4, 2023 — Initialize the lock before first use. Only lock when running from an SPDK thread. Unlock before returning from a poller or message function."
          ]
        },
        {
          "title": "Leveraging NVMe SSDs for Building a Fast, Cost-effective ...",
          "url": "https://dl.acm.org/doi/10.1145/3480963",
          "excerpts": [
            "Oct 15, 2021 — The write-friendly log-structured merge tree (LSM-tree) is widely adopted as the underlying storage engine by mainstream KV stores, such as ..."
          ]
        },
        {
          "title": "Analysis for the Performance Degradation of fsync()in F2FS",
          "url": "https://oslab.kaist.ac.kr/wp-content/uploads/esos_files/publication/conferences/international/ICICC2018_GYCHOI.pdf",
          "excerpts": [
            "by G Choi · 2010 · Cited by 8 — In addition, F2FS performs the roll- forward recovery based on the node segment processed through fsync() and the latest checkpoint pack ..."
          ]
        },
        {
          "title": "The Design and Implementation of a High-Performance ...",
          "url": "https://arxiv.org/html/2402.17963v1",
          "excerpts": [
            "Feb 28, 2024 — We present ZapRAID, a high-performance RAID system for ZNS SSDs by carefully exploiting Zone Append to achieve high write performance via intra- ..."
          ]
        },
        {
          "title": "Realtime kernel - ArchWiki",
          "url": "https://wiki.archlinux.org/title/Realtime_kernel",
          "excerpts": [
            "hwlatdetect. hwlatdetect can be used to detect SMIs taking an inordinate time, thus introducing latency by blocking normal kernel execution."
          ]
        },
        {
          "title": "12 Tuning I/O Performance",
          "url": "https://documentation.suse.com/en-us/sles/12-SP5/html/SLES-all/cha-tuning-io.html",
          "excerpts": [
            "MQ-DEADLINE is a latency-oriented I/O scheduler. It is a modification of DEADLINE scheduler for blk-mq I/O path (refer to Section 12.2.3, “ DEADLINE ”) ...See more"
          ]
        },
        {
          "title": "System Analysis and Tuning Guide | Tuning I/O performance",
          "url": "https://documentation.suse.com/pt-br/sles/15-SP7/html/SLES-all/cha-tuning-io.html",
          "excerpts": [
            "When switching from legacy block to blk-mq I/O path for a device, the none option is roughly comparable to noop , mq-deadline is comparable to deadline , and ..."
          ]
        },
        {
          "title": "Block Device Tuning of Real-Time Linux Configuration - Medium",
          "url": "https://medium.com/@eren.c.uysal/block-device-tuning-of-real-time-linux-configuration-b294386f2ce2",
          "excerpts": [
            "Real-time tuning focuses on minimizing latency by eliminating bottlenecks such as I/O scheduling queues, filesystem journaling, and CPU ..."
          ]
        },
        {
          "title": "Select latency plot of system or system group to display",
          "url": "https://www.osadl.org/Latency-plots.latency-plots.0.html",
          "excerpts": [
            "Sep 6, 2023 — Latency plots are used to graphically present the result of a latency determination of a real-time computer system.See more"
          ]
        },
        {
          "title": "Linux OS Runtime Optimizations — ECI documentation",
          "url": "https://eci.intel.com/docs/3.0/development/optimizations.html",
          "excerpts": [
            "When using the ECI Linux* Intel® LTS PREEMPT_RT kernel, all Linux kernel processes are scheduled to run on CPU 0, and CPUs 1 and 3 are configured to be isolated ..."
          ]
        },
        {
          "title": "Why choose a low latency kernel over a generic or real- ...",
          "url": "https://askubuntu.com/questions/126664/why-choose-a-low-latency-kernel-over-a-generic-or-real-time-kernel",
          "excerpts": [
            "Apr 28, 2012 — If you need a low latency system (e.g. for recording audio) then please use the -preempt kernel as a first choice. This reduces latency but ..."
          ]
        },
        {
          "title": "cyclictest - High resolution test program - Ubuntu Manpage",
          "url": "https://manpages.ubuntu.com/manpages/trusty/man8/cyclictest.8.html",
          "excerpts": [
            "It is useful to track down unexpected large latencies on a system. This option does only work with following kernel config options. For kernel < 2.6.24: * ..."
          ]
        },
        {
          "title": "Systematic Process to Reduce Linux OS Jitter",
          "url": "https://groups.google.com/g/mechanical-sympathy/c/DWlziVmyW-w",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache behavior, etc.). Same ..."
          ]
        }
      ],
      "reasoning": "The user's fine-grained field describes a strategic shift: turning away from relying on the Linux kernel's inherent jitter by building an app-specific Rust RTOS that can run alongside Linux, leveraging Rust's safety and concurrency, plus kernel-bypass techniques (DPDK/SPDK, VFIO, SR-IOV) to achieve low tail latency (p99.99) in IO-heavy workloads. The most directly relevant excerpts substantiate this intent by presenting concrete Rust-based OS and runtime projects, architectures, and related tooling that could underpin such a Rust-centric RTOS and its user-space/isolated design philosophy. Specifically:\n\n- Texts describing Theseus as an experimental OS designed with Rust, emphasizing its architecture around safe, modular components and the idea of building OS functionality in Rust that can run in a resource-partitioned, compartmentalized fashion. The material highlights Theseus's approach of structuring the system as a set of crates (cells) and its vision for isolation and modularity, which aligns with the field's concept of a Rust-based RTOS running in a contained, dedicated environment alongside Linux.\n\n- Texts describing Hubris as a Rust-based, microkernel-like or highly modular OS designed for embedded contexts, emphasizing memory protection, isolation, and the notion of a minimal, safe kernel with Rust components. This supports the idea that Rust can be used to build the core safety-critical layers of an RTOS-like environment, which is compatible with the proposed architecture in the field value.\n\n- Texts highlighting the broader rust-vmm ecosystem (vhost, vm-memory, kvm bindings, etc.), which demonstrates a strong Rust-led trajectory for building virtualization-friendly systems and user-space VMM components. This evidence supports the feasibility of Rust-based runtimes and device backends that could form part of the proposed app-specific RTOS and its peripheral I/O stack.\n\n- References to kernel-bypass IO architectures and high-performance user-space I/O stacks (DPDK, SPDK, VFIO, vDPA) and their latency/throughput characteristics, which provide the performance rationale for moving critical IO paths out of the kernel. These excerpts are functionally supportive of the field's claim that kernel bypass can yield meaningful latency improvements (e.g., p99.99 tail latency improvements) and are compatible with the Rust RTOS approach since such IO paths can be implemented in user-space components and Rust-based runtimes.\n\n- Additional latency-performance context sections discuss real-time kernels, cyclictest measurements, and tail-latency considerations. While not Rust-specific, they establish the problem space (jitter, determinism, tail latency) and the motivation for a design that reduces kernel-driven jitter through isolation and bypass. This background reinforces why the Rust RTOS path is compelling and aligns with the proposed performance goals, even if examples are not Rust-native in every cited piece.\n\nOverall, the most relevant extracts present concrete Rust-centric OS and runtime efforts (Theseus, Hubris, rust-vmm ecosystem) that illustrate a feasible path for a Rust-based RTOS and its architectural philosophy. They are followed by excerpts that show the viability and value of sandboxed, user-space, high-performance IO approaches (DPDK/SPDK/vDPA/VFIO) that the field's architecture would leverage. The remaining excerpts provide supportive latency/performance context that contextualizes the problem space but are less central to the specific Rust-RTOS feasibility claim. The supporting evidence across these excerpts collectively substantiates the field value but with varying degrees of directness; the Rust-centric OS and VMM examples offer the strongest, most direct alignment with the proposed approach.",
      "confidence": "high"
    },
    {
      "field": "linux_jitter_analysis",
      "citations": [
        {
          "title": "A realtime preemption overview - LWN.net",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "There are a few changes in PREEMPT_RT whose primary purpose is to reduce scheduling or interrupt latency. The first such change involves the x86 ..."
          ]
        },
        {
          "title": "LinuxCNC latency and jitter improvements with PREEMPT_RT ...",
          "url": "https://dantalion.nl/2024/09/29/linuxcnc-latency-jitter-kernel-parameter-tuning.html",
          "excerpts": [
            "LinuxCNC control threads can be quite sensitive to system latency and jitter\ndepending on the hardware control interface being used. Because of this, the\ninstallation ships with the so called `PREEMPT_RT` version of the Linux kernel. While this already boasts a huge improvement over the regular `PREEMPT_DYNAMIC` kernel there are many other kernel parameters that can further improve latency\nand jitter.",
            "Latency is the delay measured in time between when something is supposed to\nhappen and when it happens (in the case of OS timers). Often times this can be\naccounted for assuming the delay is constant. Jitter is the variance between the\ndelay for repeated measurements it is a measure of how constant the delay is.",
            "Linux tasks are subject to many different schedulers [1] that all exhibit\ndifferent behavior. Simply installing the `PREEMPT_RT` kernel will not lead to\nsubstantially different results when running your timing sensitive programs. For this you need to run the task with schedulers intended for realtime\napplications. On Linux there are two schedulers that are considered realtime being the fifo, `SCHED_FIFO` and `SCHED_RR` or round-robin schedulers.",
            "If you use kernel 6.6 or newer it needs to be changed to `isolcpus=managed_irq,domain,(N/2)-N` . Next are a couple of arguments to prevent unpredictable execution times:",
            "1. `skew_tick=1` , this offsets RCU timers so they do not happen at the same time",
            "2. `nosmt=force` , this disables hyperthreading",
            "Next are a couple of arguments to prevent unpredictable execution times:\n\n1. `skew_tick=1` , this offsets RCU timers so they do not happen at the same time\n2. `nosmt=force` , this disables hyperthreading\n3. `nosoftlockup`\n4. `nowatchdog`",
            "To understand the parameters for tuning one can analyze the documentation on the\ntopic from the Linux kernel [2]. We will only briefly touch upon each \nparameter here. Most importantly within Linux we can tell the kernel te reserve certain ranges\nor lists of cores for specific types of tasks. This allows to reserve cores for\nrealtime tasks while simultaneously pushing hardware and os management to\nspecific cores. The result is that we can isolate specific cores to solely run\nrealtime tasks resulting in much better predictability in turn leading to low\nlatency and low jitter. The following parameters are used to push kernel threads, regular user processes\nand interrupts to a subset of the cpus, here we use $N$ to denote the number of\nphysical processor cores the system has starting at index 0. Next we use `()` brackets to denote a mathematical formula:\n\n1. `irqaffinity=0-(N/2-1)`\n2. `kthread_cpus=0-(N/2-1)",
            "1. `intel_pstate=disable`"
          ]
        },
        {
          "title": "KVM Real-Time Virtualization and RT-Physical Isolation (LWN Article)",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Realtime is about determinism, not speed. Realtime workloads are those\nwhere missing deadlines is bad: it results in voice breaking up in telecommunications\nequipment, missed opportunities in stock trading, and exploding rockets in\nvehicle control and avionics.",
            "Bare-metal latencies\nare less than 2 microseconds, but KVM's measurement of 6-microsecond\nlatencies is also a very \ngood result"
          ]
        },
        {
          "title": "RedHawk KVM-RT Whitepaper",
          "url": "https://concurrent-rt.com/wp-content/uploads/2020/12/RedHawk_KVM-RT_Whitepaper.pdf",
          "excerpts": [
            "KVM-RT can support multiple real-time guests while maintaining real-time performance on\n\nthe host as well, while having non real-time guests like Windows 10 running alongside"
          ]
        },
        {
          "title": "Xenomai Cobalt IRQ API Documentation",
          "url": "https://www.cs.ru.nl/lab/xenomai/api3/group__cobalt__core__irq.html",
          "excerpts": [
            "Xenomai: Interrupt management"
          ]
        },
        {
          "title": "Hardware Latency Detector (hwlat_detector) - Linux Kernel Documentation",
          "url": "https://docs.kernel.org/trace/hwlat_detector.html",
          "excerpts": [
            "Hardware Latency Detector",
            "The tracer hwlat\\_detector is a special purpose tracer that is used to\ndetect large system latencies induced by the behavior of certain underlying\nhardware or firmware, independent of Linux itself",
            "# echo hwlat > /sys/kernel/tracing/current_tracer",
            "# echo 100 > /sys/kernel/tracing/tracing_thresh",
            "in /sys/kernel/tracing:",
            "By default the width is set to 500,000 and window to 1,000,000, meaning that ... If any latencies that exceed the threshold is observed then the data will be ...",
            "tracing\\_max\\_latency - maximum hardware latency actually observed (usec",
            "tracing\\_cpumask - the CPUs to move the hwlat thread acros",
            "hwlat\\_detector/width - specified amount of time to spin within window (usecs",
            "hwlat\\_detector/window - amount of time between (width) runs (usecs",
            "By default, one hwlat detector’s kernel thread will migrate across each CPU\nspecified in cpumask at the beginning of a new window, in a round-robin\nfashion.",
            "This behavior can be changed by changing the thread mode,\nthe available options are:",
            "none: do not force migration"
          ]
        },
        {
          "title": "10.5.4.1 TSC-Deadline Mode",
          "url": "https://xem.github.io/minix86/manual/intel-x86-and-64-manual-vol3/o_fe12b1e2a880e0ce-379.html",
          "excerpts": [
            "IA32_TSC_DEADLINE MSR reads zero and writes are ignored. Software can configure the TSC-deadline timer to deliver a single interrupt using the following ...",
            "Transitioning   \nbetween TSC-deadline mode and other timer modes also disarms the tim",
            "CPUID.01H:ECX.TSC\\_Deadline[bit 24] = "
          ]
        },
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual",
          "url": "https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf",
          "excerpts": [
            "CPUID.01H:ECX.TSC\\_Deadline[bit 24] = "
          ]
        },
        {
          "title": "some thoughts on HPET, from a kernel developer : r/Amd - Reddit",
          "url": "https://www.reddit.com/r/Amd/comments/uf0zdf/some_thoughts_on_hpet_from_a_kernel_developer/",
          "excerpts": [
            "TSC is a truly low overhead timing source, using a CPU register to operate. And HPET is a guaranteed timing source with high overhead."
          ]
        },
        {
          "title": "vDPA hands on: The proof is in the pudding - Red Hat",
          "url": "https://www.redhat.com/en/blog/vdpa-hands-proof-pudding",
          "excerpts": [
            "Missing: latency jitter"
          ]
        },
        {
          "title": "High Precision Event Timer Driver for Linux",
          "url": "https://www.kernel.org/doc/html/v6.1/timers/hpet.html",
          "excerpts": [
            "Missing: latency jitter"
          ]
        },
        {
          "title": "Releasing 0.5.0 of lfqueue - Lock-free MPMC queues : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/1ln3fvm/releasing_050_of_lfqueue_lockfree_mpmc_queues/",
          "excerpts": [
            "Part of that required a good lock-free queue, and a paper called: \"A Scalable, Portable, and Memory-Efficient Lock-Free FIFO Queue\" grabbed my ..."
          ]
        },
        {
          "title": "nicholassm/disruptor-rs: Low latency inter-thread ...",
          "url": "https://github.com/nicholassm/disruptor-rs",
          "excerpts": [
            "This library is a low latency, inter-thread communication library written in Rust. It's heavily inspired by the brilliant Disruptor library from LMAX."
          ]
        },
        {
          "title": "r/rust - disruptor-rs: low-latency inter-thread communication ...",
          "url": "https://www.reddit.com/r/rust/comments/1e3elbx/disruptorrs_lowlatency_interthread_communication/",
          "excerpts": [
            "The 1-element numbers seem low (for Disruptor) in either case, as just writing to a contented atomic tends to take roughly ~50ns on a 5GHz Intel ..."
          ]
        },
        {
          "title": "Crate lockfree - Rust",
          "url": "https://docs.rs/lockfree",
          "excerpts": [
            "The purpouse of this module is to solve the “ABA problem” related to pointers while still being lock-free. See documentation of the inner type for more details."
          ]
        },
        {
          "title": "3. Environment Abstraction Layer - Documentation",
          "url": "https://doc.dpdk.org/guides-17.11/prog_guide/env_abstraction_layer.html",
          "excerpts": [
            "Missing: best practices irq"
          ]
        },
        {
          "title": "DPDK Tuning Guide - Arm Developer",
          "url": "https://developer.arm.com/documentation/109701/latest/User-Guide",
          "excerpts": [
            "Missing: rte_ring MSI-",
            "Larger hugepages can cover larger memory areas without TLB misses. Verify 32*1GB hugepages are evenly distributed across all NUMA nodes. While the threads used ..."
          ]
        },
        {
          "title": "Analysis of EDF and RM scheduling algorithms for periodic and aperiodic tasks in multi-core ECU",
          "url": "https://www.researchgate.net/publication/373151884_Analysis_of_EDF_and_RM_scheduling_algorithms_for_periodic_and_aperiodic_tasks_in_multi-core_ECU",
          "excerpts": [
            "Since the first results published in 1973 by Liu and Layland on the Rate Monotonic (RM) and Earliest Deadline First (EDF) algorithms, a lot of progress has been made in the schedulability analysis of periodic task sets.",
            "Unfortunately, many misconceptions still exist about the properties of these two scheduling methods, which usually tend to favor RM more than EDF. Typical wrong statements often heard in technical conferences and even in research papers claim that RM is easier to analyze than EDF, it introduces less runtime overhead, it is more predictable in overload conditions, and causes less jitter in task execution.",
            "This paper compares RM against EDF under several aspects, using existing theoretical results, specific simulation experiments, or simple counterexamples to show that many common beliefs are either false or only restricted to specific situations.",
            "The algorithm, called RM-First-Fit-Decreasing-Utilization (or RMFFDU) , is shown to have a worst-case tight bound of 5=3 = 1:66 : : :, the lowest upper bound ever derived for the scheduling problem.",
            "the use of EDF allows a better exploitation of the available resources and significantly improves system's performance."
          ]
        },
        {
          "title": "Multiprocessor EDF and Deadline-Monotonic Schedulability Analysis - ResearchGate Publication",
          "url": "https://www.researchgate.net/publication/4049071_Multiprocessor_EDF_and_deadline_monotonic_schedulability_analysis",
          "excerpts": [
            ". Towards better processor utilization, new global algorithms such as dynamic-priority EDZL [Cho et al., 2002;Cirinei and Baker, 2007] and US-EDF{m/(2m − 1)} [Srinivasan and Baruah, 2002], and fixed-priority RM-US{m/(3m − 2)} [Andersson et al., 2001] and FP-EDF [Baruah, 2004], have also been proposed.",
            "Under global scheduling tasks are allowed to migrate across processors and algorithms that simultaneously schedule on all the processors are used.",
            "On symmetric multiprocessors, an algorithm that improves the schedulability of non-preemptive tasks was proposed by Lee et al. [8].",
            "The PARM mechanism in the system architecture shown in Fig. 1 is mainly composed of an energyconsumption controller (ECC), an admission controller (AC), and an earliest deadline first (EDF) scheduler (where an EDF scheduling algorithm is applied)."
          ]
        },
        {
          "title": "Comparative Study of RM and EDF Scheduling Algorithm ...",
          "url": "https://www.ijcsmc.com/docs/papers/March2017/V6I3201729.pdf",
          "excerpts": [
            "EDF always introduces fewer jitter than RM. ... Different properties ordinarily asserted for RM, such as consistency amid over-loaded conditions, or better jitter ..."
          ]
        },
        {
          "title": "[PDF] EDF and RM Multiprocessor Scheduling Algorithms",
          "url": "https://www.semanticscholar.org/paper/EDF-and-RM-Multiprocessor-Scheduling-Algorithms-%3A-Zapata-%C3%81lvarez/84702f8443395187f25826d4498f2504fc0d219c",
          "excerpts": [
            "A survey of the best-known realtime scheduling techniques executing on multiple processors, using the Rate Monotonic and Earliest Deadline First scheduling ..."
          ]
        },
        {
          "title": "MP scheduling is difficult",
          "url": "http://retis.sssup.it/~giorgio/slides/cbsd/mc2-sched-2p.pdf",
          "excerpts": [
            "➢ Dhall's Effect shows the limitation of global EDF and RM: both utilization bounds tend to 1, independently of the value of m. ➢ Researchers ..."
          ]
        },
        {
          "title": "Multiprocessor platforms for real-time systems - CPEN 432",
          "url": "https://cpen432.github.io/resources/bader-slides/11-Multiprocessors.pdf",
          "excerpts": [
            "The Dhall Effect. • Dhall's Effect shows the limitation of global EDF and RM: both utilization bounds tend to 1, independently of the value of m."
          ]
        },
        {
          "title": "How it works · Wult",
          "url": "https://intel.github.io/wult/pages/how-it-works.html",
          "excerpts": [
            "All modern Intel CPUs support the TSC deadline timer feature, which is provided by the LAPIC block. This feature allows for scheduling an interrupt in TSC ..."
          ]
        },
        {
          "title": "Osdev-Notes/02_Architecture/08_Timers.md at master - GitHub",
          "url": "https://github.com/dreamportdev/Osdev-Notes/blob/master/02_Architecture/08_Timers.md",
          "excerpts": [
            "However, it does come with some latency due to operating over port IO, and its frequency is low compared to the other timers available. The local APIC timer: it ...",
            "When the TSC passes the tick value in this MSR, it tells the local APIC, and if TSC deadline mode is selected in the timer LVT an interrupt is generated.See more",
            "the invariant TSC.",
            "The TSC is a bit more modern than the LAPIC timer, but still pre-dates most long mode processors, so this is another timer that should always be present.",
            "The TSC is probably the simplest timer we've covered so far: it's simply a 64-bit counter that increments every time the base clock of the processor pulses.",
            "The local APIC timer: it is also capable of generating interrupts (periodic and oneshot) and is pollable. It operates in a similar manner to the PIT where it ...",
            "We're going to focus on setting up the local APIC timer, and calibrating it with either the PIT or HPET. We'll also have a look at a how we could also use the ..."
          ]
        },
        {
          "title": "Questions on APIC timer frequency measurement (using PIT) : r/osdev",
          "url": "https://www.reddit.com/r/osdev/comments/tvff5l/questions_on_apic_timer_frequency_measurement/",
          "excerpts": [
            "Set up local APIC timer in one shot mode right up until the point that it's ready for the initial count. Set a mode 0 timer on channel 0 of ..."
          ]
        },
        {
          "title": "How can I reduce jitter by using CPU and IRQ pinning ...",
          "url": "https://access.redhat.com/solutions/2144921",
          "excerpts": [
            "Aug 2, 2024 — Resolution · Disabling automatic IRQ balancing · Disabling the kernel's built-in NUMA balancing · Isolating CPUs from the process scheduler."
          ]
        },
        {
          "title": "SPDK: Event Framework",
          "url": "https://spdk.io/doc/event.html",
          "excerpts": [
            "SPDK provides a framework for writing asynchronous, polled-mode, shared-nothing server applications. The event framework is intended to be optional.",
            "Normally, pollers are executed on every iteration of the main event loop. Pollers may also be scheduled to execute periodically on a timer if low latency is not ..."
          ]
        },
        {
          "title": "15. Poll Mode Driver - Documentation - DPDK",
          "url": "https://doc.dpdk.org/guides-24.03/prog_guide/poll_mode_drv.html",
          "excerpts": [
            "Missing: jitter SPDK",
            "A Poll Mode Driver (PMD) consists of APIs, provided through the BSD driver running in user space, to configure the devices and their respective queues."
          ]
        },
        {
          "title": "7. Poll Mode Driver - Documentation - DPDK",
          "url": "https://doc.dpdk.org/guides-16.07/prog_guide/poll_mode_drv.html",
          "excerpts": [
            "Missing: jitter SPDK",
            "A Poll Mode Driver (PMD) consists of APIs, provided through the BSD driver running in user space, to configure the devices and their respective queues."
          ]
        },
        {
          "title": "LNCS 2855 - Rate Monotonic vs. EDF: Judgment Day",
          "url": "https://www.eecs.umich.edu/courses/eecs571/reading/rm-vs-edf.pdf",
          "excerpts": [
            "by GC Buttazzo · Cited by 67 — Finally, both RM and EDF are not very well suited to work in overload con- ditions and to achieve jitter control. To cope with overloads, specific extensions."
          ]
        },
        {
          "title": "Real-Time Compute Performance - DPDK",
          "url": "https://eci.intel.com/docs/3.3/development/performance/benchmarks/rtcp-dpdk.html",
          "excerpts": [
            "Using Cache Allocation Technology improves application performance by assigning CPU affinity to cache ways, which can be dedicated to real-time applications.",
            "Linux Events Latency Histograms · Monitor CPU Utilization and Affinity · Monitor ... The example below uses neighboring memory and compute stress on Core 4.",
            "Jitter : 52566"
          ]
        },
        {
          "title": "Avoiding time jitters with periodic high frequency tasks",
          "url": "https://www.freertos.org/FreeRTOS_Support_Forum_Archive/June_2019/freertos_Avoiding_time_jitters_with_periodic_high_frequency_tasks_f79bd60d74j.html",
          "excerpts": [
            "Time slicing with tasks of equal priority would seem to increase jitter rather than decrease it as the other tasks may run first."
          ]
        },
        {
          "title": "Determinism and Jitter in a Real-Time System",
          "url": "https://www.ni.com/docs/en-US/bundle/labview-nxg-rt-module-programming-with-rt-target/page/determinism-real-time.html?srsltid=AfmBOorS1wIrOH7yfBpyGNWomGbRzvexGN78T0OHDOu8o_yQmCK6h3cf",
          "excerpts": [
            "Jitter occurs both when actual timing is later than expected timing and when actual timing is earlier than expected timing for a task."
          ]
        },
        {
          "title": "What is Offset and Jitter in an RTOS task",
          "url": "https://stackoverflow.com/questions/32675612/what-is-offset-and-jitter-in-an-rtos-task",
          "excerpts": [
            "Offset is the maximum time by which the task can be delayed once it is in READY state and jitter is the additional execution time the task takes from the ..."
          ]
        },
        {
          "title": "Timer Interrupt Sources",
          "url": "http://wiki.osdev.org/Timer_Interrupt_Sources",
          "excerpts": [
            "For local APIC timer, there's one per CPU. This is important for scalability (rather than many CPUs fighting to access the same single timer). A good OS would ...See more",
            "... TSC deadline mode, local APIC timer without TSC deadline mode, HPET, PIT. Emulation. Counters can be emulated in software using a fixed frequency IRQ (e.g. ..."
          ]
        },
        {
          "title": "Extended understanding of system timers in Windows. - Page 2",
          "url": "https://forums.blurbusters.com/viewtopic.php?t=12142&start=10",
          "excerpts": [
            "For a modern Intel CPU using a TSC timer should be the best option. The defaults are the best option. In Device manager, look for the High Precision Event timer ..."
          ]
        },
        {
          "title": "HPET and Nvidia on Linux",
          "url": "https://forums.developer.nvidia.com/t/hpet-and-nvidia-on-linux/285470",
          "excerpts": [
            "I tried using high precision event timer (HPET) as the clocksource for my system to see if it would benefit performance in gaming today."
          ]
        },
        {
          "title": "PSA: If your clocksource is HPET rather than TSC you may ... - Reddit",
          "url": "https://www.reddit.com/r/linux_gaming/comments/rsvjqb/psa_if_your_clocksource_is_hpet_rather_than_tsc/",
          "excerpts": [
            "It seems when HPET is on the main thread of the game will spam a certain system call to do with time and that kills performance."
          ]
        },
        {
          "title": "TSC Clocksource Switching to HPET During High I/O Load | Support",
          "url": "https://www.suse.com/support/kb/doc/?id=000021035",
          "excerpts": [
            "The switch to HPET may be caused by certain CPUs becoming stuck in the sbitmap_queue_clear() and __sbq_wake_up() functions."
          ]
        },
        {
          "title": "A fast multi-producer, multi-consumer lock-free concurrent queue for ...",
          "url": "https://news.ycombinator.com/item?id=13102721",
          "excerpts": [
            "Modifying IRQ Masks dynamically hits a lot of internal locking, and hurts your cache coherency. Any gains in one process will reflect ..."
          ]
        },
        {
          "title": "Data Plane Development Kit (DPDK)",
          "url": "https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/tuning-guides/58310_amd-epyc-8004-tg-data-plane-dpdk.pdf",
          "excerpts": [
            "Disabling APB by setting APBDIS to 1 and specifying a fixed Infinity Fabric SOC P-state of 0 forces the Infinity Fabric and memory controllers into full-power."
          ]
        },
        {
          "title": "A Comparison of Global and Partitioned EDF Schedulability Tests for Multiprocessors (TR-051101)",
          "url": "https://www.cs.fsu.edu/files/reports/tr-051101.pdf",
          "excerpts": [
            "The predominant approach to scheduling multiprocessor hard-real-time systems has\nbeen partitioned, in which each task is assigned statically (more or less) to one processor.",
            "Partitioned scheduling has the\nvirtue of permitting schedulability to be verified using well-understood single-processor analysis techniques.",
            "The alternative to partitioned scheduling is global scheduling, in which there is a single job queue, from which jobs\nare dispatched to any available processor according to a global priority scheme.",
            "This paper attempts to compare the present state of the art for global EDF scheduling against the state of the art for\npartitioned EDF scheduling.",
            "Because the worst-case performance of both approaches has been shown to be the same, at\nleast for the case where deadline equals period, the comparison is of empirical performance.",
            "A new hybrid EDF-based scheme is shown to perform better than previously studied priority-based global scheduling\nschemes, though not as well as EDF-based first-fit partitioned scheduling."
          ]
        },
        {
          "title": "CPU isolation troubleshooting and clocksource details (SUSE)",
          "url": "https://www.suse.com/c/cpu-isolation-nohz_full-troubleshooting-tsc-clocksource-by-suse-labs-part-6/",
          "excerpts": [
            " ## 2) TSC reliability",
            "On old intel processors, the TSC used to count at the actual CPU frequency. However if the CPU frequency were to accelerate or slowdown, the clock would then do the same, losing its synchronization with the course of time. This made the TSC unusable as a clocksource. Modern processors shouldn’t have that issue. Unaffected CPUs have the “ _constant\\_tsc_ ” flag ",
            "`$ grep constant_tsc /proc/cpuinfo`",
            "### 2\\.4) Unreliable TSC implications",
            "Now what does an unreliable TSC imply? Does it mean it’s unusable enough for the system which then needs to use another main clocksource? Not really, or rather not yet. An unreliable TSC means it has the potential for becoming unstable. And if it were to end that way, the kernel would then indeed need to switch to a trustworthier clocksource. Now before that to happen, an unreliable TSC has yet to be observed unstable. A watchdog implements that vigilance while comparing periodically a delta from two TSC snapshots against the delta from two equivalent snapshots of a clocksource considered as more reliable. If the difference between the delta of the TSC and the delta from the other clocksource goes further the minimal acceptancy, the TSC is then tagged as unstable and the kernel switches to another clocksource. This watchdog runs in a timer that executes every 0.5 second, cycling through each CPUs ",
            " ## 3) TSC stability",
            "From the kernel point of view:",
            "* An unreliable TSC means there is a suspicion that something bad can happen with the clocksource. * An unstable TSC means that something bad has eventually happened with the clocksource. When the clocksource watchdog detects too much drifts or skews from the unreliable TSC, it tags it as unstable. When that happens the generic clocksource layer arranges for switching the current clocksource from the unstable TSC to a more stable clocksource (hpet, acpi\\_pm, etc..). Unfortunately even if the current clocksource has been switched over from the bad TSC to another source, the [scheduler clock](https://docs.kernel.org/timers/timekeeping.html) is an exception as it continues to rely on the TSC, even if unstable, because it still needs the fastest source available",
            "Some processors stop the TSC while entering into low power mode (deep idle state), producing clock gaps. Unaffected CPUs have the “ _nonstop\\_tsc_ “ _flag",
            "`$ grep nonstop_tsc /proc/cpuinfo`",
            "#### 2\\.1.3) Overwrite abuses and recover",
            "[SMM](https://en.wikipedia.org/wiki/System_Management_Mode) code may interrupt the kernel via [SMIs](https://wiki.linuxfoundation.org/realtime/documentation/howto/debugging/smi-latency/smi) .",
            "If it doesn’t have the tsc\\_reliable flag, then it has to meet the 4 above requirements as a last chance to be considered reliable, and that only applies to kernels >= [5\\.16](https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.16) . [Suse Linux Enterprise Server 15 SP4](https://www.suse.com/releasenotes/x86_64/SUSE-SLES/15-SP4/index.html) ( [including its Real Time counterpart](https://documentation.suse.com/sle-rt/15-SP4/) ) supports this pretty well",
            " ## 4) Other architectures",
            "It seems that only x86 has a clocksource officially considered as unreliable (so far).",
            "It seems that only x86 has a clocksource officially considered as unreliable (so far).",
            "The CPU has the constant_tsc flag (clock rate doesn't vary with CPU frequency).",
            "The CPU has the nonstop_tsc flag (clock doesn't stop when the ... Now [the high resolution timers](https://docs.kernel.org/timers/hrtimers.html) and the [dynticks](https://docs.kernel.org/timers/no_hz.html) subsystem (on which [nohz\\_full](https://www.suse.com/c/cpu-isolation-nohz_full-part-3/) depends on) both specifically need the most precise, the most reliable, the highest capacity and if possible also the fastest clocksource"
          ]
        },
        {
          "title": "TSC - OSDev Wiki",
          "url": "https://wiki.osdev.org/TSC",
          "excerpts": [
            "The APIC timer can be set to the **TSC-deadline mode** which uses the processor's internal time-stamp counter to issue an interrupt when the counter exceeds a specific value decided by softw",
            "The **Timestamp Counter** is a 64-bit internal register which is present in all Intel processors after the Penti",
            "The TSC and related instructions are not portable and using them in **user-level software is not advised due to portability reasons** ."
          ]
        },
        {
          "title": "Multi-Queue Block IO Queueing Mechanism (blk-mq)",
          "url": "https://docs.kernel.org/block/blk-mq.html",
          "excerpts": [
            "The Multi-Queue Block IO Queueing Mechanism is an API to enable fast storage devices to achieve a huge number of input/output operations per second (IOPS)"
          ]
        },
        {
          "title": "What Do the Linux \"ethtool\" Interrupt Coalescing Values ...",
          "url": "https://knowledge.informatica.com/s/article/80204?language=en_US",
          "excerpts": [
            "Nov 19, 2010 — Interrupt coalescing batches network data at the NIC to increase data delivered to the kernel with one hardware interrupt.",
            "An algorithm to improve rx latency at low packet-receiving rates and improve throughput at high packet-receiving rates. Some NIC drivers do not ...",
            "Nov 19, 2010 — Interrupt coalescing is the standard terminology for batching network data at the network interface card (NIC) in order to increase the amount ...",
            "The Linux \"ethtool -C\" command provides a wide array of different types of parameters that can be configured in various ways to set values related to network ..."
          ]
        },
        {
          "title": "Chapter 12. Tuning interrupt coalescence settings",
          "url": "https://docs.redhat.com/de/documentation/red_hat_enterprise_linux/10/html/network_troubleshooting_and_performance_tuning/tuning-interrupt-coalescence-settings",
          "excerpts": [
            "The `ethtool` command output can vary depending on the NIC and driver in us",
            "The value of the `rx_errors` , `rx_dropped` , `tx_errors` , and `tx_dropped` fields should be 0 or close to it (up to few hundreds, depending on the network traffic and system ",
            "Verification",
            "5. Re-activate the connection:",
            "4. Modify the coalescence settings as needed. For example:",
            "3. Check the current interrupt coalescence settings:",
            "2. Monitor values of packet counters you identified in the previous step. Compare them to the expected values for your network to determine whether any particular interface experiences a bottleneck.",
            "1. Identify the network interface that is experiencing the bottleneck:",
            "The goal of coalesce tuning is to minimize the number of interrupts required for a given workload. In high-throughput situations, the goal is to have as few interrupts as possible while maintaining a high data rate. In low-latency situations, more interrupts can be used to handle traffic quickly.",
            "Interrupt coalescence is a mechanism for reducing the number of interrupts generated by a network card. Generally, fewer interrupts can enhance the latency and overall performance of your network.",
            "While ethtool.coalesce-adaptive-rx is disabled, configure ethtool.coalesce-rx-usecs to set the delay before generating an interrupt to 100 microseconds ..."
          ]
        },
        {
          "title": "intel/ethernet-linux-i40e: The i40e Linux Base Drivers for ...",
          "url": "https://github.com/intel/ethernet-linux-i40e",
          "excerpts": [
            "To set the interrupt rate manually, you must disable adaptive mode: ethtool -C <ethX> adaptive-rx off adaptive-tx off For IP forwarding: * Disable adaptive ITR ..."
          ]
        },
        {
          "title": "Ethtool",
          "url": "https://docs.nvidia.com/networking/display/MLNXOFEDv583070101/Ethtool",
          "excerpts": [
            "Nov 27, 2023 — Sets the interrupt coalescing setting. rx-frames will be enforced immediately, rx-usecs will be enforced only when adaptive moderation is ..."
          ]
        },
        {
          "title": "Intel i40e Device Driver Version 2.7.26 Installation on ...",
          "url": "https://www.concurrent-rt.co.jp/external/TechSup/PDF/Inteli40eV2.7.26.ReleaseNotes.pdf",
          "excerpts": [
            "Nov 2, 2021 — The above command would disable adaptive interrupt moderation, and allow a maximum of 5 microseconds before indicating a receive or transmit was ..."
          ]
        },
        {
          "title": "Optimizing Storage Performance with Calibrated Interrupts",
          "url": "https://www.usenix.org/system/files/osdi21-tai.pdf",
          "excerpts": [
            "by A Tai · 2021 · Cited by 34 — Interrupt coalescing is disabled by default in Linux, and real deployments use alternatives (§2). This paper addresses the challenge of dealing ..."
          ]
        },
        {
          "title": "Scaling in the Linux Networking Stack",
          "url": "https://docs.kernel.org/networking/scaling.html",
          "excerpts": [
            "Receive Packet Steering (RPS) is logically a software implementation of RSS. ... SMP IRQ affinity explains how CPUs are assigned to the bitmap. Suggested ..."
          ]
        },
        {
          "title": "RSS, IRQ affinity and RPS on Linux - Andreas Karis Blog",
          "url": "https://andreaskaris.github.io/blog/networking/rss-irq-affinity-and-rps/",
          "excerpts": [
            "We are going to learn a bit about RSS (Receive Side Scaling), IRQ SMP affinity, RPS (Receive Packet Steering) and how to analyze what's happening on your CPUs ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://docs.kernel.org/timers/no_hz.html",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "Can constant non-invariant tsc change frequency across ...",
          "url": "https://stackoverflow.com/questions/62492053/can-constant-non-invariant-tsc-change-frequency-across-cpu-states",
          "excerpts": [
            "All Intel processors support invariant TSC, which means that the TSC is incremented at a constant rate across P-, C-, and T-states (but not necessarily across ..."
          ]
        },
        {
          "title": "Full Tickless (nohz_full) Kernel and CPU Isolation for Gaming - Reddit",
          "url": "https://www.reddit.com/r/linux_gaming/comments/1crqb81/full_tickless_nohz_full_kernel_and_cpu_isolation/",
          "excerpts": [
            "I would like to have a look into CPU isolation for gaming. I know that I need to set nohz_full=1-9 to tell the kernel not ticking my last 9 cores."
          ]
        },
        {
          "title": "[PDF] NVM Express® NVMe® over PCIe® Transport Specification",
          "url": "https://nvmexpress.org/wp-content/uploads/NVMe-over-PCIe-Transport-Specification-1_0-2021.06.02-Ratified.pdf",
          "excerpts": [
            "MSI-X allows completions to be aggregated on a per vector basis. Each Completion Queue(s) may send its own interrupt message, as opposed to a ...",
            "It is recommended \nthat MSI-X be used whenever possible to enable higher performance, lower latency, and lower CPU \nutilization for processing interrupt",
            "MSI-X is the preferred interrupt behavior to use. The following configuration describes this mode of interrupt \noperation",
            "MSI-X, similar to multiple-message MSI, allows completions to be aggregated on a per vector basis.",
            "The interrupt architecture allows for efficient reporting of interrupts such that the host may service interrupts \nthrough the least amount of overhead"
          ]
        },
        {
          "title": "[PDF] NVM Express Revision 1.3a",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-1_3a-20171024_ratified.pdf",
          "excerpts": [
            "... MSI-X vector for each Completion Queue. The Table BIR and PBA BIR data ... MSI-X allows completions to be aggregated on a per vector basis."
          ]
        },
        {
          "title": "Ethernet Controller Products",
          "url": "https://cdrdv2-public.intel.com/785486/27.1_Release%20Notes_v1.0.pdf",
          "excerpts": [
            "Interrupt Moderation settings reset to default when the queue settings of a port are modified using the ethtool -L ethx combined XX command. • When a VM is ..."
          ]
        },
        {
          "title": "Linux: Disabling Interrupt Coalescing for NVMe device",
          "url": "https://serverfault.com/questions/848851/linux-disabling-interrupt-coalescing-for-nvme-device",
          "excerpts": [
            "I'm looking at testing an NVMe device with interrupt coalescing enabled, and disabled. NVMe protocol defined interrupt coalescing parameters that can be passed ..."
          ]
        },
        {
          "title": "Linux / NVMe, disable interrupt coalescing - pci express",
          "url": "https://superuser.com/questions/1207390/linux-nvme-disable-interrupt-coalescing",
          "excerpts": [
            "May 9, 2017 — I'm looking at testing an NVMe device with interrupt coalescing enabled, and disabled. NVMe protocol defined interrupt coalescing parameters ..."
          ]
        },
        {
          "title": "How can I override IRQ affinity for NVME devices",
          "url": "https://serverfault.com/questions/1052448/how-can-i-override-irq-affinity-for-nvme-devices",
          "excerpts": [
            "Feb 4, 2021 — I am trying to move all interrupts over to cores 0-3 to keep the rest of my cores free for high speed, low latency virtualization."
          ]
        },
        {
          "title": "(Nearly) full tickless operation in 3.10 - LWN.net",
          "url": "https://lwn.net/Articles/549580/",
          "excerpts": [
            "Response-time jitter due to timer interrupts will be nearly eliminated, and, according to Ingo Molnar, as much as 1% of the CPU's time will be ..."
          ]
        },
        {
          "title": "Invariant TSC support",
          "url": "https://community.intel.com/t5/Intel-ISA-Extensions/Invariant-TSC-support/m-p/772125",
          "excerpts": [
            "Dec 21, 2011 — The invariant TSC means that the TSC continues at a fixed rate regardless of the C-state or frequency of the processor (as long as the processor ...",
            "The invariant TSC means that the TSC continues at a fixed rate regardless of the C-state or frequency of the processor (as long as the processor remains in the ...",
            "This is indicated by the CPUID.80000007.EDX[8]. All E7 parts will support the invariant TSC (short of an unexpected erratum).",
            "On processors with invariant TSC\n\nsupport, the OS may use the TSC for wall clock timer services (instead of ACPI or\n\nHPET timers",
            "TSC reads are much more efficient and do not incur the overhead\n\nassociated with a ring transition or access to a platform resource",
            "Invariant            "
          ]
        },
        {
          "title": "Time Stamp Counter (TSC)",
          "url": "https://aakinshin.net/vignettes/tsc/",
          "excerpts": [
            "The invariant TSC is indicated by a combination of constant_tsc (synchronization between cores) and nonstop_tsc (power management independence) flags. In most ...",
            "Generation 3: Invariant TSC. Invariant TSC is the latest version of the counter which works well. A quote from Intel 64 and ia-32 architectures software ..."
          ]
        },
        {
          "title": "When Poll is More Energy Efficient than Interrupts (HotStorage 2022)",
          "url": "https://par.nsf.gov/servlets/purl/10410822",
          "excerpts": [
            "las-\nsic polling has the lowest latency of 8.1 µs, a 2-µs improve-\nment over interrupts",
            "Observation 6. Polling can be more energy efficient than\ninterrupts. To our surprise, for requests with low IO latencies, we\nobserved that classic polling is the most energy efficient\nmethod.",
            "Polling is commonly indicated to be a more suitable IO com-\npletion mechanism than interrupt for ultra-low latency stor-\nage devices. However, polling’s impact on overall energy\nefficiency has not been thoroughly investiga"
          ]
        },
        {
          "title": "NVMe Polling and Interrupts - LVM Vault 2017",
          "url": "https://events.static.linuxfound.org/sites/events/files/slides/lemoal-nvme-polling-vault-2017-final_0.pdf",
          "excerpts": [
            " \\-1: classic polling (default)",
            "– 0: adaptive hybrid polling",
            "– <time in ns>: fixed time hybrid polling",
            "Adaptive and fixed time polling",
            "ybrid polling results in\n\nan almost identical service time\n\ndistribution as classic pollin",
            "Adaptive hybrid polling results in\n\nan almost identical service time\n\ndistribution as classic polli",
            "Adaptive hybrid polling gives the same\n\naverage latency of 4.5us as classic pollin",
            "But with only 58% CPU load",
            "Fixed time polling efficiency\n\ndirectly depends on the sleep tim",
            "Disable useless interrupts at driver level",
            "**Linux Block I/O Hybrid Polling Implementation**",
            "Controlled using the _io\\_poll\\_delay_\n\nsysfs fil",
            "NVM is coming ! Is this relevant ?",
            "Latency relative improvements \n\ncompared to IRQ depend on the \ntypical command execution time \nof the de",
            "Polling can remove context switch (CS) overhead from I/O path",
            "Kernel based I/O polling has clear \n\nadvantages for latency aware \napplicati",
            "Classic polling is more likely to suffer\n\nfrom a longer latency distribution tail\n\nthan hybrid polling and IR",
            "Adaptive hybrid polling gives the same \n\naverage latency of 4.5us as classic polling\n– But with only 58% CPU load\n– 32% with IRQ mod",
            "Evaluation Results: Hybrid Polling",
            "by D Le Moal · 2017 · Cited by 29 — Linux Block I/O Polling Implementation. • Currently, only NVMe supports I/O polling. – poll block-mq device operation. • Polling is done on the ..."
          ]
        },
        {
          "title": "ACM: Polling vs Interrupt for Ultra-Low Latency Storage Devices",
          "url": "https://dl.acm.org/doi/10.1145/3538643.3539747",
          "excerpts": [
            "To the best of our knowledge, this is the first paper classifying polling as more energy efficient than interrupt for a real secondary storage device, and we hope that our observations will lead to more energy efficient IO completion mechanisms for new generation storage device characteristics."
          ]
        },
        {
          "title": "A Toolkit to Measure Basic System Performance and OS Jitter",
          "url": "https://highscalability.com/a-toolkit-to-measure-basic-system-performance-and-os-jitter/",
          "excerpts": [
            "May 27, 2015 — jHiccup is an open source tool designed to measure the pauses and stalls (or “hiccups”) associated with an application's underlying Java runtime ...See more"
          ]
        },
        {
          "title": "Completely eliminating the timer tick in modern Linux >=5.0",
          "url": "https://stackoverflow.com/questions/60322119/completely-eliminating-the-timer-tick-in-modern-linux-5-0",
          "excerpts": [
            "Missing: CONFIG_PREEMPT_RT suppression"
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Turn THP (Transparent Huge Pages) OFF.",
            "Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems",
            "Set Swappiness to 0.",
            "Set zone\\_reclaim\\_mode to ",
            "Turn HT (Hyper-threading) ON. (double the vcore run queues --> umpteen times lower likelihood of waiting for a cpu).",
            "Isolcpus are somewhat (but not entirely) different. With isolcpus you get the benefit of knowing that nobody will \"accidentally\" share your core.",
            "E.g. in JVMs, even an isolcpus-assigned thread's worst hiccups will be dominated by things like time-to-safepoint across all JVM threads, so you are still susceptible to hiccups outside of isolcpu-assigned threads and cores.",
            "Apr 8, 2015 — Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "Real-time Ubuntu documentation",
          "url": "https://documentation.ubuntu.com/real-time/latest/how-to/cpu-boot-configs/",
          "excerpts": [
            "\n\nReal-time tuning parameters[¶]( \"Link to this heading\")",
            "\n\nTune the performance of Real-time Ubuntu, by adding parameters from this\nsection to your bootloader configuration. ### Reduce scheduler jitter[¶]( \"Link to this heading\")",
            "ading\")\n\nReal-time Ubuntu is compiled with the `CONFIG_NO_HZ_FULL=y` kernel\nconfiguration option. This “[NO\\_HZ](https://docs.kernel.org/timers/no_hz.html)” facility can reduce jitter, by vetoing\nunnecessary scheduling-clock interrupts. (You can assist it by also [offloading\nread-copy-update]() tasks.)",
            ") #### Tickless CPUs[¶]( \"Link to this heading\")",
            "g\")\n\nAn idle CPU has no tasks and therefore no scheduling requirement. When you\nenable NO\\_HZ *without* isolating any CPUs, idle ones will receive no scheduler\nticks. They’re then “tickless” (or “dyntick-idle”). To prevent scheduler ticks\nfor tickless CPUs, configure the `nohz` kernel command-line parameter:\n\n`nohz=on`",
            "\n\n#### Adaptive-tick CPUs[¶]( \"Link to this heading\")",
            "Isolate such “adaptive-ticks CPUs”,\nto veto scheduler ticks for them (and, at the same time, for tickless CPUs)\nconfigure the following kernel command-line parameters:\n\n`nohz=on nohz_full=<CPU lis",
            "s. ### Reduce read-copy-update jitter[¶]( \"Link to this heading\")",
            "\nCallbacks for Linux’s read-copy-update (RCU) mechanism are assigned to\ncandidate CPUs. By removing a CPU from the candidate list you can reduce jitter\nin its remaining tasks. To allow that, Real-time Ubuntu is compiled with the\n`CONFIG_RCU_NOCB_CPU=y` kernel configuration option. To offload RCU callbacks from specified CPUs, use the kernel command-line\nparameter:\n\n`rcu_nocbs=<CPU list>`",
            "\n\nIsolate CPUs from SMP algorithms[¶]( \"Link to this heading\")",
            "Jitter can affect tasks assigned by the symmetric multiprocessing (SMP)\nbalancing and scheduling algorithms. You can isolate CPUs, so tasks won’t be\nassigned to them by those algorithms. To isolate CPUs from the general SMP scheduler, use the kernel command-line\nparameter:\n\n`isolcpus=<CPU list>",
            "Protecting a CPU from having to service IRQs can remove\nsignificant jitter from its remaining tasks. Change the default IRQ-affinity list, to restrict which CPUs are available for\nservicing interrupts, use the kernel command-line parameter:\n\n`irqaffinity=<CPU list>`",
            "\nNote\n\nYou can’t protect a CPU from all IRQ-handling duties. It may still have to\nservice a small number of “soft IRQs”."
          ]
        },
        {
          "title": "NAPI",
          "url": "https://docs.kernel.org/networking/napi.html",
          "excerpts": [
            "napi_schedule() is the basic method of scheduling a NAPI poll. Drivers should call this function in their interrupt handler (see Scheduling and IRQ masking for ...",
            "The NAPI budget for busy polling is lower than the default (which makes sense given the low latency intention of normal busy polling). This is not the case ..."
          ]
        },
        {
          "title": "NAPI — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.5/networking/napi.html",
          "excerpts": [
            "A successful call to napi_schedule() will take ownership of the NAPI instance. Later, after NAPI is scheduled, the driver's poll method will be called to ..."
          ]
        },
        {
          "title": "What network loads require NIC polling vs interrupts?",
          "url": "https://serverfault.com/questions/261572/what-network-loads-require-nic-polling-vs-interrupts",
          "excerpts": [
            "The best idea I have right now is to try decrease the interrupts while keeping both latency low and throughput high."
          ]
        },
        {
          "title": "embassy_executor - Rust - embassy-executor",
          "url": "https://docs.embassy.dev/embassy-executor/git/std/index.html",
          "excerpts": [
            "No “fixed capacity” data structures, executor works with 1 or 1000 tasks without needing config/tuning. Integrated timer queue: sleeping is easy, just do Timer ..."
          ]
        },
        {
          "title": "embassy-executor - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/embassy-executor/0.6.0",
          "excerpts": [
            "Aug 5, 2024 — No \"fixed capacity\" data structures, executor works with 1 or 1000 tasks without needing config/tuning. Integrated timer queue: sleeping is easy ..."
          ]
        },
        {
          "title": "embassy-executor - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/embassy-executor/0.5.1",
          "excerpts": [
            "Oct 21, 2024 — Efficient polling: a wake will only poll the woken task, not all of them. ... docs.embassy.dev/embassy-executor. Repository. github.com ..."
          ]
        },
        {
          "title": "Support generic tasks or improve documentation on why ...",
          "url": "https://github.com/embassy-rs/embassy/issues/2454",
          "excerpts": [
            "Jan 15, 2024 — To use an async function with embassy, a task is created using the embassy_executor::task annotation macro. It's implementation wraps the ..."
          ]
        },
        {
          "title": "embassy/embassy-executor/src/arch/cortex_m.rs at main",
          "url": "https://github.com/embassy-rs/embassy/blob/main/embassy-executor/src/arch/cortex_m.rs",
          "excerpts": [
            "Modern embedded framework, using Rust and async. Contribute to embassy-rs/embassy development by creating an account on GitHub."
          ]
        },
        {
          "title": "11. How to get best performance with NICs on Intel platforms",
          "url": "https://doc.dpdk.org/guides/linux_gsg/nic_perf_intel_platform.html",
          "excerpts": [
            "This document is a step-by-step guide for getting high performance from DPDK applications on Intel platforms."
          ]
        },
        {
          "title": "What? 80 Million I/O Per Second with a Standard 2U Intel® Xeon ...",
          "url": "https://spdk.io/news/2021/05/06/nvme-80m-iops/",
          "excerpts": [
            "80 MILLION IOPS at an amazing average latency of just 25.54 microseconds ... Additionally, the SPDK NVMe driver achieves linear IOPS ..."
          ]
        },
        {
          "title": "Introducing Glommio, a thread-per-core crate for Rust and ...",
          "url": "https://www.datadoghq.com/blog/engineering/introducing-glommio/",
          "excerpts": [
            "Nov 2, 2020 — Glommio allows Rust developers to write thread-per-core applications in an easy and manageable way. What is thread-per-core? We know that thread ...",
            "Glommio builds upon Rust’s native asynchronous support and Linux’s innovative event-based io\\_uring API to build a thread-per-core library that is easy to consume",
            "o\\_uring is capable of not only processing file I/O, but also network sockets, timers, and many other events over a single common API.",
            "there are no context switches.",
            "Modern applications that need to be sharded for scalability are prime candidates for using a thread-per-core architecture, where each CPU will have sole control over a fragment of the dataset."
          ]
        },
        {
          "title": "Embassy - GitHub (Embassy project documentation)",
          "url": "https://github.com/embassy-rs/embassy",
          "excerpts": [
            "Embassy is the next-generation framework for embedded applications. Write safe, correct, and energy-efficient embedded code faster, using the Rust programming language, its async facilities, and the Embassy libraries.",
            "The async executor automatically puts the core to sleep when there's no work to do. Tasks are woken by interrupts, there is no busy-loop polling while waiting.",
            "️ embedded\n\n[](️-embedded)\n\nThe Rust programming language is blazingly fast and memory-efficient, with no runtime, garbage collector, or OS. It catches a wide variety of bugs at compile time, thanks to its full memory- and thread-safety, and expressive type system. Rust's [async/await](https://rust-lang.github.io/async-book/) allows for unprecedentedly easy and efficient multitasking in embedded systems. Tasks get transformed at compile time into state machines that get run cooperatively. It requires no dynamic memory allocation and runs on a single stack, so no per-task stack size tuning is required. It obsoletes the need for a traditional RTOS with kernel context switching, and is [faster and smaller than one! ](https://tweedegolf.nl/en/blog/65/async-rust-vs-rtos-showdown)",
            "Real-time ready** \\-\n  Tasks on the same async executor run cooperatively, but you can create multiple executors with different priorities so that higher priority tasks preempt lower priority on",
            "Implementations for common classes such as USB serial (CDC ACM) and USB HID are available, and a rich builder API allows building your own.",
            "Embassy is guaranteed to compile on stable Rust 1.75 and up."
          ]
        },
        {
          "title": "Introduction to the Storage Performance Development Kit SPDK",
          "url": "https://www.intel.com/content/www/us/en/developer/articles/tool/introduction-to-the-storage-performance-development-kit-spdk.html",
          "excerpts": [
            "? The extremely high performance is achieved by combining two key techniques: running at user level and using Poll Mode Drivers (PMDs).",
            "PMDs change the basic model for an I/O. In the traditional I/O model, the application submits a request for a read or a write, and then sleeps while awaiting an interrupt to wake it up once the I/O has been completed. PMDs work differently; an application submits the request for a read or write, and then goes off to do other work, checking back at some interval to see if the I/O has yet been completed.",
            "Systems are already able to process many millions of I/Os per second, so the elimination of this overhead for millions of transactions compounds quickly into multiple cores being saved.",
            "Packets and blocks are dispatched immediately and time spent waiting is minimized, resulting in lower latency, more consistent latency (less jitter), and improved throughput.",
            "SPDK is composed of numerous subcomponents, interlinked and sharing the common elements of user-level and poll-mode operation."
          ]
        },
        {
          "title": "Task Wakeups with Waker - Asynchronous Programming in Rust",
          "url": "https://rust-lang.github.io/async-book/02_execution/03_wakeups.html",
          "excerpts": [
            "When wake() is called, the executor knows that the task associated with the Waker is ready to make progress, and its future should be polled again. Waker also ..."
          ]
        },
        {
          "title": "embassy-rs/static-cell",
          "url": "https://github.com/embassy-rs/static-cell",
          "excerpts": [
            "StaticCell provides a no-std-compatible, no-alloc way to reserve memory at compile time for a value, but initialize it at runtime, and get a 'static reference ..."
          ]
        },
        {
          "title": "AtomicWaker in futures::task - Rust",
          "url": "https://docs.rs/futures/latest/futures/task/struct.AtomicWaker.html",
          "excerpts": [
            "An AtomicWaker can coordinate concurrent notifications with the consumer potentially “updating” the underlying task to wake up."
          ]
        },
        {
          "title": "DataDog/glommio",
          "url": "https://github.com/DataDog/glommio",
          "excerpts": [
            "Glommio is a thread-per-core crate that makes writing highly parallel asynchronous applications in a thread-per-core architecture easier for rustaceans.",
            "Like other rust asynchronous crates, it allows one to write asynchronous code that takes advantage of\nrust\nasync /\nawait , but unlike its counterparts, it doesn't use helper threads anywhere.",
            "Glommio requires a kernel with a recent enough\nio_uring support, at least current enough to run discovery probes. The\nminimum version at this time is 5.8. Please also note Glommio requires at least 512 KiB of locked memory for\nio_uring to work."
          ]
        },
        {
          "title": "glommio - Rust",
          "url": "https://docs.rs/glommio/latest/glommio/",
          "excerpts": [
            "mio\n\nGlommio is a library providing a safe Rust interface for asynchronous,\nthread-local I/O, based on the linux `io_uring` interface and Rust’s `async` support. Glommio also provides support for pinning threads to CPUs, allowing\nthread-per-core applications in Rust. This library depends on linux’s `io_uring` interface, so this is Linux-only,\nwith a kernel version 5.8 or newer recommended. This library provides abstractions for timers, file I/O and networking plus\nsupport for multiple-queues and an internal scheduler, all without using\nhelper threads.",
            "\n\nGlommio is a library providing a safe Rust interface for asynchronous,\nthread-local I/O, based on the linux `io_uring` interface and Rust’s `async` support. Glommio also provides support for pinning threads to CPUs, allowing\nthread-per-core applications in Rust.",
            "\nNote that you can only have one executor per thread, so if you need more\nexecutors, you will have to create more threads. A more ergonomic interface\nfor that is planned but not yet available."
          ]
        },
        {
          "title": "RTIC.rs",
          "url": "https://rtic.rs/",
          "excerpts": [
            "RTIC is a hardware accelerated RTOS that utilizes the hardware such as the NVIC on Cortex-M MCUs, CLIC on RISC-V etc. to perform scheduling.",
            "Page Redirection If you are not redirected automatically, follow this [link](2) .",
            "RTIC the hardware accelerated real-time scheduler · is pending and enabled and has a priority higher than the (optional BASEPRI ) register, and · has the highest ..."
          ]
        },
        {
          "title": "5 Rust Runtimes Every Embedded Developer Needs to Know",
          "url": "https://www.designnews.com/embedded-systems/5-rust-runtimes-every-embedded-developer-needs-to-know",
          "excerpts": [
            "Apr 23, 2024 — RTIC, which stands for Real-Time Interrupt-Driven Concurrency, is a framework specifically designed for building real-time embedded applications ..."
          ]
        },
        {
          "title": "heapless - Rust - RTIC.rs",
          "url": "https://rtic.rs/1/api/heapless/index.html",
          "excerpts": [
            "spsc::Queue – single producer single consumer lock-free queue. §Optional Features. The heapless crate provides the following optional Cargo features: ufmt-impl ..."
          ]
        },
        {
          "title": "Embassy Executor Documentation",
          "url": "https://docs.embassy.dev/embassy-executor/git/cortex-m/struct.InterruptExecutor.html",
          "excerpts": [
            "It is even possible to run multiple interrupt mode executors at different priorities, by assigning different priorities to the interrupts.",
            "Another use case is\nto run multiple executors: one in thread mode for low priority tasks and another in\ninterrupt mode for higher priority tasks.",
            "This executor runs tasks in interrupt mode. The interrupt handler is set up to poll tasks, and when a task is woken the interrupt is pended from software.",
            "To use it, you have to pick an interrupt that won’t be used by the hardware. Some chips reserve some interrupts for this purpose, sometimes named “software interrupts” (SWI).",
            "If this is not the case, you may use an interrupt from any unused peripheral.",
            "It is somewhat more complex to use, it’s recommended to use the thread-mode [`Executor`](struct.Executor.html \"struct embassy\\_executor::Executor\") instead, if it works for your use case.",
            "This allows running async tasks at a priority higher than thread mode.",
            "Start the executor. This initializes the executor, enables the interrupt, and returns.",
            "a [`SendSpawner`](struct.SendSpawner.html \"struct embassy_executor::SendSpawner\") you can use to spawn tasks on it.",
            "The executor keeps running in the background through the interrupt.",
            "To use it, you have to pick an interrupt that won’t be used by the hardware.",
            "You must write the interrupt handler yourself, and make it call [`on_interrupt()`](struct.InterruptExecutor.html.on_interrupt \"method embassy_executor::InterruptExecutor::on_interrupt\").",
            "Higher priority tasks will preempt lower\npriority ones."
          ]
        },
        {
          "title": "Embedded Rust Options: RTIC vs Embassy",
          "url": "https://willhart.io/post/embedded-rust-options/",
          "excerpts": [
            "Embassy provides both Hardware Abstraction Layers (HALs), and an executor/runtime, while RTIC aims to only provide an execution framework.",
            "Embassy\ntakes a fairly standard approach of requiring all resources to be statically\nallocated.",
            "This requires a lot of `static` types or the use of something like\n[`StaticCell`](https://docs.rs/static_cell/latest/static_cell/).",
            "Rust projects usually have very thorough API documentation and detailed\nexamples, and both RTIC and embassy are no exception here. There are tonnes of\nexamples for both."
          ]
        },
        {
          "title": "Executor in embassy_executor - Rust",
          "url": "https://docs.embassy.dev/embassy-executor/git/cortex-m/struct.Executor.html",
          "excerpts": [
            "This is the simplest and most common kind of executor. It runs on thread mode (at the lowest priority level), and uses the WFE ARM instruction to sleep when it ..."
          ]
        },
        {
          "title": "esp_hal_embassy - Rust",
          "url": "https://docs.espressif.com/projects/rust/esp-hal-embassy/0.8.0/esp32/esp_hal_embassy/index.html",
          "excerpts": [
            "InterruptExecutor can be used to achieve preemptive multitasking in asynchronous applications, which is typically something reserved for more traditional RTOS."
          ]
        },
        {
          "title": "Real-Time Interrupt-driven Concurrency (RTIC) framework ...",
          "url": "https://github.com/rtic-rs/rtic",
          "excerpts": [
            "Real-Time Interrupt-driven Concurrency. The hardware accelerated Rust RTOS. A concurrency framework for building real-time systems."
          ]
        },
        {
          "title": "esp_hal_embassy - Rust",
          "url": "https://docs.espressif.com/projects/rust/esp-hal-embassy/0.7.0/esp_hal_embassy/index.html",
          "excerpts": [
            "If you use embassy-executor, the single-integrated queue is recommended. The generic queue allows using embassy-time without the embassy executors. single ..."
          ]
        },
        {
          "title": "Found waker not created by the Embassy executor. ...",
          "url": "https://github.com/embassy-rs/embassy/issues/2100",
          "excerpts": [
            "It is fixed if you downgrade to a prior version of Rust nightly. Seems like something changed in the last few days has caused Embassy to crash."
          ]
        },
        {
          "title": "Examining Futexes and how it tackles THUNDERING HERD - Medium",
          "url": "https://medium.com/@arkarthick/examining-futexes-and-how-it-tackles-thundering-herd-71d1e30e2887",
          "excerpts": [
            "Futex as the name suggests are fast user space mutexes. They were introduced primarily to avoid sleeps or system call latencies in the no-contention lock path."
          ]
        },
        {
          "title": "How come a futex take longer than a mutex on waking a thread?",
          "url": "https://stackoverflow.com/questions/14365288/how-come-a-futex-take-longer-than-a-mutex-on-waking-a-thread",
          "excerpts": [
            "I was trying to measure the latency when a thread calling to wake up a sleeping thread. Since it has been said that many synchronization ..."
          ]
        },
        {
          "title": "[PDF] How Low Can You Go? A Limbo Dance for Low-Latency Network ...",
          "url": "https://www.net.in.tum.de/fileadmin/bibtex/publications/papers/gallenmueller_jnsm2022.pdf",
          "excerpts": [
            "Though conserving energy, compared to DPDK's polling strategy, it leads to higher latencies for low packet rates. PF_RING [30] is a packet processing framework ..."
          ]
        },
        {
          "title": "Performing latency tests for platform verification - OKD Documentation",
          "url": "https://docs.okd.io/4.8/scalability_and_performance/cnf-performing-platform-verification-latency-tests.html",
          "excerpts": [
            "The oslat test simulates a CPU-intensive DPDK application and measures all the interruptions and disruptions to test how the cluster handles CPU heavy data ..."
          ]
        },
        {
          "title": "Rust, Embassy, and embedded-hal-async Are Such a ...",
          "url": "https://www.reddit.com/r/embedded/comments/1h9hvq4/rust_embassy_and_embeddedhalasync_are_such_a/",
          "excerpts": [
            "Do what embassy wants and wrap this led in its own task, make static commands queue, send commands to it instead of invoking them manually."
          ]
        },
        {
          "title": "PCIe Gen 5 Storage Array Optimization: Overcoming the ...",
          "url": "https://xinnor.io/blog/pcie-gen-5-storage-array-optimization-overcoming-the-obstacles-to-maximize-performance/",
          "excerpts": [
            "Sep 11, 2023 — We were able to surpass 30 million IOps in user space and nearly reached 25 million IOps in the OS kernel space. This is an excellent ..."
          ]
        },
        {
          "title": "Thread-per-core (work-stealing vs share-nothing) : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/171btot/threadpercore_workstealing_vs_sharenothing/",
          "excerpts": [
            "In the case of HFT, the first goal is the lowest tail-latency possible, and thread-per-core achieves that by minimizing cache-misses and ..."
          ]
        },
        {
          "title": "DPDK Performance Threading Guide",
          "url": "https://doc.dpdk.org/guides-16.04/sample_app_ug/performance_thread.html",
          "excerpts": [
            "In a synthetic test with many threads sleeping and resuming then the measured\njitter is typically orders of magnitude lower than the same measurement made\nfor `nanosleep()`.",
            "The L-thread subsystem resides in the examples/performance-thread/common\ndirectory and is built and linked automatically when building the\n`l3fwd-thread` example. The subsystem provides a simple cooperative scheduler to enable arbitrary\nfunctions to run as cooperative threads within a single EAL thread.",
            "The L-thread scheduler is simply the main loop of a\npthread, and in so far as the host OS is concerned it is a regular pthread\njust like any other.",
            "The simplest mitigation strategy is to use the L-thread sleep API functions,\nof which two variants exist, `lthread_sleep()` and `lthread_sleep_clks()`.",
            "Three threading models are considered: When there is one EAL thread per physical core.",
            "The OS may use sophisticated rules to determine which\nthread should be run next, threads may suspend themselves or make other threads\nready, and the scheduler may employ a time slice giving each thread a maximum\ntime quantum after which it will be preempted in favor of another thread that\nis ready to run."
          ]
        },
        {
          "title": "Sphinx: Thread-per-Core Architecture and Tail Latency (ANCS19)",
          "url": "https://penberg.org/papers/tpc-ancs19.pdf",
          "excerpts": [
            "Application tail latency is critical for services to meet their\nlatency expectations.",
            "We have shown that the thread-per-core\napproach can reduce application tail latency of a key-value\nstore by up to 71% compared to baseline Memcached running\non commodity hardware and Linux.",
            "The source code of Sphinx is available at https://github.com/\npenberg/sphinx in branch ancs19"
          ]
        },
        {
          "title": "Theseus: an Experiment in Operating System Structure and ...",
          "url": "https://www.usenix.org/system/files/osdi20-boos.pdf",
          "excerpts": [
            "Theseus enables all software written in safe Rust, including\n\napplications and libraries, to coexist alongside the core OS\n\ncomponents in a single address space (SAS) and execute at a\n\nsingle privilege level (SPL), building upon language-provided\n\ntype and memory safety to realize isolation instead of hard-\n\nware prot",
            "The SAS/SPL en-\n\nvironment augments this consistent view with _completeness_ ,\n\nin that everything from top-level applications and libraries to\n\ncore kernel components are observable as cells. This enables\n\nTheseus to ( _i_ ) implement a single mechanism, cell swapping,\n\nuniformly applicable to _any_ cell, and ( _ii_ ) jointly evolve cells\n\nfrom multiple system layers (e.g., applications and kernel\n\ncomponents) in a ",
            "Theseus is currently implemented on x86\\_64 with support\n\nfor most hardware features, such as multicore processing,\n\npreemptive multitasking, SIMD extensions, basic networking\n\nand disk ",
            "by K Boos · 2020 · Cited by 93 — We evaluate Theseus to show that it achieves easy and arbi- trary live evolution and increases system availability through fault recovery. We ...See more"
          ]
        },
        {
          "title": "Boot-up Procedure - The Theseus OS Book",
          "url": "https://www.theseus-os.com/Theseus/book/design/booting.html",
          "excerpts": [
            "Currently we use GRUB configured as a legacy bootloader (non-UEFI) and Theseus expects to be booted by a Multiboot2-compliant bootloader. In the future, we ..."
          ]
        },
        {
          "title": "The Theseus OS Blog",
          "url": "https://www.theseus-os.com/",
          "excerpts": [
            "Welcome to the Theseus OS blog, where Theseus OS maintainers announce development and progress updates. ... Theseus OS on GitHub · Theseus OS Organization ..."
          ]
        },
        {
          "title": "Theseus is a new OS written from scratch in Rust ...",
          "url": "https://www.reddit.com/r/rust/comments/jpfuwe/theseus_is_a_new_os_written_from_scratch_in_rust/",
          "excerpts": [
            "Theseus is a new OS written from scratch in Rust to experiment with novel OS structure, better state management, and how to shift OS responsibilities."
          ]
        },
        {
          "title": "rust-vmm/vhost-device",
          "url": "https://github.com/rust-vmm/vhost-device",
          "excerpts": [
            "This repository hosts various 'vhost-user' device backends in their own crates. See their individual README.md files for specific information about those ...",
            "* [CAN](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-can/README.md)",
            "* [Console](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-console/README.md)",
            "* [GPIO](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-gpio/README.md)",
            "* [GPU] ( <https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-gpu/README.md> )",
            "* [I2C](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-i2c/README.md)",
            "* [Input](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-input/README.md)",
            "* [RNG](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-rng/README.md)",
            "* [SCMI](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-scmi/README.md)",
            "* [SCSI](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-scsi/README.md)",
            "* [Sound](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-sound/README.md)",
            "* [SPI](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-spi/README.md)",
            "* [VSOCK](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-vsock/README.md)",
            "The vhost-device workspace also provides a",
            "The vhost-device workspace also provides a",
            "The vhost-device workspace also provides a",
            "The vhost-device workspace also provides a",
            "The binaries built by this repository can be run with any VMM which\ncan act as a vhost-user frontend.",
            "The binaries built by this repository can be run with any VMM which\ncan act as a vhost-user frontend.",
            "The binaries built by this repository can be run with any VMM which\ncan act as a vhost-user frontend.",
            "The binaries built by this repository can be run with any VMM which\ncan act as a vhost-user frontend.",
            "The binaries built by this repository can be run with any VMM which\ncan act as a vhost-user frontend.",
            "The binaries built by this repository can be run with any VMM which\ncan act as a vhost-user frontend.",
            "The vhost-device workspace also provides a\n[template](https://github.com/rust-vmm/vhost-device/blob/main/vhost-device-template/README.md) to help new developers understand how to write their own vhost"
          ]
        },
        {
          "title": "rust-vm-memory+backend-mmap-devel",
          "url": "https://packages.fedoraproject.org/pkgs/rust-vm-memory/rust-vm-memory+backend-mmap-devel",
          "excerpts": [
            "rust-vm-memory+backend-mmap-devel: Safe abstractions for accessing the VM physical memory. ... License(s): Apache-2.0 OR BSD-3-Clause; Maintainer: slp. Related ..."
          ]
        },
        {
          "title": "vmm - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/vmm",
          "excerpts": [
            "vmm is a Rust crate that provides a collection of types and utilities for mathematical operations, specifically tailored for vectors and ..."
          ]
        },
        {
          "title": "Do you need the Virtio driver if you are passing a whole disk ... - Reddit",
          "url": "https://www.reddit.com/r/VFIO/comments/14e28k6/do_you_need_the_virtio_driver_if_you_are_passing/",
          "excerpts": [
            "If you're using PCI passthrough to pass through an entire NVME device then you don't. If you're using virtio-scsi or viotio-blk to pass ..."
          ]
        },
        {
          "title": "flosse/rust-os-comparison",
          "url": "https://github.com/flosse/rust-os-comparison",
          "excerpts": [
            "Theseus is approaching maturity with the ability to execute legacy components in a WASM sandboxed environment. Redox (repository / homepage); Theseus OS ( ..."
          ]
        },
        {
          "title": "vhost - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/vhost",
          "excerpts": [
            "Jun 4, 2025 — A pure rust library for vDPA, vhost and vhost-user. The vhost crate aims to help implementing dataplane for virtio backend drivers."
          ]
        },
        {
          "title": "vm_memory - Rust",
          "url": "https://docs.rs/vm-memory/",
          "excerpts": [
            "This crate aims to provide a set of stable traits to decouple VM memory consumers from VM memory providers. Based on these traits, VM memory consumers could ..."
          ]
        },
        {
          "title": "rust-vmm/.github - Security Vulnerability Process",
          "url": "https://github.com/rust-vmm/.github",
          "excerpts": [
            "Our process for handling reported security concerns and protecting rust-vmm customers through coordinated vulnerability disclosure (CVD) is described below."
          ]
        },
        {
          "title": "vmm - Rust - Docs.rs",
          "url": "https://docs.rs/vmm",
          "excerpts": [
            "vmm is a Rust crate that provides a collection of types and utilities for mathematical operations, specifically tailored for vectors and matrices."
          ]
        },
        {
          "title": "Theseus OS and Rust OS Ecosystem (Theseus, Redox OS, rust-vmm)",
          "url": "https://github.com/theseus-os/Theseus",
          "excerpts": [
            "Theseus is under active development, and although it is not yet mature, we envision that Theseus will be useful in high-end embedded systems or edge datacenter environments.",
            "Languages\n\n* [Rust 96\\.5%](/theseus-os/Theseus/search?l=rust",
            "* [Makefile 1\\.4%](/theseus-os/Theseus/search?l=makefile",
            "* [Assembly 1\\.0%](/theseus-os/Theseus/search?l=assembly",
            "* [Shell 0\\.5%](/theseus-os/Theseus/search?l=shell",
            "* [C++ 0\\.2%](/theseus-os/Theseus/search?l=c%2B%2B",
            "* [C 0\\.2%](/theseus-os/Theseus/search?l=c",
            "\n* Other 0\\.2%\n\n## Footer\n\n[](https://github.com) © 2025 GitHub, Inc.",
            "git clone --recurse-submodules --depth 1 https://github.com/theseus-os/Theseus.git",
            "Linux, 64-bit Debian-based distributions like Ubuntu, tested on Ubuntu 16.04, 18.04, 20.04. + Arch Linux and Fedora have also been reported to work correctly.",
            "To boot over USB, simply run `make usb drive=sdc` , in which `sdc` is the device node for the USB disk itself _( **not a partition** like sdc2)_ to which you want to write the OS image.",
            "Theseus includes two forms of documentation:",
            "We have tested Theseus on a variety of real machines, including Intel NUC devices, various Thinkpad laptops, and Supermicro servers.",
            "Currently, we have only tested booting Theseus via USB or PXE using a traditional BIOS bootloader rather than UEFI, but UEFI is fully supported so it should work."
          ]
        },
        {
          "title": "Theseus's Design and Structure",
          "url": "https://www.theseus-os.com/Theseus/book/design/design.html",
          "excerpts": [
            "Theseus is a safe-language OS, in which everything runs in a single address space (SAS) and single privilege level (SPL).",
            "This includes everything from low-level kernel components to higher-level OS services, drivers, libraries, and more, all the way up to user applications.",
            "Currently, there is a one-to-one relationship between a cell and a Rust crate . The crate is Rust's project container that consists of source code and a dependency manifest file. The crate also serves as Rust's translation unit (elementary unit of compilation); in Theseus we configure each Rust crate to be built into a single\n.o object file (a relocatable ELF file).",
            "Theseus OS does not base its structure on any aspect of the underlying hardware, unlike the above three system designs. Everything, including applications, system services, and core kernel components, exists and runs in a single address space and a single privilege level (in \"kernel space\").",
            "The structure of Theseus is purely software-defined and based on the modularity concept of cells.",
            "The cell concept is a term we coined to represent an individual entity of code and/or data that can be loaded into Theseus.",
            "Theseus is implemented as a collection of many small entities called cells , a software-defined unit of modularity that acts as the core building block of Theseus"
          ]
        },
        {
          "title": "Redox OS and rust-vmm on GitHub",
          "url": "https://github.com/redox-os",
          "excerpts": [
            "Redox OS is a microkernel-based, complete and general-purpose operating system written in Rust created in 2015"
          ]
        },
        {
          "title": "Redox OS and related Rust OS tooling",
          "url": "https://github.com/redox-os/relibc",
          "excerpts": [
            "relibc is a portable POSIX C standard library written in Rust and is under heavy development."
          ]
        },
        {
          "title": "vmm-sys-util - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/vmm-sys-util",
          "excerpts": [
            "Aug 7, 2025 — This crate is a collection of modules that provides helpers and utilities used by multiple rust-vmm components.",
            "crates.io: Rust Package Registry For full functionality of this site it is necessary to enable JavaScript."
          ]
        },
        {
          "title": "rust-vmm/vm-device",
          "url": "https://github.com/rust-vmm/vm-device",
          "excerpts": [
            "The vm-device is tested using unit tests and integration tests. It leverages rust-vmm-ci for continuous testing. All tests are ran in the rustvmm/dev container."
          ]
        },
        {
          "title": "A Low-Latency Optimization of a Rust-Based Secure Operating ...",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC9692816/",
          "excerpts": [
            "Hubris provides memory protection out of the box, requiring chips to provide hardware memory protection mechanisms. At the time of writing, it runs on ARM ...",
            "by I Culic · 2022 · Cited by 6 — The result is a secure low-latency operating system for embedded devices and microcontrollers capable of handling interrupts at latencies as low as 60 µs."
          ]
        },
        {
          "title": "Hubris Reference",
          "url": "https://hubris.oxide.computer/reference/",
          "excerpts": [
            "Hubris is a small operating system intended for deeply-embedded computer systems — the kind that usually don't have any user interface, or way for an operator ...",
            "While we expect interrupts to be handled in unprivileged tasks in general, we have left allowances for applications to handle interrupts in lower-latency but ..."
          ]
        },
        {
          "title": "Hubris - Open Source Real-Time Operating Systems (RTOS)",
          "url": "https://www.osrtos.com/rtos/hubris/",
          "excerpts": [
            "7 days ago — Hubris is an operating environment tailored for 32-bit microcontrollers (specifically ARM Cortex-M variants like STM32 and NXP LPC families) ..."
          ]
        },
        {
          "title": "Tock - Open Source Real-Time Operating Systems (RTOS)",
          "url": "https://www.osrtos.com/rtos/tock/",
          "excerpts": [
            "Tock is an embedded operating system designed for running multiple concurrent, mutually distrustful applications on ARM Cortex-M and RISC-V microcontrollers."
          ]
        },
        {
          "title": "Tock Embedded OS",
          "url": "https://github.com/tock",
          "excerpts": [
            "Tock is an embedded operating system designed for running multiple concurrent, mutually distrustful applications on low-memory and low-power microcontrollers."
          ]
        },
        {
          "title": "TockOS Enablement on i.MX RT 1052 - NXP Community",
          "url": "https://community.nxp.com/t5/NXP-Tech-Blog/TockOS-Enablement-on-i-MX-RT-1052/ba-p/1131151",
          "excerpts": [
            "Tock OS implements memory isolation through the MPU (Memory Protection Unit) already existent in the ARM Cortex-M devices. It is an open ..."
          ]
        },
        {
          "title": "smoltcp-rs/smoltcp: a smol tcp/ip stack",
          "url": "https://github.com/smoltcp-rs/smoltcp",
          "excerpts": [
            "smoltcp is a standalone, event-driven TCP/IP stack that is designed for bare-metal, real-time systems. Its design goals are simplicity and robustness."
          ]
        },
        {
          "title": "Tock Embedded Operating System",
          "url": "https://www.tockos.org/",
          "excerpts": [
            "Missing: target x86_64 ARM Cortex- M"
          ]
        },
        {
          "title": "The Tock Book",
          "url": "https://book.tockos.org/doc/overview",
          "excerpts": [
            "Tock is a secure, embedded operating system for Cortex-M and RISC-V microcontrollers. T",
            ". Tock assumes the hardware includes a memory protection unit (MPU).",
            " The Tock kernel and\nits extensions (called capsules ) are written in Rust.",
            ". Tock can run multiple, independent untrusted processes written in any language.",
            "t the default Tock scheduler is preemptive and uses a round-robin policy.",
            ". Tock can be configured to use different scheduling algorithms"
          ]
        },
        {
          "title": "Tock Design",
          "url": "https://www.tockos.org/documentation/design/",
          "excerpts": [
            "Tock's architecture resolves this trade-off by using a language sandbox to isolated components and a cooperative scheduling model for concurrency in the kernel.",
            "Inside the kernel, Tock achieves this with a\nlanguage-based isolation abstraction called *capsules* that incurs no memory or\ncomputation overhea",
            "In user-space, Tock uses (more-or-less) a traditional\nprocess model where process are isolated from the kernel and each other using\nhardware protection mechanisms.",
            "The MPU limits which\nmemory addresses a process can access.",
            "A process’s code, stored in flash, is made\naccessible with a read-only memory protection region while its memory is allocated\nas a contiguous region of RAM.",
            "Processes are isolated from each other, the kernel, and the underlying hardware\nexplicitly by the hardware Memory Protection Unit (MPU).",
            "The\nkernel schedules processes preemptively, so processes have stronger system\nliveness guarantees than capsules."
          ]
        },
        {
          "title": "Tock (operating system) - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Tock_(operating_system)",
          "excerpts": [
            "Tock is a free and open source embedded operating system for microcontrollers written in Rust.",
            "Development and release",
            "License \"MIT License\"), [Apache Software License",
            "Official website | [www.tockos.or"
          ]
        },
        {
          "title": "oxidecomputer/hubris",
          "url": "https://github.com/oxidecomputer/hubris",
          "excerpts": [
            "\nHubris is a microcontroller operating environment designed for deeply-embedded\nsystems with reliability requirements."
          ]
        },
        {
          "title": "Hubris FAQ (Hubris Fervently Anticipated Questions)",
          "url": "https://github.com/oxidecomputer/hubris/blob/master/FAQ.mkdn",
          "excerpts": [
            "A lightweight, memory-protected, message-passing kernel for deeply embedded systems.",
            "Hubris doesn't provide these at the operating system level.",
            " Hubris-the-system primarily targets the ST STM32H74/5 and NXP LPC55S",
            "The Hubris kernel is designed to be fairly small with a nearly-minimal set of\nabstractions and operations.",
            "Hubris is a flat addressed system. Specifically, Hubris uses physical\n  addresses without translatio",
            "Because of our reliance on nightly toolchain features, we support exactly one\nversion of the toolchain -- the one listed in our `rust-toolchain.toml` file at\nthe top of the repo.",
            "Hubris-the-kernel is essentially device-independent and runs on any ARMv6-M\nprocessor, or any ARMv7E-M or ARMv8-M Main-profile processor with a floating\npoint unit.",
            "Not usefully, no. QEMU's emulation of Cortex-M system peripherals -- specifically, the interrupt\nmodel, NVIC, and SysTick timer, at minimum -- is buggy, and has been for about a\ndecade."
          ]
        },
        {
          "title": "Hubris and Humility (Oxide Computer blog)",
          "url": "https://oxide.computer/blog/hubris-and-humility",
          "excerpts": [
            "Hubris is a memory-protected system, with tasks, the kernel, and drivers all in disjoint protection domains.",
            "As time went on in early 2020 and we found ourselves increasingly forcing existing systems out of the comfort of their design centers, we wondered: was our assumption of using an existing system wrong? Should we in fact be exploring our own de novo operating system? In particular, our colleague [Cliff Biffle](http://cliffle.com/) , who had a ton of experience with both Rust and embedded systems, had a vision for what such a system might look like (namely, the system that he had always wanted for himself!). Cliff dove into a sketch of his ideas, giving the nascent system a name that felt p",
            "Hubris was an all-Rust system that was not distracting itself by accommodating other runtimes; it was microkernel-based, allowing for safety and isolation; it employed a strictly synchronous task model, allowing for it be easily developed and comprehended; and it was small and light, allowing it to fit into some of the tight spots we envisioned for it",
            "Following Cliff’s lead, I dubbed it Humility, and it’s been exciting for me to see the debugger and the operating system work together to yield a higher-quality system.",
            "We have known for quite some time that we would open source Hubris: not only is open source core to our own commercial thesis at Oxide, we also believe that the open source revolution — and its many advantages for customers — are long overdue in the lowest layers of the software stack."
          ]
        },
        {
          "title": "Hubris",
          "url": "https://hubris.oxide.computer/",
          "excerpts": [
            " small open-source operating system for deeply-embedded computer systems. Hubris is ",
            "Hubris provides preemptive multitasking, memory isolation between\nseparately-compiled components, the ability to isolate crashing drivers and\nrestart them without affecting the rest of the system, and flexible\ninter-component messaging that eliminates the need for most syscalls — in about\n2000 lines of Rust.",
            "However, Hubris may be more interesting for what it doesn't have. There are no\noperations for creating or destroying tasks at runtime, no dynamic resource\nallocation, no driver code running in privileged mode, and no C code in the\nsystem.",
            "Hubris source code](https://github.com/oxidecomputer/hubris)"
          ]
        },
        {
          "title": "rust-vmm/vm-memory",
          "url": "https://github.com/rust-vmm/vm-memory",
          "excerpts": [
            "The `vm-memory` crate\nprovides a set of traits to decouple VM memory consumers from VM memory\nproviders. Based on these traits, VM memory consumers can access the physical\nmemory of the VM without knowing the implementation details of the VM memory\nprovide",
            "Platform Support\n\n[]()\n\n* Arch: x86\\_64, ARM64, RISCV64\n* OS: Linux/Unix/",
            "This project is licensed under either of\n\n* [Apache License](http://www.apache.org/licenses/LICENSE-2.0) , Version 2.0\n* [BSD-3-Clause License](https://opensource.org/licenses/BSD-3-Clause)"
          ]
        },
        {
          "title": "rust-vmm / vmm-sys-util",
          "url": "https://github.com/rust-vmm/vmm-sys-util",
          "excerpts": [
            "Helpers and utilities used by multiple rust-vmm components and VMMs",
            "This crate is a collection of modules that provides helpers and utilities\nused by multiple rust-vmm components. The crate implements safe wrappers around common utilities for working\nwith files, event file descriptors, ioctls and others.",
            "Platforms :\n    * x86_64\n    * aarch64\n    * riscv64",
            "License\nThis code is licensed under BSD-3-Clause .",
            "rust-vmm / vmm-sys-util"
          ]
        },
        {
          "title": "Rust-vmm crates on crates.io",
          "url": "https://crates.io/teams/github:rust-vmm:gatekeepers?sort=downloads",
          "excerpts": [
            "rust-vmm\n========\n\ngatekeepers\n-----------\n\nDisplaying\n1-10\nof\n35\ntotal results\n\nSort by\n\nAll-Time Downloads\n\n* [Alphabetical](/teams/github:rust-vmm:gatekeepers)\n* [All-Time Downloads](/teams/github:rust-vmm:gatekeepers?sort=downloads)\n* [Recent Downloads](/teams/github:rust-vmm:gatekeepers?sort=recent-downloads)\n* [Recent Updates](/teams/github:rust-vmm:gatekeepers?sort=recent-updates)\n* [Newly Added](/teams/github:rust-vmm:gatekeepers?sort=new)\n\n1. [seccompiler](/crates/seccompiler)\n   v0.5.0\n\n   Provides easy-to-use seccomp-bpf jailing. All-Time:\n\n   7,130,207\n\n   Recent:\n\n   591,257\n\n   Updated:\n\n\n   6 months ago\n\n   * [Repository](https://github.com/rust-vmm/seccompiler)\n2.\n[vmm-sys-util](/crates/vmm-sys-util)\n   v0.15.0\n\n   A system utility set\n\n   All-Time:\n\n   5,138,439\n\n   Recent:\n\n   869,576\n\n   Updated:\n\n\n   21 days ago\n\n   * [Repository](https://github.com/rust-vmm/vmm-sys-util)\n3. [vm-memory](/crates/vm-memory)\n   v0.16.2\n\n   Safe abstractions for accessing the VM physical memory\n\n   All-Time:\n\n   3,083,484\n\n   Recent:\n\n   306,382\n\n   Updated:\n\n\n   3 months ago\n\n   * [Repository](https://github.com/rust-vmm/vm-memory)\n4. [kvm-ioctls](/crates/kvm-ioctls)\n   v0.24.0\n\n   Safe wrappers over KVM ioctls\n\n   All-Time:\n\n   2,610,138\n\n   Recent:\n\n   187,032\n\n   Updated:\n\n\n   13 days ago\n\n   * [Repository](https://github.com/rust-vmm/kvm)\n5. [linux-loader](/crates/linux-loader)\n   v0.13.0\n\n   A Linux kernel image loading crate. All-Time:\n\n   1,990,059\n\n   Recent:\n\n   141,256\n\n   Updated:\n\n\n   10 months ago\n\n   * [Homepage](https://github.com/rust-vmm/linux-loader)\n   * [Repository](https://github.com/rust-vmm/linux-loader)\n6. [event-manager](/crates/event-manager)\n   v0.4.1\n\n   Abstractions for implementing event based systems\n\n   All-Time:\n\n   1,643,538\n\n   Recent:\n\n   112,158\n\n   Updated:\n\n\n   3 months ago\n\n   * [Repository](https://github.com/rust-vmm/event-manager)\n7. [vm-superio](/crates/vm-superio)\n   v0.8.0\n\n   Emulation for legacy devices\n\n   All-Time:\n\n   1,633,966\n\n   Recent:\n\n   109,680\n\n   Updated:\n\n\n   over 1 year ago\n\n   * [Repository](https://github.com/rust-vmm/vm-superio)\n8.\n[vhost](/crates/vhost)\n   v0.14.0\n\n   a pure rust library for vdpa, vhost and vhost-user\n\n   All-Time:\n\n   1,536,795\n\n   Recent:\n\n   245,411\n\n   Updated:\n\n\n   3 months ago\n\n   * [Documentation](https://docs.rs/vhost)\n   * [Repository](https://github.com/rust-vmm/vhost)\n9. [kvm-bindings](/crates/kvm-bindings)\n   v0.14.0\n\n   Rust FFI bindings to KVM generated using bindgen. All-Time:\n\n   1,435,319\n\n   Recent:\n\n   193,634\n\n   Updated:\n\n\n   13 days ago\n\n   * [Repository](https://github.com/rust-vmm/kvm)\n10. [vm-fdt](/crates/vm-fdt)\n    v0.3.0\n\n    Crate for writing Flattened Devicetree blobs\n\n    All-Time:\n\n    1,432,728\n\n    Recent:\n\n    97,416\n\n    Updated:\n\n\n    almost 2 years ago\n\n    * [Repository](https://github.com/rust-vmm/vm-fdt)\n\n1. [1](/teams/github:rust-vmm:gatekeepers?sort=downloads \"Go to page 1\")\n2. [2](/teams/github:rust-vmm:gatekeepers?page=2&sort=downloads \"Go to page 2\")\n3. [3](/teams/github:rust-vmm:gatekeepers?page=3&sort=downloads \"Go to page 3\")\n4.\n[4](/teams/github:rust-vmm:gatekeepers?page=4&sort=downloads \"Go to page 4\")\n\n\n\nRust\n====\n\n* [rust-lang.org](https://www.rust-lang.org/)\n* [Rust Foundation](https://foundation.rust-lang.org/)\n* [The crates.io team](https://www.rust-lang.org/governance/teams/dev-tools)\n\nGet Help\n========\n\n* [The Cargo Book](https://doc.rust-lang.org/cargo/)\n* [Support](/support)\n* [System Status](https://status.crates.io/)\n* [Report a bug](https://github.com/rust-lang/crates.io/issues/new/choose)\n\nPolicies\n========\n\n* [Usage Policy](/policies)\n* [Security](/policies/security)\n* [Privacy Policy](https://foundation.rust-lang.org/policies/privacy-policy/)\n* [Code of Conduct](https://www.rust-lang.org/policies/code-of-conduct)\n* [Data Access](/data-access)\n\nSocial\n======\n\n* [rust-lang/crates.io](https://github.com/rust-lang/crates.io/)\n* [](https://rust-lang.zulipchat.com//318791/t-crates-io)\n* [@cratesiostatus](https://twitter.com/cratesiostatus)"
          ]
        },
        {
          "title": "[PDF] Theseus: an Experiment in Operating System Structure and State ...",
          "url": "https://systems-rg.github.io/slides/2022-05-06-theseus.pdf",
          "excerpts": [
            "The primary goal is to design and implement an OS with the maximum degree of state spill freedom possible. • State spill is defined as one software component ..."
          ]
        },
        {
          "title": "Task Management - The Theseus OS Book",
          "url": "https://www.theseus-os.com/Theseus/book/subsystems/task.html",
          "excerpts": [
            "You can use any standard synchronization types for inter-task communication, e.g., shared memory or channels. You can catch the action of stack unwinding after ..."
          ]
        },
        {
          "title": "Theseus: an Experiment in Operating System Structure and State ...",
          "url": "https://www.academia.edu/51083894/Theseus_an_Experiment_in_Operating_System_Structure_and_State_Management",
          "excerpts": [
            "This paper describes an operating system (OS) called Theseus. Theseus is the result of multi-year experimentation to redesign and improve OS modularity by ..."
          ]
        },
        {
          "title": "[PDF] Theseus: a State Spill-free Operating System - Efficient Computing Lab",
          "url": "https://www.yecl.org/publications/boos2017plos.pdf",
          "excerpts": [
            "This paper establishes the design principles and high-level implementation details of Theseus, an OS free from state spill. Our future work ..."
          ]
        },
        {
          "title": "Booting PowerEdge Server w/PCIe NVMe Using Clover Boot",
          "url": "https://www.youtube.com/watch?v=wqBa-eAnkCw&pp=0gcJCf0Ao7VqN5tD",
          "excerpts": [
            "Missing: Theseus x86_64 portability SMP VirtIO"
          ]
        },
        {
          "title": "Multiboot USB UEFI & Legacy All In One - YouTube",
          "url": "https://www.youtube.com/watch?v=mZB8gwkBtvA",
          "excerpts": [
            "Missing: Theseus x86_64 server portability SMP PCIe NVMe VirtIO"
          ]
        },
        {
          "title": "Microkernels - The Redox Operating System",
          "url": "https://doc.redox-os.org/book/microkernels.html",
          "excerpts": [
            "In microkernels the kernel components (drivers, filesystems, etc) are moved to user-space, thus bugs on them don't crash the kernel. This is very important to ...",
            "Microkernels are very modular by nature. You can replace, reload, modify, change, and remove modules, on runtime, without even touching the kernel. Modern ..."
          ]
        },
        {
          "title": "Our Philosophy - The Redox Operating System",
          "url": "https://doc.redox-os.org/book/philosophy.html",
          "excerpts": [
            "Redox OS is predominately MIT X11-style licensed, including all software, documentation, and fonts. There are only a few exceptions to this, which are all ..."
          ]
        },
        {
          "title": "POSIX Conformance Testing for the Redox Signals Project",
          "url": "https://www.redox-os.org/news/posix-conformance-testing/",
          "excerpts": [
            "We are developing tests to verify that the new functionality is in reasonable compliance with the POSIX.1-2024 standard."
          ]
        },
        {
          "title": "System Services in User Space",
          "url": "https://doc.redox-os.org/book/user-space.html",
          "excerpts": [
            "As any microkernel-based operating system, most kernel components are moved to user-space and adapted to work on it. Monolithic kernels in general have ..."
          ]
        },
        {
          "title": "redox-os / drivers · GitLab",
          "url": "https://gitlab.redox-os.org/redox-os/drivers",
          "excerpts": [
            "Jun 10, 2018 — A device driver on Redox is an user-space daemon that use system calls and schemes to work. For operating systems with monolithic kernels, drivers use internal ..."
          ]
        },
        {
          "title": "Redox OS: Real hardware breakthroughs, and focusing on ...",
          "url": "https://www.reddit.com/r/linux/comments/e2uj7k/redox_os_real_hardware_breakthroughs_and_focusing/",
          "excerpts": [
            "redox is an OS with a microkernel built on Rust language. Rust was designed to prevent a lot of the most common bugs and sources of security ..."
          ]
        },
        {
          "title": "Using GPLv-3 instead of MIT license in Redox os",
          "url": "https://www.reddit.com/r/Redox/comments/mfr749/using_gplv3_instead_of_mit_license_in_redox_os/",
          "excerpts": [
            "It is licensed MIT. That means, Everyone who copies this would be just can be made proprietary. Yes, this is benifit for the company that copied and added ..."
          ]
        },
        {
          "title": "rust-vmm/vhost - GitHub",
          "url": "https://github.com/rust-vmm/vhost",
          "excerpts": [
            "vhost -> A pure rust library for vDPA, vhost and vhost-user. · vhost-user-backend -> It provides a framework to implement vhost-user backend services."
          ]
        },
        {
          "title": "vmm-reference/LICENSE-BSD-3-CLAUSE at main · rust-vmm/vmm ...",
          "url": "https://github.com/rust-vmm/vmm-reference/blob/main/LICENSE-BSD-3-CLAUSE",
          "excerpts": [
            "A VMM implementation based of rust-vmm components. Contribute to rust-vmm/vmm-reference development by creating an account on GitHub."
          ]
        },
        {
          "title": "vmm - Keywords - crates.io: Rust Package Registry",
          "url": "https://crates.io/keywords/vmm",
          "excerpts": [
            "All Crates for keyword 'vmm' · dbs-address-space v0.3.0. address space manager for virtual machines. · dbs-boot v0.4.0. Traits and structs for booting sandbox."
          ]
        },
        {
          "title": "Redox OS FAQ and Rust VMM overview",
          "url": "https://www.redox-os.org/faq/",
          "excerpts": [
            "Redox is a microkernel-based, complete, fully-functioning and general-purpose operating system created in 2015, with a focus on safety, freedom, reliability, correctness, and pragmatism.",
            "It has advanced features such as system modularity, kernel panic resistence, driver reincarnation, protection against bad drivers and secure interfaces for ..."
          ]
        },
        {
          "title": "kvm-ioctls - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/kvm-ioctls",
          "excerpts": [
            "kvm-ioctls. The kvm-ioctls crate provides safe wrappers over the KVM API, a set of ioctls used for creating and configuring Virtual Machines ..."
          ]
        },
        {
          "title": "rust-vmm",
          "url": "https://buildkite.com/rust-vmm",
          "excerpts": [
            "vm-device rust-vmm crate CI. main. Speed3.3mReliability100%Builds- ... vm-memory-ci. Public. main. Speed11.0mReliability75%Builds- ..."
          ]
        },
        {
          "title": "rust-vmm",
          "url": "https://www.cl.cam.ac.uk/research/security/seminars/archive/slides/2019-05-07-b.pdf",
          "excerpts": [
            "Faster development for new custom VMMs. - Security & Testability. - Clean interface. - Reduce code duplication (CrosVM & Firecracker)."
          ]
        },
        {
          "title": "boot_info - Rust - Theseus OS",
          "url": "https://www.theseus-os.com/Theseus/doc/boot_info/index.html",
          "excerpts": [
            "This crate provides an abstraction over multiboot2 and UEFI boot information. It allows the kernel's initialisation to be the same, regardless of whether it ..."
          ]
        },
        {
          "title": "Drivers - The Redox Operating System",
          "url": "https://doc.redox-os.org/book/drivers.html",
          "excerpts": [
            "On Redox the device drivers are user-space daemons, being a common Unix process they have their own namespace with restricted schemes."
          ]
        },
        {
          "title": "Boot Process - The Redox Operating System",
          "url": "https://doc.redox-os.org/book/boot-process.html",
          "excerpts": [
            "Redox supports UEFI booting on x86-64, ARM64, and RISC-V 64-bit machines. UEFI starts up in 64-bit mode; thus, the boot process doesn't need multiple stages."
          ]
        },
        {
          "title": "Redox: An operating system in Rust - LWN.net",
          "url": "https://lwn.net/Articles/979524/",
          "excerpts": [
            "The Redox kernel is a microkernel — meaning that core components such as drivers and filesystems can run in user space — but rather than invent ..."
          ]
        },
        {
          "title": "Introduction - The Redox Operating System",
          "url": "https://doc.redox-os.org/book/",
          "excerpts": [
            "Redox OS is a general-purpose operating system written in Rust. Our aim is to provide a fully functioning Unix-like microkernel-based operating system."
          ]
        },
        {
          "title": "Hardware Support - The Redox Operating System",
          "url": "https://doc.redox-os.org/book/hardware-support.html",
          "excerpts": [
            "Compatibility Table ; CPU, - Intel 64-bit (x86_64) - Intel 32-bit (i686) from Pentium II and after with limitations - AMD 32/64-bit - ARM 64-bit (Aarch64) with ..."
          ]
        },
        {
          "title": "vmm_sys_util - Rust",
          "url": "https://docs.rs/vmm-sys-util/",
          "excerpts": [
            "vmm_sys_util is a collection of modules providing helpers and utilities used by multiple rust-vmm components."
          ]
        },
        {
          "title": "Real-time optimization",
          "url": "https://www.osadl.org/Real-time-optimization.qa-farm-latency-optimization.0.html",
          "excerpts": [
            "May 20, 2023 — The worst-case latency of a system depends on a large number of variables; some of them are part of the kernel configuration."
          ]
        },
        {
          "title": "AM625: Linux cyclictest results - Processors forum - TI E2E",
          "url": "https://e2e.ti.com/support/processors-group/processors/f/processors-forum/1228035/am625-linux-cyclictest-results",
          "excerpts": [
            "The average latency improves significantly (4x better as you show) and worst case latency stays somewhat similar is an expect result. With core isolation ..."
          ]
        },
        {
          "title": "From epoll to io_uring's Multishot Receives — Why 2025 Is the Year ...",
          "url": "https://codemia.io/blog/path/From-epoll-to-iourings-Multishot-Receives--Why-2025-Is-the-Year-We-Finally-Kill-the-Event-Loop",
          "excerpts": [
            "In the above example, io_uring might reduce p99 latency by ~25% and p99.9 by ~30% versus epoll (actual numbers vary by workload, but this trend ...",
            "Aug 13, 2025 — In benchmarking, io_uring shows massive throughput and latency ... In the above example, io_uring might reduce p99 latency by ~25% and p99."
          ]
        },
        {
          "title": "Understanding Delays in AF_XDP-based Applications",
          "url": "https://arxiv.org/html/2402.10513v1",
          "excerpts": [
            "Feb 16, 2024 — We conduct an experimental study to understand the XDP/AF_XDP ecosystem and detect microseconds delays to better architect future latency-sensitive ...See more"
          ]
        },
        {
          "title": "Understanding Delays in AF_XDP-based Applications",
          "url": "https://hal.science/hal-04458274v1/file/main.pdf",
          "excerpts": [
            "by KC du Perron · 2024 · Cited by 4 — In this section we investigate the impact of the various optimization parameters on the latency, for both Intel and. Mellanox cards. To do so, ...See more"
          ]
        },
        {
          "title": "P99 Conf: 3 Ways to Squash Application Latency - The New Stack",
          "url": "https://thenewstack.io/p99-conf-3-ways-to-squash-application-latency/",
          "excerpts": [
            "To reduce latency, minimize data movement as much as possible, Enberg advised. Put the data as close as possible to where it is being used."
          ]
        },
        {
          "title": "Reduce microsecond jitter (using realtime kernel)",
          "url": "https://unix.stackexchange.com/questions/706405/reduce-microsecond-jitter-using-realtime-kernel",
          "excerpts": [
            "I've written a small program in C++ to read the current pin state, as well as print out the time in microseconds since the start of the program."
          ]
        },
        {
          "title": "Real-time Linux is officially part of the kernel after decades ...",
          "url": "https://www.reddit.com/r/technews/comments/1fktivs/realtime_linux_is_officially_part_of_the_kernel/",
          "excerpts": [
            "It doesn't have as much delay as regular Linux so you can use it for time sensitive applications where delay is unacceptable. We use it (RT ..."
          ]
        },
        {
          "title": "Benchmarking Report: Theseus Engine",
          "url": "https://voltrondata.com/benchmarks/theseus",
          "excerpts": [
            "This latest update (March 2025) includes refreshed results for Voltron Data's Theseus engine at 10TB, 30TB, and 100TB TPC-H scale factors.See more"
          ]
        },
        {
          "title": "[Perf Question] How to \"benchmark(?)\"/investigate latency ...",
          "url": "https://www.reddit.com/r/rust/comments/17dfmmk/perf_question_how_to_benchmarkinvestigate_latency/",
          "excerpts": [
            "I am currently writing a custom implementation of a distributed consensus algorithm for replicated state machines and benchmarking/testing it locally."
          ]
        },
        {
          "title": "Hubris – A small operating system for deeply-embedded ...",
          "url": "https://news.ycombinator.com/item?id=29390751",
          "excerpts": [
            "Nov 30, 2021 — This does increase the minimum effective interrupt latency in Hubris, and if not very carefully implemented, the jitter also. 2. Hubris ..."
          ]
        },
        {
          "title": "(PDF) A Low-Latency Optimization of a Rust-Based Secure ...",
          "url": "https://www.researchgate.net/publication/365315287_A_Low-Latency_Optimization_of_a_Rust-Based_Secure_Operating_System_for_Embedded_Devices",
          "excerpts": [
            "Nov 8, 2022 — The only downside of Tock is that it lacks the low-latency real-time component. Therefore, the purpose of this research is to leverage the ..."
          ]
        },
        {
          "title": "ELC-Linux-RT.pdf",
          "url": "https://www.rocketboards.org/foswiki/pub/Documentation/AlteraSoCLTSIRTKernel/ELC-Linux-RT.pdf",
          "excerpts": [
            "Vanilla 3.10 Kernel, Fully Loaded",
            "**μ** **= ~75**",
            "\n\n**Kurtosis = ~1000",
            "\n\n**95th Perc = ~100**",
            "\n\n**99th Perc = ~110**",
            "**μ** **= ~67**",
            "**μ** **= ~67**",
            "\n\n**5th Perc = ~46**",
            "\n\n**5th Perc = ~46**",
            "\n\n**95th Perc = ~86**",
            "\n\n**99th Perc = ~100**",
            "Scheduling Latency Jitter Comparison"
          ]
        },
        {
          "title": "From the analysis: DPDK vs io_uring vs Linux network stack",
          "url": "http://liu.diva-portal.org/smash/record.jsf?pid=diva2:1789103",
          "excerpts": [
            "DPDK is widely adopted in the telecommunication industry and has been for several years. io\\_uringis a new Linux system call interface, as of this writing, used for asynchronous I/O operations and could potentially offer better network performance than the standard Linux network stack.",
            "From the analysis, it was concluded that DPDK had the best overall performance when considering packet loss, throughput, latency, and packet rate.io\\_uring performed better than the Linux network stack in terms of throughput, packet loss, and packet rate but was not considerably better in terms of latency.",
            "The Linux kernel network parameters were tuned in order to investigate what kind of performance increase can be achieved for an application using io\\_uring to gain a better understanding of howio\\_uring compares to DPDK.",
            "We conclude that io\\_uring shows some promising results compared to the standard Linux network stack, but is not yet able to compare with DPDK in network performance."
          ]
        },
        {
          "title": "Latency and Performance Comparison of DPDK vs AF_XDP with TRex (DIVa/Theses context)",
          "url": "http://www.diva-portal.org/smash/get/diva2:1897043/FULLTEXT01.pdf",
          "excerpts": [
            "When measuring the latency and PDV all output from TRex was saved every 0.1 seconds for the 300 seconds. That generates 3000 samples per test. This results in a ...See more On the other hand, AF\\_XDP\n\nempl",
            "employs in-kernel processing, leveraging a specialized data path in the Linux\n\noperating system to bypass the entire network stack while keeping the interface\n\naccessible within the O",
            "DPDK has supported it for a long time, while AF\\_XDP\n\nintroduced it just before the start of the theses. DPDK terms this feature\n\n“indirect mbufs”, while AF\\_XDP refers to it as “multi-buffe",
            "The average latency in Table 5.1 shows that the average latency for DPDK is still lower than AF_XDP. The reason is the logarithmic binning used by TRex does ...",
            "Since the device undergoing testing operates at full load,\n\nmeasurements must be taken on an external node",
            "TRex provides the average latency measurement.",
            "DPDK have an overall lower latency than\n\nAF\\_XD",
            "This\n\ncapability allows advanced packet operations to be executed without data\n\ncopying, which is essential for specific workloads, as data copying often\n\nsignificantly degrades performan",
            "With both techniques implementing this\n\nfeature, the question arises: which one offers superior performance"
          ]
        },
        {
          "title": "SPDK (Storage Performance Development Kit)",
          "url": "https://www.simplyblock.io/glossary/spdk/",
          "excerpts": [
            "Low Latency: Eliminates system calls and kernel intervention, leading to microsecond-level latencies. Scalability: Supports large-scale storage deployments, ..."
          ]
        },
        {
          "title": "Xenomai latency test on RPI2",
          "url": "https://community.emlid.com/t/xenomai-latency-test-on-rpi2/1136",
          "excerpts": [
            "Nov 11, 2015 — Hi Emlid team, I finally completed the linux kernel xenomai ipipe patch for the raspberryPie2 and was able to run some latency test: min ..."
          ]
        },
        {
          "title": "Porting Linux Drivers to RTOS - Electronics Stack Exchange",
          "url": "https://electronics.stackexchange.com/questions/17336/porting-linux-drivers-to-rtos",
          "excerpts": [
            "Jul 25, 2011 — Three months is probably a conservative estimate for the porting effort; if you can identify the APIs used (network, interrupt, timers, waiting/ ..."
          ]
        },
        {
          "title": "How Genode came to the Pandaboard",
          "url": "https://genode.org/documentation/articles/pandaboard",
          "excerpts": [
            "This article describes the journey of the Genode developers to make the Genode OS Framework available on this platform.",
            "This is a significant improvement but still well below our goal to achieve at least 90% of the throughput of native Linux. Cached versus uncached memory objects."
          ]
        },
        {
          "title": "PREEMPT_RT vs. Xenomai on TX2 - NVIDIA Developer Forums",
          "url": "https://forums.developer.nvidia.com/t/preempt-rt-vs-xenomai-on-tx2/161061",
          "excerpts": [
            "In many publications, it is reported that Xenomai has better hard realtime performance with less jitter than the PREEMPT_RT patch."
          ]
        },
        {
          "title": "Real-time Linux explained (LinuxGizmos article, 2017)",
          "url": "https://linuxgizmos.com/real-time-linux-explained/",
          "excerpts": [
            "Xenomai performed better on most tests, and offered far less jitter, the differences were not as great as the 300 to 400 percent latency superiority claimed by some Xenomai boosters, said Altenberg.",
            "When tests were performed on userspace tasks — which Altenberg says is the most real-world, and therefore the most important, test — the worst-case reaction was about 90 to 95 microseconds for both Xenomai and RTL/PREEMPT.RT, he claimed.",
            "When they isolated a single CPU in the dual Cortex-A9 system for handling the interrupt in question, which Altenberg says is fairly common, PREEMPT.RT performed slightly better, coming in around 80 microseconds.",
            " (For more details, check out the video about 33 minutes in.) Altenberg acknowledges that his 12-hour test is the bare minimum, compared to OSADL’s two- to three-year tests, and that it is “not a mathematical proof.",
            "In any case, he suggests that RTL deserves a handicap considering its easier development process. "
          ]
        },
        {
          "title": "Defining SLA/SLO-Driven Monitoring Requirements in 2025",
          "url": "https://uptrace.dev/blog/sla-slo-monitoring-requirements",
          "excerpts": [
            "Jun 14, 2025 — SLA/SLO-driven monitoring aligns your observability strategy with business objectives by defining measurable service targets and implementing monitoring ..."
          ]
        },
        {
          "title": "When the Long Tail Turns Against You - LinkedIn",
          "url": "https://www.linkedin.com/pulse/when-long-tail-turns-against-you-colin-gallagher-nbvic",
          "excerpts": [
            "You should always look at percentile latencies (p99, p99.9, p99.99) to understand the true impact on your AI workloads. By measuring and ..."
          ]
        },
        {
          "title": "Every 100ms in Added Page Load Time Cost 1% in Revenue",
          "url": "https://www.conductor.com/academy/page-speed-resources/faq/amazon-page-speed-study/",
          "excerpts": [
            "Amazon found that every 100ms in added page load time cost them 1% in sales. This has now become one of the most referenced data points surrounding page speed ..."
          ]
        },
        {
          "title": "Amazon Found Every 100ms of Latency Cost them 1% in ...",
          "url": "https://www.gigaspaces.com/blog/amazon-found-every-100ms-of-latency-cost-them-1-in-sales",
          "excerpts": [
            "10 years ago, Amazon found that every 100ms of latency cost them 1% in sales. Google found an extra .5 seconds in search page generation time dropped ..."
          ]
        },
        {
          "title": "Using netperf and ping to measure network latency",
          "url": "https://cloud.google.com/blog/products/networking/using-netperf-and-ping-to-measure-network-latency",
          "excerpts": [
            "Jun 17, 2020 — After 100 packets, the summary reports that we observed an average latency of 0.146 milliseconds, or 146 microseconds. For comparison, let's run ..."
          ]
        },
        {
          "title": "Attack of the killer microseconds",
          "url": "https://research.google/pubs/attack-of-the-killer-microseconds/",
          "excerpts": [
            "by LA Barroso · 2017 · Cited by 351 — The computer systems we use today make it easy for programmers to mitigate event latencies in the nanosecond and millisecond time scales."
          ]
        },
        {
          "title": "Understanding the Long Tail Latency of TCP in Large-Scale Cloud ...",
          "url": "https://dl.acm.org/doi/10.1145/3735358.3735393",
          "excerpts": [
            "The P99 latency exhibits an even more dramatic deterioration - with just 0.5% packet loss, it spikes to approximately 175 ms and plateaus at ..."
          ]
        },
        {
          "title": "RTOS vs Linux: Essential decision-making factors",
          "url": "https://lembergsolutions.com/blog/rtos-vs-linux-essential-decision-making-factors",
          "excerpts": [
            "Linux provides quicker development time and lower initial costs; however, production expenses will be higher.",
            "Linux allows for faster deployment and reduces software development costs.",
            "RTOS may be more cost-effective for mass production.",
            "The key factors influencing the decision include the complexity of developing business logic, development costs, and performance.",
            "Our team spent over a month resolving driver issues, even with the manufacturer’s support."
          ]
        },
        {
          "title": "RTOS vs Embedded Linux: A Decision Guide",
          "url": "https://usa.seco.com/blog/details/rtos-vs-embedded-linux-a-decision-guide",
          "excerpts": [
            "High-volume products often benefit from RTOS's lower per-unit costs, while products requiring frequent feature updates may find Linux's rich ecosystem more cost",
            "System Resources"
          ]
        },
        {
          "title": "Google SRE Workbook - Implementing SLOs",
          "url": "https://sre.google/workbook/implementing-slos/",
          "excerpts": [
            "This process may allow you to mathematically identify a relationship between a key business metric (e.g., sales) and a measurable technical metric (e.g., latency).",
            "A latency SLO can capture this user base by setting multiple thresholds: 90% of requests are faster than 100 ms, and 99% of requests are faster than 400 ms.",
            "You may want to experiment with the reliability of your application and measure which changes in reliability (e.g., adding latency into page load times) have a measurably adverse impact on user behavior (e.g., percentage of users completing a purchase). We recommend performing this sort of analysis only if you are confident that you have error budget to burn. There are many subtle interactions between latency, availability, customers, business domains, and competition (or lack thereof)."
          ]
        },
        {
          "title": "Tail latency and performance primer",
          "url": "https://last9.io/blog/tail-latency/",
          "excerpts": [
            "The Importance of Tail Latency in Web Services",
            "1. **User Experience** : Today, users expect instant responses from web services. Even if 99% of requests are fast, that 1% of high latency responses can frustrate users and potentially drive them away.",
            "2. **System Reliability** : High tail latency can be an early warning sign of underlying system issues in the backend. Ignoring it is akin to disregarding warning signs in any complex system.",
            "3. **Resource Allocation** : Understanding tail latency helps in allocating resources more efficiently across data center cores. Optimization efforts may need to focus on edge cases rather than just average throughput.",
            "4. **SLOs and SLAs** : Many [service level objectives (SLOs) and agreements (SLAs)](https://last9.io/blog/sla-vs-slo-vs-sli/) are based on percentile latencies, often including the 99th percentile latency. Missing these targets can have significant business conse",
            "Focusing solely on average latency can be misleading. It's crucial to look at percentile latencies:",
            "* **p50 (median)** : This indicates what the \"typical\" request looks like.",
            "* **p95** : 95% of requests are faster than this value.",
            "## **Measuring Tail Latency: Beyond Averages**",
            "Tail latency significantly impacts large-scale systems. This blog covers its importance, contributing factors, and effective reduction ... [Image](https://last9.ghost.io/content/images/",
            "* **p99** : This is where tail latency becomes apparent. Only 1% of requests are slower than this."
          ]
        },
        {
          "title": "SPDK: Storage Performance Development Kit",
          "url": "https://spdk.io/doc/",
          "excerpts": [
            "SPDK is a Storage Performance Development Kit. It includes applications like iSCSI Target, NVMe over Fabrics Target, and vhost Target."
          ]
        },
        {
          "title": "Usenix HotStorage 2019 paper (EvFS and SPDK Blobstore)",
          "url": "https://www.usenix.org/system/files/hotstorage19-paper-yoshimura.pdf",
          "excerpts": [
            " is a user-level file system in SPDK,\nexposes its optimized user-level storage stack to applications\n\nthrough its non-POSIX interface. BlobFS ",
            "EvFS builds a UNIX file-directory structure within an\nNVM namespace by using SPDK Blobstore [4].",
            "Blobstore provides interfaces for reads, writes, and resizes on\na BLOB, and thus, we regard a BLOB as an inode.",
            "Blobstore updates these metadata in mem-\nory and writes it back to storage when we explicitly invoke a\nsynchronization AP",
            "Blobstore has the crash safety at page granularity, but EvFS handles the file-level consistency by using the synchronization API.",
            "Blobstore has the crash safety at page granularity, but EvFS handles the file-level consistency by using the synchronization API."
          ]
        },
        {
          "title": "littlefs — MCUXpresso SDK Documentation",
          "url": "https://mcuxpresso.nxp.com/mcuxsdk/25.09.00-pvw1/html/middleware/littlefs/README.html",
          "excerpts": [
            "littlefs is a block based filesystem that uses small logs to\nstore metadata and larger copy-on-write (COW) structures to store file data.",
            "Jul 17, 2025 — Power-loss resilience - littlefs is designed to handle random power failures. All file operations have strong copy-on-write guarantees and ...",
            "Power-loss resilience** \\- littlefs is designed to handle random power\nfailures.\nAll file operations have strong copy-on-write guarantees and if\npower is lost the filesystem will fall back to the last known good sta",
            "Bounded RAM/ROM** \\- littlefs is designed to work with a small amount of\nmemory. RAM usage is strictly bounded, which means RAM consumption does not\nchange as the filesystem grows. The filesystem contains no unbounded\nrecursion and dynamic memory is limited to configurable buffers that can be\nprovided statical",
            "Dynamic wear leveling** \\- littlefs is designed with flash in mind, and\nprovides wear leveling over dynamic bloc"
          ]
        },
        {
          "title": "WHAT IS Flash-Friendly File System (F2FS)?",
          "url": "https://docs.kernel.org/filesystems/f2fs.html",
          "excerpts": [
            "F2FS is a file system exploiting NAND flash memory-based storage devices, which is based on Log-structured File System (LFS).",
            "f2fs is a tool to check the consistency of an f2fs-formatted partition, which examines whether the filesystem metadata and user-made data are cross-referenced ..."
          ]
        },
        {
          "title": "NVMe Atomic Write with QEMU",
          "url": "https://blogs.oracle.com/linux/post/nvme-atomic-write-with-qemu",
          "excerpts": [
            "By default, a single NVMe logical block write (512 bytes) is guaranteed to be atomic .",
            "If a NVMe controller provides an atomic guarantee for writes larger than a single logical block, its atomic write behavior must be specified using the Atomicity Parameters defined in the NVMe Specification (2).",
            "An informative write-up of how to configure NVMe atomic write support using QEMU."
          ]
        },
        {
          "title": "Durability: NVMe disks",
          "url": "https://www.evanjones.ca/durability-nvme.html",
          "excerpts": [
            "The first NVMe mechanism that can be used to ensure data is durably written is the Flush command (Base Specification 7.1 \"Flush command\"). It writes everything in the write cache to non-volatile memory. More specifically, \"The flush applies to all commands [...] completed by the controller prior to the submission of the Flush command\".",
            "From the Command Set Specification Section 2.1.2 \"Command Ordering Requirements\": \"each command is processed as an independent entity without reference to other commands [...]. If there are ordering requirements between these commands, host software or the associated application is required to enforce that ordering\".",
            "The couple of NVMe devices I looked at have these values set to zero (according to the `nvme id-ctrl` command). Somewhat confusingly, this means writes of a single logical block are atomic.",
            "NVMe includes *optional* support for atomic writes, with different values for \"normal operation\" and after power failure. These are defined by the Atomic Write Unit Normal (AWUN) and Atomic Write Unit Power Fail (AWUPF) settings for the device.",
            "The Flush command\n-----------------\n\nWithout special commands, NVMe provides no guarantees about what data will survive a power failure (Command Set 2.1.4.2 \"AWUPF/NAWUPF\"",
            "The disk could have any of the writes, with no relation to the order they were submitted or completed. It could also choose to return an error for all the ranges.",
            "The second mechanism to ensure durability is to set the Force Unit Access option on Write commands. This means that \"the controller shall write that data and metadata, if any, to non-volatile media before indicating command completion\" (Command Specification 3.2.6 \"Write Command\" Figure 63).",
            "Disabling write caching\n-----------------------\n\nThe last mechanism that may ensure durability is to explicitly disable the write cache. If an NVMe device has a volatile write cache, it must be controllable. This means you can disable it (Base Specification 5.27.1.4 \"Volatile Write Cache\""
          ]
        },
        {
          "title": "USENIX 2024 USENIX Annual Technical Conference - SPDK-focused storage research",
          "url": "https://www.usenix.org/system/files/atc24-yi-shushu.pdf",
          "excerpts": [
            "ts\n\nUser-space storage stack. The user-space storage framework,\nsuch as Storage Performance Development Kit (SPDK) [70],\nis a promising solution to address Challenge 1, that is, the\nsoftware overheads in the storage stack.",
            "First, two-phase write introduces extra metadata, which in-\n\ncreases crash consistency cost (Challenge 3). Specifically, to\nsupport out-of-place updates, the AFA engine maintains extra\nmapping tables in host memory to record where data is actu-\nally st",
            "In the conversion phase, it reads 1⇥ data out for\nparity computation and writes 1⇥ data and 0.5⇥ parity back\nto SSDs. The total write amplification is 3.5⇥.",
            "wo-phase write causes significant write amplifica-\n\ntion (Challenge 4",
            "wo-phase write AFA en-\ngines impose two more challenges. First, two-phase write introduces extra metadata, which in-\n\ncreases crash consistency cost (Challenge 3). Specifically, to\nsupport out-of-place updates, the AFA engine maintains extra\nmapping tables in host memory to record where data is actu-\nally store",
            "ework,\nsuch as Storage Performance Development Kit (SPDK) [70],\nis a promising solution to address Challenge 1, that is, the\nsoftware overheads in the storage stack."
          ]
        },
        {
          "title": "NVME_NVM_COMMANDS - Win32 apps",
          "url": "https://learn.microsoft.com/en-us/windows/win32/api/nvme/ne-nvme-nvme_nvm_commands",
          "excerpts": [
            "Mar 13, 2023 — NVME_NVM_COMMAND_FLUSH The Flush command that commits data and metadata associated with the specified namespace(s) to nonvolatile media."
          ]
        },
        {
          "title": "The semantics of a FLUSH command (per NVMe spec) ...",
          "url": "https://news.ycombinator.com/item?id=30421010",
          "excerpts": [
            "Feb 21, 2022 — A flush command only guarantees, upon completion, that all writes COMPLETED prior to submission of the flush are non-volatile. Not all ..."
          ]
        },
        {
          "title": "Ive just been scanning through linux kernel code (inc ext4). Are you ...",
          "url": "https://news.ycombinator.com/item?id=30372498",
          "excerpts": [
            "What you're looking for is the actual NVMe commands. There's two things: FLUSH (which flushes the whole cache), and a WRITE with the FUA bit ...",
            "Feb 17, 2022 — There's two things: FLUSH (which flushes the whole cache), and a WRITE with the FUA bit set (which basically turns that write into write-through ..."
          ]
        },
        {
          "title": "BlobFS (Blobstore Filesystem)",
          "url": "https://spdk.io/doc/blobfs.html",
          "excerpts": [
            "BlobFS is a Blobstore Filesystem, primarily tested with RocksDB, and can be mounted as a kernel filesystem using FUSE. It has a flat namespace.",
            "BlobFS provides a FUSE plug-in to mount an SPDK BlobFS as a kernel filesystem for inspection or debug purposes."
          ]
        },
        {
          "title": "SPDK 17.03 Release: VM Optimization & Blobstore",
          "url": "https://spdk.io/release/2017/03/31/17.03_release/",
          "excerpts": [
            "Blobstore and BlobFS have been designed specifically to enable accelerated performance for these types of applications. See Blobstore Design ..."
          ]
        },
        {
          "title": "YangKeao/rocksdb-spdk: A library that provides an ... - GitHub",
          "url": "https://github.com/YangKeao/rocksdb-spdk",
          "excerpts": [
            "A library that forms the core building block for a fast key value server, especially suited for storing data on flash drives."
          ]
        },
        {
          "title": "A Robust Flash File System Since 2002 | Yaffs - A Flash File System ...",
          "url": "https://yaffs.net/",
          "excerpts": [
            "This is write amplification - a single small write causing a cascade of writes (and erases) of whole blocks. Yaffs avoids much of this problem by being log ..."
          ]
        },
        {
          "title": "Decentralized, Epoch-based F2FS Journaling with Fine- ...",
          "url": "https://www.usenix.org/system/files/osdi25-cui.pdf",
          "excerpts": [
            "As such, in journal apply and crash recovery, with both file and filesystem metadata in journal records, we can reach to a consistent state."
          ]
        },
        {
          "title": "Comparing 4 Popular Embedded File Systems - JBLopen",
          "url": "https://www.jblopen.com/comparing-embedded-file-systems/",
          "excerpts": [
            "YAFFS2 does not support chunks smaller than 1KiB, so smaller updates will always result in a read-modify-write sequence which further decreases ...",
            "In the case of LittleFS, read performances are excellent with no caching at all, while write performances are terrible for fundamental design ..."
          ]
        },
        {
          "title": "NVM Express NVM Command Set Specification, Revision 1.1",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-NVM-Command-Set-Specification-Revision-1.1-2024.08.05-Ratified.pdf",
          "excerpts": [
            "command is submitted that has a size greater than the AWUN/NAWUN value or crosses an atomic  \nboundary, then:\n\n• In Single Atomicity Mode (refer to sectio [n 2.1.4.1)](-.html) , there is no guarantee of command atomicity;\n\nand\n\n• In Multiple Atomicity Mode (refer to section [2\\.1.4.5)](-.html) , atomicity is guaranteed for each portion of the\n\ncommand that falls within an atom",
            "*Force Unit Access (FUA):** If this bit is\n\nset to ‘1’, then the controller shall flush any data and metadata\n\nspecified by the Verify command from any volatile cache before performing the Verify operation and  \nshall perform the Verify operation on data and metadata that have been committed to non-volatile ",
            "If a write command is submitted that has a size less than or equal to the AWUN/NAWUN value and the  \nwrite command does not cross an atomic boundary (refer to section [2\\.1.4.4)](-.html) , then the host is guaranteed  \nthat the write command is atomic to the NVM with respect to other read or",
            "The boundary sizes shall be greater than or equal to the corresponding atomic write sizes:\n\n• NABSN shall be greater than or equal to AWUN;  \n• NABSN shall be greater than or equal to NAWUN if NAWUN is reported;  \n• NABSPF shall be greater than or equal to AWUPF; and  \n• NABSPF shall be greater than or equal to NAWUPF if NAWUPF is reported. In addition, NABO shall be less than or equal to NA",
            "\n\nNamespace Atomic Boundary Size  \nNormal (NABSN)\n\n= NAWUN,\n\n= AWUN if NAWUN\n\nis not reported, and\n\n= NABSPF\n\n≥ NAWUN and\n\n≥ AWUN\n\nNamespace Atomic Boundary Size  \nPower Fail (NABSPF)\n\n= NAWUPF,\n\n= AWUPF if NAWUPF is\n\nnot reported, and\n\n= NABSN\n\n≥ NA",
            " AWUN/NAWUN Example Results ",
            " AWUN/NAWUN Example Results "
          ]
        },
        {
          "title": "SPDK Blobstore and BlobFS Documentation",
          "url": "https://spdk.io/doc/blob.html",
          "excerpts": [
            "Atomicity\n\nFor all Blobstore operations regarding atomicity, there is a dependency on the underlying device to guarantee atomic operations of at least one page size.",
            "Blobstore is callback driven; in the event that any Blobstore API is unable to make forward progress it will not block but instead return control at that point and make a call to the callback function provided in the API, along with arguments, when the original call is completed.",
            "Blobstore requires a backing storage device that can be integrated using the `bdev` layer, or by directly integrating a device driver to Blobstore."
          ]
        },
        {
          "title": "Havelsan's Revenue in 2024",
          "url": "https://github.com/zephyrproject-rtos/littlefs/blob/zephyr/DESIGN.md",
          "excerpts": [
            "Power-loss resilience** \\- On these systems, power can be lost at any time. If a power loss corrupts any persistent data structures, this can cause the\n   device to become unrecove",
            "Bounded RAM/ROM** \\- If the above requirements weren't enough, these\n   systems also have very limited amounts of memory. This prevents many\n   existing filesystem designs, which can lean on relatively large amounts of\n   RAM to temporarily store filesystem metadata. For ROM, this means we need to keep our design simple and reuse code paths\n   where possible. For RAM we have a stronger requirement, all RAM us",
            "ittlefs has two methods of protecting against wear:\n\n1. Detection and recovery from bad blocks\n2. Evenly distributing wear across dynamic blocks",
            "The block allocator has a secondary role: wear leveling. Wear leveling is the process of distributing wear across all blocks in the\nstorage to prevent the filesystem from experiencing an early death due to\nwear on a single block in the storage.",
            "Together, bad block detection and dynamic wear leveling provide a best effort\nsolution for avoiding the early death of a filesystem due to wear."
          ]
        },
        {
          "title": "Design of littlefs - NXP MCUXpresso documentation",
          "url": "https://mcuxpresso.nxp.com/mcuxsdk/25.09.00-pvw1/html/middleware/littlefs/DESIGN.html",
          "excerpts": [
            "\n\nA little fail-safe filesystem designed for microcontrollers.",
            "\n\nThe embedded systems littlefs targets are usually 32-bit microcontrollers with\naround 32 KiB of RAM and 512 KiB of ROM. These are often paired with SPI NOR\nflash chips with about 4 MiB of flash storage.",
            "Power-loss resilience** \\- On these systems, power can be lost at any time. If a power loss corrupts any persistent data structures, this can cause the\n   device to become unrecovera",
            ". littlefs takes a cautious approach. Instead of trusting a free list on disk,\nlittlefs relies on the fact that the filesystem on disk is a mirror image of\nthe free blocks on the disk. The block allocator operates much like a garbage\ncollector in a scripting language, scanning for unused blocks on demand.",
            " role: wear leveling. Wear leveling is the process of distributing wear across all blocks in the\nstorage to prevent the filesystem from experiencing an early death due to\nwear on a single block in the storage.",
            ". At the sub-block level, littlefs is built\nout of small, two block logs that provide atomic updates to metadata anywhere\non the filesystem.",
            ". At the super-block level, littlefs is a CObW tree of blocks",
            "\n\nThe block allocator has a secondary role: wear leveling. Wear leveling is the process of distributing wear across all blocks"
          ]
        },
        {
          "title": "LittleFS embedded filesystem overview",
          "url": "https://os.mbed.com/blog/entry/littlefs-high-integrity-embedded-fs/",
          "excerpts": [
            "**Power-loss resilience",
            "We designed this file system for systems that may have random power failures. It has strong copy-on-write guarantees, and storage on disk is always kept in a valid state.",
            "We designed this file system for systems that may have random power failures. It has strong copy-on-write guarantees, and storage on disk is always kept in a valid state.",
            " Wear-leveling",
            "RAM/ROM size",
            "Littlefs, the high-integrity embedded file system in Mbed OS is optimized to work with a limited amount of RAM and ROM. It avoids recursion, limits dynamic memory to configurable buffers and at no point stores an entire storage block in RAM.",
            "Power-loss resilience",
            "Most storage chips that embedded devices use support a limited set of erases per sector. If you do not have a storage controller with support for wear leveling, the longevity of your embedded device could be compromised. The embedded file system provides dynamic wear leveling to spread data across sectors throughout the full size of the flash."
          ]
        },
        {
          "title": "NVM Command Set Specification",
          "url": "https://nvmexpress.org/wp-content/uploads/NVMe-NVM-Command-Set-Specification-1.0a-2021.07.26-Ratified.pdf",
          "excerpts": [
            "Jul 23, 2021 — 30. Force Unit Access (FUA): If set to '1', then the controller shall flush any data and metadata specified by the Verify command from any ...",
            "In this example, AWUPF/NAWUPF has a value of 1KiB (equivalent to two 512-byte logical blocks),. AWUN/NAWUN has a value of 2KiB (equivalent to ..."
          ]
        },
        {
          "title": "NVM Command Set Specification 1.0e",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-NVM-Command-Set-Specification-1.0e-2024.07.29-Ratified.pdf",
          "excerpts": [
            "Jul 29, 2024 — If a write command crosses the atomic boundary specified by the NABSN value, then the atomicity based on the NAWUN parameters is not guaranteed.See more"
          ]
        },
        {
          "title": "[PDF] NVM Express Base Specification 2.0d",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-Base-Specification-2.0d-2024.01.11-Ratified.pdf",
          "excerpts": [
            "This NVM Express Base Specification, Revision 2.0d is proprietary to the NVM Express, Inc. (also referred to as “Company”) and/or its successors ..."
          ]
        },
        {
          "title": "Minimizing latency · Issue #891 · littlefs-project/littlefs",
          "url": "https://github.com/littlefs-project/littlefs/issues/891",
          "excerpts": [
            "Nov 14, 2023 — littlefs does have a couple cases where latency can spike. And to make things worse these latency spikes could be described as extreme due to poor scalability."
          ]
        },
        {
          "title": "LittleFileSystem - API references and tutorials",
          "url": "https://os.mbed.com/docs/mbed-os/v6.16/apis/littlefilesystem.html",
          "excerpts": [
            "Bounded RAM/ROM - This file system works with a limited amount of memory. · Power-loss resilient - We have designed this for operating systems that may have ..."
          ]
        },
        {
          "title": "How to get best performance from LittleFS #1033 - GitHub",
          "url": "https://github.com/littlefs-project/littlefs/issues/1033",
          "excerpts": [
            "Another option is to fork littlefs's test/bench runner, which is what littlefs uses to run its test suite locally, simulate powerloss, etc. It' ..."
          ]
        },
        {
          "title": "Open and Close File Performance · Issue #214 · littlefs- ...",
          "url": "https://github.com/ARMmbed/littlefs/issues/214",
          "excerpts": [
            "Jun 25, 2019 — I've done a number of tests: one of them erases the flash, creates a new FS and proceeds to create files of a constant size on the FS until we run out of space."
          ]
        },
        {
          "title": "NVM Express NVM Command Set Specification (Draft excerpts on atomicity and durability)",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-NVM-Command-Set-Specification-Revision-1.2-2025.08.01-Ratified.pdf",
          "excerpts": [
            "Namespace Atomic Write Unit Normal (NAWUN): This field indicates the namespace \nspecific size of the write operation guaranteed to be written atomically to the NVM \nduring normal operatio",
            "Atomic Write Unit Power Fail (AWUPF): This field indicates the size of the write operation \nguaranteed to be written atomically to the NVM across all namespaces with any supported \nnamespace format during a power fail or error conditio",
            "If a write command is submitted with size less than or equal to the AWUPF/NAWUPF value and the write \ncommand does not cross an atomic boundary (refer to section 2.1.4.4), the controller guarantees that if the \ncommand fails due to a power failure or other error condition, then subsequent read commands for the \nlogical blocks associated with the write command shall return one of the following: \n\n• \nAll old data (i.e., original data on the NVM in the LBA range addressed by the interrupted write); or \n\n• \nAll new data (i.e., all data to be written to the NVM by the interr",
            "The NVM subsystem reports in the Identify Controller data structure the size in logical blocks of the write \noperation guaranteed to be written atomically under various conditions, including: \n\n• \nnormal operation;  \n\n• \npower fail; \n\n• \nthe write portion of a Compare and Write fused operation (refer to section 2.1.3.1); and \n\n• \nthe write portion of a Copy command (refer to section 3.",
            "If a write \ncommand is submitted that has a size greater than the AWUN/NAWUN value or crosses an atomic \nboundary, the",
            "In Single Atomicity Mode (refer to section 2.1.4.1), there is no guarantee of command atomicity; \nand",
            "In Multiple Atomicity Mode (refer to section 2.1.4.5), atomicity is guaranteed for each portion of the \ncommand that falls within an atomic LBA subrange",
            "Atomicity guarantees for Multiple Atomicity Mode \nare specified in section 2.1.4.5"
          ]
        },
        {
          "title": "NVMe Durability and ccNVMe Commands",
          "url": "https://dl.acm.org/doi/full/10.1145/3568428",
          "excerpts": [
            "Hence, the commit request implicitly flushes the device to ensure durability, by issuing a flush command first and setting the FUA bit in the I/O command, if the volatile write cache is present in the SSD.",
            "the basic operational unit is the *transaction* (a set of operations that need to be executed atomically) rather than the request from each slot of the queues."
          ]
        },
        {
          "title": "littlefs project repository (Power-loss resilience and design goals)",
          "url": "https://github.com/littlefs-project/littlefs",
          "excerpts": [
            "The filesystem contains no unbounded\nrecursion and dynamic memory is limited to configurable buffers that can be\nprovided statically.",
            "Power-loss resilience** \\- littlefs is designed to handle random power\nfailures. All file operations have strong copy-on-write guarantees and if\npower is lost the filesystem will fall back to the last known good sta",
            "Dynamic wear leveling** \\- littlefs is designed with flash in mind, and\nprovides wear leveling over dynamic bloc",
            "Power-loss resilience - littlefs is designed to handle random power failures.",
            "Littlefs is written in C, and specifically should compile with any compiler\nthat conforms to the `C99` standard.",
            "In littlefs, these ingredients form a sort of two-layered cake, with the small\nlogs (called metadata pairs) providing fast updates to metadata anywhere on\nstorage, while the COW structures store file data compactly and without any\nwear amplification cost."
          ]
        },
        {
          "title": "MDPI 2025: Embedded File Systems and Crash Consistency",
          "url": "https://www.mdpi.com/2079-9268/15/2/22",
          "excerpts": [
            "Some filesystems handle updates to existing data by copying the updated data to a new location and marking the old data as invalid. In such cases, the filesystem must implement an algorithm that periodically erases blocks containing invalid data to free up space for new writes.",
            "In many filesystems, wear leveling and GC are managed by the flash translation layer (FTL), which acts as an abstraction layer between the filesystem and raw flash memory. FTL not only distributes wear evenly but can also provide crash recovery mechanisms by maintaining metadata and journaling techniques to restore consistency after unexpected power loss [[5](),[6](),[7]()]."
          ]
        },
        {
          "title": "[PDF] NVMe Base Specification, Revision 1.4c",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-1_4c-2021.06.28-Ratified.pdf",
          "excerpts": [
            "**Atomic Write Unit Normal (AWUN):** This field indicates the size of the write operation\n\nguaranteed to be written atomically to the NVM across all namespaces with any\n\nsupported namespace format during normal operatio",
            "\nAWUN/NAWUN control the atomicity of command execution in relation to other commands. They impose\n\ninter-command serialization of writing of blocks of data to the NVM and prevent blocks of data ending up\n\non the NVM containing partial data from one new command and partial data from one or more other new\n\ncommands.",
            "If a write command is submitted with size less than or equal to the AWUN/NAWUN value and the write\n\ncommand does not cross an atomic boundary (refer to section 6.4.3), then the host is guaranteed that the\n\nwrite command is atomic to the NVM with respect to other read or write command",
            "If a write command is\n\nsubmitted with size greater than the AWUN/NAWUN value or crosses an atomic boundary, then there is no\n\nguarantee of command atomicit",
            "y. AWUN/NAWUN does not have any applicability to write errors caused\n\nby power failure or other error conditions (refer to Atomic Write Unit Power Fail).",
            "ush applies to all\n\ncommands for the specified namespace(s) completed by the controller prior to the submission of the Flush\n\ncommand. The controller ",
            "If bits 2:1 are set to 11b in the VWC field (refer to Figure 251) and the specified NSID is FFFFFFFFh, then\n\nthe Flush command applies to all namespaces attached to the controller processing the Flush command",
            "If bits 2:1 are set to 10b in the VWC field and the specified NSID is FFFFFFFFh, then the controller fails\n\nthe command with status code Invalid Namespace or Format",
            "If bits 2:1 are cleared to 00b in the VWC field,\n\nthen the controller behavior if the specified NSID is FFFFFFFFh is not indicated"
          ]
        },
        {
          "title": "Specifications - NVM Express",
          "url": "https://nvmexpress.org/specifications/",
          "excerpts": [
            "Missing: FUA flush AWUN AWUPF NAWUN NAWUPF NABSN NABSPF ordering pcisig."
          ]
        },
        {
          "title": "TP4159 PCIe Infrastructure for Live Migration",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-Revision-Changes-2025.08.01.pdf",
          "excerpts": [
            "Aug 5, 2025 — Defined a new Multiple Atomicity Mode (MAM) that provides write guarantees between Namespace Atomic Boundaries for commands that cross ..."
          ]
        },
        {
          "title": "Efficient Crash Consistency for NVMe over PCIe and RDMA",
          "url": "https://dl.acm.org/doi/fullHtml/10.1145/3568428",
          "excerpts": [
            "by X Liao · 2023 · Cited by 9 — Crash consistency includes atomicity and ordering guarantees. We present how ccNVMe provides these two guarantees during normal execution and after a crash."
          ]
        },
        {
          "title": "SPDK: blob.h File Reference",
          "url": "https://spdk.io/doc/blob_8h.html",
          "excerpts": [
            "Dump a blobstore's metadata to a given FILE in human-readable format. ... The metadata thread is the thread which called spdk_bs_init() or spdk_bs_load()."
          ]
        },
        {
          "title": "Storage Developer Conference - #70: SPDK Blobstore",
          "url": "https://podscripts.co/podcasts/storage-developer-conference/70-spdk-blobstore-a-look-inside-the-nvm-optimized-allocator",
          "excerpts": [
            "And really, the metadata is nothing more than an extent list, a list of clusters that have been allocated for that blob, and a series of, we call them XADDRs,."
          ]
        },
        {
          "title": "[PDF] F2FS: A New File System for Flash Storage - USENIX",
          "url": "https://www.usenix.org/sites/default/files/conference/protected-files/fast15_slides_lee.pdf",
          "excerpts": [
            "In F2FS, more than 90% of writes are sequential. F2FS reduces write amount per fsync by using roll-forward recovery. If checkpoint is done per fsync, write ..."
          ]
        },
        {
          "title": "[PDF] exF2FS: Transaction Support in Log-Structured Filesystem - USENIX",
          "url": "https://www.usenix.org/system/files/fast22-oh.pdf",
          "excerpts": [
            "F2FS's transaction support has a specific target application: SQLite. With atomic write of F2FS, SQLite can implement the transaction without ..."
          ]
        },
        {
          "title": "LittleFS performance issue for QSPI flash from 1.54 beta10 to ...",
          "url": "https://forum.pjrc.com/index.php?threads/littlefs-performance-issue-for-qspi-flash-from-1-54-beta10-to-1-56.70467/",
          "excerpts": [
            "Jun 13, 2022 — I'm pretty sure this problem is you're running out pre-erased sectors and LittleFS is having to do slow erase operations. Sadly, LittleFS doesn ..."
          ]
        },
        {
          "title": "[PDF] F2FS: A New File System for Flash Storage - cs.wisc.edu",
          "url": "https://pages.cs.wisc.edu/~bart/736/papers/fast15-paper-lee.pdf",
          "excerpts": [
            "For this workload, the roll-forward recovery policy of F2FS produces huge benefits. In fact, F2FS reduces the amount of data writes by about 46% over EXT4 in."
          ]
        },
        {
          "title": "Bad performance / big number of reads when using LittleFS",
          "url": "https://forums.mbed.com/t/bad-performance-big-number-of-reads-when-using-littlefs/5855",
          "excerpts": [
            "Nov 4, 2019 — The spikes you see are caused by directory-level garbage collection. This happens with the directory fills with commits (file changes)."
          ]
        },
        {
          "title": "Design notes for littlefs",
          "url": "https://docs.mcuxpresso.nxp.com/mcuxsdk/25.06.00/html/middleware/littlefs/DESIGN.html",
          "excerpts": [
            "littlefs was originally built as an experiment to learn about filesystem design\nin the context of microcontrollers. The question was: How would you build a\nfilesystem that is resilient to power-loss and flash wear without using\nunbounded memory?",
            "This leaves us with three major requirements for an embedded filesystem. 1. **Power-loss resilience** - On these systems, power can be lost at any time.",
            "m. littlefs[]( \"Permalink to this heading\")\n==================================================\n\nSo what does littlefs do? If we look at existing filesystems, there are two interesting design patterns\nthat stand out, but each have their own set of problems. Logging, which\nprovides independent atomicity, has poor runtime performance. And COW data\nstructures, which perform well, push the atomicity problem upwards.",
            "littlefs has two methods of protecting against wear:\n\n1. Detection and recovery from bad blocks\n2. Evenly distributing wear across dynamic blocks",
            "The block allocator has a secondary role: wear leveling. Wear leveling is the process of distributing wear across all blocks in the\nstorage to prevent the filesystem from experiencing an early death due to\nwear on a single block in the storage.",
            "littlefs uses a statistical wear leveling algorithm. What this\nmeans is that we don’t actively track wear, instead we rely on a uniform\ndistribution of wear across storage to approximate a dynamic wear leveling\nalgorithm.",
            "Together, bad block detection and dynamic wear leveling provide a best effort\nsolution for avoiding the early death of a filesystem due to wear."
          ]
        },
        {
          "title": "NVM Express Revision 1.3",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM_Express_Revision_1.3.pdf",
          "excerpts": [
            "Force Unit Access (FUA): This field specifies that the data read shall be read from non-volatile media. 29:26. Protection Information Field ..."
          ]
        },
        {
          "title": "[PDF] NVM ExpressTM Base Specification",
          "url": "https://nvmexpress.org/wp-content/uploads/NVM-Express-1_3c-2018.05.24-Ratified.pdf",
          "excerpts": [
            "6.4.1 AWUN/NAWUN. AWUN/NAWUN control the atomicity of command execution in relation to other commands. They impose inter-command ..."
          ]
        },
        {
          "title": "[PDF] UNH–IOL NVMe Testing Service",
          "url": "https://www.iol.unh.edu/sites/default/files/testsuites/nvme/UNH-IOL_NVM_Command_Set_Conformance_v18.0_2022.07.13.pdf",
          "excerpts": [
            "Atomic Boundaries Not Supported (NABSN/NABSPF = 0) (M) ... This version takes into account the NVMexpress.org's 2.0 effort in refactoring NVMe ..."
          ]
        },
        {
          "title": "NVMe API Changes",
          "url": "https://spdk.io/feature/2016/03/03/nvme-api/",
          "excerpts": [
            "Mar 3, 2016 — First, we added support for Force Unit Access and Limited Retry by adding an io_flags parameter to the read and write commands. We also added a ..."
          ]
        },
        {
          "title": "spdk/lib/nvme/nvme_ns.c at master",
          "url": "https://github.com/spdk/spdk/blob/master/lib/nvme/nvme_ns.c",
          "excerpts": [
            "Storage Performance Development Kit. Contribute to spdk/spdk development by creating an account on GitHub."
          ]
        },
        {
          "title": "spdk/lib/blob/blobstore.c at master - GitHub",
          "url": "https://github.com/spdk/spdk/blob/master/lib/blob/blobstore.c",
          "excerpts": [
            "Storage Performance Development Kit. Contribute to spdk/spdk development by creating an account on GitHub."
          ]
        },
        {
          "title": "F2FS",
          "url": "https://wiki.archlinux.org/title/F2FS",
          "excerpts": [
            "May 13, 2025 — If a file is linked, F2FS may loose its parent inode number so that fsync calls for the linked file need to perform the checkpoint every time."
          ]
        },
        {
          "title": "[PDF] SPDK Blobstore - SNIA.org",
          "url": "https://www.snia.org/sites/default/files/SDC/2017/presentations/Solid_State_Stor_NVM_PM_NVDIMM/Luse_Paul_Verma_Vishal_SPDK_Blobstore_A_Look_Inside_the_NVM_Optimized_Allocator.pdf",
          "excerpts": [
            "❒ Github : https://github.com/spdk/spdk. ❒ Trello : https://trello.com/spdk. ❒ GerritHub : https://review.gerrithub.io/#/q/project:spdk/spdk+status:open."
          ]
        },
        {
          "title": "[blobstore] crash during copy-on-write can leak data #2866",
          "url": "https://github.com/spdk/spdk/issues/2866",
          "excerpts": [
            "A crash at a critical point can lead to dirty clusters being used for new allocations with the assumption that they have already been zeroed."
          ]
        },
        {
          "title": "[PDF] UNH-IOL NVMe Conformance Test Plan",
          "url": "https://www.iol.unh.edu/sites/default/files/UNH-IOL_NVMe_Conformance_Test_Suite_v14.0_Clean.pdf",
          "excerpts": [
            "NABSN, NABO, NABSPF values. Support for Atomic Writes is indicated ... Discussion: AWUN/NAWUN control the atomicity of command execution in ..."
          ]
        },
        {
          "title": "SPDK Blobstore support object? · Issue #183 - GitHub",
          "url": "https://github.com/spdk/spdk/issues/183",
          "excerpts": [
            "Someone implemented a block device in Ceph using SPDK, so you can technically use SPDK with Bluestore to accelerate it."
          ]
        },
        {
          "title": "UNH–IOL NVMe Testing Service",
          "url": "https://www.iol.unh.edu/sites/default/files/testsuites/nvme/UNH-IOL_NVM_Command_Set_Conformance_v17.0.pdf",
          "excerpts": [
            "Jan 21, 2022 — The UNH-IOL NVMe Testing Service is located at 21 Madbury Rd Suite 100 Durham, NH 03824. Contact them at +1 603–862–0090 or nvmelab@iol.unh.edu."
          ]
        },
        {
          "title": "SPDK Spinlocks Introduction",
          "url": "https://spdk.io/spdk_spinlock/2023/01/04/spdk_lock/",
          "excerpts": [
            "Jan 4, 2023 — Initialize the lock before first use. Only lock when running from an SPDK thread. Unlock before returning from a poller or message function."
          ]
        },
        {
          "title": "Leveraging NVMe SSDs for Building a Fast, Cost-effective ...",
          "url": "https://dl.acm.org/doi/10.1145/3480963",
          "excerpts": [
            "Oct 15, 2021 — The write-friendly log-structured merge tree (LSM-tree) is widely adopted as the underlying storage engine by mainstream KV stores, such as ..."
          ]
        },
        {
          "title": "Analysis for the Performance Degradation of fsync()in F2FS",
          "url": "https://oslab.kaist.ac.kr/wp-content/uploads/esos_files/publication/conferences/international/ICICC2018_GYCHOI.pdf",
          "excerpts": [
            "by G Choi · 2010 · Cited by 8 — In addition, F2FS performs the roll- forward recovery based on the node segment processed through fsync() and the latest checkpoint pack ..."
          ]
        },
        {
          "title": "The Design and Implementation of a High-Performance ...",
          "url": "https://arxiv.org/html/2402.17963v1",
          "excerpts": [
            "Feb 28, 2024 — We present ZapRAID, a high-performance RAID system for ZNS SSDs by carefully exploiting Zone Append to achieve high write performance via intra- ..."
          ]
        },
        {
          "title": "Realtime kernel - ArchWiki",
          "url": "https://wiki.archlinux.org/title/Realtime_kernel",
          "excerpts": [
            "hwlatdetect. hwlatdetect can be used to detect SMIs taking an inordinate time, thus introducing latency by blocking normal kernel execution."
          ]
        },
        {
          "title": "12 Tuning I/O Performance",
          "url": "https://documentation.suse.com/en-us/sles/12-SP5/html/SLES-all/cha-tuning-io.html",
          "excerpts": [
            "MQ-DEADLINE is a latency-oriented I/O scheduler. It is a modification of DEADLINE scheduler for blk-mq I/O path (refer to Section 12.2.3, “ DEADLINE ”) ...See more"
          ]
        },
        {
          "title": "Block Device Tuning of Kernel Internals",
          "url": "https://medium.com/@eren.c.uysal/block-device-tuning-of-kernel-internals-5291676bfc28",
          "excerpts": [
            "Tune for your workload: use none or mq-deadline for low latency; bfq for fairness. For high concurrency, favor blk-mq with increased queue depth ...See more"
          ]
        },
        {
          "title": "SLES 15 SP7 | System Analysis and Tuning Guide",
          "url": "https://documentation.suse.com/en-us/sles/15-SP7/html/SLES-all/cha-tuning-io.html",
          "excerpts": [
            "The blk-mq I/O path replaces cfq and does not include the elevator boot parameter. 14.2 Available I/O elevators with blk-mq I/O path ...See more"
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "Feb 14, 2022 — The low latency Ubuntu kernel is suitable for low-jitter workloads and industrial embedded applications with latency requirements in the milliseconds' range."
          ]
        },
        {
          "title": "Minimising latency in your edge cloud with real-time kernel | Ubuntu",
          "url": "https://ubuntu.com/blog/minimising-latency-in-your-edge-cloud-with-real-time-kernel",
          "excerpts": [
            "We used the cyclictest from the rt-tests suite to measure the latency experienced inside a system container and a virtual machine as workloads."
          ]
        },
        {
          "title": "SPDK 19.04 NVMeOF RDMA Benchmark Report-2 | PDF",
          "url": "https://www.scribd.com/document/465681854/SPDK-19-04-NVMeOF-RDMA-benchmark-report-2",
          "excerpts": [
            "4k Random Read results. Results in the table represent aggregate performance (IOPS & Avg. latency) observed: Test Result: 4K 100% Random Read, QD ..."
          ]
        },
        {
          "title": "Should I use real-time or low latency kernel in Ubuntu ...",
          "url": "https://www.reddit.com/r/Ubuntu/comments/178bz4o/should_i_use_realtime_or_low_latency_kernel_in/",
          "excerpts": [
            "Real-time kernels won't make your computer smoother or faster. On the contrary, it runs markedly slower than the normal Linux kernel. There's a ...",
            "Have you guys used Real Time kernel or Low Latency ones which come with Ubuntu Pro? Is it recommended? What benefits would I see in normal ..."
          ]
        },
        {
          "title": "OS in Rust: Building kernel for custom target: Part-4 | by Knoldus Inc.",
          "url": "https://medium.com/@knoldus/os-in-rust-building-kernel-for-custom-target-part-4-d539f87f988e",
          "excerpts": [
            "This article pertains to building a custom kernel for the custom target that we created in our previous post."
          ]
        },
        {
          "title": "apic - Rust Package Registry - Crates.io",
          "url": "https://crates.io/crates/apic/dependencies",
          "excerpts": [
            "Interface to the Advanced Programmable Interrupt Controller (APIC) ... Support for x86_64 specific instructions, registers, and structures. Rust. rust-lang ..."
          ]
        },
        {
          "title": "A Minimal Rust Kernel | Writing an OS in Rust",
          "url": "https://os.phil-opp.com/minimal-rust-kernel/",
          "excerpts": [
            "In this post, we create a minimal 64-bit Rust kernel for the x86 architecture. We build upon the freestanding Rust binary from the previous post"
          ]
        },
        {
          "title": "rust-osdev/bootloader - GitHub",
          "url": "https://github.com/rust-osdev/bootloader",
          "excerpts": [
            "An experimental x86_64 bootloader that works on both BIOS and UEFI systems. Written in Rust and some inline assembly, buildable on all platforms.",
            "You need a nightly [Rust](https://www.rust-lang.org) compiler with the `llvm-tools-preview` component, which can be installed through `rustup component add llvm-tools-preview` .",
            "To use this crate, you need to adjust your kernel to be bootable first.",
            "The function name can be arbitrary. + The `boot_info` argument provides information about available memory, the framebuffer, and more."
          ]
        },
        {
          "title": "nvme - Rust Package Registry",
          "url": "https://crates.io/crates/nvme",
          "excerpts": [
            "Apr 11, 2025 — A simple NVMe driver. NVMe. A no-std compatible NVMe driver for embedded and operating system development."
          ]
        },
        {
          "title": "Achieve High-Performance Storage with DAOS",
          "url": "https://www.solidigm.com/content/dam/solidigm/en/site/products/technology/achieve-high-performance/documents/daos-high-performance-storage-brief.pdf",
          "excerpts": [
            "DAOS 1.0 read latency with Intel. Optane PMem yields sub-40 us latency at four nines (P99.99), and just over 50 us latency at five nines (P99.999). When."
          ]
        },
        {
          "title": "low-latency-for-storage-intensive-workloads- ...",
          "url": "https://www.intel.com/content/dam/www/public/us/en/documents/technology-briefs/low-latency-for-storage-intensive-workloads-tech-brief.pdf",
          "excerpts": [
            "Intel Optane technology SSDs deliver more reliable performance through highly predictable access latency. Memory and Storage. Technical Series. The Direct ...",
            "This paper applies those percentile measurement principles to Intel Optane SSDs and 3D NAND SSDs to fully describe their relative performance. These ..."
          ]
        },
        {
          "title": "SR-IOV VF/PF Latency Insights",
          "url": "https://blog.emumba.com/sr-iov-vf-pf-latency-insights-60264b131666",
          "excerpts": [
            "May 20, 2021 — The latency increases visibly as the number of VFs increases. We noticed a drastic increase of 155% in the median as we moved from 0 to 30 VFs."
          ]
        },
        {
          "title": "VFIO: Tuning your Windows gaming VM for optimal performance",
          "url": "https://angrysysadmins.tech/index.php/2022/07/grassyloki/vfio-tuning-your-windows-gaming-vm-for-optimal-performance/",
          "excerpts": [
            "Jul 29, 2022 — Latencymon. Latencymon is a tool used to see what is causing stuttering in Windows. It is great for finding what driver is causing lag. Load ..."
          ]
        },
        {
          "title": "4 Tips to Improve P99 Latency - Control Plane",
          "url": "https://controlplane.com/community-blog/post/4-tips-to-improve-p99-latency",
          "excerpts": [
            "P99 latency refers to the 99th percentile of latency measurements. It's a performance metric that indicates how long it takes to complete 99% of requests ..."
          ]
        },
        {
          "title": "Unikernels in rust",
          "url": "https://internals.rust-lang.org/t/unikernels-in-rust/2494",
          "excerpts": [
            "Aug 12, 2015 — MirageOS is a library operating system that constructs unikernels for secure, high-performance network applications across a variety of ..."
          ]
        },
        {
          "title": "What is Single Root I/O Virtualization (SR-IOV)",
          "url": "https://techdocs.broadcom.com/us/en/vmware-cis/vsphere/vsphere-supervisor/8-0/sr-iov.html",
          "excerpts": [
            "Jul 16, 2025 — You can use SR-IOV for networking of virtual machines that are latency sensitive or require more CPU resources. Overview of SR-IOV. SR-IOV is ..."
          ]
        },
        {
          "title": "PCIE passthrough stuttering/lag spikes - Virtualization",
          "url": "https://forum.level1techs.com/t/pcie-passthrough-stuttering-lag-spikes/170635",
          "excerpts": [
            "Apr 9, 2021 — I've been trying to setup a Windows 10 virtual machine mainly for gaming. I had issues where vfio would give me BAR 0 error and it could not reserve memory."
          ]
        },
        {
          "title": "RDP into a VirtIO GPU guest = huge latency? : r/Proxmox - Reddit",
          "url": "https://www.reddit.com/r/Proxmox/comments/1gi9uy0/rdp_into_a_virtio_gpu_guest_huge_latency/",
          "excerpts": [
            "Put it back to Spice GPU and the latency comes back with in a couple seconds. Going in via Spice no latency/issues. Just odd that RDP would ..."
          ]
        },
        {
          "title": "grafana - How to find the p99 in prometheus? - Stack Overflow",
          "url": "https://stackoverflow.com/questions/77266325/how-to-find-the-p99-in-prometheus",
          "excerpts": [
            "My goal is to determine the P99 percentile for this metric within the specified time range (from 10:30:00 to 13:30:00 in grafana timepicker)"
          ]
        },
        {
          "title": "NVMe Driver",
          "url": "https://rust-for-linux.com/nvme-driver",
          "excerpts": [
            "The Rust NVMe driver is an effort to implement a PCI NVMe driver in safe Rust for use in the Linux Kernel. The purpose of the driver is to provide a vehicle for ..."
          ]
        },
        {
          "title": "nvme - Rust",
          "url": "https://docs.rs/nvme/",
          "excerpts": [
            "A no-std compatible NVMe driver for embedded and operating system development. This crate provides functionality for interacting with NVMe (Non-Volatile ..."
          ]
        },
        {
          "title": "virtio_drivers - Rust - Docs.rs",
          "url": "https://docs.rs/virtio-drivers",
          "excerpts": [
            "These drivers can be used by bare-metal code (such as a bootloader or OS kernel) running in a VM to interact with VirtIO devices provided by the VMM."
          ]
        },
        {
          "title": "Low-latency hardware-assisted virtual networking",
          "url": "https://labs.ripe.net/author/florian-wiedner/low-latency-hardware-assisted-virtual-networking/",
          "excerpts": [
            "Mar 27, 2025 — We can show that virtualisation effectively supports low-latency applications with minimal overhead and improved tail latency compared to bare-metal systems.",
            "rthermore, the most influential factor towards latency on both containers, lightweight virtualisation, and virtual machines, as full OS virtualisation, comes from interrupts stopping the currently executed process and requiring it to be handled immediately",
            "Moving interrupts to cores not involved in low-latency packet processing significantly improves the performance.",
            "The diagram in figure 1 - based on the data published by Wiedner et al.3 - provides a comparison of optimised containers and virtual machines when processing packets towards end-to-end latency on systems of different vendors.",
            "s. It illustrates that after carefully optimising the system, we can achieve latencies with a big marginal towards the URLLC requirements for further processing in the network.",
            "Moreover, it shows that the remaining latency outside of queuing can be stated towards the hardware design and TLB shootdowns, which cannot be disabled.",
            "Further causes and especially high outliers can be accounted towards rescheduling interrupts.",
            "Figure 1 shows an analysis based on DPDK as user-space networking; kernel-space networking reduces the amount of possible isolation with, for example, pinning and isolation of the cores dedicated to packet processing.",
            "DPDK as user-space networking; kernel-space networking reduces the amount of possible isolation with, for example, pinning and isolation of the cores dedicated to packet processing.",
            "Our analysis in different papers (see notes 2-5 below), shows that it is generally possible to use virtualised network functions towards URLLC flows, but careful planning and optimisation are required."
          ]
        },
        {
          "title": "RIPE 90 Presentation: Hardware-assisted Virtual Networking for low-latency network services",
          "url": "https://ripe90.ripe.net/wp-content/uploads/presentations/3-slides.pdf",
          "excerpts": [
            "Traffic: UDP Traffic with 64 B packets",
            "Loadgen runs a packet generator (MoonGen) creating UDP packets",
            "Device under Test (DuT) runs containers/VMs/packet processing application",
            "Hardware-timestamping of entire network traffic (timer resolution 1.25 ns)",
            "Determine worst-case latencies",
            "Percentiles [%]",
            "Kernel Variants on LXC Container [1 Mpackets/s]",
            "Evaluation\n\nContainers vs. Bare-metal [1 Mpackets/s]",
            "The kernel variant is significantly influencing the tail-latency. • Nearly similar results until 99.999th percentile\n\n• Real-time kernel performs most deterministi"
          ]
        },
        {
          "title": "How to enable Real-time Ubuntu - Ubuntu Pro Client ...",
          "url": "https://documentation.ubuntu.com/pro-client/en/docs/howtoguides/enable_realtime_kernel/",
          "excerpts": [
            "The Real-time Ubuntu kernel package is installed using the APT package manager when you enable realtime-kernel through the Pro Client. If you want to access ..."
          ]
        },
        {
          "title": "How to enable Real-time Ubuntu on your machine",
          "url": "https://ubuntu.com/blog/enable-real-time-ubuntu",
          "excerpts": [
            "Jul 30, 2025 — How to enable Real-time Ubuntu on your machine · Generic on Ubuntu 22.04 LTS or Ubuntu 24.04 LTS: sudo pro enable realtime-kernel · Raspberry Pi 4 ..."
          ]
        },
        {
          "title": "Best Practices of Real-Time Linux Configuration",
          "url": "https://medium.com/@eren.c.uysal/best-practices-of-real-time-linux-configuration-195dc44c4105",
          "excerpts": [
            "Real-Time preemption (PREEMPT_RT) and tuned kernel parameters play critical roles in achieving predictable scheduling and interrupt handling."
          ]
        },
        {
          "title": "[PDF] Intel® Optane™ Persistent Memory and SPDK Power Baidu's Next ...",
          "url": "https://www.intel.com/content/dam/www/central-libraries/us/en/documents/baidu-optane-pmem-spdk-white-paper.pdf",
          "excerpts": [
            "Missing: tail 99.99"
          ]
        },
        {
          "title": "SPDK Whitepapers",
          "url": "https://spdk.io/whitepaper/",
          "excerpts": [
            "Missing: tail latency 99.99 QD1 4k"
          ]
        },
        {
          "title": "Linux OS Runtime Optimizations — ECI documentation",
          "url": "https://eci.intel.com/docs/3.3/development/performance/optimizations.html",
          "excerpts": [
            "When using the ECI Linux Intel LTS PREEMPT_RT kernel, all Ethernet device MSI interrupts Linux network device are scheduled to run on CPU 0. This creates a side ..."
          ]
        },
        {
          "title": "System Analysis and Tuning Guide | Tuning I/O performance",
          "url": "https://documentation.suse.com/pt-br/sles/15-SP7/html/SLES-all/cha-tuning-io.html",
          "excerpts": [
            "When switching from legacy block to blk-mq I/O path for a device, the none option is roughly comparable to noop , mq-deadline is comparable to deadline , and ..."
          ]
        },
        {
          "title": "Block Device Tuning of Real-Time Linux Configuration - Medium",
          "url": "https://medium.com/@eren.c.uysal/block-device-tuning-of-real-time-linux-configuration-b294386f2ce2",
          "excerpts": [
            "Real-time tuning focuses on minimizing latency by eliminating bottlenecks such as I/O scheduling queues, filesystem journaling, and CPU ..."
          ]
        },
        {
          "title": "Select latency plot of system or system group to display",
          "url": "https://www.osadl.org/Latency-plots.latency-plots.0.html",
          "excerpts": [
            "Sep 6, 2023 — Latency plots are used to graphically present the result of a latency determination of a real-time computer system.See more"
          ]
        },
        {
          "title": "Linux OS Runtime Optimizations — ECI documentation",
          "url": "https://eci.intel.com/docs/3.0/development/optimizations.html",
          "excerpts": [
            "When using the ECI Linux* Intel® LTS PREEMPT_RT kernel, all Linux kernel processes are scheduled to run on CPU 0, and CPUs 1 and 3 are configured to be isolated ..."
          ]
        },
        {
          "title": "Why choose a low latency kernel over a generic or real- ...",
          "url": "https://askubuntu.com/questions/126664/why-choose-a-low-latency-kernel-over-a-generic-or-real-time-kernel",
          "excerpts": [
            "Apr 28, 2012 — If you need a low latency system (e.g. for recording audio) then please use the -preempt kernel as a first choice. This reduces latency but ..."
          ]
        },
        {
          "title": "cyclictest - High resolution test program - Ubuntu Manpage",
          "url": "https://manpages.ubuntu.com/manpages/trusty/man8/cyclictest.8.html",
          "excerpts": [
            "It is useful to track down unexpected large latencies on a system. This option does only work with following kernel config options. For kernel < 2.6.24: * ..."
          ]
        },
        {
          "title": "Systematic Process to Reduce Linux OS Jitter",
          "url": "https://groups.google.com/g/mechanical-sympathy/c/DWlziVmyW-w",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache behavior, etc.). Same ..."
          ]
        },
        {
          "title": "Unexplained CPU latency spikes - Jetson AGX Xavier",
          "url": "https://forums.developer.nvidia.com/t/unexplained-cpu-latency-spikes/83172",
          "excerpts": [
            "The kernel parameters include “isolcpus=nohz,domain,6-7 nohz_full=6-7 rcu_nocbs=6-7”, and after booting, the following steps are used to run a ..."
          ]
        },
        {
          "title": "virtio vs e1000 vs rtl8139: what's the difference?",
          "url": "https://unix.stackexchange.com/questions/597789/virtio-vs-e1000-vs-rtl8139-whats-the-difference",
          "excerpts": [
            "Jul 10, 2020 — e1000 emulates an Intel NIC, rtl8139 emulates a Realtek NIC, and virtio is a para-virtualized driver, ie it knows it's operating in a VM.See more"
          ]
        },
        {
          "title": "“Degrees of Real-Time” 6.6.x/i.MX6 Quad - ...",
          "url": "https://reliableembeddedsystems.com/blog/degrees-of-real-time-6-6-x-i-mx6-quad/",
          "excerpts": [
            "Oct 11, 2024 — This post shows you “degrees of real-time” by observing the maximum jitter/latencies of cyclictest over an upstream kernel configured and ..."
          ]
        },
        {
          "title": "Launchpad Ubuntu Answers - Question 809210 (Cyclictest latency discussion)",
          "url": "https://answers.launchpad.net/ubuntu/+question/809210",
          "excerpts": [
            "st application. Cyclictest is used for benchmarking performence of Real Time Systems (User is running ubuntu 22.04 Real Time Kernel) and is used to measure latencies . User has observed latency spikes, the value almost under 10us, but there are two threads are 39 us and 31us. Would you please let us know the spikes is normal? cyclictest -m -p95 -d0 -a 1-38 -t 16"
          ]
        },
        {
          "title": "Linux scheduling latency debug and analysis",
          "url": "https://bristot.me/linux-scheduling-latency-debug-and-analysis/",
          "excerpts": [
            "This page will be updated as new features are added to rtla timerlat. Reducing the scheduling latency is one of the main goals of the PREEMPT_RT patchset. The ..."
          ]
        },
        {
          "title": "How to install the real-time kernel on Ubuntu Core",
          "url": "https://documentation.ubuntu.com/real-time/latest/how-to/uc-install-real-time-kernel/",
          "excerpts": [
            "24-rt-hwe/stable contains the real-time Hardware Enablement (HWE) kernel for Ubuntu 24.04 LTS. The kernel version is 6.11.0. 24-rt-hwe-edge ..."
          ]
        },
        {
          "title": "higher latency than Ubuntu? - Kernel & Hardware",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=293713",
          "excerpts": [
            "Mar 11, 2024 — The Ubuntu rt kernel has latencies in `cyclictest` with less than 15us as maximum (similar to the Wiki), whereas Arch linux-rt-lts has maximum values in the ..."
          ]
        },
        {
          "title": "Release notes for the Genode OS Framework 22.11",
          "url": "https://genode.org/documentation/release-notes/22.11",
          "excerpts": [
            "Over the past 18 months, we have steadily expanded the ... Thereby, we've used the recent DDE Linux porting approach, first described in the 21.08 release."
          ]
        },
        {
          "title": "Genode Porting Guide: Porting devices drivers",
          "url": "https://genode.org/documentation/developer-resources/porting_device_drivers",
          "excerpts": [
            "The first step in porting a driver is to identify the driver code that has to be ported. Once the code is located, we usually create a new Genode repository ..."
          ]
        },
        {
          "title": "Fault isolation for device drivers - IEEE Xplore",
          "url": "https://ieeexplore.ieee.org/document/5270357/",
          "excerpts": [
            "Abstract: This work explores the principles and practice of isolating low-level device drivers in order to improve OS dependability."
          ]
        },
        {
          "title": "Programming Device Drivers in Minix",
          "url": "https://wiki.minix3.org/doku.php?id=releases:3.2.0:developersguide:driverprogramming",
          "excerpts": [
            "The Reincarnation server will attempt to restart device drivers when it notices they are abruptly killed by the kernel due to a crash, or in ..."
          ]
        },
        {
          "title": "Programming Device Drivers in Minix - developersguide",
          "url": "https://wiki.minix3.org/doku.php?id=developersguide:driverprogramming",
          "excerpts": [
            "The Reincarnation server will attempt to restart device drivers when it notices they are abruptly killed by the kernel due to a crash, or in ..."
          ]
        },
        {
          "title": "MINIX 3 isolates drivers in unprivileged processes. - ResearchGate",
          "url": "https://www.researchgate.net/figure/MINIX-3-isolates-drivers-in-unprivileged-processes_fig5_224596767",
          "excerpts": [
            "This work explores the principles and practice of isolating low-level device drivers in order to improve OS dependability."
          ]
        },
        {
          "title": "[PDF] Building a Dependable Operating System: Fault Tolerance in MINIX 3",
          "url": "https://www.cs.vu.nl/~ast/Theses/herder-thesis.pdf",
          "excerpts": [
            "PCI-bus driver. Restrict access to a class of PCI devices. Figure 2.6: Resources that can be configured via per-driver isolation policies in MINIX 3. By ..."
          ]
        },
        {
          "title": "Multimedia support for MINIX 3",
          "url": "http://www.sce.carleton.ca/faculty/wainer/papers/moolenbroek_thesis.pdf",
          "excerpts": [
            "by D van Moolenbroek · 2007 · Cited by 3 — Whenever a crash occurs in a block device driver, the FS servers operating on any of ... Or, the MINIX Reincarnation Server (RS) could detect a crashed."
          ]
        },
        {
          "title": "A performance study on 10 GbE NICs with SR-IOV support",
          "url": "https://www.researchgate.net/publication/224140605_Evaluating_standard-based_self-virtualizing_devices_A_performance_study_on_10_GbE_NICs_with_SR-IOV_support",
          "excerpts": [
            "... (10GbE) NICs, and commonly used software-based I/O virtualization approaches usually suffer significant performance degradation compared with native hardware."
          ]
        },
        {
          "title": "Native vs. VM Benchmarks. Using passthrough for GPU ...",
          "url": "https://www.reddit.com/r/VFIO/comments/n3mjj3/native_vs_vm_benchmarks_using_passthrough_for_gpu/",
          "excerpts": [
            "In both cases (native and VM) the values are pretty similar (around 6500mb/s). Since I haven't yet been able to pass through a graphics card ( ..."
          ]
        },
        {
          "title": "E1000 vNIC and 10GBe pNIC | VMware vSphere",
          "url": "https://community.broadcom.com/vmware-cloud-foundation/discussion/e1000-vnic-and-10gbe-pnic",
          "excerpts": [
            "Aug 18, 2024 — The e1000(e) can exceed 1Gbps or even 10Gbps for that matter (of course if your physical links allow this or e.g. on the same host), the vmxnet3 can also well ..."
          ]
        },
        {
          "title": "Optimizing Nested Virtualization Performance Using Direct ...",
          "url": "https://www.cs.columbia.edu/~nieh/pubs/asplos2020_dvh.pdf",
          "excerpts": [
            "by JT Lim · 2020 · Cited by 35 — Our experimental results show that DVH can provide near native execution speeds and improve KVM performance by more than an order of ..."
          ]
        },
        {
          "title": "What sort of performance should I expect from VirtIO? - Reddit",
          "url": "https://www.reddit.com/r/homelab/comments/hz4ald/what_sort_of_performance_should_i_expect_from/",
          "excerpts": [
            "So I tested this some more, and if I use iperf3, I can saturate the link at around 9.3 to 9.4 Gbps. CPU usage is a bit high on the VM while ..."
          ]
        },
        {
          "title": "kvm_ioctls - Rust",
          "url": "https://docs.rs/kvm-ioctls/latest/kvm_ioctls/",
          "excerpts": [
            "Use the KVM object to create a VM. The VM is used for running VM specific ioctls. Initialize the guest memory for the created VM. In this dummy example we are ..."
          ]
        },
        {
          "title": "virtio_queue - Rust",
          "url": "https://docs.rs/virtio-queue",
          "excerpts": [
            "Struct to maintain information and manipulate state of a virtio queue for multi-threaded context. Reader: Provides high-level interface over the sequence of ..."
          ]
        },
        {
          "title": "codenet/understanding-rust-vmm",
          "url": "https://github.com/codenet/understanding-rust-vmm",
          "excerpts": [
            "Design. vmm-reference project uses kvm-ioctls, kvm-bindings, vm-memory, vm-superio, etc. vmm-reference is relatively simpler to understand since it does not ..."
          ]
        },
        {
          "title": "VSOCK: From Convenience to Performant VirtIO ...",
          "url": "https://lpc.events/event/17/contributions/1626/attachments/1334/2674/VSOCK_%20From%20Convenience%20to%20Performant%20VirtIO%20Communication.pdf",
          "excerpts": [
            "VSOCK Performance. • VSOCK is optimized for convenient usage by applications ... • One reason: VSOCK datagram enqueueing has comparatively low latency."
          ]
        },
        {
          "title": "VirtIO Vsock Performance To Improve With Linux 6.12",
          "url": "https://www.phoronix.com/news/Linux-6.12-Faster-VirtIO-Vsock",
          "excerpts": [
            "Sep 26, 2024 — This VirtIO Vsock change can make a significant impact to lowering the latency and boosting the throughput as shown in the prior patch comments:."
          ]
        },
        {
          "title": "Accelerate Your NVMe Drives with SPDK - Intel",
          "url": "https://www.intel.com/content/www/us/en/developer/articles/technical/accelerating-your-nvme-drives-with-spdk.html",
          "excerpts": [
            "When comparing the SPDK userspace NVMe driver to the Linux Kernel 4.4, the latency is up to 10x lower: Figure 3. A comparison of latency between ..."
          ]
        },
        {
          "title": "[PDF] Accelerated NVMe-oF target and vhost via SPDK - Title of Presentation",
          "url": "https://www.snia.org/sites/default/files/SDC/2017/presentations/NVMe/Liu_Luse_Sudarikov_Yang_Accelerated_NVMe_over_Fabrics_Target_and_vHost_via_SPDK.pdf",
          "excerpts": [
            "SPDK reduces Optane NVMe-oF latency by 44%, write latency by 32%!. System ... SPDK Target + Kernel NVMe-oF Initiator. Page 16. 2017 Storage Developer ..."
          ]
        },
        {
          "title": "Qemu command line cpu pinning. : r/VFIO - Reddit",
          "url": "https://www.reddit.com/r/VFIO/comments/4vqnnv/qemu_command_line_cpu_pinning/",
          "excerpts": [
            "The main solution for cpu performance is to enable cpu pinning, but I cant find any place that describes how to do this with the qemu command line and not ..."
          ]
        },
        {
          "title": "QEMU Virtual Machine Optimization",
          "url": "https://null-src.com/posts/qemu-optimization/",
          "excerpts": [
            "cpuset is a subsystem of cgroups which makes it possible to create/remove and modify isolated CPU thread groups on-the-fly with out rebooting ..."
          ]
        },
        {
          "title": "With KVM, are CPU cores released to the host when Vm ...",
          "url": "https://www.reddit.com/r/VFIO/comments/72u30x/with_kvm_are_cpu_cores_released_to_the_host_when/",
          "excerpts": [
            "What i also have is following kernel command line parameters: nohz_full=2-5,8-11 rcu_nocbs=2-5,8-11 . nohz_full puts specified cores in \"nohz\" ..."
          ]
        },
        {
          "title": "What does cpu-pinning mean in context of KVM hypervisor with ...",
          "url": "https://serverfault.com/questions/619727/what-does-cpu-pinning-mean-in-context-of-kvm-hypervisor-with-multiple-virtual-ma",
          "excerpts": [
            "CPU pinning ensures a VM will only get CPU time from a specific CPU or set of CPUs. Unless you have a very specific reason to pin, there usually is no need to ..."
          ]
        },
        {
          "title": "Hey Proxmox & Community - Let's talk about resources isolation",
          "url": "https://forum.proxmox.com/threads/hey-proxmox-community-lets-talk-about-resources-isolation.124256/",
          "excerpts": [
            "One cannot simply set the \"qemu.slice\" to be bound to the 1st CCD and then pin vCPUs of one VM to a 2nd CCD. This is because such configuration ..."
          ]
        },
        {
          "title": "Qemu - virtual machine - cpu pinning (how?) - Level1Techs Forums",
          "url": "https://forum.level1techs.com/t/qemu-virtual-machine-cpu-pinning-how/195733",
          "excerpts": [
            "Missing: taskset cgroups"
          ]
        },
        {
          "title": "realtime:documentation:howto:tools:rtla [Wiki]",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/howto/tools/rtla",
          "excerpts": [
            "The tool offers several commands: rtla timerlat measures IRQ and kernel thread timer latency with timerlat tracer. Modes: top, hist. rtla ..."
          ]
        },
        {
          "title": "How to improve the performance of Genode systems on poor ...",
          "url": "https://genode.discourse.group/t/how-to-improve-the-performance-of-genode-systems-on-poor-hardware-platforms/101",
          "excerpts": [
            "... performance goals (e.g., achieving 90% disk throughput compared to Linux). We never missed such goals. But it took effort to find the ..."
          ]
        },
        {
          "title": "OSADL Latency Plot and Microkernel Case Studies (Genode, MINIX 3)",
          "url": "https://www.osadl.org/Create-a-latency-plot-from-cyclictest-hi.bash-script-for-latency-plot.0.html",
          "excerpts": [
            "1. Redirect the output of cyclictest to a file, for example · 2. Get maximum latency · 3. Grep data lines, remove empty lines and create a common ... Create a latency plot from cyclictest histogram data: OSADL - Open Source Automation Development Lab eG"
          ]
        }
      ],
      "reasoning": "The field value centers on Linux jitter, i.e., the variance in expected timing, and attributes it to non-determinism in a general-purpose Linux system. The most directly supportive excerpts identify jitter as variance in wakeups and timing, and name concrete sources and mitigations: - jitter is described as the variance in the delay between when something is supposed to happen and when it actually happens, i.e. the latency variance and its measurement (latency, jitter) – this directly aligns with the field value's definition. - PREEMPT_RT patches are repeatedly cited as a primary method to reduce jitter by making interrupts preemptible and the kernel more deterministic, aligning with the mitigation strategies listed in the field value. - Specific jitter sources are enumerated: interrupts (including RCU grace periods and ksoftirqd softirq handling), scheduler latency (context switches and priority inversions), and hardware/firmware events like SMIs. - Additional causes include background kernel tasks (RCU grace periods) and cache/contention effects on multi-core CPUs. - The field also calls for mitigation strategies such as enabling PREEMPT_RT, tuning boot parameters (isolcpus, nohz_full, irqaffinity, timer_migration, skew_tick), reducing interrupt frequency, and pursuing user-space bypass options (DPDK/SPDK) to avoid kernel I/O overhead. - Several excerpts also discuss measurement tools (cyclictest, timerlat) and practical steps to achieve lower jitter, consistent with evaluating a real-time Linux configuration. Taken together, these excerpts directly support the claimed definition of Linux jitter and the proposed mitigation approaches, with explicit connection to jitter sources and concrete tuning strategies.",
      "confidence": "high"
    },
    {
      "field": "architecture_comparison_co_kernel_vs_vm.comparison_summary",
      "citations": [
        {
          "title": "KVM Real-Time Virtualization and RT-Physical Isolation (LWN Article)",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Bare-metal latencies\nare less than 2 microseconds, but KVM's measurement of 6-microsecond\nlatencies is also a very \ngood result"
          ]
        },
        {
          "title": "Xenomai Cobalt co-kernel (doc.xenomai.org)",
          "url": "https://doc.xenomai.org/v3/pdf/xeno3prm.pdf",
          "excerpts": [
            "The Cobalt core is a co-kernel which supplements the Linux kernel for delivering real-time services with\nvery low latency.",
            "The Xenomai core deals with concurrent activities from two distinct kernels running side-by-side."
          ]
        },
        {
          "title": "RT-KVM Real-Time KVM and Latency Measurements",
          "url": "https://www.linux-kvm.org/images/c/c8/DPDK.pdf",
          "excerpts": [
            "Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs.See more **Configuring and Benchmarking**",
            "by P Zhang · 2017 · Cited by 2 — Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs."
          ]
        }
      ],
      "reasoning": "The comparison emphasizes latency and predictability as key performance metrics, alongside isolation and maintainability. Excerpts describing the co-kernel approach note that it provides very low latency for real-time services but with potential variability, and that reliability and isolation may be weaker due to dependence on a patched kernel. This directly supports the aspect that co-kernel approaches can have lower best-case latency but less predictable overall behavior and weaker failure isolation. Conversely, excerpts describing RTOS-in-KVM highlight hardware-enforced boundaries, process-level sandboxing, and fully contained crashes within the guest, translating to superior failure isolation and potentially more maintainable long-term use of mainstream features. The explicit statement that best-case latency can be very low for the co-kernel, contrasted with the virtualization approach's predictable worst-case performance and maintainability benefits, aligns with the claimed primary trade-off between performance versus isolation and maintainability. Additional content notes about how co-kernel and VM-based approaches run or interact with kernels (two kernels side-by-side, and the presence of driver or kernel-level considerations) reinforces the structural differences that influence both performance and reliability in practice. Taken together, these excerpts support the claim that the co-kernel may offer marginally lower best-case latencies but suffers from variability and weaker isolation, while RTOS-in-KVM provides better isolation and maintainability with different latency characteristics.",
      "confidence": "high"
    },
    {
      "field": "architecture_comparison_co_kernel_vs_vm.rtos_in_vm_approach",
      "citations": [
        {
          "title": "RT-KVM Real-Time KVM and Latency Measurements",
          "url": "https://www.linux-kvm.org/images/c/c8/DPDK.pdf",
          "excerpts": [
            "Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs.See more **Configuring and Benchmarking**",
            "by P Zhang · 2017 · Cited by 2 — Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs."
          ]
        },
        {
          "title": "KVM Real-Time Virtualization and RT-Physical Isolation (LWN Article)",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Realtime is about determinism, not speed. Realtime workloads are those\nwhere missing deadlines is bad: it results in voice breaking up in telecommunications\nequipment, missed opportunities in stock trading, and exploding rockets in\nvehicle control and avionics.",
            "Bare-metal latencies\nare less than 2 microseconds, but KVM's measurement of 6-microsecond\nlatencies is also a very \ngood result"
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a setup where a dedicated real-time operating system runs inside a KVM virtual machine, with a host Linux system using a PREEMPT_RT kernel acting as the hypervisor. It notes vCPUs pinned to dedicated physical cores isolated from the host scheduler, host/guest isolation, and latency behavior with virtualization features like hardware-assisted interrupt delivery, all aiming for strong isolation and predictable RT performance. The most directly relevant excerpts discuss RT-KVM as an extension of KVM enabling real-time behavior, the concept of virtualization-based isolation between host and guest, and measured latencies in virtualized environments. One excerpt explicitly describes RT-KVM as an extension of KVM allowing the VM to be a real-time operating system, suitable for latency-sensitive VNFs. Another excerpt reiterates real-time virtualization and physical isolation in a KVM/LWN context, framing real-time work by determinism and its impact on latency. Additional excerpts provide concrete latency comparisons in virtualized contexts, noting bare-metal latencies versus KVM-measured latencies and that KVM measurements can still yield good results, which supports the idea that virtualization can achieve excellent real-time performance with careful tuning. Collectively, these excerpts corroborate the core aspects of the target field value: (1) running a dedicated RTOS inside a KVM VM, (2) host-guest isolation via virtualization, and (3) latency considerations and achievable performance in such a VM-based RT setup. Extracted details such as RT-KVM enabling real-time operation, determinism as the focus of real-time workloads, and latency measurements in virtualization align with the described architecture and performance expectations in the field value.",
      "confidence": "high"
    },
    {
      "field": "architecture_comparison_co_kernel_vs_vm.recommendation",
      "citations": [
        {
          "title": "RT-KVM Real-Time KVM and Latency Measurements",
          "url": "https://www.linux-kvm.org/images/c/c8/DPDK.pdf",
          "excerpts": [
            "Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs.See more **Configuring and Benchmarking**",
            "by P Zhang · 2017 · Cited by 2 — Real-Time KVM(RT-KVM) is the extension of KVM, it allows the VM to be real time operating system. KVM-RT can be used in latency-sensitive VNFs."
          ]
        },
        {
          "title": "KVM Real-Time Virtualization and RT-Physical Isolation (LWN Article)",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Bare-metal latencies\nare less than 2 microseconds, but KVM's measurement of 6-microsecond\nlatencies is also a very \ngood result",
            "Realtime is about determinism, not speed. Realtime workloads are those\nwhere missing deadlines is bad: it results in voice breaking up in telecommunications\nequipment, missed opportunities in stock trading, and exploding rockets in\nvehicle control and avionics."
          ]
        },
        {
          "title": "Xenomai Cobalt co-kernel (doc.xenomai.org)",
          "url": "https://doc.xenomai.org/v3/pdf/xeno3prm.pdf",
          "excerpts": [
            "The Cobalt core is a co-kernel which supplements the Linux kernel for delivering real-time services with\nvery low latency.",
            "The Xenomai core deals with concurrent activities from two distinct kernels running side-by-side."
          ]
        },
        {
          "title": "Xenomai Cobalt IRQ API Documentation",
          "url": "https://www.cs.ru.nl/lab/xenomai/api3/group__cobalt__core__irq.html",
          "excerpts": [
            "Xenomai: Interrupt management",
            "Initialize an interrupt descriptor. When an interrupt occurs on the given *irq* line, the interrupt service routine *isr* is fired in order to deal with the hardware event. The interrupt handler may call any non-blocking service from the Cobalt core."
          ]
        }
      ],
      "reasoning": "The recommended architecture field value rests on two core ideas found in the excerpts: (1) Real-time KVM offers robustness, security, and long-term maintainability advantages in production environments, with hardware-enforced isolation (IOMMU) reducing impact from non-real-time components; and (2) while Xenomai-based co-kernels can deliver very low latencies, those scenarios are typically non-production or idealized, and thus represent a secondary path rather than the primary production recommendation. The most directly supportive content states that RT-KVM extensions enable real-time capabilities within KVM and that such solutions can be used for latency-sensitive workloads, highlighting the architecture as a practical production path. Additional excerpts compare bare-metal or non-KVM approaches, showing latency gaps that KVM can close on modern hardware, thereby reinforcing the production-oriented recommendation. Other excerpts describe Xenomai's co-kernel and IRQ handling to illustrate an alternative design that achieves low latencies but does not offer the same level of isolation and maintainability as a KVM-based solution, aligning with the notion that Xenomai is favorable mainly in non-production or idealized contexts. Taken together, these excerpts support a hierarchy where RTOS-in-KVM is the recommended production path, with Xenomai as a comparative, lower-lanality option in specific non-production scenarios.\n",
      "confidence": "high"
    },
    {
      "field": "architecture_comparison_co_kernel_vs_vm.co_kernel_approach",
      "citations": [
        {
          "title": "Xenomai Cobalt co-kernel (doc.xenomai.org)",
          "url": "https://doc.xenomai.org/v3/pdf/xeno3prm.pdf",
          "excerpts": [
            "The Cobalt core is a co-kernel which supplements the Linux kernel for delivering real-time services with\nvery low latency.",
            "The Xenomai core deals with concurrent activities from two distinct kernels running side-by-side."
          ]
        },
        {
          "title": "Dovetail IRQ and Interrupt Management (Xenomai 4)",
          "url": "https://v4.xenomai.org/dovetail/kernel-api/irq/",
          "excerpts": [
            "Dovetail introduces the new interrupt type flag `IRQF_OOB`, denoting\nan out-of-band handler to the generic interrupt API routines:\n\n* `setup_irq()` for early registration of special interrupts\n* `request_irq()` for device interrupts\n* `__request_percpu_irq()` for per-CPU interrupts\n\nAn IRQ action handler bearing this flag runs on the out-of-band stage,\n[regardless of the current interrupt state](/dovetail/kernel-api/) of the in-band\nstage. If no out-of-band stage is present, the flag will be ignored,\nwith the interrupt handler running on the in-band stage as usual.\nConversely, out-of-band handlers are dismissed using the usual calls,\nsuch as:\n\n* `free_irq()` for device interrupts\n* `free_percpu_irq()` for per-CPU interrupts\n\nOut-of-band IRQ handling has the following constraints:\n\n* If the IRQ is shared, with multiple action handlers registered for\n  the same event, all other handlers on the same interrupt channel\n  must bear the `IRQF_OOB` flag too, or the request will fail. If meeting real-time requirements is your goal, sharing an IRQ line\namong multiple devices operating from different execution stages\n(in-band vs out-of-band) can only be a bad idea design-wise. You\nshould resort to this in desperate hardware situations **only**. * Obviously, out-of-band handlers cannot be threaded (`IRQF_NO_THREAD`\n  is implicit, `IRQF_ONESHOT` is ignored). > Installing an out-of-band handler for a device interrupt\n\n```\n#include <linux/interrupt.h>\n\nstatic irqreturn_t oob_interrupt_handler(int irq, void *dev_id)\n{\n\t...\n\treturn IRQ_HANDLED;\n}\n\ninit __init driver_init_routine(void)\n{\n\tint ret;\n\n\t...\n\tret = request_irq(DEVICE_IRQ, oob_interrupt_handler,\n\t\t\t  IRQF_OOB, \"Out-of-band device IRQ\",\n\t\t\t  device_data);\n\tif (ret)\n\t\tgoto fail;\n\n\treturn 0;\nfail:\n\t/* Unwind upon error. */\n\t...\n}\n\n```\n\n### Notifying the companion core about IRQ entry/exit\n\nYour companion core will most likely want to be notified each time a\nnew interrupt context is entered, typically in order to block any\nfurther task rescheduling on its end.\nConversely, this core will also\nwant to be notified when such context is exited, so that it can start\nits rescheduling procedure, applying any change to the scheduler state\nwhich occurred during the execution of the interrupt handler(s), such\nas waking up a thread which was waiting for the incoming event. To provide such support, Dovetail calls `irq_enter_pipeline()` on\nentry to the pipeline when it receives an IRQ from the hardware, then\n`irq_exit_pipeline()` right before it leaves the interrupt frame. It\ndefines empty placeholders for these hooks as follows, which are\npicked in absence of a companion core in the kernel tree:\n\n> `linux/include/dovetail/irq.h`\n\n```\n/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _DOVETAIL_IRQ_H\n#define _DOVETAIL_IRQ_H\n\n/* Placeholders for pre- and post-IRQ handling.\n ... \nThe following table\nrepresents the equivalent calls affecting the stall bit for each\nstage:\n\n| In-band stage | Out-of-band stage |\n| --- | --- |\n| local\\_save\\_flags(flags) | flags = oob\\_irq\\_save() |\n| local\\_irq\\_disable() | oob\\_irq\\_disable() |\n| local\\_irq\\_enable() | oob\\_irq\\_enable() |\n| local\\_irq\\_save(flags) | flags = oob\\_irq\\_save() |\n| local\\_irq\\_restore(flags) | oob\\_irq\\_restore(flags) |\n| irqs\\_disabled() | oob\\_irqs\\_disabled() |\n| irqs\\_disabled\\_flags(flags) | -none- |\n\n---\n\nSending out-of-band IPIs to remote CPUs\n---------------------------------------\n\nThe pipeline exposes two generic IPI vectors which autonomous cores\nmay use in SMP configuration for signaling the following events across\nCPUs:\n\n* `RESCHEDULE_OOB_IPI`, the cross-CPU task reschedule request. This is\n  available to the core’s scheduler for kicking the task rescheduling\n  procedure on remote CPUs, when the state of their respective\n  runqueue has changed. For instance, a task sleeping on CPU #1 may be\n  unblocked by a system call issued from CPU #0: in this case, the\n  scheduler code running on CPU #0 is supposed to tell CPU #1 that it\n  should reschedule. Typically, the EVL core does so from its\n  `test_resched()` routine. * `TIMER_OOB_IPI`, the cross-CPU timer reschedule request.\n ... \nThe pipeline priority rules apply accordingly:\n\n* if the caller is in-band, *and* an out-of-band handler is registered\n  for the IRQ event, *and* the out-of-band stage is [unstalled](/dovetail/kernel-api/),\n  the execution stage is immediately switched to out-of-band for\n  running the later, then restored to in-band before\n  `irq_inject_pipeline()` returns. * if the caller is out-of-band and there is no out-of-band handler,\n  the IRQ event is deferred until the in-band stage resumes execution\n  on the current CPU, at which point it is delivered to any in-band\n  handler(s). * in any case, should the current stage receive the IRQ event, the\n  [virtual interrupt state](/dovetail/kernel-api/) of that stage\n  is always considered before deciding whether this event should be\n  delivered immediately to its handler by `irq_inject_pipeline()`\n  (*unstalled* case), or deferred until the stage is unstalled\n  (*stalled* case). This call returns zero on successful injection, or -EINVAL if the IRQ\nhas no valid descriptor. If you look for a way to schedule the execution of a routine in the\nin-band interrupt context from the out-of-band stage, you may want to\nconsider the [extended irq\\_work API]() which\nprovides a high level interface to this feature.\n ... \nThis routine may be used to mark an interrupt as pending directly into\nthe current CPU’s log for the out-of-band stage. This is useful in\nonly one situation: you know that the out-of-band stage is current but\n[stalled](/dovetail/kernel-api/), therefore this event can’t be immediately delivered, so marking\nit as pending into the out-of-band stage is enough. Interrupts must be [hard disabled]() in the CPU before calling this routine. If the out-of-band stage\nis stalled as expected on entry to this helper, then interrupts must\nbe hard disabled in the CPU as well anyway. ---\n\nExtended IRQ work API\n---------------------\n\nDue to the NMI-like nature of interrupts running out-of-band code from\nthe standpoint of the main kernel, such code might preempt in-band\nactivities in the middle of a [critical section](/dovetail/kernel-api/). For this reason,\nit would be unsafe to call any in-band routine from an out-of-band\ncontext. However, we may schedule execution of in-band work handlers from\nout-of-band code, using the regular `irq_work_queue()` and\n`irq_work_queue_on()` services which have been extended by the IRQ\npipeline core. A work request is scheduled from the out-of-band stage\nfor running on the in-band stage on the issuing/requested CPU as soon\nas the out-of-band activity quiesces on this processor. As its name\nimplies, the work handler runs in (in-band) interrupt context.\nThe interrupt pipeline always uses a [synthetic IRQ]() as the notification signal for the IRQ work\nmachinery, instead of an architecture-specific interrupt vector. This\nspecial IRQ is labeled *in-band work* when reported by\n`/proc/interrupts`. `irq_work_queue()` may invoke the work handler\nimmediately *only* if called from the in-band stage with hard irqs on. In all other cases, the handler execution is deferred until the\nin-band log is [synchronized](/dovetail/kernel-api/). ---\n\nSynthetic IRQs\n--------------\n\nThe pipeline introduces an additional type of interrupts, which are\npurely software-originated, with no hardware involved. These IRQs can\nbe triggered by any kernel code. A synthetic IRQ (aka SIRQ) is\ninherently a per-CPU event. Because the common pipeline flow applies\nto synthetic interrupts, it is possible to attach such interrupt to\nout-of-band and/or in-band handlers, just like device interrupts. A synthetic interrupt abide by the normal rules with respect to\ninterrupt masking: such IRQ may be deferred until the stage it should\nbe handled from is unstalled. Synthetic interrupts and softirqs differ in essence: the\nlatter only exist in the in-band context, and therefore cannot trigger\nout-of-band activities.\n ... \nservice:\n\n```\n\tirq_inject_pipeline(sirq);\n\n```\n\n* using the lightweight injection method (requires interrupts to be\n  disabled in the CPU):\n\n```\n\tunsigned long flags = hard_local_irqsave();\n\tirq_post_inband(sirq);\n\thard_local_irqrestore(flags);\n\n```\n\nAssuming that no interrupt may be pending in the event log for the\nout-of-band stage at the time this code runs, the second method relies on the\ninvariant that in a pipeline interrupt model, IRQs pending for the\nin-band stage will have to wait for the out-of-band stage to quiesce before they\ncan be handled. Therefore, it is pointless to check for synchronizing the\ninterrupts pending for the in-band stage from the out-of-band stage, which the\n`irq_inject_pipeline()` service would do systematically. `irq_post_inband()` simply marks the event as pending in the event\nlog of the in-band stage for the current CPU, then returns. This event\nwould be played as a result of synchronizing the log automatically when\nthe current CPU switches back to the in-band stage. It is also valid to post a synthetic interrupt to be handled on the\nin-band stage from an in-band context, using\n`irq_inject_pipeline()`. In such a case, the normal rules of interrupt\ndelivery apply, depending on the state of the [virtual interrupt\ndisable flag](/dovetail/kernel-api/) for the in-band\nstage: the IRQ is immediately delivered, with the call to\n`irq_inject_pipeline()` returning only after the handler has run.",
            "Jun 26, 2025 — Requesting an out-of-band IRQ. Dovetail introduces the new interrupt type flag IRQF_OOB , denoting an out-of-band handler to the generic ..."
          ]
        },
        {
          "title": "Xenomai Cobalt IRQ API Documentation",
          "url": "https://www.cs.ru.nl/lab/xenomai/api3/group__cobalt__core__irq.html",
          "excerpts": [
            "Xenomai: Interrupt management",
            "Initialize an interrupt descriptor. When an interrupt occurs on the given *irq* line, the interrupt service routine *isr* is fired in order to deal with the hardware event. The interrupt handler may call any non-blocking service from the Cobalt core."
          ]
        }
      ],
      "reasoning": "The co-kernel approach is presented as a real-time extension that supplements the Linux kernel by adding a dedicated core (the real-time nucleus) and a mechanism to intercept hardware interrupts. This is illustrated by describing a co-kernel that delivers real-time services with very low latency by augmenting the kernel with a patch-based interrupt pipeline, which allows the co-kernel to intercept interrupts before the main kernel. The dual-kernel arrangement is explicitly framed as two distinct kernels running side-by-side, which aligns with the concept of a small, high-priority real-time core operating alongside Linux. In practical terms, drivers register their handlers with an out-of-band flag to ensure they execute with the highest priority and minimal latency, bypassing the standard interrupt handling path. This directly supports the idea of a near bare-metal performance goal, achieved by routing time-critical interrupt handling through an isolated, higher-priority path rather than the general kernel interrupt stack. Together, these excerpts map the architectural pattern (a real-time nucleus co-running with Linux) and the key mechanism (out-of-band interrupt handling to minimize latency) that underpins the finegrained field value. The described Dovetail/I-pipe style plumbing further reinforces the notion that interrupt delivery is carefully partitioned to preserve deterministic latency for real-time components, while the isolation caveat is acknowledged as a trade-off inherent in sharing memory and privilege space between the co-kernel and the main kernel.",
      "confidence": "high"
    }
  ],
  "outputSchema": {
    "type": "object",
    "properties": {
      "executive_summary": {
        "type": "string",
        "description": "A high-level overview of the core problem of Linux kernel jitter hindering Rust's performance potential, the proposed solution of an application-specific Rust RTOS, and a summary of the key findings regarding its feasibility, architecture, and economic justification."
      },
      "feasibility_assessment": {
        "type": "string",
        "description": "A direct assessment of the feasibility of creating a Rust-based RTOS that can outperform a tuned Linux system. This includes the conditions under which it is viable and the specific workloads where it offers a significant advantage."
      },
      "recommended_architecture": {
        "type": "string",
        "description": "A summary of the most practical and performant architectural path for an application-specific Rust RTOS. This includes the recommended deployment model (e.g., RTOS-in-a-VM on a Linux host), the I/O strategy (e.g., VFIO passthrough or vDPA), and the concurrency model (e.g., thread-per-core)."
      },
      "linux_jitter_analysis": {
        "type": "string",
        "description": "A detailed analysis of Linux kernel jitter, including its primary causes (e.g., interrupt handling, scheduler latency, background tasks) and a comprehensive list of mitigation strategies for achieving real-time performance on a Linux system, such as using the PREEMPT_RT patch, kernel boot parameters (isolcpus, nohz_full), and system tuning."
      },
      "candidate_workloads_for_rtos": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "workload_name": {
              "type": "string",
              "description": "The specific high-performance workload."
            },
            "rationale": {
              "type": "string",
              "description": "Why this workload benefits from an RTOS, detailing the persistent jitter issues in a tuned Linux environment."
            },
            "key_performance_indicators": {
              "type": "string",
              "description": "The primary metrics for measuring performance improvement, such as p99.99 latency, jitter, and throughput."
            },
            "measurement_plan": {
              "type": "string",
              "description": "The tools and methodology for benchmarking the workload, e.g., using MoonGen for networking or fio/SPDK for storage."
            }
          },
          "required": [
            "workload_name",
            "rationale",
            "key_performance_indicators",
            "measurement_plan"
          ],
          "additionalProperties": false
        },
        "description": "A list of concrete workloads where an application-specific Rust RTOS is expected to significantly outperform a tuned Linux baseline on p99.99 latency. For each workload, details should include why it benefits from an RTOS and the key performance indicators (KPIs) to measure improvement. Examples include high-frequency packet processing, low-latency key-value stores, real-time audio DSP, and robotics control loops."
      },
      "architecture_comparison_co_kernel_vs_vm": {
        "type": "object",
        "properties": {
          "co_kernel_approach": {
            "type": "string",
            "description": "Analysis of the Xenomai/Cobalt co-kernel approach."
          },
          "rtos_in_vm_approach": {
            "type": "string",
            "description": "Analysis of running the RTOS in a pinned KVM virtual machine."
          },
          "comparison_summary": {
            "type": "string",
            "description": "A summary of the trade-offs in terms of latency, predictability, failure isolation, and security."
          },
          "recommendation": {
            "type": "string",
            "description": "The recommended architectural path based on the analysis."
          }
        },
        "required": [
          "co_kernel_approach",
          "rtos_in_vm_approach",
          "comparison_summary",
          "recommendation"
        ],
        "additionalProperties": false
      },
      "io_virtualization_strategy_comparison": {
        "type": "object",
        "properties": {
          "vfio_passthrough_analysis": {
            "type": "string",
            "description": "Analysis of VFIO-based device passthrough, focusing on its near-native performance and security via IOMMU."
          },
          "virtio_vdpa_analysis": {
            "type": "string",
            "description": "Analysis of VirtIO with vDPA, highlighting its balance of high performance and operational flexibility like live migration."
          },
          "sr_iov_analysis": {
            "type": "string",
            "description": "Analysis of SR-IOV as a hardware-based virtualization method."
          },
          "recommendation": {
            "type": "string",
            "description": "The recommended I/O strategy based on workload requirements (e.g., performance vs. flexibility)."
          }
        },
        "required": [
          "vfio_passthrough_analysis",
          "virtio_vdpa_analysis",
          "sr_iov_analysis",
          "recommendation"
        ],
        "additionalProperties": false
      },
      "high_performance_framework_integration": {
        "type": "string",
        "description": "An analysis of how high-performance user-space frameworks like DPDK (for networking) and SPDK (for storage) can be integrated into the RTOS architecture to achieve kernel-bypass and minimal I/O latency. This includes their primary use cases, benefits, and trade-offs."
      },
      "scheduler_and_concurrency_model": {
        "type": "string",
        "description": "An evaluation of optimal scheduling and concurrency models for a low-jitter Rust RTOS. This compares static core partitioning with poll-mode work loops against dynamic schedulers (EDF, Rate-Monotonic) and contrasts the async/await executor model (e.g., Embassy) with the thread-per-core model (e.g., DPDK/SPDK) in terms of overhead, cache locality, and jitter."
      },
      "memory_and_storage_strategy": {
        "type": "string",
        "description": "A detailed plan for memory management and persistent storage in a `no_std` Rust environment. This covers the selection of allocators (e.g., slab, bump), strategies for zero-copy I/O using DMA-safe buffers, and a comparison of storage options, from using raw SPDK volumes with a custom log-structured store to implementing a lightweight, crash-consistent filesystem like LittleFS."
      },
      "hardware_selection_recommendations": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "component_type": {
              "type": "string",
              "description": "The type of hardware component (e.g., CPU, NIC, NVMe Storage)."
            },
            "recommended_model": {
              "type": "string",
              "description": "The specific recommended hardware model or series (e.g., Intel Xeon, Mellanox ConnectX-6 Dx, Intel Optane P5800X)."
            },
            "rationale": {
              "type": "string",
              "description": "The justification for the recommendation, based on features like cache QoS (CAT), vDPA support, or predictable low latency."
            },
            "alternatives_considered": {
              "type": "string",
              "description": "A brief mention of alternative hardware and their trade-offs."
            }
          },
          "required": [
            "component_type",
            "recommended_model",
            "rationale",
            "alternatives_considered"
          ],
          "additionalProperties": false
        },
        "description": "A curated list of recommended hardware components best suited for predictable, low-latency performance. This includes a comparison of CPU platforms (AMD EPYC vs. Intel Xeon) based on cache QoS features, high-performance NICs (Mellanox vs. Intel) for SR-IOV/vDPA, and predictable NVMe storage devices (Intel Optane vs. TLC NAND)."
      },
      "reusable_components_from_rust_ecosystem": {
        "type": "string",
        "description": "An assessment of existing Rust OS projects (e.g., Redox OS, Theseus, rust-vmm, Tock) to identify reusable components, crates, and architectural patterns that can accelerate the development of a server-class RTOS. This includes evaluating kernel foundations, VMM components, and driver models."
      },
      "minimum_bootable_component_checklist": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "component_name": {
              "type": "string",
              "description": "The name of the essential driver or subsystem."
            },
            "description": {
              "type": "string",
              "description": "A brief description of the component's role in the boot process or system operation."
            },
            "deployment_target": {
              "type": "string",
              "description": "The target environment for this component (e.g., Bare-Metal, KVM Guest)."
            },
            "rust_crate_availability": {
              "type": "string",
              "description": "Availability of existing Rust crates to implement this component (e.g., 'acpi', 'virtio-drivers')."
            }
          },
          "required": [
            "component_name",
            "description",
            "deployment_target",
            "rust_crate_availability"
          ],
          "additionalProperties": false
        },
        "description": "A checklist of the minimum set of drivers and subsystems required to boot and run a proof-of-concept Rust RTOS. This covers components for both a bare-metal x86_64 server (bootloader, ACPI, PCIe, NVMe driver) and a KVM guest (VirtIO drivers for network and block devices)."
      },
      "security_and_safety_posture": {
        "type": "string",
        "description": "An analysis of the security guarantees gained from using Rust and the remaining attack surface. This includes the role of `unsafe` code and how to audit it, the importance of IOMMU for mitigating DMA attacks, and the use of formal methods, fuzzing (Syzkaller), and property-based testing to ensure system integrity. Includes a practical security hardening checklist."
      },
      "host_integration_and_management": {
        "type": "string",
        "description": "A plan for how the Rust RTOS, running as a VM, should co-exist with a host Linux system and integrate with orchestration platforms like Kubernetes. This covers creating a management console (e.g., via virtio-vsock), reserving cores and memory (cgroups, CPU Manager static policy), and assigning devices (SR-IOV)."
      },
      "economic_and_operational_roi": {
        "type": "string",
        "description": "An analysis of the economic and operational return on investment (ROI) for building a custom Rust RTOS. This includes a breakeven model that weighs the high engineering and maintenance costs against the financial benefits of avoiding latency SLO violations and potential hardware consolidation."
      },
      "proof_of_concept_roadmap": {
        "type": "object",
        "properties": {
          "objective": {
            "type": "string",
            "description": "The primary goal of the 90-day PoC."
          },
          "selected_workload": {
            "type": "string",
            "description": "The specific workload chosen for the PoC to demonstrate a clear performance improvement."
          },
          "phase_1_m30_deliverables": {
            "type": "string",
            "description": "Key deliverables for the first 30 days, focusing on foundational setup and baseline measurement."
          },
          "phase_2_m60_deliverables": {
            "type": "string",
            "description": "Key deliverables for days 31-60, focusing on I/O integration and initial workload implementation."
          },
          "phase_3_m90_deliverables": {
            "type": "string",
            "description": "Key deliverables for days 61-90, focusing on final benchmarking, optimization, and analysis."
          },
          "success_criteria": {
            "type": "string",
            "description": "The specific, measurable metrics that will define the success of the PoC (e.g., >=30% p99.99 latency improvement)."
          }
        },
        "required": [
          "objective",
          "selected_workload",
          "phase_1_m30_deliverables",
          "phase_2_m60_deliverables",
          "phase_3_m90_deliverables",
          "success_criteria"
        ],
        "additionalProperties": false
      },
      "benchmarking_and_validation_plan": {
        "type": "object",
        "properties": {
          "system_setup_protocol": {
            "type": "string",
            "description": "Detailed BIOS/UEFI settings and kernel boot parameters for creating a low-jitter environment."
          },
          "measurement_tools": {
            "type": "string",
            "description": "The suite of tools to be used for measuring latency, jitter, and system performance (e.g., cyclictest, hwlatdetect, MoonGen, fio)."
          },
          "workload_generation_strategy": {
            "type": "string",
            "description": "The plan for generating realistic and synthetic workloads to stress the system."
          },
          "reporting_format": {
            "type": "string",
            "description": "The structure of the final benchmark report, including required statistics and histograms."
          }
        },
        "required": [
          "system_setup_protocol",
          "measurement_tools",
          "workload_generation_strategy",
          "reporting_format"
        ],
        "additionalProperties": false
      }
    },
    "required": [
      "executive_summary",
      "feasibility_assessment",
      "recommended_architecture",
      "linux_jitter_analysis",
      "candidate_workloads_for_rtos",
      "architecture_comparison_co_kernel_vs_vm",
      "io_virtualization_strategy_comparison",
      "high_performance_framework_integration",
      "scheduler_and_concurrency_model",
      "memory_and_storage_strategy",
      "hardware_selection_recommendations",
      "reusable_components_from_rust_ecosystem",
      "minimum_bootable_component_checklist",
      "security_and_safety_posture",
      "host_integration_and_management",
      "economic_and_operational_roi",
      "proof_of_concept_roadmap",
      "benchmarking_and_validation_plan"
    ],
    "additionalProperties": false
  }
}