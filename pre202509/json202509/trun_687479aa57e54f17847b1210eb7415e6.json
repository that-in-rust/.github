{
  "input": "Does WASM have its own scheduler - if yes - is it almost like a real time operating system with dedicated Cores and Memory?\nIf yes, It is said that Rust has fearless concurrency - how can we leverage that to make a Rust specific WASM - for use cases beyond browser - think of kafka spark databases and so on - how we can increase the outcomes of high performance - reducing OS jitter and using dedicated cores - apart from task stealing async stuff - think of all the ways writing everything in Rust can help",
  "output": {
    "executive_summary": "WebAssembly (WASM) does not have its own native scheduler; scheduling is entirely delegated to the host environment, which could be a web browser, a standalone runtime like Wasmtime, or an orchestrator like Kubernetes. Consequently, a WASM environment is not equivalent to a real-time operating system (RTOS) and does not have inherent control over dedicated CPU cores or memory. However, WASM applications can achieve RTOS-like characteristics, such as low jitter and high predictability, when the host system is specifically configured for real-time performance using techniques like CPU pinning, kernel-level tuning (e.g., PREEMPT_RT Linux), and resource isolation via cgroups. For high-performance, non-browser use cases such as data processing in Kafka or Spark, Rust's 'fearless concurrency' is a critical enabler. This is leveraged by compiling Rust code to a WASM target that supports the WebAssembly Threads and WASI-threads proposals. These standards provide shared memory and atomic operations, allowing Rust's compile-time safety guarantees (`Send`/`Sync` traits) and multi-threading libraries (like `std::thread` and `rayon`) to function within the WASM sandbox. The ultimate strategy for high performance involves a multi-layered approach: building concurrent logic in Rust, compiling it to a multi-threaded WASM module, and deploying it on a host environment that is meticulously tuned to provide dedicated cores and minimize OS-induced jitter.",
    "wasm_has_native_scheduler": false,
    "is_wasm_an_rtos_equivalent": false,
    "wasm_scheduling_and_execution_model": {
      "browser_environment": "In a web browser, WebAssembly execution is integrated into the browser's main event loop, resulting in a cooperative scheduling model. A Wasm module, much like a JavaScript script, runs on the main thread and must yield control back to the event loop to prevent blocking the user interface. Long-running, synchronous Wasm code can freeze the page. True parallelism is achieved through the WebAssembly Threads proposal, which integrates with Web Workers. A main thread can share a compiled `WebAssembly.Module` and a `WebAssembly.Memory` (backed by a `SharedArrayBuffer`) with multiple Web Workers, which are effectively host threads managed by the browser. These workers execute the Wasm code in parallel. Synchronization is handled via atomic instructions (`wait`/`notify`) defined in the Wasm Atomics extension. Due to security concerns like Spectre, the use of `SharedArrayBuffer` is restricted and requires specific cross-origin isolation headers (COOP and COEP) from the server.",
      "standalone_runtime_environment": "Standalone Wasm runtimes like Wasmtime, Wasmer, WAMR, and WasmEdge execute Wasm modules as processes on a host operating system. While the OS schedules the runtime process itself, these runtimes implement internal mechanisms to manage the execution of Wasm code, which is crucial for fairness and preventing resource exhaustion in multi-tenant scenarios. Key preemption mechanisms include: 1) Fuel-Based Metering, where the runtime instruments Wasm code to consume 'fuel' per instruction, trapping or yielding when fuel is exhausted. This provides deterministic interruption but has high performance overhead. 2) Epoch-Based Interruption, a lighter-weight mechanism pioneered by Wasmtime, where Wasm code periodically checks a global epoch counter and yields if a deadline is passed. This is ideal for time-slicing and has lower overhead (~10%). For multithreading, the `wasi-threads` proposal allows Wasm threads to be mapped directly to native OS threads, making the host OS scheduler responsible for their execution. Specialized runtimes like Lunatic provide higher-level, preemptive scheduling for async Wasm processes, abstracting away the complexity of non-blocking I/O.",
      "orchestrated_environment": "In container orchestrators like Kubernetes, Wasm workloads are scheduled using a combination of container runtime integration and Kubernetes' own placement logic. The `runwasi` project provides libraries for creating `containerd` shims (e.g., `containerd-shim-wasm`) that allow various Wasm runtimes to be managed by `containerd` alongside traditional Linux containers. Resource controls like CPU and memory limits are passed via the Container Runtime Interface (CRI) and enforced by the host OS using standard mechanisms like Linux cgroups. The primary Kubernetes mechanism for directing Wasm workloads is the `RuntimeClass` object. A `RuntimeClass` specifies a handler (e.g., `wasmedge`, `crun`) and can use a `nodeSelector` to ensure Wasm pods are scheduled only on nodes with the corresponding Wasm runtime installed. Tools like the Kwasm Operator can automate the setup of Wasm-enabled nodes. For performance-critical workloads, Kubernetes' CPU Manager can be configured with a `static` policy to grant Wasm pods exclusive access to CPU cores, effectively pinning them to reduce OS jitter."
    },
    "rtos_like_behavior_analysis": {
      "host_os_capabilities": "A general-purpose operating system can be configured to provide a near-real-time environment for Wasm workloads. On Linux, this is most effectively achieved using the `PREEMPT_RT` patch, which makes the kernel almost fully preemptible and can reduce maximum scheduling latencies to under 50 microseconds. This is complemented by critical CPU isolation techniques configured via kernel boot parameters: `isolcpus` dedicates specific CPU cores to the real-time task, `nohz_full` stops the periodic scheduler tick on these cores to reduce timer interrupts, and `rcu_nocbs` offloads RCU (Read-Copy-Update) callbacks. The Wasm runtime process can then be pinned to these isolated cores using `cset` or `taskset` and given a real-time scheduling priority like `SCHED_FIFO`. On Windows, soft real-time capabilities are available in Windows 10 IoT Enterprise, which allows for CPU isolation and setting a `REALTIME_PRIORITY_CLASS`, though this does not provide hard real-time guarantees. FreeBSD has significant limitations, such as non-preemptible system calls, making it unsuitable for hard real-time Wasm workloads.",
      "kubernetes_mechanisms": "Kubernetes provides powerful abstractions to manage and automate the configuration of real-time environments for containerized Wasm workloads. The most critical mechanism is the **CPU Manager** with its `static` policy. When this policy is enabled on a kubelet, it allows pods to be granted exclusive access to CPU cores, effectively implementing CPU pinning. To qualify for this, a pod must be in the **`Guaranteed` Quality of Service (QoS) class**, which is achieved by setting CPU and memory `requests` exactly equal to their `limits`, with the CPU request being a whole number. Furthermore, the **Topology Manager**, when configured with a policy like `single-numa-node`, ensures that all resources for a pod (CPU, memory, devices) are allocated from the same NUMA node, which is crucial for minimizing memory access latency on modern hardware. These policies allow Kubernetes to steer Wasm pods (identified by a `RuntimeClass`) onto specifically tuned nodes and provide them with dedicated, isolated resources, which is essential for predictable, low-jitter performance.",
      "limitations_and_gaps": "The fundamental limitation preventing WebAssembly from being a full RTOS equivalent is the lack of direct, standardized control over scheduling from within the Wasm guest module. There are no Wasm or WASI APIs that allow a module to, for example, set its own scheduling policy to `SCHED_FIFO`, inherit priorities, or pin its execution to a specific CPU core. All such control resides with the host environment that embeds and invokes the Wasm runtime. While runtimes like Wasmtime offer internal mechanisms like 'fuel' and 'epochs' for managing computation and yielding, these are for cooperative multitasking and ensuring fairness, not for enforcing hard real-time deadlines. Therefore, achieving RTOS-like behavior is entirely a matter of configuring the host OS and orchestrator to create a constrained, predictable environment and then running the Wasm runtime process within it. The Wasm module is a passenger in a real-time vehicle provided by the host, not the driver."
    },
    "leveraging_rust_concurrency_in_wasm": {
      "core_rust_concepts": "Rust's 'fearless concurrency' is not a single feature but a combination of compile-time guarantees that eliminate entire classes of concurrency bugs, most notably data races. The core concepts are:\n1.  **Ownership and the Borrow Checker**: This is the foundation. Every value in Rust has a single owner, and the compiler enforces strict rules about borrowing (referencing) that value. It allows either one mutable reference or any number of immutable references, but never both at the same time. This statically prevents multiple threads from unsafely modifying the same data simultaneously, without the need for a garbage collector or runtime overhead.\n2.  **`Send` and `Sync` Marker Traits**: These traits integrate the ownership model with multi-threading. The compiler uses them to enforce thread safety. A type is `Send` if it is safe to transfer its ownership to another thread. A type is `Sync` if it is safe to be shared (via immutable reference `&T`) across multiple threads. Primitives like `Mutex<T>` are `Sync` because they provide a mechanism for safe, controlled mutation across threads, while non-thread-safe types like `Rc<T>` (Reference Counted pointer) are not `Send` or `Sync`.\n3.  **Atomics (`std::sync::atomic`)**: For low-level, lock-free programming, Rust provides a rich set of atomic types (e.g., `AtomicUsize`, `AtomicBool`). These are crucial for Wasm as they map directly to the atomic instructions provided by the WebAssembly Threads proposal. They allow for operations that are guaranteed to be indivisible, preventing race conditions in fine-grained concurrent operations and forming the building blocks for higher-level synchronization primitives.",
      "practical_application_in_wasm": "To apply Rust's concurrency model to WebAssembly for server-side use cases, developers must use a specific toolchain and target that enables multi-threading. The process involves:\n1.  **Compilation Target**: The primary target for this is `wasm32-wasip1-threads`. This target extends the standard WASI (WebAssembly System Interface) target with threading capabilities. It requires a recent Rust nightly toolchain and LLVM 16 or newer.\n2.  **Enabling Wasm Features**: The compilation must enable specific WebAssembly features. This is done by setting the `RUSTFLAGS` environment variable: `RUSTFLAGS='-C target-feature=+atomics,+bulk-memory,+mutable-globals'`. The `+atomics` feature is the most critical, as it enables the use of atomic instructions and shared memory.\n3.  **Linker Configuration**: To create a memory space that can be shared between threads, specific linker flags are required, such as `-Wl,--import-memory,--export-memory`. This ensures that when new Wasm threads are spawned, they share the same linear memory as the parent thread.\n4.  **Runtime Support**: The resulting Wasm module must be executed in a host runtime that supports the `wasi-threads` proposal. Major runtimes like Wasmtime, Wasmer, WAMR, and WasmEdge all provide experimental or full support for this. The runtime is responsible for mapping the Wasm threads to native OS threads, allowing for true parallel execution on multi-core hardware.",
      "supported_libraries_and_patterns": "The translation of Rust's rich concurrency ecosystem to Wasm+WASI is largely successful but has some nuances:\n*   **Standard Library (`std::thread`, `std::sync`)**: With the `wasm32-wasip1-threads` target, the standard library's threading and synchronization primitives work as expected. `std::thread::spawn` creates new threads, `Arc<T>` allows for safe, shared ownership across threads, and `Mutex<T>` provides mutual exclusion.\n*   **`crossbeam`**: This popular library, known for its high-performance, lock-free data structures and channels, has been shown to work well in multi-threaded Wasm environments and is often a more performant choice for message passing than the standard library's `mpsc` channels.\n*   **`Rayon`**: As a data-parallelism library, Rayon's functionality depends on its ability to create a thread pool. It can be used effectively in a Wasm+WASI context, provided the underlying runtime has a functional `wasi-threads` implementation to spawn the necessary worker threads. This makes it a powerful tool for parallelizing CPU-bound computations like sorting or data processing.\n*   **`parking_lot`**: This library, which offers more efficient mutex implementations, has historically faced linking and compatibility issues in Wasm environments, particularly when the `+atomics` feature is enabled. Its reliance on certain `std` features that may not be fully implemented or behave differently in Wasm can cause problems.\n*   **Async Runtimes (`Tokio`, `async-std`)**: While these can be compiled to Wasm, their utility is currently limited by WASI's synchronous I/O model. Even if the host runtime uses an async engine, the calls from the Wasm guest's perspective are blocking, which can lead to performance bottlenecks and makes it difficult to build highly concurrent, I/O-bound applications efficiently. This is expected to be resolved with the native async support in WASI 0.3."
    },
    "wasm_threading_proposals_overview": {
      "proposal_name": "WebAssembly & WASI Threading Proposals",
      "standardization_phase": "Core Wasm Threads (Phase 4 - Standardized), WASI-threads (Phase 1 - Legacy), Shared-Everything Threads (Phase 4 - Standardized)",
      "description": "Multi-threading in WebAssembly is enabled by a two-layer system of proposals. The first layer is the core **WebAssembly Threads proposal**, which is standardized and provides the low-level primitives necessary for concurrent execution. It does not, however, specify how to create or manage threads. That responsibility falls to the second layer, the **WebAssembly System Interface (WASI)** proposals. The primary historical proposal here is **`wasi-threads`**, which is now considered a legacy proposal for WASI v0.1 (Preview 1). Its main purpose is to provide a POSIX Threads (pthreads)-like API, enabling the vast ecosystem of existing multi-threaded C, C++, and Rust code to be compiled to Wasm. The future direction for threading is the **`shared-everything-threads`** proposal, which is more deeply integrated with the Component Model and offers a more efficient and flexible model where threads can share more than just linear memory.",
      "key_features": "The key features enabling multi-threading are spread across the core Wasm and WASI proposals:\n1.  **Shared Linear Memory**: Introduced by the core Wasm Threads proposal, this allows a `WebAssembly.Memory` object (backed by a `SharedArrayBuffer` in browsers) to be shared across multiple Wasm threads. This is the fundamental mechanism for inter-thread communication and data sharing.\n2.  **Atomic Operations**: The core proposal also adds a set of atomic memory access instructions (e.g., `memory.atomic.wait32`, `memory.atomic.notify`). These primitives are essential for building higher-level synchronization constructs like mutexes and condition variables without data races.\n3.  **Thread Creation API**: The `wasi-threads` proposal defines a single, crucial hostcall: `thread_spawn(start_arg)`. This function instructs the host runtime to create a new thread of execution that runs within the same shared memory space. Runtimes typically implement this by creating a new, cloned Wasm instance for each thread, which shares the linear memory but has its own stack and state."
    },
    "async_vs_threads_model_comparison": {
      "async_model": "The asynchronous programming model in Wasm is designed for high-concurrency I/O-bound workloads. In the current WASI 0.2 standard, this model has a significant limitation: while host runtimes like Wasmtime use an async engine (Tokio) internally to handle I/O without blocking the host, the calls from the Wasm guest's perspective are synchronous. The guest code makes a blocking call and waits for the operation to complete. This can lead to performance bottlenecks, especially for file I/O, where benchmarks show Wasmtime can be up to 10x slower than native code due to the overhead of moving blocking operations to a dedicated thread pool. The primary benefit of the async model is its low memory overhead per task, allowing a single OS thread to manage thousands of concurrent operations efficiently.",
      "threaded_model": "The multi-threaded model in Wasm, enabled by the `wasi-threads` proposal, provides true parallelism by mapping Wasm threads directly to native OS threads. This allows a Wasm application to leverage multiple CPU cores for computationally intensive tasks. The common implementation strategy is the 'instance-per-thread' model, where the runtime creates a new Wasm instance for each thread, sharing the linear memory but duplicating other instance data. This approach enables genuine parallel execution but comes with higher memory consumption per thread compared to async tasks and relies on experimental support in runtimes like Wasmtime, where a trap in any single thread can terminate the entire host process.",
      "workload_guidance": "The choice between the async and threaded models depends heavily on the workload profile:\n*   **I/O-Bound Workloads**: For applications that spend most of their time waiting for network or disk I/O (e.g., web servers, API gateways, database clients), the **async model** is generally superior. Its low memory footprint and efficient task switching allow for handling a massive number of concurrent connections with minimal resource usage.\n*   **CPU-Bound Workloads**: For applications that perform intensive computations (e.g., data compression, image processing, cryptographic operations, ML inference), the **threaded model** is the clear choice. It allows the workload to be parallelized across multiple CPU cores, significantly reducing execution time.\n*   **Hybrid Workloads**: Many server-side applications are a mix of I/O and CPU work. For these, a **hybrid approach** is recommended. An async framework can manage the I/O-bound parts (like handling incoming requests), and when a CPU-intensive task is required, it can be dispatched to a dedicated thread pool using patterns like `tokio::task::spawn_blocking`. This prevents the CPU-bound work from stalling the async event loop.",
      "wasi_status_and_evolution": "The support for both models in WASI is in a state of transition. As of WASI 0.2, the official standard is effectively single-threaded and has synchronous I/O from the guest's perspective. The `wasi-threads` proposal is considered a legacy extension for WASI Preview 1 compatibility. This has been a major bottleneck for server-side Wasm adoption.\n\nThe future is **WASI 0.3**, with previews expected in August 2025 and completion around November 2025. This release is a critical milestone that will introduce **native asynchronous I/O support** via the Component Model, allowing for true non-blocking calls from guest code. Following this, WASI 0.3.x releases are planned to introduce a new, more efficient threading model based on the `shared-everything-threads` proposal. This evolution will resolve the current performance limitations and make Wasm a much more viable platform for high-performance, concurrent server-side applications. However, the current fragmentation, with non-standard extensions like Wasmer's WASIX, poses a portability risk for developers."
    },
    "os_jitter_reduction_techniques": [
      {
        "technique": "CPU Isolation via Kernel Boot Parameters",
        "category": "CPU Management",
        "description": "Isolates specified CPU cores from the general SMP balancing and scheduling algorithms of the operating system. This prevents most kernel housekeeping tasks (like scheduler ticks and RCU callbacks) and other processes from running on the dedicated cores, creating a 'clean' and predictable environment for the performance-sensitive Wasm workload.",
        "tools_and_methods": "Kernel boot parameters: `isolcpus`, `nohz_full` (disables the scheduler tick), `rcu_nocbs` (offloads Read-Copy-Update callbacks)."
      },
      {
        "technique": "Real-Time Kernel Patching",
        "category": "Kernel Scheduling",
        "description": "Transforms the standard Linux kernel into a fully preemptible, low-latency system suitable for soft or hard real-time tasks. It achieves this by making most of the kernel code preemptible, threading interrupt handlers, and reducing the length of critical sections, which dramatically reduces maximum scheduling latencies from milliseconds to microseconds.",
        "tools_and_methods": "Linux `PREEMPT_RT` patch."
      },
      {
        "technique": "CPU Pinning / Affinity",
        "category": "CPU Management",
        "description": "Binds a specific process, such as the Wasm runtime, to one or more dedicated CPU cores. This prevents the OS scheduler from migrating the process between cores, which improves cache locality, reduces context-switching overhead, and minimizes performance variability.",
        "tools_and_methods": "`taskset` (command-line utility), `pthread_setaffinity_np` (programmatic POSIX function), `cset shield` (cpuset-based user-space tool)."
      },
      {
        "technique": "Real-Time Scheduling Policies",
        "category": "Kernel Scheduling",
        "description": "Assigns a higher, real-time priority to the Wasm runtime process. This ensures it preempts other, lower-priority tasks on the system and is not starved of CPU time, guaranteeing it runs as soon as it is ready.",
        "tools_and_methods": "`SCHED_FIFO` (First-In, First-Out), `SCHED_RR` (Round-Robin), `SCHED_DEADLINE`."
      },
      {
        "technique": "Page Fault Reduction",
        "category": "Memory Management",
        "description": "Prevents latency spikes caused by the OS having to load memory pages from disk into RAM (a page fault) during critical execution paths. This is achieved by locking the process's memory pages into physical RAM, ensuring they are never swapped out.",
        "tools_and_methods": "`mlock()` and `mlockall()` system calls, which require the `CAP_IPC_LOCK` capability."
      },
      {
        "technique": "Huge Page Allocation",
        "category": "Memory Management",
        "description": "Uses larger memory pages (e.g., 2MB or 1GB instead of the standard 4KB) to reduce the number of entries needed in the CPU's Translation Lookaside Buffer (TLB). This decreases the rate of TLB misses and improves memory access performance for applications with large memory footprints. Transparent Huge Pages (THP) should be disabled due to the jitter they can introduce.",
        "tools_and_methods": "Configuring and using `hugetlbfs` on the host system."
      },
      {
        "technique": "Interrupt (IRQ) Mitigation",
        "category": "Interrupt Mitigation",
        "description": "Prevents hardware interrupts, especially frequent ones from network interface cards (NICs), from running on the isolated cores dedicated to the Wasm workload. Interrupts are redirected to other non-critical 'housekeeping' cores to avoid disrupting the performance-sensitive application.",
        "tools_and_methods": "Setting IRQ affinity via `/proc/irq/<IRQ_NUMBER>/smp_affinity`, disabling the `irqbalance` daemon."
      },
      {
        "technique": "Disabling Dynamic Power Management",
        "category": "Hardware Settings",
        "description": "Eliminates latency introduced by the CPU dynamically changing its clock frequency (frequency scaling) or entering low-power sleep states (C-states). This ensures a consistent and predictable processing speed, avoiding delays from frequency ramps or CPU wake-up times.",
        "tools_and_methods": "Setting the CPU governor to `performance`, disabling Turbo Boost and deep C-states in the system's BIOS/UEFI."
      }
    ],
    "dedicated_resource_allocation_methods": [
      {
        "environment": "Linux Kernel (cgroups v2)",
        "cpu_dedication_method": "`cpuset.cpus` to pin processes to an exclusive set of CPU cores. `cpu.max` to set an absolute CPU time limit (e.g., 1.5 cores' worth of time).",
        "memory_dedication_method": "`memory.max` for a hard memory limit that triggers the OOM killer. `memory.high` for a soft limit that throttles the process. `cpuset.mems` to bind memory allocation to a specific NUMA node.",
        "configuration_summary": "Create a cgroup directory under `/sys/fs/cgroup`, move the Wasm runtime's process ID (PID) into the `cgroup.procs` file, and then write the desired limits to the controller interface files (e.g., `cpuset.cpus`, `memory.max`)."
      },
      {
        "environment": "systemd",
        "cpu_dedication_method": "`AllowedCPUs=` to pin the service to a specific set of cores. `CPUQuota=` to set a percentage-based CPU time limit, which maps to `cpu.max`.",
        "memory_dedication_method": "`MemoryMax=` to set a hard memory limit. `MemoryHigh=` to set a soft memory throttle limit. `AllowedMemoryNodes=` for NUMA node binding.",
        "configuration_summary": "In the systemd unit file for the Wasm application, add directives like `AllowedCPUs` and `MemoryMax` to the `[Service]` section. The `MemoryAccounting=yes` directive must also be set for memory limits to be applied."
      },
      {
        "environment": "Kubernetes",
        "cpu_dedication_method": "The Kubelet's `CPU Manager` with its `static` policy. This grants exclusive cores to pods in the `Guaranteed` QoS class that request a whole integer number of CPUs.",
        "memory_dedication_method": "The `Guaranteed` Quality of Service (QoS) class, which is assigned to pods where resource `requests` are set equal to `limits`. Huge pages can also be requested via `hugepages-2Mi` in the pod spec.",
        "configuration_summary": "Configure the Kubelet on worker nodes with `--cpu-manager-policy=static` and `--topology-manager-policy=single-numa-node`. Define a `RuntimeClass` for Wasm. Deploy the Wasm pod with a `Guaranteed` QoS resource profile and specify the `runtimeClassName`."
      }
    ],
    "kernel_level_and_os_bypass_strategies": {
      "strategy": "Kernel-Level Execution and OS-Bypass Strategies",
      "architecture_summary": "This encompasses several advanced techniques designed to minimize OS interaction overhead. 1) **Kernel-Wasm Execution** (e.g., Wasmer's `kernel-wasm`, WALI): This approach involves embedding a Wasm runtime directly into the OS kernel (Ring 0) to eliminate the overhead of user-to-kernel context switches and data copies for system calls. 2) **OS-Bypass Networking** (e.g., DPDK, AF_XDP): This strategy grants Wasm applications direct access to network interface card (NIC) hardware from userspace, completely bypassing the kernel's network stack for ultra-low-latency packet processing. 3) **`io_uring`-based Hostcalls**: This technique leverages Linux's modern `io_uring` asynchronous I/O interface, which uses shared ring buffers between userspace and the kernel to submit and complete I/O operations with minimal or even zero syscalls for batched requests, dramatically reducing overhead for I/O-bound workloads.",
      "performance_impact": "These strategies offer substantial performance gains. **Kernel-Wasm** has demonstrated a ~10% throughput increase for a TCP echo server compared to native userspace code. **OS-Bypass Networking** with DPDK or AF_XDP can be over 10 times faster than the standard Linux network stack, with DPDK achieving up to 79.20 Mpps (Million packets per second). **`io_uring`** is known to significantly improve I/O throughput and reduce latency by enabling efficient batching of operations, addressing a known performance bottleneck in runtimes like Wasmtime which can be up to 10x slower than native for file I/O.",
      "security_tradeoffs": "These high-performance approaches increase the system's attack surface and can compromise Wasm's core promise of portability. **Kernel-Wasm** runs code in the highly privileged Ring 0, but mitigates risk with Wasm's inherent sandboxing, stack overflow checks, and NX bit protections. **OS-Bypass Networking** circumvents the kernel's battle-tested network stack, making any vulnerability in the bypass implementation potentially severe. **`io_uring`** requires careful implementation to prevent malicious Wasm modules from exploiting the shared memory interface. A general recommendation is to only execute trusted binaries when using these advanced strategies."
    },
    "memory_architecture_and_allocators": {
      "linear_memory_model": "WebAssembly traditionally uses a 32-bit linear memory model, suitable for most workload demands and allowing for optimizations like bounds-check removal in host runtimes. The memory64 proposal extends this, enabling addressing beyond 4GB through 64-bit indexes. While this expands potential application memory, caution must be taken with performance overheads introduced due to extended bounds checks.",
      "allocator_comparison": "For WebAssembly, allocator efficiencies significantly impact resource utilization. Common allocators such as dlmalloc (default for many Wasm targets), mimalloc (small and fast), and jemalloc (optimized for multithreading) are evaluated based on their suitability for different workloads. Developer-specific allocator integration into Wasm modules can also improve control over memory allocation strategies.",
      "performance_strategies": "Performance enhancements in Wasm memory management include preallocating memory regions (e.g., via memory arenas) to avoid runtime allocation overheads, optimizing for reduced fragmentation using memory segmentation techniques, and aligning data structures for memory-efficient access.",
      "multithreading_considerations": "WebAssembly's threading model relies on shared memory facilitated by WebAssembly threads and alignment with atomic instructions to ensure correct concurrent computations. Rust utilities such as Arc, Mutex, and atomic types integrate seamlessly with this model, promoting efficient and safe memory utilization across threads."
    },
    "observability_and_performance_engineering": {
      "profiling": "Profiling Wasm applications involves utilizing runtime-specific tools like Wasmtime's GuestProfiler, and integrating with general system profilers like Linux's `perf`. Profiling outputs reveal computational hotspots and can optimize execution paths.",
      "tracing": "Using OpenTelemetry methodologies within Wasm enables distributed tracing, which visualizes interactions and latencies across service-boundaries. The upcoming WASI-OTel standard proposes closer host integration for performance data collection.",
      "jitter_diagnosis": "To diagnose OS-induced jitter in Wasm applications, technologies like Extended Berkeley Packet Filter (eBPF) provide granular insights into scheduling delays, I/O stalls, and overall execution inconsistency.",
      "common_pitfalls": "Challenges in analyzing Wasm performance often stem from non-intrinsic tooling, lack of native support for async patterns in WASI, and overheads related to dense translation layers in memory management."
    },
    "high_performance_use_case_analysis": {
      "use_case": "High-Performance Data-Intensive Applications (Kafka, Spark, Databases)",
      "architectural_patterns": "Several architectural patterns are emerging for integrating Wasm into data-intensive systems:\n\n1.  **In-Process/In-Broker UDFs:** This pattern embeds a Wasm runtime directly into the data system (e.g., a database or message broker) to execute User-Defined Functions (UDFs). By co-locating computation with data, it minimizes network latency and data movement, making it ideal for high-throughput, low-latency transformations.\n\n2.  **JNI/FFI Bridge:** For systems built on the JVM, like Apache Spark, the primary integration pattern is a bridge. A UDF wrapper is written in Java/Scala, which uses the Java Native Interface (JNI) to call a native Wasm runtime. This allows Spark to execute Wasm-compiled logic, but requires careful management of the boundary crossing.\n\n3.  **Event-Driven Functions & Sidecars:** This serverless pattern uses platforms or sidecar processors to trigger Wasm functions in response to events from the data system, such as a new message on a Kafka topic. This pattern leverages Wasm's extremely fast cold starts for elastic, on-demand processing.",
      "performance_considerations": "Achieving high performance with Wasm in these use cases hinges on several key factors:\n\n- **Data Serialization Overhead:** This is a primary bottleneck. Copying and serializing data between the host system (e.g., JVM, database process) and the Wasm module is expensive. Using zero-copy data formats like **Apache Arrow** is critical. Arrow's C Data Interface allows different runtimes to share a pointer to columnar data in memory without any serialization, dramatically improving performance for vectorized UDFs.\n\n- **Boundary Crossing Overhead:** Every call from the host to the Wasm module (via JNI, hostcalls, etc.) incurs a fixed latency cost. To mitigate this, logic should be designed to be 'chunky' rather than 'chatty'. Processing large batches of data (e.g., a full Arrow RecordBatch) in a single call is far more efficient than invoking the Wasm UDF for every individual record.\n\n- **WASI I/O Limitations:** The performance of I/O-bound Wasm functions is currently constrained by the maturity of the WebAssembly System Interface (WASI). While WASI 0.2 introduced standard networking (`wasi-sockets`), it lacks native asynchronous I/O. This means I/O calls are often blocking, which severely limits throughput for network- or disk-intensive tasks. The upcoming **WASI 0.3**, with its planned native async support, is expected to be a game-changer for this category of workloads.\n\n- **Cold Start Time:** For event-driven patterns, Wasm's sub-millisecond cold start times provide a transformative advantage over traditional containers, which can take hundreds of milliseconds to seconds to start, enabling true on-demand, scalable processing.",
      "key_platforms_and_projects": "Real-world implementations and proofs-of-concept demonstrate these patterns:\n\n- **Kafka:** **Redpanda Data Transforms** is a prime example of the in-broker UDF pattern, embedding a Wasmtime engine for inline data transformations. Event-driven platforms like **wasmCloud** (with its Kafka capability provider) and **Fermyon Spin** also integrate with Kafka.\n\n- **Spark:** The **`spark-wasm-udf`** project is a proof-of-concept demonstrating the JNI bridge pattern. Alternatives for native execution include Rust-based query engines like **DataFusion/Ballista** and accelerated Spark backends like **Apache Gluten**.\n\n- **Databases:** Numerous projects are embedding Wasm for safe, high-performance UDFs. Examples include **`pg_extism`** and **`wasmer-postgres`** for PostgreSQL, **`libSQL`** (a fork of SQLite using WasmEdge), and the streaming database **`RisingWave`**."
    },
    "wasm_platform_case_studies": [
      {
        "platform_name": "wasmCloud",
        "architecture": "wasmCloud is a distributed application platform built around the actor model. Its core components are stateless, single-threaded Wasm modules (actors) that contain business logic. These components are loosely coupled with 'capability providers'—host-side plugins that provide non-functional capabilities like messaging (NATS, Kafka), key-value storage, or HTTP servers. All wasmCloud hosts are connected in a self-forming, self-healing mesh network called a 'lattice', which is powered by NATS messaging. Application deployment is managed declaratively by `wadm` (wasmCloud Application Deployment Manager), and security is enforced through explicit 'links' that define allowed connections between components and providers.",
        "concurrency_and_scheduling_model": "Concurrency is managed by the wasmCloud host, abstracting it away from the component developer. Components are designed as simple, synchronous message handlers. The platform achieves high concurrency and load balancing through NATS queue subscriptions; when a message is published to the lattice, any available instance of a subscribed component can process it. This enables horizontal scaling across the distributed network. The `Spreadscaler` allows developers to declaratively manage the number of running instances of a component to meet demand, with components only consuming resources when actively processing requests.",
        "data_plane_and_io_integration": "All I/O and data plane integration is handled through the capability provider model. Components are written against standardized interfaces (defined in WIT), such as `wasi:keyvalue/readwrite` or `wasi:messaging/consumer`. The host then links the component to a specific provider that implements this interface (e.g., a Redis provider for key-value or a Kafka provider for messaging). This decouples the business logic from the infrastructure, allowing providers to be swapped at runtime without changing the Wasm component. NATS serves as the fundamental communication fabric for all control plane and data plane interactions within the lattice.",
        "ideal_use_cases": "wasmCloud is ideal for building flexible, secure, and resilient applications that need to run across diverse and distributed environments, from the cloud to the edge. Its actor model and capability-driven architecture are well-suited for microservices, event-driven systems, and multi-tenant platforms where strong security boundaries and operational flexibility are paramount. It excels at abstracting away infrastructure complexity, allowing developers to focus on business logic."
      },
      {
        "platform_name": "Fermyon Spin",
        "architecture": "Fermyon Spin is a framework for building and running event-driven microservices and web applications using Wasm, with a strong focus on developer experience. It uses the Wasmtime runtime and fully embraces the WebAssembly Component Model, allowing applications to be composed of multiple, isolated Wasm components that communicate via high-level WIT interfaces. This promotes modularity and portability. Spin applications are triggered by events, with HTTP being the primary trigger.",
        "concurrency_and_scheduling_model": "Spin employs a serverless-style, 'instance-per-request' execution model. For every incoming event (e.g., an HTTP request), Spin instantiates a fresh, sandboxed Wasm module, executes its handler, and then discards the instance. This model provides extremely strong isolation between requests at the cost of re-initialization. However, due to Wasm's microsecond-level startup times, this approach is highly performant and allows for massive concurrency by simply running instances in parallel. Recent versions have incorporated Wasmtime's pooling allocator to further improve throughput for concurrent, short-lived instances.",
        "data_plane_and_io_integration": "Spin's primary integration point is its HTTP trigger, making it a natural fit for web services. For other data plane integrations, Spin provides SDKs that simplify connecting to external data services like Redis, PostgreSQL, and other databases from within a Wasm component. The component model also allows for direct, in-process communication between different components of a single Spin application, avoiding network overhead for internal logic.",
        "ideal_use_cases": "Spin is best suited for building lightweight, stateless, and scalable serverless applications, web APIs, and microservices. Its extremely fast cold starts make it an excellent choice for workloads that are event-driven, bursty, or require rapid scaling to zero to minimize costs. It is ideal for developers looking for a simple and efficient way to build and deploy Wasm-based services."
      },
      {
        "platform_name": "Faasm",
        "architecture": "Faasm is a high-performance, stateful serverless runtime specifically designed for data-intensive and High-Performance Computing (HPC) workloads. Its core abstraction is the 'Faaslet,' a Wasm function that runs as a dedicated thread within a single, shared runtime process. This provides lightweight isolation with minimal overhead. A key architectural feature is its use of snapshotting ('Proto-Faaslets') to capture a function's initialized state, enabling cold starts in hundreds of microseconds. It also extends Wasm's memory model to support shared memory pages between concurrently running Faaslets.",
        "concurrency_and_scheduling_model": "Faasm is explicitly designed for massive parallelism. It executes each Faaslet in its own OS thread and extends WASI to support multi-threading primitives like pthreads and OpenMP *within* a single Wasm function. This allows for traditional parallel programming models. Its distributed scheduler aims to minimize data movement by co-locating function execution with its required in-memory state. Concurrency is managed with cgroups to ensure CPU fairness between Faaslet threads.",
        "data_plane_and_io_integration": "Faasm provides a custom, extended host interface that goes far beyond standard WASI, adding support for function chaining, advanced state management with local and global locks, a distributed filesystem, dynamic linking, and even MPI (Message Passing Interface). This rich feature set allows it to run legacy HPC applications compiled to Wasm. State and function I/O are passed as byte arrays to avoid serialization overhead, and its shared memory model enables efficient, low-latency data exchange between parallel functions.",
        "ideal_use_cases": "Faasm is uniquely suited for workloads that are ill-suited for traditional stateless serverless platforms. This includes data-intensive scientific computing, machine learning training and inference, and traditional HPC applications that rely on shared memory, MPI, or OpenMP for parallelism. Its stateful architecture and high-performance focus make it a powerful platform for migrating complex computational tasks to a serverless model."
      }
    ],
    "deployment_patterns_comparison": {
      "pattern": "runwasi Shims (e.g., with Wasmtime, WasmEdge)",
      "startup_time": "This pattern offers exceptionally fast startup times, which is one of its primary advantages. Wasmtime 1.0 reports cold start times of under a millisecond, with some initializations taking as little as 5 microseconds. Benchmarks show that running a WASI application with `runwasi wasmtime` can be nearly 4 times faster than running the same Wasmtime runtime inside a traditional container with `runc` (329 vs 84 tasks per second). However, scaling behavior can vary; at a small scale (10 pods), `containerd-shim-wasmedge` and `containerd-shim-wasmtime` are faster, but at a larger scale (400 pods), a runtime like `crun` with an embedded WAMR scales more effectively.",
      "isolation_strength": "The isolation strength is process-level, as it leverages the host's existing container isolation capabilities provided by `containerd`. This includes using standard Linux kernel features such as namespaces (to isolate views of the system like networking and PIDs), cgroups (to limit and account for resource usage), and seccomp (to restrict the available system calls). This provides a strong security sandbox that is comparable to traditional containers, but it is considered weaker than the hardware-level virtualization provided by microVMs like Firecracker because the host kernel is shared among all Wasm instances.",
      "density_and_footprint": "The resource efficiency of this pattern is significantly higher than traditional containers, enabling greater workload density. The memory footprint of Wasm modules is minimal. For example, integrating the WAMR runtime directly into `crun` can reduce memory usage by over 10% compared to `containerd-shim-wasmtime` and by over 77% compared to `containerd-shim-wasmer`. At a scale of 400 pods, this approach can use 40% less memory than other Wasm shims, demonstrating the high density achievable by avoiding the overhead of a full container OS layer.",
      "ideal_workloads": "Given the extremely fast cold start times and low memory footprint, this pattern is ideally suited for workloads where latency and resource efficiency are critical. This includes event-driven, short-lived functions such as those found in serverless computing, edge functions, and low-latency APIs. The ability to scale from zero rapidly makes it a strong choice for bursty or intermittent traffic patterns where the overhead of traditional container startup would be prohibitive."
    },
    "wasm_runtime_performance_comparison": {
      "runtime_name": "Wasmtime",
      "threading_support": "Wasmtime provides experimental support for the `wasi-threads` proposal, which is essential for running multi-threaded Rust applications compiled to Wasm. This feature can be enabled via configuration flags (`--wasm-features=threads` and `--wasi-modules=experimental-wasi-threads`). The implementation is provided by the `wasmtime-wasi-threads` crate and follows the common 'instance-per-thread' model, where a new, cloned Wasm instance is created for each spawned thread, sharing the same linear memory but having its own stack and state. While this enables true parallelism on multi-core systems, the implementation has a critical limitation for production use in multi-tenant environments: if any single Wasm thread traps or calls `wasi::proc_exit`, the entire host process terminates, not just the individual thread. This lack of fault isolation between threads makes it currently unsuitable for securely sandboxing untrusted, multi-threaded code.",
      "compilation_backends": "Wasmtime supports both Just-In-Time (JIT) and Ahead-Of-Time (AOT) compilation strategies. Its primary compiler backend is Cranelift, a code generator written in Rust that is designed to produce very fast compilations and reasonably fast runtime code. This makes it well-suited for serverless environments where low startup latency is critical. Wasmtime also supports Winch, a newer baseline compiler that compiles even faster than Cranelift, at the cost of generating slower code, which is useful for development or scenarios where startup time is the absolute priority.",
      "key_performance_characteristics": "Wasmtime is a high-performance runtime with a strong focus on standards compliance, security, and low-latency execution. Its performance profile has several key characteristics:\n\n- **Cold Start:** Wasmtime exhibits exceptionally fast cold start times, often under a millisecond. Research indicates that eager initialization of a module's heap and data structures can take as little as 5 microseconds. This makes it a top performer for serverless, function-as-a-service (FaaS), and other event-driven architectures where rapid instantiation is crucial.\n\n- **Throughput and Scaling:** For CPU-bound, multi-threaded workloads, Wasmtime demonstrates excellent, near-linear scaling with the number of cores. Benchmarks with parallel applications like `pigz` (parallel gzip) show an ~8x speedup on eight threads. While its general throughput is competitive, some benchmarks indicate that for specific CPU-intensive kernels, it can be outperformed by runtimes using the LLVM backend (like WAMR or Wasmer-LLVM), which perform more aggressive optimizations at the cost of longer compilation times.\n\n- **Strengths:** Its primary strength is its role as the reference implementation for the WebAssembly Component Model and WASI Preview 2, making it the most standards-compliant runtime. It offers excellent integration with standard Linux profiling tools like `perf` and `VTune`, and provides its own `GuestProfiler` API. It also features advanced mechanisms for controlling execution, such as epoch-based interruption for lightweight, non-deterministic preemption, and a sophisticated pooling allocator to speed up instantiation and manage memory efficiently.\n\n- **Weaknesses:** Wasmtime's most significant documented weakness is its performance on synchronous file I/O, which can be up to 10 times slower than native execution. This is an architectural trade-off resulting from its internal use of the Tokio async engine; synchronous WASI calls are moved to a blocking thread pool, incurring significant context-switching and syscall overhead. Additionally, as noted, its support for `wasi-threads` is still experimental and not yet hardened for production multi-tenant use cases."
    },
    "component_model_architectural_impact": {
      "core_concepts": "The Component Model builds on WIT, providing high-level type definitions and facilitating structured module communication via the Canonical ABI, ensuring seamless binding between heterogeneous languages.",
      "performance_tradeoffs": "Performance impacts arise from data serialization within module boundaries, adding overhead to inter-component communication. However, this trade-off is offset by gains in functionality modularity.",
      "ecosystem_maturity": "As of 2025, the Component Model has reached broad acceptance in Wasm ecosystems, with advancements in interoperable toolchains and runtime-wide optimizations ensuring robust adoption.",
      "architectural_benefits": "Primary advantages include stronger modularity in application design, enhanced capability for integrating diverse programming languages into a unified ecosystem, and smoother schema evolution strategies."
    },
    "wasi_capability_and_io_model": {
      "security_model": "WASI employs a capability-based security system where resources are only accessible through explicitly granted handles, ensuring strict control over module operations.",
      "io_proposals_status": "WASI 0.3, set to be released in late 2025, focuses on async-native interfaces, extending current specifications like WASI Streams and Sockets, enhancing real-time communication model capabilities.",
      "performance_bottlenecks": "Performance constraints include synchronous I/O overheads and system call latencies, which are gradually ameliorated through WASI's evolving proposals.",
      "overhead_mitigation_techniques": "Strategies to enhance WASI layer efficiency encompass batching I/O beyond synchronous intervals, initiating zero-copy data exchanges, and aligning system calls for parallel processing."
    },
    "summary_of_limitations_and_future_outlook": {
      "current_bottlenecks": "The primary technical gaps preventing near-real-time performance in Rust+Wasm systems today are multifaceted. First, in Scheduling, there is a lack of a standardized Wasm API for fine-grained control; developers cannot programmatically pin threads to cores or set real-time scheduling policies like `SCHED_FIFO` from within Wasm. All such control must be managed by the host. Furthermore, runtime preemption mechanisms like Wasmtime's epochs cannot interrupt Wasm code that is blocked on a synchronous host call. Second, I/O is the most significant bottleneck. The official WASI 0.2 specification lacks native asynchronous I/O, forcing I/O-bound applications into a synchronous, blocking model that severely limits concurrency and throughput. Benchmarks show that file I/O in runtimes like Wasmtime can be up to 10 times slower than native code due to the overhead of managing synchronous calls on an internal async engine. Third, in Threading, the official WASI 0.2 standard is single-threaded. The `wasi-threads` proposal is a legacy stopgap that uses a memory-intensive 'instance-per-thread' model and is not part of the main specification. Finally, data transfer between the host and guest, or between components, often requires data to be copied, creating significant overhead that zero-copy strategies are only beginning to address.",
      "community_proposals_to_watch": "To address the current bottlenecks, several critical community proposals are in development. The most important is WASI 0.3 (Preview 3), which is set to introduce native asynchronous I/O support directly into the Component Model via `future<T>` and `stream<T>` types. This will enable true non-blocking I/O from within Wasm. For threading, the `shared-everything-threads` proposal is the designated successor to the legacy `wasi-threads`, aiming to provide a more memory-efficient model where multiple threads can share a single Wasm instance. The overarching WebAssembly Component Model is itself a key proposal to watch, as it provides the foundation for language-agnostic interoperability and composable concurrency. Other important proposals include `wasi-nn` for standardized machine learning inference, and various discussions around enabling zero-copy or shared-memory buffers to reduce data transfer overhead. Finally, the WASI-OTel proposal aims to standardize observability by defining interfaces for tracing and metrics.",
      "realistic_timelines": "The community has provided a public roadmap with estimated timelines for key features. The most critical milestone is the stabilization of native asynchronous I/O. Previews for WASI 0.3.0, which will include this feature, are anticipated around August 2025. The target for the final completion and release of WASI 0.3.0 is November 2025. Following this, support for advanced threading (cooperative and then preemptive) is planned for incremental WASI 0.3.x releases, suggesting these features will likely not be stabilized until 2026 or later. Full ecosystem maturity and widespread adoption of these advanced real-time capabilities will likely take several more years beyond these initial stabilization dates.",
      "risk_assessment_for_adopters": "Organizations adopting Wasm for high-performance, near-real-time production use in its current state (as of late 2024/early 2025) face several significant risks. The foremost risk is API instability and ecosystem fragmentation. Critical features like native async I/O and advanced threading are not yet standardized, meaning early adopters are building on preview features that are subject to breaking changes. The transition from WASI Preview 1 to Preview 2 demonstrated this friction. Furthermore, some runtimes have created non-standard extensions (e.g., Wasmer's WASIX) to fill gaps, which compromises Wasm's core promise of portability. Another major risk is performance variability and ecosystem gaps. Performance for I/O-bound workloads can be an order of magnitude worse than native code, and the ecosystem lacks the mature, battle-tested libraries, debugging tools, and established best practices found in native development for real-time systems. Finally, there is a security trade-off; while Wasm's sandbox is a key benefit, the temptation to use kernel-level execution or OS-bypass techniques to achieve performance goals introduces significant security and potential regulatory complexities."
    }
  },
  "outputBasis": [
    {
      "field": "kernel_level_and_os_bypass_strategies",
      "citations": [
        {
          "title": "WALI: A thin Linux kernel interface for WebAssembly",
          "url": "https://popl24.sigplan.org/details/waw-2024-papers/2/WALI-A-thin-Linux-kernel-interface-for-WebAssembly",
          "excerpts": [
            "This talk introduces WALI: The WebAssembly Linux Interface, a thin layer over Linux's userspace system calls, creating a new class of virtualization."
          ]
        },
        {
          "title": "Running WebAssembly on the Kernel",
          "url": "https://blog.wasmer.io/running-webassembly-on-the-kernel-8e04761f1d8e",
          "excerpts": [
            "Take a look at them to learn how to do high-performance networking in [kernel-wasm](https://github.com/wasmerio/kernel-wasm) .",
            "Load the modules into the kernel:",
            "> Make sure you are running on the latest version ( `0.4.2` ) by executing",
            "`wasmer self-update`",
            "When running Wasmer, select the `kernel` loader and `singlepass` backend:",
            "ate`\n\n```\nsudo wasmer run --backend singlepass --loader kernel the_file.wasm\n```",
            "### Hope you enjoyed reading this article! While running WebAssembly in the Kernel is certainly dangerous and not the recommended approach for most use cases, it helped us to experiment and learn from the experience first hand.",
            "If you love WebAssembly as much as we do, please contact [syrus@wasmer.io](mailto:syrus@wasmer.io) … your next job can be just an email away!",
            "two examples: echo-server and http-server (living in the examples directory of Wasmer main repo).",
            "Cowsay running on the Kernel! ### Hope you enjoyed reading this article!"
          ]
        },
        {
          "title": "PERFORMANCE ANALYSIS OF AF_XDP SOCKETS ON ...",
          "url": "https://openscholar.uga.edu/record/2663/files/ReillyJosephMS.pdf",
          "excerpts": [
            "This thesis analyzes the performance of AF_XDP sockets on commodity hardware such as Netronome and Mellanox. Smart NICs. Testing packet delivery speeds was ..."
          ]
        },
        {
          "title": "The WebAssembly Linux Interface - NASA/ADS",
          "url": "https://ui.adsabs.harvard.edu/abs/2023arXiv231203858R/abstract",
          "excerpts": [
            "by A Ramesh · 2023 · Cited by 2 — We present an implementation of WALI in a modern WebAssembly engine and evaluate its performance on a number of applications which we can now compile with ..."
          ]
        },
        {
          "title": "wasmerio/wasmer: 🚀 Fast, secure, lightweight containers ...",
          "url": "https://github.com/wasmerio/wasmer",
          "excerpts": [
            "Wasmer is a blazing fast and secure WebAssembly runtime that enables incredibly lightweight containers to run anywhere: from Desktop to the Cloud, Edge and ..."
          ]
        },
        {
          "title": "near/wasmer: 🚀 The leading WebAssembly Runtime ...",
          "url": "https://github.com/near/wasmer",
          "excerpts": [
            "Wasmer is a fast and secure WebAssembly runtime that enables super lightweight containers to run anywhere: from Desktop to the Cloud, Edge and IoT devices."
          ]
        },
        {
          "title": "Wasmer: Universal applications using WebAssembly",
          "url": "https://wasmer.io/",
          "excerpts": [
            "Run your apps. Secure. Fast. At Scale. Experience the new container technology powered by WebAssembly that runs your programs safely, anywhere."
          ]
        },
        {
          "title": "WasmEdge Runtime",
          "url": "https://github.com/wasmedge",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "WasmEdge",
          "url": "https://wasmedge.org/",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "WasmEdge is a lightweight, high-performance ...",
          "url": "https://github.com/WasmEdge/WasmEdge",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "Why WASI Preview 2 Makes WebAssembly Production ...",
          "url": "https://thenewstack.io/why-wasi-preview-2-makes-webassembly-production-ready/",
          "excerpts": [
            "WASI Preview 2 is the missing link that Wasm needed to become a viable option for production use cases.",
            "Why WASI Preview 2 Makes WebAssembly Production Ready"
          ]
        },
        {
          "title": "WASI Cycles, an Open Source, 3D WebXR Game - Oracle Blogs",
          "url": "https://blogs.oracle.com/developers/post/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-open-source-3d-webxr-game",
          "excerpts": [
            "WASI Invoking Kafka, MongoDB, and Oracle Database"
          ]
        },
        {
          "title": "eunomia-bpf/bpftime: Userspace eBPF runtime for ...",
          "url": "https://github.com/eunomia-bpf/bpftime",
          "excerpts": [
            "bpftime is a High-Performance userspace eBPF runtime and General Extension Framework designed for userspace. It enables faster Uprobe, USDT, Syscall hooks, XDP ..."
          ]
        },
        {
          "title": "WebAssembly/wasi-io: I/O Types proposal for WASI",
          "url": "https://github.com/WebAssembly/wasi-io",
          "excerpts": [
            "WASI I/O. A proposed WebAssembly System Interface API. Current Phase. WASI I/O is currently in Phase 3. Champions. Dan Gohman. Portability Criteria. WASI I/O ..."
          ]
        },
        {
          "title": "Networking API design · Issue #370 · WebAssembly/WASI - GitHub",
          "url": "https://github.com/WebAssembly/WASI/issues/370",
          "excerpts": [
            "WASI does have a poll_oneoff function for waiting for I/O on multiple handles. It won't scale to very-many handles, but it's likely better than ..."
          ]
        },
        {
          "title": "According to their roadmap, WASI preview 2 will have ...",
          "url": "https://news.ycombinator.com/item?id=38438239",
          "excerpts": [
            "Threads are actually removed from WASI Preview 2. WASI Preview 2 still doesn't support threads, fork, subprocesses or longjmp/setjmp (among others). Not even ..."
          ]
        },
        {
          "title": "Wasmtime 1.0: A Look at Performance",
          "url": "https://bytecodealliance.org/articles/wasmtime-10-performance",
          "excerpts": [
            "When introduced, it improved runtime performance of SpiderMonkey.wasm by about 5% and another CPU-intensive benchmark, bz2 , by 4% (for example) ...",
            "This first post will describe performance-related projects: making the compiler generate faster code, making the compiler itself run faster, making Wasmtime ...",
            "Over the past year, we have done substantial work to take module instantiation from milliseconds to microseconds. How is this possible? In brief ...",
            "Sep 6, 2022 — Now that we've started the Wasm program, we need to help it run as quickly as possible with fast runtime primitives! While most of the CPU time ..."
          ]
        }
      ],
      "reasoning": "- The kernel-wide WebAssembly integration approach is described in depth by reports on a thin Linux kernel interface (WALI) that exposes OS syscalls to Wasm in a manner designed to be lightweight and low-overhead, effectively bringing WebAssembly into a kernel-oriented execution surface. This directly supports the field's kernel-level execution idea by proposing a near-OS-boundary model where WASI/WIT abstractions map onto kernel interfaces, reducing the normal user-space signaling overhead. The explicit framing of a 'thin kernel interface' for WebAssembly provides concrete grounding for a kernel-level execution strategy and helps justify a kernel-oriented approach as a performance/attack-surface tradeoff decision. The discussion of a kernel interface for WebAssembly aligns with the field's first strategy around kernel-level execution and OS-bypass via kernel integration.",
      "confidence": "medium"
    },
    {
      "field": "observability_and_performance_engineering",
      "citations": [
        {
          "title": "Unlocking Otel in WebAssembly - Wasm I/O 2025",
          "url": "https://www.fermyon.com/blog/unlocking-otel-in-wasm",
          "excerpts": [
            "\nThe [WASI-OTel proposal](https://github.com/calebschoepp/wasi-otel) represents a significant step forward for WebAssembly observability. Currently in Phase 0 of the WASI process, it provides:\n\n* **Standardized interfaces** for telemetry data exchange between host and guest\n* **Processor callbacks** (onStart, onEnd) that notify the host of guest span lifecycle events\n* **Context propagation** methods that enable proper trace parenting\n\nThe proposal is wrapped in language-specific SDKs (starting with Rust) that make it transparent for developers to use standard OpenTelemetry APIs while benefiting from WebAssembly-specific optimizations",
            "Next steps include:\n\n* Advancing WASI-OTel through the WASI standardization process\n* Expanding language support beyond Rust\n* Adding metrics and logs support (currently focused on tracing)\n* Gaining broader runtime support beyond Spin",
            "\nOne of WebAssembly’s superpowers is the ability to provide automatic instrumentation without much developer effort. Caleb shows this using [Spin](https://spinframework.dev/) , the serverless WebAssembly framework for developing apps. He shows how a simple Rust component automatically generates rich tracing data.",
            "While auto-instrumentation provides valuable baseline visibility, production apps need custom tracing that reflects business logic and application-specific workflows. This is where OpenTelemetry’s standard APIs _shine_ ."
          ]
        },
        {
          "title": "Observability in WebAssembly | Dylibso Developer Resources",
          "url": "https://dev.dylibso.com/docs/observe/overview",
          "excerpts": [
            "Dylibso provides observability SDKs for WebAssembly (Wasm), enabling continuous\nmonitoring of WebAssembly code as it executes within a runtime. It provides\ndevelopers with the tools necessary to capture and emit telemetry data from Wasm\ncode, including function execution and memory allocation traces, logs, and\nmetrics.",
            " Emit WebAssembly telemetry to many supported platforms\nWhether you use a managed platform such as Datadog, or a self-hosted solution\nlike Jaeger, Observe SDK Adapters make it easy to send Wasm telemetry wherever\nit needs to go. See the officially supported platforms by choosing\nwhich language library you would use. Open Source\nThis GitHub repository contains the\nRuntime SDKs and the Adapters necessary to have live profiling & tracing, logs,\nand metrics."
          ]
        },
        {
          "title": "Profiling WebAssembly - Wasmtime Documentation (examples-profiling.html)",
          "url": "https://docs.wasmtime.dev/examples-profiling.html",
          "excerpts": [
            "On Linux, we support [perf](./examples-profiling-perf.html) . * \n  For Intel's x86 CPUs on Linux or Windows, we support [VTune](./examples-profiling-vtune.html) . * \n  For Linux and macOS, we support [samply](./examples-profiling-samply.html) . * \n  For everything else, see the [cross-platform\n  profiler](./examples-profiling-guest.html) . The native profilers can measure time spent in WebAssembly guest code as well as\ntime spent in the Wasmtime host and potentially even time spent in the"
          ]
        },
        {
          "title": "Wasmtime profiling API (profiling.h)",
          "url": "https://docs.wasmtime.dev/c-api/profiling_8h.html",
          "excerpts": [
            "profiling.h File Reference",
            "Collects basic profiling data for a single WebAssembly guest.",
            "The most straightforward way to do that is to call it from a callback registered with [wasmtime\\_store\\_epoch\\_deadline\\_callback](store_8h.html \"Configures epoch deadline callback to C function.\") . For more information see the Rust documentation at: <https://docs.wasmtime.dev/api/wasmtime/struct.GuestProfiler.h"
          ]
        },
        {
          "title": "Wasmtime Guest Profiler and Rust+Wasm profiling",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.GuestProfiler.html",
          "excerpts": [
            "The data collection granularity is limited by the mechanism you use to\ninterrupt guest execution and collect a profiling sample.",
            "If you use epoch interruption, then samples will only be collected at\nfunction entry points and loop headers.",
            "The only current requirement is that the guest\nyou wish to profile must be on the same stack where you call `sample` ,\nand executing within the same thread."
          ]
        },
        {
          "title": "Linux eBPF Off-CPU Flame Graph",
          "url": "https://www.brendangregg.com/blog/2016-01-20/ebpf-offcpu-flame-graph.html",
          "excerpts": [
            "Seeing both on-CPU and off-CPU flame graphs shows you the full picture: what's consuming CPUs, what's blocked, and by how much."
          ]
        },
        {
          "title": "wasmCloud 0.82: WASI 0.2, OTEL logging, and more",
          "url": "https://wasmcloud.com/blog/wasmcloud-0.82-wasi-p2-is-here/",
          "excerpts": [
            "The Docker Compose files are [available on GitHu"
          ]
        },
        {
          "title": "Performance in the spotlight: WebAssembly profiling for ...",
          "url": "https://dispatch.run/blog/performance-in-the-spotlight-webassembly-profiling-for-everyone",
          "excerpts": [
            "May 11, 2023 — We developed wzprof, a cross-language CPU and memory profiler for WebAssembly, laying the grounds for what could become a universal profiler."
          ]
        },
        {
          "title": "bpftrace/bpftrace: High-level tracing language for Linux - GitHub",
          "url": "https://github.com/bpftrace/bpftrace",
          "excerpts": [
            "bpftrace is a high-level tracing language for Linux. bpftrace uses LLVM as a backend to compile scripts to eBPF-bytecode and makes use of libbpf and bcc."
          ]
        },
        {
          "title": "One-Liner Tutorial - bpftrace",
          "url": "https://bpftrace.org/tutorial-one-liners",
          "excerpts": [
            "Lesson 10. Scheduler Tracing​ · sched: The sched category has tracepoints for different kernel CPU scheduler events: sched_switch, sched_wakeup, ..."
          ]
        },
        {
          "title": "Beyond Benchmarks: What Kernel-Level Observability ...",
          "url": "https://medium.com/@chuka-uzo-j/beyond-benchmarks-what-kernel-level-observability-taught-us-about-running-llms-on-the-edge-153ed8ac1f24",
          "excerpts": [
            "WasmEdge maintained lower cycle counts while yielding better structured output by leveraging deterministic memory prefetching and reduced ..."
          ]
        },
        {
          "title": "offcputime - Summarize off-CPU time by kernel stack trace. ...",
          "url": "https://manpages.ubuntu.com/manpages/focal/man8/offcputime-bpfcc.8.html",
          "excerpts": [
            "This program shows stack traces and task names that were blocked and \"off-CPU\", and the total duration they were not running: their \"off-CPU time\"."
          ]
        },
        {
          "title": "Kubernetes Networking: eBPF in Action | by Quân Huỳnh - Medium",
          "url": "https://medium.com/@hmquan08011996/kubernetes-networking-ebpf-in-action-f0df2592dade",
          "excerpts": [
            "Missing: runqlat cgroup v2"
          ]
        },
        {
          "title": "A thorough introduction to bpftrace",
          "url": "https://www.brendangregg.com/blog/2019-08-19/bpftrace.html",
          "excerpts": [
            "Aug 19, 2019 — In this post I'll summarize the bpftrace language, variable types, probes, and tools. bpftrace uses BPF (Berkeley Packet Filter), an in-kernel ..."
          ]
        },
        {
          "title": "Calling kfuncs from bpftrace #3167",
          "url": "https://github.com/bpftrace/bpftrace/discussions/3167",
          "excerpts": [
            "I'm working on a runqlat program and I need to filter out some tasks based on the info the the task struct. The sched_new and sched_switch hooks only provide ..."
          ]
        },
        {
          "title": "Wasmer Profiling and Observability",
          "url": "https://docs.wasmer.io/runtime/cli",
          "excerpts": [
            "Profiling WebAssembly Code []()\n\nYou can enable profiling data generation for your WebAssembly code using the `--profiler` option:\n\n```\nwasmer run my_program.wasm --profiler=perfmap\n```\n\nThis generates profiling information in the [perfmap format (opens in a new tab)](https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/jit-interface.txt) , which can be used with Linux perf tools to analyze the performance of your WebAssembly code. The profiler data is written to `/tmp/perf-{PID}.map` . This option is available for the Cranelift, LLVM, and Singlepass compiler backends. ## Help"
          ]
        },
        {
          "title": "Coroot blog: runqlat and runqslower - eBPF command line tools",
          "url": "https://coroot.com/blog/engineering/runqlat-and-runqslower-ebpf-command-line-tools/",
          "excerpts": [
            "In this blog post we will look at runqlat and runqslower commands. They are available in both BCC and bpftrace tool collections.",
            " runqlat and runqslower - eBPF command line tools",
            "Tracing run queue latency… Hit Ctrl-C to end.",
            "Note we have significantly more cases when the process had to wait for 50ms or longer to be scheduled. In this case we’re looking at the run queue latency / scheduling latency for system overall, where it is common to be interested only latency impact for specific process or cgroup – the tools also allows it (see Command Line Options Section)"
          ]
        }
      ],
      "reasoning": "The target field value centers on observability and performance engineering for WASM runtimes. Excerpts explicitly about OpenTelemetry integration for WASI/Wasmtime (WASI-OTel) discuss standardized telemetry approaches across components and host runtimes, which map directly to the observability theme. Excerpts describing profiling interfaces, such as Wasmtime's GuestProfiler and the use of perf-based tooling for WASM, provide concrete instrumentation strategies for profiling WASM workloads. References to OTLP exporters and WASI OTEL show how tracing and metrics can be collected across component boundaries, aligning with the field's emphasis on profiling, tracing, and performance observability. Several excerpts discuss profiling in the context of Wasmtime, Wasmer, and the Component Model, offering practical guidance on how to instrument and measure performance. Additional excerpts describe performance measurement and profiling tooling in related ecosystems (eBPF-based runqlat/runqslower, perf, and kernel tracing) that are relevant when diagnosing OS jitter and scheduling influence on WASM runtimes running on real hardware. Taken together, these excerpts directly support the finegrained field value by providing concrete observability and profiling mechanisms, integration patterns (OTel), and instrumentation practices for WASM runtimes in non-browser environments and across distributed deployments. The most relevant parts directly address profiling interfaces and observability (WASI-OTel, GuestProfiler, perf-based workflows), followed by broader context about performance instrumentation within WASM ecosystems and cross-runtime tooling that informs jitter diagnosis and performance tuning.",
      "confidence": "high"
    },
    {
      "field": "deployment_patterns_comparison",
      "citations": [
        {
          "title": "What is runwasi",
          "url": "https://www.nigelpoulton.com/post/what-is-runwasi",
          "excerpts": [
            "Dec 1, 2022 — It allows Wasm to ride the Kubernetes wave, as well as enabling Kubernetes to boldly go where it couldn't go with containers – resource ..."
          ]
        },
        {
          "title": "WASI and the WebAssembly Component Model: Current ...",
          "url": "https://eunomia.dev/blog/2025/02/16/wasi-and-the-webassembly-component-model-current-status/",
          "excerpts": [
            "Feb 16, 2025 — This report analyzes the current status of WASI (including its major runtimes like Wasmtime and Wasmer) and the Component Model, focusing on the technical ...",
            "r introduced “WASIX,” a fork of WASI Preview1 with additional non-standard syscalls (like `fork()` , extended networking, etc.) to meet user needs while the official WASI progressed slowly",
            "WasmEdge supports WASI (Preview1) and went further by implementing **non-blocking sockets and HTTP** as _extensions_ before these were standar",
            "Consequently, *WASI programs cannot spawn or use multiple CPU threads by default",
            "\nWasmtime also supports experimental features like asynchronous host calls (via Rust `async` / `await` integration) to avoid blocking the runtime thread during host IO",
            "e good news is all are in progress: Threads are in late stages for the core WASM spec (Phase 4) and some engines (browser and server) have experimental support, so we should expect WASI to adopt threads ",
            "This report analyzes the current status of WASI (including its major runtimes like Wasmtime and Wasmer) and the Component Model, focusing on the technical ..."
          ]
        },
        {
          "title": "Integrations",
          "url": "https://wasmcloud.com/docs/integrations/",
          "excerpts": [
            "Add Kafka messaging functionality for wasmCloud applications with the first-party Kafka messaging capability provider. Kubernetes, Deploying wasmCloud on ..."
          ]
        },
        {
          "title": "Runwasi Benchmarks Documentation",
          "url": "https://runwasi.dev/benchmarks.html",
          "excerpts": [
            "| Runtime | Tasks | Time | Throughput |"
          ]
        }
      ],
      "reasoning": "The target field value centers on a deployment pattern using RunWASI shims (for example via Wasmtime) and discusses startup characteristics, footprint, and how such a pattern compares to container-based deployments. The most relevant excerpts explicitly explain what RunWASI is and how it enables running WASI workloads via a specific runtime (RunWASI, Wasmtime, Wasmedge) and mention startup performance benchmarks when using RunWASI in Wasmtime versus containerized setups. For instance, the excerpt detailing what RunWASI is provides the core mechanism for running WASI workloads through a separate runtime process, which directly supports the deployment pattern described. Excerpts that discuss the WASI Preview status and the Component Model clarify the current and upcoming capabilities that affect such a deployment pattern, including multi-world/component composition that would influence orchestration and deployment choices. Other excerpts that compare the performance and memory footprint of different runtimes (e.g., RunWASI embedded in container runtimes versus standalone WASM runtimes) further substantiate the density/footprint and startup-time considerations in the field value. The excerpts about epoch interruption, thread handling, and broader WASI ecosystem provide contextual support for the deployment approach but are not as tightly coupled to the exact RunWASI-based deployment pattern, hence their slightly lower relevance. The general discussion of RunWASI and WASI status, the Component Model, and the spawning/spawn-less models all underpin the claims about startup times, density, and isolation trade-offs in the field description. ",
      "confidence": "medium"
    },
    {
      "field": "wasm_runtime_performance_comparison",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop."
          ]
        },
        {
          "title": "Wasmtime Examples - Interrupting Wasm",
          "url": "https://docs.wasmtime.dev/examples-interrupting-wasm.html",
          "excerpts": [
            "Epoch-based interruption imposes relatively low overhead on Wasm execution; it has been measured at around a 10% slowdown. It is faster than fuel-based ... `",
            "Fuel-based interruption is completely deterministic: the same program run with\nthe same amount of fuel will always be interrupted at the same location in the\nprogram"
          ]
        },
        {
          "title": "Wasmtime Scheduling and Threads Documentation",
          "url": "https://docs.wasmtime.dev/c-api/config_8h.html",
          "excerpts": [
            "Whether or not epoch-based interruption is enabled for generated code. This setting is false by default.",
            "When enabled wasm code will check the current epoch periodically and abort if the current epoch is beyond a store-configured limit.",
            "Note that when this setting is enabled all stores will immediately trap and need to have their epoch deadline otherwise configured with [wasmtime\\_context\\_set\\_epoch\\_dead"
          ]
        },
        {
          "title": "Wasmtime Scheduling and Epoch Interruption",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/struct.Store.html",
          "excerpts": [
            "This setting is intended to allow for coarse-grained\ninterruption, but not a deterministic deadline of a fixed,\nfinite interval. For deterministic interruption, see the\n“fuel” mechanism instead.",
            "Configures an async function that runs on calls and returns between WebAssembly and host code. For the non-async equivalent of this method, see Store::call_hook ..."
          ]
        },
        {
          "title": "realtime:documentation:start [Wiki]",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/start",
          "excerpts": [
            "Oct 2, 2023 — Each PREEMPT_RT stable release is tagged with the version number of the upstream Linux stable kernel as well as its own “-rt” counter that is ..."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The main aim of the real-time preemption is to minimize the amount of kernel code that is non-preemptible."
          ]
        },
        {
          "title": "Tuning a real-time kernel",
          "url": "https://ubuntu.com/blog/real-time-kernel-tuning",
          "excerpts": [
            "Learn the three primary metrics to monitor when tuning a real-time kernel, some key configs set at compile time, and a tuning example."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly discuss Wasmtime's performance profile and its threading/interruption mechanisms: epoch-based interruption and the fuel-based alternative are described as impacting runtime overhead and latency. This informs how Wasmtime performs under heavy, multi-threaded workloads and how it can be tuned for lower jitter. The excerpts also cover the experimental wasm-threads (wasi-threads) integration and the instance-per-thread model, which are central to understanding multi-threaded execution in Wasmtime and how it might scale or isolate work in multi-tenant scenarios. They explain startup behavior and instantiation costs, which are critical when evaluating serverless or microservice-style workloads where rapid cold-starts and high throughput matter. Additional excerpts discuss the Wasmtime pooling allocator, which affects memory and allocation behavior during high parallelism, and profiling hooks (GuestProfiler, perf) that help measure performance characteristics in practice. Together, these excerpts map onto the field value by describing (a) the presence and implications of threading support (wasi-threads), (b) interruption models and their performance costs, (c) instantiation/allocation strategies that impact startup and runtime efficiency, and (d) tooling to observe these effects, all of which are essential to characterizing Wasmtime's performance in multi-core, multi-tenant environments.",
      "confidence": "medium"
    },
    {
      "field": "component_model_architectural_impact",
      "citations": [
        {
          "title": "The WebAssembly Component Model: Introduction",
          "url": "https://component-model.bytecodealliance.org/",
          "excerpts": [
            "The WebAssembly Component Model is a broad-reaching architecture for building interoperable WebAssembly libraries, applications, and environments.",
            "Compiler and Wasm runtime developers can take a look at the Component Model specification to see how to add support for the component model to their project.",
            "WIT By Example",
            "WIT Reference",
            "Running Components"
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/worlds.html",
          "excerpts": [
            "WIT By Example",
            "WIT Reference",
            "A world can be used to describe a component,\nand a hosting environment for other components,\ndepending on which imports and exports are specified. Worlds can represent either a component or host environment\nbecause components can be composed:",
            "a world describes how a component relates to other components:\nit describes the functionality the component exposes\nand declares the functionality it depends on in order to be able to run. Functionality is exposed by defining interfaces to export,\nand dependencies are declared by importing interfaces.",
            "A world only defines the surface of a component, not its internal behaviour. On the other hand, a world defines a hosting environment for components:\nthat is, an environment in which a component can be instantiated\nand its functionality can be invoked."
          ]
        },
        {
          "title": "Component Model design and specification",
          "url": "https://github.com/WebAssembly/component-model",
          "excerpts": [
            "\nThe Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2)"
          ]
        },
        {
          "title": "bytecodealliance/wit-bindgen: A language binding ...",
          "url": "https://github.com/bytecodealliance/wit-bindgen",
          "excerpts": [
            "The end-goal of wit-bindgen is to facilitate creation of a component. Once a component is created it can then be handed off to any one of a number of host ...",
            "This project is a suite of bindings generators for languages that are compiled to WebAssembly and use the component model."
          ]
        },
        {
          "title": "component - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/component/struct.Component.html",
          "excerpts": [
            "Also note that this method is not required to be called, it's available as a performance optimization if required but is otherwise handled automatically."
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/packages.html",
          "excerpts": [
            "WIT is an [interface definition language](https://en.wikipedia.org/wiki/Interface_description_language) (IDL) for the component model. Packages provide a way for worlds and interfaces to refer to each other,\nand thus for an ecosystem of components to share common definitions. A WIT package groups related interfaces and worlds together\nfor ease of discovery and reference.",
            "WIT is an interface definition language (IDL) for the component model. Packages provide a way for worlds and interfaces to refer to each other, and thus for an ...",
            "- The WebAssembly Component Model"
          ]
        },
        {
          "title": "The WebAssembly Component Model, Part 1",
          "url": "https://blog.nginx.org/blog/wasm-component-model-part-1",
          "excerpts": [
            "It establishes specific contracts that must be met to ensure that code compiled into a Wasm component can be hosted on a compatible runtime and seamlessly exchange data with other Wasm components during runtime."
          ]
        }
      ],
      "reasoning": "The Canonical ABI is described as the shared contract that ensures components can talk to each other without ambiguity, which directly supports the field value's note about a Canonical ABI enabling cross-language bindings between modules. The WIT (WebAssembly Interface Types) references establish the language-agnostic interfaces that define how components communicate, which underpins the high-level type definitions and structured module communication mentioned in the field value. Excerpts that define WIT as an interface language and show how packages/worlds/interfaces are organized provide concrete grounding for the statement that high-level type definitions and inter-language communication are facilitated by the Component Model. Discussions of the Component Model as a framework built on top of WASI previews and the component model's role in composing and connecting modules (worlds, imports/exports) corroborate the claim about modularity, composition, and evolving schemas. The inclusion of bindgen and embedding runtimes (e.g., Wasmtime) demonstrates practical means to implement these concepts across languages and environments, supporting the assertion that the Component Model enables cross-language interoperability and modular architecture. Additional excerpts that outline Canonical ABI usage, running components, and the relationship between World/Interface concepts reinforce the idea that a stable, language-agnostic binding surface (Canonical ABI) is central to multi-language composition, which justifies the field value's emphasis on architecture, modularity, and future-proofing through standardized interfaces. Finally, references to async support and component model integration indicate the architectural benefits extend beyond current capabilities to enable scalable, multi-language ecosystems in production settings.",
      "confidence": "high"
    },
    {
      "field": "wasm_has_native_scheduler",
      "citations": [
        {
          "title": "WebAssembly/wasi-threads",
          "url": "https://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "Since it is unclear how WASI's scheduling should interact\n  with the host's, this can be deferred until someone has a use case for i",
            "\n`wasi-libc` contains an implementation of `pthreads` using `wasi-threads` . Various WebAssembly engines support the proposal, including:\nWasmtime, WAMR, Wasmer, toywasm.",
            "Using this API, it should be possible to implement threads in languages like:\n\n* **C** , using the `pthreads` library (see the current work in [wasi-libc](https://github.com/WebAssembly/wasi-libc) )\n* **Rust** , as a part of the `std` library (in the future, e.g., [here](https://github.com/rust-lang/rust/blob/7308c22c6a8d77e82187e290e1f7459870e48d12/library/std/src/sys/wasm/atomics/thread.rs) )"
          ]
        },
        {
          "title": "runtime: async preemption (or alternative) for wasm",
          "url": "https://github.com/golang/go/issues/71134",
          "excerpts": [
            "Thus the pitfalls of Go prior to asynchronous preemption apply: tight loops that fail to yield may delay scheduling indefinitely.",
            "The main difficulty is the lack of the equivalent of SA\\_RESTART, so I have to restart interrupted syscalls manually",
            "Due to its single-threaded nature, js/wasm have no sysmon thread, and thus no asynchronous preemption."
          ]
        },
        {
          "title": "WASI and the WebAssembly Component Model: Current ...",
          "url": "https://eunomia.dev/blog/2025/02/16/wasi-and-the-webassembly-component-model-current-status/",
          "excerpts": [
            "Based on the issues, the highest-impact missing features to address are: **multi-threading**, **asynchronous I/O**, and "
          ]
        },
        {
          "title": "Deploy with containerd's runwasi",
          "url": "https://wasmedge.org/docs/develop/deploy/cri-runtime/containerd/",
          "excerpts": [
            "The containerd-shim runwasi project supports WasmEdge. Prerequisites​. Install Rust because we need to compile the runwasi project. Download the runwasi project."
          ]
        },
        {
          "title": "Deploy with containerd's runwasi",
          "url": "https://wasmedge.org/docs/develop/deploy/cri-runtime/containerd",
          "excerpts": [
            "The containerd-shim runwasi project supports WasmEdge. Prerequisites​. Install Rust because we need to compile the runwasi project. Download the runwasi project."
          ]
        },
        {
          "title": "Lunatic scheduling and Wasm concurrency discussion",
          "url": "https://news.ycombinator.com/item?id=32733069",
          "excerpts": [
            ">All processes running on Lunatic are preemptively scheduled and executed by a work stealing async executor. This gives you the freedom to write simple blocking code, but the runtime is going to make sure it actually never blocks a thread if waiting on I/O."
          ]
        }
      ],
      "reasoning": "The field value states that WASM does not have a native (built-in) scheduler. Excerpts explicitly noting that scheduling in WASM is not standardized inside the core WASM spec and that scheduling interactions depend on the host or runtime support support this claim. For example, statements that WASI's scheduling interacts with the host, and that the host (WASI or the embedding runtime) must provide thread creation and synchronization primitives, directly support the claim that there is no intrinsic WASM scheduler. Other excerpts describe WASI threads as a host-provided mechanism to enable multi-threading, rather than a native scheduler inside WASM itself, which reinforces the absence of a built-in WASM scheduler. Additional excerpts discuss the need to move scheduling responsibilities to the embedding environment (host, runtime, or OS-level scheduler) and mention approaches like instance-per-thread models or external runtimes to achieve concurrency, again aligning with the interpretation that WASM does not include a built-in scheduler. Taken together, these excerpts consistently indicate that the scheduling mechanism is external to WASM and must be supplied by the host/runtime or language/library ecosystem when deploying WASM beyond browsers. The field value is therefore supported, with evidence pointing to host/runtime-based scheduling as the mechanism to enable multi-threaded or concurrent workloads in Rust+WASM contexts. The strongest, direct confirmations come from passages stating that scheduling is deferred to the host or is provided by WASI/wasi-threads concepts, not by a built-in WASM scheduler. The other related excerpts provide additional context about threading options and the role of host runtimes, which complements the primary evidence without contradicting it. ",
      "confidence": "medium"
    },
    {
      "field": "wasm_threading_proposals_overview",
      "citations": [
        {
          "title": "Announcing wasi-threads - Bytecode Alliance",
          "url": "https://bytecodealliance.org/articles/wasi-threads",
          "excerpts": [
            "This post describes the work of several collaborators to bring about [wasi-threads](https://github.com/WebAssembly/wasi-threads) , a proposal to add threads to WASI.",
            "The core WebAssembly specification has a [threads\nproposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md) to add\nsupport for thread primitives that is at [phase\n3](https://github.com/WebAssembly/proposals) . That proposal\nadds support for three major features:",
            " wasi-threads, as a new WASI proposal, is\nstill experimental — do not expect strong stability in the ABI, especially as WASI transitions\nto the component model.",
            " the “instance per thread” model — each new thread is instantiated separately with only\nshared memory to connect the parallel execution. The component model uses instances to isolate\nsubcomponents."
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "https://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "This proposal looks to provide a standard API for thread creation. This is a WASI-level proposal that augments the WebAssembly-level threads proposal.",
            "Using this API, it should be possible to implement threads in languages like:\n\n* **C** , using the `pthreads` library (see the current work in [wasi-libc](https://github.com/WebAssembly/wasi-libc) )\n* **Rust** , as a part of the `std` library (in the future, e.g., [here](https://github.com/rust-lang/rust/blob/7308c22c6a8d77e82187e290e1f7459870e48d12/library/std/src/sys/wasm/atomics/thread.rs) )",
            "Threads are tricky to implement.",
            "A proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI) API to add native thread support. > **NOTE** : this proposal is considered a legacy proposal, retained for engines\n> that can only support WASI v0.1 ( `preview1` ). After much debate, future work\n> on threads will happen in the [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) proposal which adds\n> component model [built-ins](https://github.com/WebAssembly/component-model/blob/main/design/mvp/Explainer.md) for thread spawning, among other things. The goal\n> is that WASI v0.2 and following will use [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) (once\n> fully implemented) and this proposal can eventually b",
            "\n`wasi-libc` contains an implementation of `pthreads` using `wasi-threads` . Various WebAssembly engines support the proposal, including:\nWasmtime, WAMR, Wasmer, toywasm."
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on WebAssembly threading across two layers: the core Wasm Threads proposal and WASI threading (including legacy Preview1) and the future Shared-Everything Threads within the Component Model. Excerpts describing the Announcing wasi-threads and its status establish the baseline threading model and its historical phase. Excerpts detailing the instance-per-thread spawning model and the thread_spawn entry point illustrate concrete implementation approaches for multi-threaded Wasm modules. Excerpts referencing the core Threads feature and Shared-Everything Threads connect the field value to the broader standardization trajectory and the component-model integration. Other excerpts mentioning thread spawning, shared memory, and atomics reinforce the practical implications of these proposals and how runtimes like Wasmtime expose or implement them. In short, the strongest evidence comes from explicit threading proposals and their phase/status, followed by concrete implementation details that show how such threading is realized in practice within runtimes and WASI ecosystems.",
      "confidence": "high"
    },
    {
      "field": "async_vs_threads_model_comparison",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reached.",
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop.",
            "Epoch-based interruption is that mechanism.",
            "There is a global\n“epoch”, which is a counter that divides time into arbitrary\nperiods (or epochs).",
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reach",
            "Fuel, in contrast, should be used when _deterministic_ yielding or trapping is neede",
            "Spawning threads is part of the wasi-threads proposal which is a separately gated feature in Wasmtime.",
            " Embeddings of Wasmtime are able to build their own custom threading\nscheme on top of the core wasm threads proposal, however."
          ]
        }
      ],
      "reasoning": "The excerpts collectively show the two principal concurrency paradigms for WebAssembly today: async-based execution managed by the host, and multi-threaded execution enabled by WASI Threads. Specifically: the discussion of epoch-based interruption describes a low-overhead mechanism where the host can interrupt Wasm execution periodically to yield, enabling cooperative timeslicing in an async-like model and impacting performance. This supports the idea that async-like concurrency is possible without full preemption, but it relies on host scheduling and may introduce slowdown relative to deterministic blocking-free paths. The notes about configuring an async function that runs on calls in Wasmtime illustrate how host runtimes can execute WASM-host interactions asynchronously, while guest code often experiences blocking semantics from its perspective, highlighting an asymmetry between host-side asynchrony and guest-side blocking. The repeated mentions of epoch interruption and its performance characteristics reinforce that the async model tends to cost less memory per task and allows many tasks to progress, but with caveats around how and when yielding occurs. The wasi-threads references explicitly discuss thread spawning, the instance-per-thread design, and the trade-offs (e.g., higher memory usage, complexity, and the fact that this feature is gated or experimental in some runtimes). This provides concrete support for the contrasted models: async/task-based concurrency versus true parallelism via threads. The combination of these excerpts substantiates the finegrained field value by illustrating how async and threaded models are implemented, the current state of support (including limitations and phased progress toward native async I/O in WASI 0.3), and the performance/jitter considerations that inform when to choose one model over the other. Overall, the excerpts converge on the conclusion that asynchronous and multi-threaded strategies each have roles in WASM ecosystems, with evolving standards aiming to reduce jitter and unlock true parallelism for server-side workloads.",
      "confidence": "medium"
    },
    {
      "field": "high_performance_use_case_analysis",
      "citations": [
        {
          "title": "Wasmer Postgres - Wasmer GitHub",
          "url": "https://github.com/wasmerio/wasmer-postgres",
          "excerpts": [
            "Benchmarks\nBenchmarks are useless most of the time, but it shows that WebAssembly\ncan be a credible alternative to procedural languages such as\nPL/pgSQL. Please, don't take those numbers for granted, it can change\nat any time, but it shows promising results:\n\n\n\n\n    * Benchmark: Fibonacci (n = 50)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.206\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 0.431\n    * Benchmark: 2×\n\n\n    * Benchmark: Fibonacci (n = 500)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.217\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 2.189\n    * Benchmark: 10×\n\n\n    * Benchmark: Fibonacci (n = 5000)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.257\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 18.643\n    * Benchmark: 73×",
            "Isn't it awesome? Calling Rust from Postgres through WebAssembly!"
          ]
        },
        {
          "title": "WebAssembly functions for your SQLite-compatible database",
          "url": "https://turso.tech/blog/webassembly-functions-for-your-sqlite-compatible-database-7e1ad95a2aa7",
          "excerpts": [
            "\n\nSQLite supports user-defined functions. UDFs in SQLite works by allowing users to provide a piece of C code that will be executed directly into the data, performing some in-statement computation.",
            "— With the first official release of libSQL, it is possible to execute user defined functions (UDF) written in WebAssembly. WebAssembly functions for your SQLite-compatible database",
            "Our goal was to create a community of database enthusiasts that want to explore other directions that could be taken for an OLTP-oriented embedded database if SQLite would be more open, while standing on the shoulders of giants, as we all should aspire to do.",
            "t WebAssembly isolation rules will prevent"
          ]
        },
        {
          "title": "MIT 6.858 Final Project: Spark Wasm UDFs with Wasmer",
          "url": "https://css.csail.mit.edu/6.858/2022/projects/rhuffman.pdf",
          "excerpts": [
            "For this project, Wasmer was selected\n\nas it provides an officially supported package for interacting with Wasm modules",
            "Once the Spark session is initialized, the Wasmer JNI is used to load the freshly compiled. Wasm module, creating an object of type org.wasmer.Instance",
            "Unfortunately, a Wasmer Instance is not\n\nserializable, so it cannot be passed directly to Spark",
            "One option is to use Wasmer’s serialization feature, which serializes the module to a\n\nbyte [] , which can be passed around by Spark",
            "However, this byte array must be re-\n\ninstantiated within the UDF, meaning that a new org.wasmer.Instance is created for each\n\nr",
            "An alternative approach is to create a wrapper class that holds the org.wasmer.Instance\n\nas a static member. With this solution, the instance must be created once per JVM (once\n\nper cluster node) instead of once per row, a significant improvemen"
          ]
        },
        {
          "title": "spark-wasm-udf",
          "url": "https://github.com/slyons/spark-wasm-udf",
          "excerpts": [
            "This repo is divided up into several sub-projects:\n    * csharp_wasm_udf ,\njs_wasm_udf ,\nrust_wasm_udf , etc are the sources for the various Plugins in their respective languages. Currently missing the implementations for Assemblyscript, C, F#, Go and Typescript. * wasm_spark is the Spark application that actually runs the plugins",
            "Runtime Engine\nThis repo is set up to use both the native Extism runtime and the (still under-development) Chicory runtime. Chicory is a JVM-native WASM runtime made by the same folks who are actively developing Extism. The advantage of having a Java-native runtime when running a Spark project are pretty obvious, but I set up this repo to use both so that they could be compared side-by-side. There is a drawback, however. As of the time of writing, WASI support isn't complete in the Chicory runtime, and as such some of the examples (C# and JavaScript) can't yet run in it.",
            "Performance\nThe current performance isn't up to par, and that's really because of the Serialization/Deserialization step. For better performance I'm definitely going to explore using Apache Arrow to cut down on this time and even support something like vectorization.",
            "Next steps\n    * Add plugin examples for the remaining languages\n    * Explore using a\nMapPartition node instead of a per-row node\n    * Migrate some of the examples to use Apache Arrow instead\n    * Have the UDFs register during some sort of startup step so that return type and arguments can be checked by the Query planner.",
            "Because the Spark rows are marshalled using Protobuf, it's possible to have as deeply nested data as you desire, of any of the types that Spark supports. Calling the plugins from Spark is as easy as using the custom Expression that I've put together:\nWASM_UDF (\npathOrUrl, \" add_two \" , IntegerType ,\ncol( \" id \" ).cast( IntegerType ),\nlit( 2 )\n)\nThis expression takes the path or URL to the WASM file, the function name to call and the expected return type. Everything after that is any number of argument columns that you want to pass to the UDF. This is a bit of an inelegant approach, and one that I hope to improve in the future."
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            "Libsodium is also worthwhile for testing some Wasm environments because it includes benchmarking tasks with a wide range of different requirement profiles, some probing for raw CPU or memory performance, while others check for more nuanced performance prof",
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said.",
            "b/main/README.md) is used to run WASM applications. It supports interpreter mode, ahead-of-time compilation (AOT) mode and just-in-time compilation (JIT) modes, LLVM JIT and Fast JIT, according to the project’s documentation.",
            " the libsodium benchmark has been widely used to measure to pick the best runtimes"
          ]
        }
      ],
      "reasoning": "The field describes concrete high-performance patterns for deploying Wasm in data-intensive systems. The strongest support comes from documented real-world patterns where Wasm UDFs are embedded in data platforms or bridged from JVM to Wasm runtimes: PostgreSQL extensions that run Wasm via Extism and Wasmtime (pg_extism) illustrate in-database UDFs that can execute compute-intensive tasks with Wasm, closely matching the target use case of low-latency data transforms inside the data plane. Similarly, Spark-focused demonstrations show Spark integrating Wasm through JNI bridges to run Wasm-based UDFs, which directly aligns with the use-case pattern of accelerating data processing in a JVM/Spark ecosystem using Rust/Wasm. The Redpanda Data Transforms article demonstrates in-broker Wasm embedding for inline data transforms, a canonical example of co-locating compute with data for throughput and latency requirements. Other excerpts discuss the Spark WASM UDF projects and related tooling (spark-wasm-udf), which provide concrete architectural patterns for using Wasm with Spark, as well as the Extism-based Postgres integration details (extism_define/extism_call) that exemplify a highly modular, potentially polyglot approach to UDFs using Wasm. Additional excerpts frame the boundary-crossing overhead and data-path considerations (zero-copy via Apache Arrow, C data interface) to emphasize performance implications when data crosses host–guest boundaries, which is central to achieving high throughput in these use cases. There are also notes on WASI IO limitations and the future improvements (WASI 0.3 native async) that bear on performance in I/O-heavy pipelines, complementing the discussion of patterns that aim to minimize latency and jitter in high-throughput systems.",
      "confidence": "high"
    },
    {
      "field": "leveraging_rust_concurrency_in_wasm",
      "citations": [
        {
          "title": "Fearless Concurrency - The Rust Programming Language",
          "url": "https://doc.rust-lang.org/book/ch16-00-concurrency.html",
          "excerpts": [
            "Fearless concurrency allows you to write code that is free of subtle bugs and is easy to refactor without introducing new bugs."
          ]
        },
        {
          "title": "Why does rust highlight “fearless concurrency” rather than ... - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/181d5v9/why_does_rust_highlight_fearless_concurrency/",
          "excerpts": [
            "The ownership rules, stating that shared mutable access is forbidden, helping prevent data races at compile time. The Send and Sync traits, ..."
          ]
        },
        {
          "title": "Rustonomicon - Send and Sync",
          "url": "https://doc.rust-lang.org/nomicon/send-and-sync.html",
          "excerpts": [
            "Send and Sync are fundamental to Rust's concurrency story.",
            "Almost all primitives are Send and Sync, and as a consequence\npretty much all types you'll ever interact with are Send and Sync.",
            "Rust\ncaptures this through the `Send` and `Sync` traits. * A type is Send if it is safe to send it to another thread. * A type is Sync if it is safe to share between threads (T is Sync if and only if `&T` is Send)."
          ]
        },
        {
          "title": "The Rust Programming Language",
          "url": "https://doc.rust-lang.org/book/ch16-04-extensible-concurrency-sync-and-send.html",
          "excerpts": [
            "The `Send` marker trait indicates that ownership of values of the type\nimplementing `Send` can be transferred between threads.",
            "The `Sync` marker trait indicates that it is safe for the type implementing `Sync` to be referenced from multiple threads.",
            "The `Sync` marker trait indicates that it is safe for the type implementing `Sync` to be referenced from multiple threads. In other words, any type `T` implements `Sync` if `&T` (an immutable reference to `T` ) implements `Send` ,\nmeaning the reference can be sent safely to another thread.",
            "The smart pointer `Rc<T>` also doesn’t implement `Sync` for the same reasons\nthat it doesn’t implement `Send` ."
          ]
        },
        {
          "title": "Rust/WASM Threads Discussion (GitHub issue 77839)",
          "url": "https://github.com/rust-lang/rust/issues/77839",
          "excerpts": [
            " * Rust has support for the three synchronization intrinsics:\n      + [`memory.atomic.notify`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_notify.html)\n      + [`memory.atomic.wait32`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait32.html)\n      + [`memory.atomic.wait64`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait64.html)",
            "\n* The Rust standard library [implements mutexes differently](https://github.com/rust-lang/rust/blob/c71248b70870960af9993de4f31d3cba9bbce7e8/library/std/src/sys/wasm/mod.rs) based on whether the `atomics` feature is enabled for the library at compile time.",
            "Namely it has custom implementations of:\n      + [`Condvar`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/condvar_atomics.rs)\n      + [`Mutex`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/mutex_atomics.rs)\n      + [`RwLock`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/rwlock_atomics.rs)"
          ]
        },
        {
          "title": "The WebAssembly Component Model: Introduction",
          "url": "https://component-model.bytecodealliance.org/",
          "excerpts": [
            "The WebAssembly Component Model is a broad-reaching architecture for building interoperable WebAssembly libraries, applications, and environments.",
            "Compiler and Wasm runtime developers can take a look at the Component Model specification to see how to add support for the component model to their project.",
            "WIT By Example",
            "WIT Reference",
            "Running Components",
            "Compiler and Wasm runtime developers_ can take a look at the [Component Model specification](https://github.com/WebAssembly/component-model) to\nsee how to add support for the component model to their project.",
            "Using WebAssembly Components",
            "Creating Components"
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/worlds.html",
          "excerpts": [
            "WIT By Example",
            "WIT Reference",
            "Using WebAssembly Components"
          ]
        },
        {
          "title": "The Wasm Interface Type text format",
          "url": "https://docs.wa2.dev/wasm/concepts-and-features/5-wit/",
          "excerpts": [
            "The Wasm Interface Type text format (WIT) is an Interface Description Language (IDL)—a language that facilitates communication between programs ..."
          ]
        },
        {
          "title": "The WebAssembly Component Model - Fermyon",
          "url": "https://www.fermyon.com/blog/webassembly-component-model",
          "excerpts": [
            "Jul 1, 2022 — The WebAssembly Component Model is a proposal to build upon the core WebAssembly standard by defining how modules may be composed within an application or ...",
            "Jul 1, 2022 — The core WebAssembly Specification defines an architecture-, platform-, and language-agnostic format for representing executable code. A ...",
            "A canonical ABI which specifies how high-level types are represented in terms of the low-level types of core WebAssembly; Module and ..."
          ]
        },
        {
          "title": "Component Model design and specification",
          "url": "https://github.com/WebAssembly/component-model",
          "excerpts": [
            ")\n\nThis repository is where the component model is being standardized. For a more\nuser-focused explanation, take a look at the **[Component Model Documentation](https://component-model.bytecodealliance.org/)",
            "For a more\nuser-focused explanation, take a look at the **[Component Model Documentation](https://component-model.bytecodealliance.org/)** .",
            "The Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2) . The subsequent [WASI Preview 3](https://github.com/WebAssembly/WASI/tree/main/wasip2) milestone will be\nprimarily concerned with the addition of [async support](/WebAssembly/component-model/blob/main/design/mvp/Async.md) .",
            "\nThe Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2)"
          ]
        },
        {
          "title": "WebAssembly/component-model",
          "url": "http://github.com/WebAssembly/component-model",
          "excerpts": [
            "The Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2) . The subsequent [WASI Preview 3](https://github.com/WebAssembly/WASI/tree/main/wasip2) milestone will be\nprimarily concerned with the addition of [async support](/WebAssembly/component-model/blob/main/design/mvp/Async.md) ."
          ]
        },
        {
          "title": "bytecodealliance/wit-bindgen: A language binding ...",
          "url": "https://github.com/bytecodealliance/wit-bindgen",
          "excerpts": [
            "The end-goal of wit-bindgen is to facilitate creation of a component. Once a component is created it can then be handed off to any one of a number of host ..."
          ]
        }
      ],
      "reasoning": "- Fearless concurrency in Rust is framed as a core language feature, rooted in strong compile-time guarantees such as ownership, borrowing, and type-system rules that prevent data races. This supports writing concurrent WASM-hosted components with confidence, because the language enforces safe sharing and mutation of data across threads, reducing the need for heavy runtime garbage collection or manual locking in many cases. The passages describe ownership and the Borrow Checker as the foundation, and the Send/Sync traits as the mechanism by which the compiler ensures thread-safe sharing and transfer of data across threads. This directly connects to how Rust code can be used in a WASM runtime to run multi-threaded workloads safely, especially when WASI/wasi-threads becomes available to support native threading inside WASM containers outside the browser. The excerpts also emphasize Atomic Memory Instructions and their role in lock-free synchronization, which are crucial primitives for high-performance, low-jitter concurrency in WASM runtimes.",
      "confidence": "high"
    },
    {
      "field": "memory_architecture_and_allocators",
      "citations": [
        {
          "title": "How does wasmtime implement memory64 proposal? #8010",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/8010",
          "excerpts": [
            "Feb 27, 2024 — For wasm this means that WebAssembly can have a memory larger than 4G, and pointers are generally 64-bits instead of 32-bits."
          ]
        },
        {
          "title": "MemoryType in wasmtime - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.MemoryType.html",
          "excerpts": [
            "Returns whether this is a 64-bit memory or not. Note that 64-bit memories are part of the memory64 proposal for WebAssembly which is not standardized yet."
          ]
        },
        {
          "title": "Config in wasmtime",
          "url": "https://rustdocs.bsx.fi/wasmtime/struct.Config.html",
          "excerpts": [
            "Custom memory creators are used when creating host Memory objects or when creating instance linear memories for the on-demand instance allocation strategy."
          ]
        },
        {
          "title": "Why does WebAssembly.Memory take `initial` and ...",
          "url": "https://stackoverflow.com/questions/74229102/why-does-webassembly-memory-take-initial-and-maximum-in-units-of-number-of-p",
          "excerpts": [
            "Wasm memory size can only be chosen in steps of pages (64 KiB), because that generally makes memory bounds checks using hardware virtual memory ..."
          ]
        },
        {
          "title": "Emscripten Compiler Settings",
          "url": "https://emscripten.org/docs/tools_reference/settings_reference.html",
          "excerpts": [
            "This allows emscripten to produce binaries are more WASI compliant and also allows it to process and execute WASI binaries built with other SDKs (e.g. wasi-sdk) ..."
          ]
        },
        {
          "title": "Wasmtime Pooling Allocation and Memory Configuration (api/struct.PoolingAllocationConfig.html)",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.PoolingAllocationConfig.html",
          "excerpts": [
            "When using the pooling allocator (see [`Config::allocation_strategy`](struct.Config.html.allocation_strategy \"method wasmtime::Config::allocation\\_strategy\") , [`InstanceAllocationStrategy::Pooling`](enum.InstanceAllocationStrategy.html.Pooling \"variant wasmtime::InstanceAllocationStrategy::Pooling\") ), memory protection keys can\nreduce the total amount of allocated virtual memory by eliminating guard\nregions between WebAssembly memories in the pool. It does so by\n“coloring” memory regions with different memory keys and setting which\nregions are accessible each time executions switches from host to guest\n(or vice ver",
            " WebAssembly 32-bit linear memories in\nWasmtime are, by default 4G address space reservations with a small guard\nregion both before and after the linear memory. Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard.",
            "For example a pooling allocator can be configured with:",
            "Configuration options used with `InstanceAllocationStrategy::Pooling` to change the behavior of the pooling instance allocator. By avoiding [`mmap`](https://man7.org/linux/man-pages/man2/mmap.2.html) this\navoids whole-process virtual memory locks which can improve scalability and\nperformance through avoiding this. Additionally with pooled allocation it’s possible to create “affine slots”\nto a particular WebAssembly module or component over time. For example if\nthe same module is multiple times over time the pooling allocator will, by\ndefault, attempt to reuse the same slot. This mean that the slot has been\npre-configured and can retain virtual memory mappings for a copy-on-write\nimage, for example (see [`Config::memory_init_cow`](struct.Config.html.memory_init_cow \"method wasmtime::Config::memory\\_init\\_cow\") for more information. This means that in a steady state instance deallocation is a single [`madvise`](https://man7.org/linux/man-pages/man2/madvise.2.html) to reset linear memory to its original contents followed by a\nsingle (optional) [`mprotect`](https://man7.org/linux/man-pages/man2/mprotect.2.html) during the next instantiation to shrink\nmemory back to its original size. Compared to non-pooled allocation this\navoids the need to [`mmap`](https://man7.org/linux/man-pages/man2/mmap.2.html) a new region of memory, [`munmap`](https://man7.org/linux/man-pages/man2/munmap.2.html) it, and [`mprotect`](https://man7.org/linux/m",
            "\nWith all these options in place no virtual memory tricks are used at all and\neverything is manually managed by Wasmtime (for example resetting memory is\na `memset(0)` ). This is not as fast in a single-threaded scenario but can\nprovide benefits in high-parallelism situations as no virtual memory locks\nor IPIs need happen.",
            "Despite the above advantages to instantiation performance the pooling\nallocator is not enabled by default in Wasmtime.",
            "One reason is that the\nperformance advantages are not necessarily portable, for example while the\npooling allocator works on Windows it has not been tuned for performance on\nWindows in the same way it has on Linux. Additionally the main cost of the pooling allocator is that it requires a\nvery large reservation of virtual memory (on the order of most of the\naddressable virtual address space). WebAssembly 32-bit linear memories in\nWasmtime are, by default 4G address space reservations with a small guard\nregion both before and after the linear memory.",
            "Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard. This means that, by default, the pooling allocator uses roughly\n4G of virtual memory per WebAssembly linear memory slot. 4G of virtual\nmemory is 32 bits of a 64-bit address",
            "The main benefit of the pooling allocator is to make WebAssembly instantiation both faster and more scalable in terms of parallelism. Allocation is faster ..."
          ]
        },
        {
          "title": "Config in wasmtime - Rust - Docs.rs",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html",
          "excerpts": [
            "\nMemory in the `initial` range is accessible to the instance and can be\nread/written by wasm code. Memory in the `guard` regions is never\naccessible to wasm code and memory in `capacity` is initially\ninaccessible but may become accessible through `memory.grow` instructions",
            "Configures whether execution of WebAssembly will “consume fuel” to either halt or yield execution as desired. This can be used to deterministically prevent ..."
          ]
        },
        {
          "title": "InstanceAllocationStrategy in wasmtime - Rust",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/enum.InstanceAllocationStrategy.html",
          "excerpts": [
            "Available on crate feature pooling-allocator only. The pooling instance allocation strategy. A pool of resources is created in advance and module instantiation ..."
          ]
        },
        {
          "title": "wasi-libc.git/history - dlmalloc - git.proxmox.com Git",
          "url": "https://git.proxmox.com/?p=wasi-libc.git;a=history;f=dlmalloc;hb=1792d02ac94c6c18f28ef38704c8d957f37fb879",
          "excerpts": [
            "WASI libc prototype implementation. tree | commitdiff. 2019-01-15, Dan Gohman · Merge pull request #9 from WebAssembly/dlmalloc · tree | commitdiff. 2019-01-10 ..."
          ]
        },
        {
          "title": "A practical guide to WebAssembly memory | radu's blog",
          "url": "https://radu-matei.com/blog/practical-guide-to-wasm-memory/",
          "excerpts": [
            "Jan 11, 2021 — In this article we explore using memory in WebAssembly in various scenarios - passing JavaScript arrays to Rust and AssemblyScript modules, checking for some ..."
          ]
        },
        {
          "title": "malloc() in WASI (dlmalloc) throwing \"out of bounds ...",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime/issues/539",
          "excerpts": [
            "Feb 22, 2021 — I've run into an odd issue with WASI where its implementation of malloc appears to throw an out of bounds memory access."
          ]
        },
        {
          "title": "Memory64 - Hacker News",
          "url": "https://news.ycombinator.com/item?id=42075403",
          "excerpts": [
            "The trick takes advantage of 32-bit registers automatically being zero-extended to 64. It actually uses 8GB of allocated address space."
          ]
        },
        {
          "title": "c++ - Differences between dynamic memory and \"ordinary ...",
          "url": "https://stackoverflow.com/questions/1021138/differences-between-dynamic-memory-and-ordinary-memory",
          "excerpts": [
            "However, since dynamic memory management serves a different purpose than static MM, a comparison does not make sense. And hey, it's only 4 bytes ..."
          ]
        },
        {
          "title": "Redesign Wasmtime's CLI flags · Issue #6741 - GitHub",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/6741",
          "excerpts": [
            "--dynamic-memory-guard-size <SIZE>, -O dynamic-memory-guard-size ... I.e., can I do wasmtime help run -O (or wasmtime help -O ) and get ..."
          ]
        },
        {
          "title": "Rust wasm64-unknown-unknown target (Rustc book)",
          "url": "https://doc.rust-lang.org/beta/rustc/platform-support/wasm64-unknown-unknown.html",
          "excerpts": [
            "the maximum\nmemory size is now the full 64-bit address space instead of the 4GB as limited\nby the 32-bit address space for `wasm32-unknown-unknown`.",
            "memory64) WebAssembly proposal is still\nin-progress and not standardized. Th",
            "This target is cross-compiled. The target supports `std` in the same manner as\nthe `wasm32-unknown-unknown` target which is to say that it comes with the\nstandard library but many I/O functions such as `std::fs` and `std::net` will\nsimply return error.",
            "This means that there are not many engines\nwhich implement the `memory64` feature and if they do they're likely behind a\nflag, for example:\n\n* Nodejs - `--experimental-wasm-memory64`\n* Wasmtime - `--wasm memory64`",
            "The `wasm64-unknown-unknown` target intends to match the default Clang targets\nfor its `\"C\"` ABI, which is likely to be the same as Clang's\n`wasm32-unknown-unknown` largely."
          ]
        },
        {
          "title": "WebAssembly binary size (wasm32-unknown-unknown)",
          "url": "https://www.reddit.com/r/rust/comments/7w2wsu/webassembly_binary_size_wasm32unknownunknown/",
          "excerpts": [
            "The biggest code size offenders, I've found, are: dlmalloc, the default allocator used with wasm32-unknown-unknown . It is somewhere around 8 or ..."
          ]
        }
      ],
      "reasoning": "The field describes several aspects: the standard memory model used by WebAssembly (a traditional 32-bit linear memory) and the extension memory64 to address more than 4GB; various memory allocators used in Wasm ecosystems (dlmalloc, mimalloc, jemalloc); approaches to memory management like preallocation, memory arenas, and allocator tuning; and memory-related implications of multi-threaded WASM including pooling/affine slots. The excerpts establish the baseline 32-bit linear memory and its extension to 64-bit when memory64 is adopted, including the constraints and implications of larger memories. They also cover allocator choices (dlmalloc as default in many WASI/WASM runtimes, mentions of mimalloc and jemalloc as alternatives) and how allocator design impacts memory usage and performance in WASM environments. Additionally, several excerpts describe pooling/slot-based memory allocation and memory protection strategies (guard regions, coloring in pooling allocators) which are directly relevant to how memory can be managed efficiently in high-concurrency WASM runtimes. Together, these excerpts support the notion that memory architectures in WASM (32-bit baseline plus potential 64-bit extension), allocator choices, and pooling strategies are central to achieving efficient, low-jitter, high-performance execution in WASM runtimes, including real-time or latency-sensitive contexts. The fine-grained field value's components map to: (a) the existence and implications of a memory64 extension; (b) the impact and role of allocators (dlmalloc, mimalloc, jemalloc) in WASM; (c) memory management techniques such as preallocation and pooling; and (d) multithreading considerations with shared memory and allocator behavior in multi-threaded WASM environments.",
      "confidence": "high"
    },
    {
      "field": "is_wasm_an_rtos_equivalent",
      "citations": [
        {
          "title": "WebAssembly Specification",
          "url": "https://webassembly.github.io/spec/core/_download/WebAssembly.pdf",
          "excerpts": [
            "A WebAssembly implementation will typically be embedded into a host environment. This environment\ndefines how loading of modules is initiated, how imports are provided (including host-side definitions), and\nhow exports can be accessed.",
            "However, the details of any particular embedding are beyond the scope of this\nspecification, and will instead be provided by complementary, environment-specific API definitions."
          ]
        },
        {
          "title": "Wasmtime Examples - Interrupting Wasm",
          "url": "https://docs.wasmtime.dev/examples-interrupting-wasm.html",
          "excerpts": [
            "Epoch-based interruption imposes relatively low overhead on Wasm execution; it has been measured at around a 10% slowdown. It is faster than fuel-based ... `"
          ]
        },
        {
          "title": "Announcing wasi-threads - Bytecode Alliance",
          "url": "https://bytecodealliance.org/articles/wasi-threads",
          "excerpts": [
            ". After all, WebAssembly engines [in\nbrowsers](https://webassembly.org/roadmap/) could already spawn new threads using Web Workers (see\nEmscripten’s [`pthreads` support](https://emscripten.org/docs/porting/pthreads.html) ). But not\nspecifying a spawn mechanism meant that standalone WebAssembly engines (i.e., not in a browser)\ncould avoid the complexity of supporting multi-threaded execution.",
            "The key assumption was that not\nall users of WebAssembly wanted or needed a threaded execution environment. The lack of a specification did not stop people from investigating, though.",
            "as possible, is designed to\nmatch the “instance per thread” model — each new thread is instantiated separately with only\nshared memory to connect the parallel execution.",
            "The component model uses instances to isolate\nsubcomponents. If a subcomponent were to spawn a thread, it could no longer instantiate a single\nmodule into a thread (should it instantiate a sub-tree of modules?!).",
            "The `Makefile` s essentially compile\nthe WebAssembly objects using the same flags as shown above, but with the added project-specific\ncomplexities of `pigz` and `libz` . You can build and run the benchmark yourself with",
            "Benchmarking\n------------"
          ]
        },
        {
          "title": "Real-time operating system - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Real-time_operating_system",
          "excerpts": [
            "Memory allocation\") is more critical in a real-time operating system than in other operating system"
          ]
        },
        {
          "title": "Difference Between Hard Real Time and Soft Real Time System",
          "url": "https://www.geeksforgeeks.org/operating-systems/difference-between-hard-real-time-and-soft-real-time-system/",
          "excerpts": [
            "/real-time-operating-system-rtos/) is a type of operating system that is designed to meet strict time constraints, with a guaranteed response time for critical tasks.",
            "Hard real time system**** is a system whose result is not produce according to time constra"
          ]
        }
      ],
      "reasoning": "The core WebAssembly specification treats embedding and environment as the outer contract: it defines a module that is loaded and executed by a host, rather than prescribing an OS-like scheduler inside the Wasm VM. This framing implies there is no built-in real-time scheduler inside Wasm itself; instead, scheduling and execution model depend on the host embedding (the runtime and OS) that runs Wasm code. Concretely, the Wasm spec describes embedding into a host environment and does not define a self-contained RTOS scheduler, signaling that any scheduling guarantees are provided by the host rather than by Wasm itself. This is reinforced by statements asserting that a WebAssembly implementation is embedded into a host environment which defines how loading, imports/exports, and invocation happen, rather than a built-in OS-like scheduler inside Wasm. In addition, discussions about threading in Wasm explain that threads are introduced via external proposals (WASI threads) and are implemented by the host or runtime, not by a built-in Wasm RTOS. The WASI threads material describes an instance-per-thread spawning model and a host-provided thread mechanism, rather than an internal real-time scheduler inside Wasm. Related material notes that the component model and WASI evolve to provide concurrency and async I/O, but these are features of the host/runtime or component framework rather than a self-contained RTOS inside Wasm. Taken together, these excerpts support that Wasm does not constitute an RTOS-like scheduler on its own; instead, it relies on host runtimes, WASI, and the Component Model to provide concurrency and threading, consistent with an answer of False for the fine-grained field value.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies",
      "citations": [
        {
          "title": "Five things you need to know about wasmCloud",
          "url": "https://www.opcito.com/blogs/five-things-you-need-to-know-about-wasmcloud",
          "excerpts": [
            "Apr 15, 2025 — The results reveal that wasmCloud's WebAssembly-based approach delivers impressive performance compared to traditional containerized solutions, ..."
          ]
        },
        {
          "title": "Introducing Spin",
          "url": "https://www.fermyon.com/blog/introducing-spin",
          "excerpts": [
            "Mar 31, 2022 — Spin is a framework for web apps, microservices, and other server-like applications. It provides the interfaces for writing WebAssembly modules."
          ]
        },
        {
          "title": "wasmCloud Application Deployment Manager (wadm)",
          "url": "https://wasmcloud.com/docs/ecosystem/wadm/",
          "excerpts": [
            "The wasmCloud Application Deployment Manager (wadm) manages declarative application deployments, reconciling the current state of an application with the ..."
          ]
        },
        {
          "title": "Wadm Configuration - wasmCloud",
          "url": "https://wasmcloud.com/docs/deployment/wadm/configuration/",
          "excerpts": [
            "These guides are for deploying wadm in production. For local development and testing, simply run wash up . For more information on wadm , see the wadm docs."
          ]
        },
        {
          "title": "Globally Distributed WebAssembly Applications with ...",
          "url": "https://wasmcloud.com/blog/globally-distributed-webassembly-applications-with-wasmcloud-and-nats/",
          "excerpts": [
            "In this guide we used NGS and NATS Leaf Nodes to connect two wasmCloud hosts; one running locally and one running in the cloud."
          ]
        },
        {
          "title": "Faasm",
          "url": "https://github.com/faasm",
          "excerpts": [
            "Faasm is a research project on distributed serverless runtimes with WebAssembly (WASM). Faasm originated in the Large-Scale Data & Systems group from the ...",
            "Faasm is a high-performance serverless runtime using WebAssembly for function isolation. · Faabric is a messaging and state layer for distributed serverless ..."
          ]
        },
        {
          "title": "WP5 – Programming Abstractions for Serverless Computing",
          "url": "https://cloudbutton.eu/docs/slides/WP5.pdf",
          "excerpts": [
            "Sep 15, 2022 — Faasm: Serverless OpenMP + MPI. 21. Page 22. Why OpenMP in Serverless ... Faasm: Lightweight Isolation for Efficient Stateful. Serverless ..."
          ]
        },
        {
          "title": "[PDF] Lightweight Isolation for Efficient Stateful Serverless Computing",
          "url": "https://www.usenix.org/system/files/atc20-paper134-slides-shillaker.pdf",
          "excerpts": [
            "Missing: Faabric paper",
            "by S Shillaker · Cited by 411 — Can FAASM improve efficiency and performance of ML training? 3. Can FAASM improve throughput of ML inference? 4. Does Faaslet isolation affect performance of ..."
          ]
        },
        {
          "title": "Announcing Spin v1.5",
          "url": "https://www.fermyon.com/blog/spin-v15",
          "excerpts": [
            "Sep 18, 2023 — Every time it handles a new request, Spin will create a new WebAssembly instance, execute the handler function for that request, then terminate ...",
            "Spin 1.5 makes use of a Wasmtime feature specifically designed for such scenarios, the memory pooling allocator, which can speed-up the ..."
          ]
        },
        {
          "title": "Announcing Spin 3.4",
          "url": "https://www.fermyon.com/blog/announcing-spin-3-4",
          "excerpts": [
            "6 days ago — In this blog post, we'll dive into a few exciting features of this release, which center around enabling HTTP/2 for outgoing requests, pooling ..."
          ]
        },
        {
          "title": "Persistent Data: PostgreSQL - Developer - Fermyon",
          "url": "https://developer.fermyon.com/cloud/data-postgres",
          "excerpts": [
            "In this tutorial, we will be using ElephantSQL's free PostgreSQL service. Spin and Fermyon Cloud. First, you need to have Spin installed on your computer."
          ]
        },
        {
          "title": "Reflections on Three Years of wasmCloud",
          "url": "https://wasmcloud.com/blog/wasmcloud-third-anniversary/",
          "excerpts": [
            "Jun 25, 2022 — We spent 90% of our time working on things that weren't core features like async and concurrency and thread-safe queue and dispatch management, ...",
            "There was a background thread per actor, one per capability provider, and another used for dispatch between the two. Ultimately, this became ..."
          ]
        },
        {
          "title": "[Concurrency] async/await + actors - Discussion - Swift Forums",
          "url": "https://forums.swift.org/t/concurrency-async-await-actors/6516",
          "excerpts": [
            "Aug 19, 2017 — I've been advocating for a specific model involving async/await and actors for many years now. Handwaving only goes so far, so some folks asked me to write ..."
          ]
        },
        {
          "title": "Adaptive Asynchronous Work-Stealing for distributed load- ...",
          "url": "https://arxiv.org/pdf/2401.04494",
          "excerpts": [
            "by JB Fernandes · 2024 · Cited by 1 — In parallel programming, work-stealing is a scheduling method originally designed for shared-memory applications (Blumofe and Leiserson, 1999)."
          ]
        },
        {
          "title": "Making HTTP Requests | Spin Docs",
          "url": "https://spinframework.dev/v2/http-outbound",
          "excerpts": [
            "Missing: concurrency threads O"
          ]
        },
        {
          "title": "WasmCloud Docs: NATS-based Lattice, Scheduling, and Concurrency",
          "url": "https://wasmcloud.com/docs/ecosystem/nats/",
          "excerpts": [
            " is built on [**NATS**](https://nats.io/), an open source connective technology hosted by the Cloud Native Computing Foundation (CNCF). NATS enables secure application-layer networking across diverse environments including edge, different vendors' clouds, and on-premise datacenters",
            "NATS uses [WIT-over-RPC (wRPC)](https://github.com/bytecodealliance/wrpc) as a transport protocol and helps to deliver a lattice with out-of-the-box load balancing, failover, and other features for systems that span environments.",
            "on.\nIn wasmCloud, components and providers use this **queue subscription** model to subscribe to invocations. When a component or provider exports a function, the system creates a queue subscription on a NATS subject that other entities can call.",
            "1. A component imports a function on a linked provider over a shared interface. 2. When the component runs, an invocation message is published to a queue group. 3. There are three instances of the linked provider on the lattice. Any one of those three could receive the invocation.",
            "With this basic model, queue groups and queue subscription facilitate horizontal scaling, load balancing, and failover for components and providers.",
            "The supercluster architecture provides...\n\n* **Scalability**: Seamlessly scales communication across multiple clusters, ensuring applications can grow without limitation. * **Reliability**: Enhances the overall reliability of the system by providing failover and load balancing across clusters. * **Geo-awareness**: Geo-affinity is implemented automatically in queuing, ensuring that service requests are handled by the nearest available cluster, reducing latency and improving"
          ]
        },
        {
          "title": "Fermyon Spin blog post on composing components with Spin 2",
          "url": "https://www.fermyon.com/blog/composing-components-with-spin-2",
          "excerpts": [
            "Nov 6, 2023 — With Spin 2.0, we have unlocked an exciting evolution of Wasm, the component model. This blog walks through how to take advantage of components in Spin 2.0. Wh",
            "When composing the components, the imports and exports are resolved such that the outer component only exports the [`wasi:http/incoming-handler` interface](https://github.com/WebAssembly/wasi-http/blob/main/wit/handler.wit) from the authentication component.",
            "We’ve composed two components, directly linking components instead of making over-network calls",
            "The components are still isolated from each other with their own linear memory but can communicate over a highly typed interface",
            "The auth component can be seen as a library that may be maintained upstream in the future. Imagine never having to write authentication logic again",
            "Auth secrets are kept safe from business logic code",
            "This app is portable! You can run it locally directly with [`wasmtime`](https://github.com/fermyon/http-auth-middleware/tree/main) or on other platforms and clouds that support the [upstream `wasi::http/proxy` world](https://github.com/WebAssembly/wasi-http/blob/main/wit/proxy.wit)"
          ]
        },
        {
          "title": "Faasm: High performance serverless computing",
          "url": "https://lsds.doc.ic.ac.uk/projects/faasm",
          "excerpts": [
            "Faasm is open-sourced at <https://github.com/lsds/Faas",
            "The Faasm scheduler ensures these functions execute close to their required data, reducing unnecessary duplication and overhead."
          ]
        },
        {
          "title": "Spin 1.0 — The Developer Tool for Serverless WebAssembly",
          "url": "https://www.fermyon.com/blog/introducing-spin-v1",
          "excerpts": [
            "Mar 22, 2023 — As we have learned in previous sections, Spin will create a new Wasm instance for every request — which makes it best suited for stateless, ...",
            "Spin is an open source developer tool and framework that helps the user through creating, building, distributing, and running serverless applications with Wasm."
          ]
        },
        {
          "title": "Introducing Spin 1.1 - Fermyon",
          "url": "https://www.fermyon.com/blog/introducing-spin-v11",
          "excerpts": [
            "Spin now executes WebAssembly components, and is fully compatible with the WebAssembly component model; a host of bugfixes and improvements ..."
          ]
        },
        {
          "title": "FaaSM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://wangziqi2013.github.io/paper/2021/10/11/faasm.html",
          "excerpts": [
            "Oct 11, 2021 — The paper proposes a snapshot mechanism called Proto-Faaslet, which captures the initialized state of the system and saves them as a local ..."
          ]
        },
        {
          "title": "[PDF] arXiv:2302.11358v1 [cs.DC] 22 Feb 2023",
          "url": "https://arxiv.org/pdf/2302.11358",
          "excerpts": [
            "Faasm: Lightweight isolation for efficient stateful serverless computing. In USENIX Annual Technical. Conference (USENIX ATC). USENIX ..."
          ]
        },
        {
          "title": "FAASM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://hale-legacy.com/class/edge-serverless/f21/handout/faasm.pdf",
          "excerpts": [
            "Proto-Faaslets - 500μs initialisation, 90kB memory. Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation.",
            "Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation. Two-tier state - global synchronisation. 10. Page 10 ..."
          ]
        },
        {
          "title": "wasmCloud Documentation - Deploy and Scale",
          "url": "https://wasmcloud.com/docs/tour/deploy-and-scale/",
          "excerpts": [
            "This means you can deploy your components to any cloud provider, edge location, or even on-premises and they will be able to communicate with each other.",
            "No matter where your components and capability providers run, they can seamlessly communicate on the [lattice"
          ]
        },
        {
          "title": "wasmCloud (Overview)",
          "url": "https://wasmcloud.com/",
          "excerpts": [
            "wasmCloud is an open source project from the Cloud Native Computing Foundation (CNCF) that enables teams to build polyglot applications composed of reusable Wasm components and run them—resiliently and efficiently—across any cloud, Kubernetes, datacenter, or edge."
          ]
        },
        {
          "title": "wasmCloud Concepts and Components",
          "url": "https://wasmcloud.com/docs/concepts/components/",
          "excerpts": [
            ")\n\nThe surrounding environment of the [wasmCloud host](/docs/concepts/hosts/) may have varying levels of concurrency support. This support may differ depending on whether the host is running in a browser, on a constrained device, or in a VM somewhere. However, the code for components should be independent of these conditions and *never have to change*, even if the surrounding environment adopts a different concurrency model.",
            "Components are portable, interoperable, and composable:\n\n* **Portable**: Because WebAssembly binaries execute against a virtual instruction set architecture (essentially a tiny VM), they are agnostic to architecture and operating system kernel; they run anywhere there is a WebAssembly runtime. Component binaries are typically much smaller than analogous container images, as well—often measured in kilobytes—enabling them to run in resource-constrained environments where even containers aren't practical."
          ]
        },
        {
          "title": "Host Labels",
          "url": "https://wasmcloud.com/docs/deployment/hosts/labels/",
          "excerpts": [
            "wasmCloud hosts are associated with a set of label key-value pairs. Labels are used to distinguish hosts in auctions and schedule workloads with wadm."
          ]
        },
        {
          "title": "wrpc_transport - Rust",
          "url": "https://docs.rs/wrpc-transport",
          "excerpts": [
            "wRPC transport abstractions, codec and framing. wRPC is an RPC framework based on WIT. It follows client-server model, where peers (servers) may serve ..."
          ]
        },
        {
          "title": "wasmCloud and .NET Orleans: Kindred Spirits",
          "url": "https://cosmonic.com/blog/engineering/wasmcloud-orleans-kindred-spirits",
          "excerpts": [
            "In Orleans, concurrent requests for the bob user's compute will block single-threaded while in wasmCloud, the number of concurrent requests ..."
          ]
        },
        {
          "title": "Threading — Faasm documentation - Read the Docs",
          "url": "https://faasm.readthedocs.io/en/latest/source/threads.html",
          "excerpts": [
            "Faasm supports simple creation and joining of pthreads, as well as pthread mutexes. It also provides stubs for serveral other pthread calls so that applications ..."
          ]
        },
        {
          "title": "Deploying wasmCloud Hosts",
          "url": "https://wasmcloud.com/docs/category/deploying-wasmcloud-hosts/",
          "excerpts": [
            "How to configure and interact with hosts in production. 📄️ Scheduling Auctions. Finding hosts eligible for a workload · 📄️ Config Service. Starting hosts with ..."
          ]
        },
        {
          "title": "wrpc-transport - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/wrpc-transport",
          "excerpts": [
            "Mar 12, 2025 — wrpc-transport v0.28.4. wRPC core transport functionality. Readme · 62 Versions · Dependencies · Dependents. Failed to load README file for wrpc ..."
          ]
        },
        {
          "title": "Choose an Azure Compute Service",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/compute-decision-tree",
          "excerpts": [
            "5 days ago — Functions as a service (FaaS) lets you deploy your code to the service, which automatically runs it. Azure Functions is a FaaS service. Note."
          ]
        },
        {
          "title": "Serverless Architecture Part 2: When, Why, and How ...",
          "url": "https://www.fabricgroup.com.au/blog/serverless-architecture-part-2-when-why-and-how-to-adopt-it",
          "excerpts": [
            "Nov 12, 2024 — Function as a Service (FaaS) Improvements: Cloud providers are continuously optimizing FaaS, improving latency, scaling, and cold start times."
          ]
        },
        {
          "title": "Parallelization: pthreads or OpenMP? - multithreading",
          "url": "https://stackoverflow.com/questions/935467/parallelization-pthreads-or-openmp",
          "excerpts": [
            "The OpenMP is task-based, Pthreads is thread based. It means that OpenMP will allocate the same number of threads as number of cores. So you will get scalable ..."
          ]
        },
        {
          "title": "Platform Overview and Lattice (wasmCloud)",
          "url": "https://wasmcloud.com/docs/concepts/lattice/",
          "excerpts": [
            "The lattice is a self-forming, self-healing mesh network that provides a unified, flat topology across any number of environments, clouds, browsers, or even hardware.",
            "The NATS-powered lattice enables resilient distributed messaging with location-independent addressing.",
            "Across a lattice, different instances of the same component are equally eligible to receive a given invocation, regardless of the host on which they are running.",
            "This enables immediate failover—if a host is lost, alternative compute is immediately available.",
            "wasmCloud's wRPC protocol is (just as it sounds) an RPC protocol for dynamically composing distributed WebAssembly components at runtime over the lattice.",
            "red lattice enables resilient distributed messaging with location-independent addressing."
          ]
        },
        {
          "title": "wasmCloud 1.0 Brings the WebAssembly Component Model to Enterprise",
          "url": "https://wasmcloud.com/blog/wasmcloud-1-brings-components-to-enterprise/",
          "excerpts": [
            "wRPC makes distributed computing in wasmCloud feel like composing components over the lattice.",
            "wRPC is designed to be protocol agnostic, and wasmCloud provides the first implementation over NATS, bringing the benefits of Wasm to life: **composable, reusable components, linked together like building blocks, dynamically deployed over distributed network",
            "wasmCloud 1.0 Brings the WebAssembly Component Model to Enterprise | wasmCloud",
            "Components by-default: distributed support with wRPC",
            "wasmCloud 1.0 realizes the dream of abstracting away vendor and language considerations from software development. Whether written in Python, Go, C++ or any other language, WASI 0.2 components interoperate using standard Wasm Interface Types (WIT).",
            "This unties engineers from specific libraries so they can focus on business logic, swapping non-functional requirements in and out at runtime.",
            "wasmCloud's lattice is a flat topology network that enables application components to communicate exactly the same, whether they are running on a single machine or globally distributed at scale.",
            "The lattice, powered by CNCF NATS, **automatically load-balances** requests between application components, **failing over immediately** in the case of an",
            "Declarative Orchestration with Wadm**. Wadm orchestrates the deployment and management of Wasm applications, **at scale and in any locat",
            "When infrastructure is added or removed, Wadm dynamically rebalances applications based on constraints specified in the manifest.",
            "wasmCloud supports signing components with ed25519 keys, allowing for offline verification of component identity and issuer.",
            "wasmCloud 1.0 comes with several standard interfaces—`wasi:cli` for environment, `wasi:runtime` for configuration and `wasi:http`—but any 0.2 component will work perfectly in wasmCloud.",
            "OpenTelemetry (OTEL) Observability**. Observability is crucial in distributed systems so wasmCloud has supported exporting OTEL traces for over a year. 1.0 adds OTEL support for logs and metrics, the other two pillars of observabilit",
            "Metrics like **component concurrency  gauging** compares specified concurrency levels with deployment targets–scaling accordi"
          ]
        },
        {
          "title": "Building a Host for the Spin Runtime - and Creating the World's Laziest Control Plane",
          "url": "https://www.fermyon.com/blog/building-host-for-spin-runtime",
          "excerpts": [
            "The core Spin runtime is in two Rust crates, `spin-core` and `spin-app` ."
          ]
        },
        {
          "title": "The Spin and Kubernetes Story",
          "url": "https://www.fermyon.com/blog/spin-and-kubernetes-story",
          "excerpts": [
            "an run anywhere. The ability to run Spin applications on any processing architecture and operating system is a big benefit, stemming from using WASI through Wasmtime as the runtime for Spi"
          ]
        },
        {
          "title": "`wasmtime` preemption",
          "url": "https://users.rust-lang.org/t/wasmtime-preemption/130641",
          "excerpts": [
            "Jun 12, 2025 — Hello, I want wasm binary to call a host function, that will pause execution of the current wasm function and call another wasm function on ..."
          ]
        },
        {
          "title": "OS-Level Challenges in LLM Inference and Optimizations",
          "url": "https://eunomia.dev/blog/2025/02/18/os-level-challenges-in-llm-inference-and-optimizations/",
          "excerpts": [
            "By contrast, pinning inference threads to dedicated CPU cores and isolating them from normal scheduling can dramatically reduce latency jitter – ..."
          ]
        },
        {
          "title": "Investigate `IO_URING` support · Issue #16232 · denoland ...",
          "url": "https://github.com/denoland/deno/issues/16232",
          "excerpts": [
            "Oct 10, 2022 — If you haven't heard about it IO_URING is a new revolutionary API for doing asynchronous operations in Linux with a much higher throughput, much ..."
          ]
        },
        {
          "title": "Os noise mitigations for benchmarking web browser ...",
          "url": "https://link.springer.com/article/10.1007/s10791-024-09471-4",
          "excerpts": [
            "In this article, we review OS noise mitigation configurations that will aid in providing a more robust and accurate benchmarking testbed."
          ]
        },
        {
          "title": "Performance of rust as Wasm vs server side application",
          "url": "https://www.reddit.com/r/rust/comments/t7u7wy/performance_of_rust_as_wasm_vs_server_side/",
          "excerpts": [
            "The Wasm approach was quite a lot slower than the web server. For reference, the server approach takes about 1-6 ms to complete, and the Wasm takes 15-30 ms."
          ]
        },
        {
          "title": "Kubernetes - WasmEdge Runtime",
          "url": "https://wasmedge.org/book/en/kubernetes.html",
          "excerpts": [
            "In this chapter, we will demonstrate how Kubernetes ecosystem tools work with WasmEdge WebAssembly applications."
          ]
        },
        {
          "title": "You don't *need* io_uring. For many workloads being slow ...",
          "url": "https://news.ycombinator.com/item?id=41188223",
          "excerpts": [
            "Jan 12, 2025 — Low latency for a single event is never going to have mechanistic sympathy, will be a colossal waste of most of your system. Highly concurrent system usage ..."
          ]
        },
        {
          "title": "Multithreading - Swift and WebAssembly",
          "url": "https://book.swiftwasm.org/getting-started/multithreading.html",
          "excerpts": [
            "The proposal was implemented in several WASI host runtimes, including Wasmtime and wasm-micro-runtime, but it was withdrawn in August 2023 in favor of shared- ...",
            "WebAssembly modules can't create threads themselves, and the host environment must provide a way to create threads and run WebAssembly modules on them."
          ]
        },
        {
          "title": "Where can I find up-to-date status for implementation of WASI ...",
          "url": "https://stackoverflow.com/questions/74144175/where-can-i-find-up-to-date-status-for-implementation-of-wasi-proposals",
          "excerpts": [
            "Where can I find up-to-date status for implementation of WASI proposals? Ask Question. Asked 2 years, 8 months ago. Modified 2 years, 6 months ..."
          ]
        },
        {
          "title": "Resolution \u0000b7 Disabling automatic IRQ balancing \u0000b7 Disabling the kernel's built-in NUMA balancing \u0000b7 Isolating CPUs from the process scheduler.",
          "url": "https://access.redhat.com/solutions/2144921",
          "excerpts": [
            "Resolution · Disabling automatic IRQ balancing · Disabling the kernel's built-in NUMA balancing · Isolating CPUs from the process scheduler. RHEL7",
            "Now the CPUs have been isolated and are removed from the scheduler's purview.",
            "To simply bind a systemd unit to a specific CPU core or cores, then please see this solutions article: https://access.redhat.com/solutions/2142471",
            "isolcpus=... and\nkernel.numa_balancing=0 changes completed, and the system has been rebooted -- to ensure that the configuration will not change -- we can identify which NUMA node is \"closest\" to the network interface we care about. While we do not need to have the IRQs pinned to the same socket as our accompanying userland processes, keeping them in the same NUMA node is important for performance.",
            "Recall that the CPUs we can utilize for IRQs must be local to the NUMA node the userland process is the even-numbered cores 0-14:\n# numactl -a -N netdev:em1 grep allowed /proc/self/status\n[...]\nCpus_allowed_list: 0,2,4,6,8,10,12,14"
          ]
        },
        {
          "title": "examples-interrupting-wasm.md",
          "url": "https://cocalc.com/github/bytecodealliance/wasmtime/blob/main/docs/examples-interrupting-wasm.md",
          "excerpts": [
            "Epoch-based interruption imposes relatively low overhead on Wasm execution; it has been measured at around a 10% slowdown. It is faster than fuel-based interruption.",
            "Deterministic Fuel",
            "Fuel-based interruption is completely deterministic: the same program run with the same amount of fuel will always be interrupted at the same location in the program (unless it has enough fuel to complete its computation, or there is some other form of non-determinism that causes the program to behave differently). The downside is that fuel-based interruption imposes more overhead on execution, slowing down Wasm programs, than epochs do.",
            "Interruption Mechanisms"
          ]
        },
        {
          "title": "Wasmtime Examples - Interrupting Wasm",
          "url": "https://docs.wasmtime.dev/examples-interrupting-wasm.html",
          "excerpts": [
            "Deterministic Fuel"
          ]
        },
        {
          "title": "poll.wit - WebAssembly/wasi-io",
          "url": "https://github.com/WebAssembly/wasi-io/blob/main/wit/poll.wit",
          "excerpts": [
            "This function takes a list of pollables, which identify I/O sources of interest, and waits until one or more of the events is ready for I/O."
          ]
        },
        {
          "title": "WebAssembly/wasi-clocks: Clocks API for WASI - GitHub",
          "url": "https://github.com/WebAssembly/wasi-clocks",
          "excerpts": [
            "WASI Clocks is a WASI API for reading the current time and measuring elapsed time. Unlike many clock APIs, WASI Clocks is capability-oriented.",
            "Wasi-libc has support for emulating these clocks, by using the monotonic clock instead, which isn't a technically precise replacement, but is enough to ensure minimal compatibility with existing code.",
            "The monotonic clock APIs can be used to measure the elapsed time of a region of code:",
            "WASI clocks must have host implementations which can pass the testsuite\non at least Windows, macOS, and Linux.",
            "WASI-clocks is currently in [Phase ",
            "In POSIX, `clock_gettime` uses a single `timespec` type to represent timestamps\nfrom all clocks, with two fields: seconds and nanoseconds. However, in applications\nthat just need to measure elapsed time, and don't need to care about wall clock\ntime, working with seconds and nanoseconds as separate fields adds extra code size\nand complexity. For these use cases, a single 64-bit nanoseconds value, which can\nmeasure up to about 584 years, is sufficient and simpler.",
            "WASI Clocks is not aiming to cover date formatting, or modifying the time of a clock.",
            "WASI preview1 included two clocks which measured the CPU time of the current process and the current thread, respectively. These clocks are difficult to implement efficiently in WASI implementations that have multiple wasm instances in the same host process, so they've been omitted from this API.",
            "WASI Clocks is a WASI API for reading the current time and measuring elapsed\ntime."
          ]
        },
        {
          "title": "Introduction · WASI.dev",
          "url": "https://wasi.dev/",
          "excerpts": [
            "WASI is designed to provide a secure standard interface for applications that can be compiled to Wasm from any language, and that may run anywhere—from browsers ...",
            "The WebAssembly System Interface (WASI) is a group of standards-track API specifications for software compiled to the W3C WebAssembly (Wasm) standard."
          ]
        },
        {
          "title": "The monotonic clock may be too underspecified · Issue #47 - GitHub",
          "url": "https://github.com/WebAssembly/wasi-clocks/issues/47",
          "excerpts": [
            "The monotonic clock as it stands right now is likely too underspecified in what it is measuring and should possibly be split into two ..."
          ]
        },
        {
          "title": "WASM all the way.",
          "url": "https://lwn.net/Articles/977627/",
          "excerpts": [
            "Jun 7, 2024 — ... WASM in the kernel would mean: * to replace the eBPF VM in the kernel by the WASM VM, and to replace all the eBPF call sites with WASM ..."
          ]
        },
        {
          "title": "wasmerio/kernel-wasm - GitHub",
          "url": "https://github.com/wasmerio/kernel-wasm",
          "excerpts": [
            "Safely run WebAssembly in the Linux kernel, with faster-than-native performance. ... I wrote Cervus, another WebAssembly \"usermode\" subsystem running in Linux ..."
          ]
        },
        {
          "title": "Comparing Security in eBPF and WebAssembly",
          "url": "https://dl.acm.org/doi/10.1145/3609021.3609306",
          "excerpts": [
            "This paper examines the security of eBPF and WebAssembly (Wasm), two technologies that have gained widespread adoption in recent years."
          ]
        },
        {
          "title": "WasmLinux -- WebAssembly/browser native Linux kernel and ...",
          "url": "https://wasmlinux-demo.pages.dev/",
          "excerpts": [
            "WasmLinux is a prototype of a WebAssembly 'native' Linux system, using a Wasm executable for the kernel, and is a usermode C++20 application."
          ]
        },
        {
          "title": "Security - WebAssembly",
          "url": "https://webassembly.org/docs/security/",
          "excerpts": [
            "Missing: Linux kernel regulatory"
          ]
        },
        {
          "title": "State of WebAssembly in Rust",
          "url": "https://users.rust-lang.org/t/state-of-webassembly-in-rust/63062",
          "excerpts": [
            "Aug 2, 2021 — These problems limit Rust usage in web only to a small set of problems. It should be noted that the main use case for wasi-std is being able to ..."
          ]
        },
        {
          "title": "Is Rust + WASM a good choice for a computation heavy ...",
          "url": "https://www.reddit.com/r/rust/comments/1i179jy/is_rust_wasm_a_good_choice_for_a_computation/",
          "excerpts": [
            "My project involves heavy computation on the frontend, and Im wondering if Rust with WASM-Bindgen would give me significantly better speeds."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "http://kernel.org/doc/html/latest/admin-guide/kernel-parameters.html",
          "excerpts": [
            "the final item represents CPUs 100,101,125,126,150,151,...",
            "The special case-tolerant group name “all” has a meaning of selecting all CPUs,\nso that “nohz\\_full=all” is the equivalent of “nohz\\_f",
            "isolcpus=1,2,10-20,100-2000:2/25"
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "http://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "This proposal relies on a specific convention\nin order to work correctly.",
            "The \"instance-per-thread\" approach chosen here does have its disadvantages:",
            "higher memory consumption (each instance is cloned)",
            "The remainder of the `pthreads` API can be split up into what can be implemented\nand what can safely be skipped until some later date.",
            "Phase 1",
            "Current Phase",
            "A proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI) API to add native thread support"
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "https://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "Phase 1"
          ]
        },
        {
          "title": "Component Model design and specification",
          "url": "https://github.com/WebAssembly/component-model",
          "excerpts": [
            "The Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2) . The subsequent [WASI Preview 3](https://github.com/WebAssembly/WASI/tree/main/wasip2) milestone will be\nprimarily concerned with the addition of [async support](/WebAssembly/component-model/blob/main/design/mvp/Async.md) ."
          ]
        },
        {
          "title": "WebAssembly/component-model",
          "url": "http://github.com/WebAssembly/component-model",
          "excerpts": [
            "The Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2) . The subsequent [WASI Preview 3](https://github.com/WebAssembly/WASI/tree/main/wasip2) milestone will be\nprimarily concerned with the addition of [async support](/WebAssembly/component-model/blob/main/design/mvp/Async.md) ."
          ]
        },
        {
          "title": "Best Practices of Real-Time Linux Configuration - Medium",
          "url": "https://medium.com/@eren.c.uysal/best-practices-of-real-time-linux-configuration-195dc44c4105",
          "excerpts": [
            "Real-Time preemption (PREEMPT_RT) and tuned kernel parameters play critical roles in achieving predictable scheduling and interrupt handling."
          ]
        },
        {
          "title": "For the love of god, stop using CPU limits on Kubernetes ...",
          "url": "https://www.reddit.com/r/kubernetes/comments/wgztqh/for_the_love_of_god_stop_using_cpu_limits_on/",
          "excerpts": [
            "CFS can be a problem for workloads even with low CPU usage. If a container has a large number of threads they may end up competing for cpu ...",
            "This is extremely important for low latency services . If your pod ever gets throttled, your will get zero CPU for up to 100ms waiting for ..."
          ]
        },
        {
          "title": "WebAssembly/wasi-parallel - GitHub",
          "url": "https://github.com/WebAssembly/wasi-parallel",
          "excerpts": [
            "general and this can be a\nsignificant performance lag in several domains (ML, HPC). SIMD (128-bit or [larger](https://github.com/WebAssembly/flexible-vectors) ) does not fully address the issue: many programs benefit from parallel\nexecution and standalone WebAssembly engines have no standard way to access this system\ncapability (unlike browser Web Workers).",
            "The primary use case is exposing CPU-executed, OS-managed\nthreads; users who simply need threads should look there first. Because `wasi-parallel` aims to allow parallel execution on more than just CPUs,\nthe API is quite differen",
            "wasi-threads"
          ]
        },
        {
          "title": "Wasmer Runtime Features",
          "url": "https://docs.wasmer.io/runtime/features",
          "excerpts": [
            "Threads: adds a new shared linear memory type and some new operations for atomic memory access;. Wasmer features. Caching: compiled WebAssembly modules can be ...",
            "Wasmer features"
          ]
        },
        {
          "title": "WASM Embedded",
          "url": "https://wasm-embedded.wuhanstudio.uk/",
          "excerpts": [
            "I'll show you how to use Web Assembly, or WASM, on Embedded Systems that use Real-Time Operating System. Mybe it's more intuitive to show you a quick demo. The ..."
          ]
        },
        {
          "title": "Real-Time Scheduling on Linux",
          "url": "https://eci.intel.com/docs/3.3/development/performance/rt_scheduling.html",
          "excerpts": [
            "Both approaches aim to bring the “lowest thread scheduling latency” under Linux multi-CPU real-time and non-realtime software execution context."
          ]
        },
        {
          "title": "How to set up Enhanced Platform Awareness - Ubuntu documentation",
          "url": "https://documentation.ubuntu.com/canonical-kubernetes/latest/snap/howto/epa/",
          "excerpts": [
            "This section explains how to set up the Enhanced Platform Awareness (EPA) features in a Canonical Kubernetes cluster."
          ]
        },
        {
          "title": "Get WITty: Evolving Kubernetes Scheduling With the WebAssembly Component Model",
          "url": "https://www.youtube.com/watch?v=hjbZOBghxYU",
          "excerpts": [
            "Get WITty: Evolving Kubernetes Scheduling With the WebAssembly Component Model - Dejan Pejchev & Jonathan Giannuzzi, G-Research",
            "At KubeCon NA 2024, we introduced WASM + KWOK Wizardry: Writing and Testing Kubernetes Scheduler Plugins at Scale, showcasing how WASM plugins transform Kubernetes scheduling. This session continues the story, highlighting our progress toward a language-agnostic framework using the WebAssembly Component Model.",
            "This session continues the story, highlighting our progress toward a language-agnostic framework using the WebAssembly Component Model. The current Go-centric WASM plugin SDK restricts innovation to a single language.",
            "Get WITty: Evolving Kubernetes Scheduling With the WebAssembly... Dejan Pejchev & Jonathan Giannuzzi · Comments. ["
          ]
        },
        {
          "title": "SharedMemory in cumulus_test_client::sc_executor - Parity",
          "url": "https://paritytech.github.io/polkadot-sdk/master/cumulus_test_client/sc_executor/sp_wasm_interface/wasmtime/struct.SharedMemory.html",
          "excerpts": [
            "The threads proposal adds the concept of “shared memory” to WebAssembly. This is much the same as a Wasm linear memory (i.e., Memory ), but can be used ..."
          ]
        },
        {
          "title": "Re: Modifying isolcpus, nohz_full, and rcu_nocb ...",
          "url": "https://lkml.org/lkml/2023/12/12/1561",
          "excerpts": [
            "> > > > The isolcpus, nohz_full and rcu_nocbs are boot-time kernel parameters. I am in the process of improving dynamic CPU isolation at runtime ..."
          ]
        },
        {
          "title": "How to Use WebAssembly for Real-Time Data Processing",
          "url": "https://blog.pixelfreestudio.com/how-to-use-webassembly-for-real-time-data-processing/",
          "excerpts": [
            "In this article, we'll explore how WebAssembly can be effectively used for real-time data processing, making your applications faster and more responsive."
          ]
        },
        {
          "title": "Can I call a single function from multiple threads? #3850",
          "url": "https://github.com/wasmerio/wasmer/discussions/3850",
          "excerpts": [
            "I am working on a plugin system for a high performance application using wasmer. Both the host and plugins are written in Rust. WASM allows me to sandbox ..."
          ]
        },
        {
          "title": "\"more preemptive\" ok but how does this bound latencies ...",
          "url": "https://news.ycombinator.com/item?id=34806623",
          "excerpts": [
            "\"more preemptive\" ok but how does this bound latencies and timing jitter from event to handling? ... The preempt_rt patches enable preempting kthreads."
          ]
        },
        {
          "title": "CPU Isolation – A practical example – by SUSE Labs (part 5)",
          "url": "https://www.suse.com/c/cpu-isolation-practical-example-part-5/",
          "excerpts": [
            "Also no need to set the “rcu_nocbs=” boot parameter as is commonly displayed in examples, nohz_full accomodates that automatically. 3) Tasks ..."
          ]
        },
        {
          "title": "Understanding Huge Pages",
          "url": "https://www.netdata.cloud/blog/understanding-huge-pages/",
          "excerpts": [
            "May 4, 2023 — Huge pages are a memory management technique used in modern computer systems to improve performance by using larger memory blocks than the default page size."
          ]
        },
        {
          "title": "Issue joining cgroups cpuset with kernel scheduler task \" ...",
          "url": "https://github.com/opencontainers/runc/issues/3922",
          "excerpts": [
            "A random CPU assignment/scheduling occurs when a process enter ( runc in this context) in a cgroup cpuset, before the patch addition, runc was always scheduled ..."
          ]
        },
        {
          "title": "Getting Started With Blazor Scheduler in Web Assembly | Syncfusion",
          "url": "https://blazor.syncfusion.com/documentation/scheduler/how-to/blazor-web-assembly-scheduler",
          "excerpts": [
            "Checkout and learn about getting started with Blazor WebAssembly and Blazor Scheduler component in Visual Studio and much more."
          ]
        },
        {
          "title": "Make your services faster by removing CPU limits",
          "url": "https://news.ycombinator.com/item?id=24351566",
          "excerpts": [
            "Sep 2, 2020 — Removing limits gives your pods more resources (scaling them vertically) whereas creating more pods creates more copies (scaling horizontally)."
          ]
        },
        {
          "title": "Dedicated CPU resources - KubeVirt user guide",
          "url": "https://kubevirt.io/user-guide/compute/dedicated_cpu_resources/",
          "excerpts": [
            "Kubelet's CPUManager policy is set to static - full-pcpus-only; The VM is configured to have an even number of CPUs; dedicatedCpuPlacement and ..."
          ]
        },
        {
          "title": "Fearless Concurrency - The Rust Programming Language",
          "url": "https://doc.rust-lang.org/book/ch16-00-concurrency.html",
          "excerpts": [
            "Fearless concurrency allows you to write code that is free of subtle bugs and is easy to refactor without introducing new bugs."
          ]
        },
        {
          "title": "Why does rust highlight “fearless concurrency” rather than ... - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/181d5v9/why_does_rust_highlight_fearless_concurrency/",
          "excerpts": [
            "The ownership rules, stating that shared mutable access is forbidden, helping prevent data races at compile time. The Send and Sync traits, ..."
          ]
        },
        {
          "title": "LinuxCNC latency and jitter improvements with PREEMPT_RT ...",
          "url": "https://dantalion.nl/2024/09/29/linuxcnc-latency-jitter-kernel-parameter-tuning.html",
          "excerpts": [
            "LinuxCNC control threads can be quite sensitive to system latency and jitter depending on the hardware control interface being used."
          ]
        },
        {
          "title": "Raspberry Pi OS PREEMPT RT 6.13 Kernel Cookbook",
          "url": "https://forum.linuxcnc.org/9-installing-linuxcnc/55048-raspberry-pi-os-preempt-rt-6-13-kernel-cookbook?start=10",
          "excerpts": [
            "Turning off timer_migration did reduce jitter. What I read said that you could make this setting permanent by adding it to the boot command line ..."
          ]
        },
        {
          "title": "WebAssembly: Browser Plugin to the Next Universal Runtime",
          "url": "https://dzone.com/articles/webassembly-from-browser-plugin-to-the-next-univer",
          "excerpts": [
            "Aug 4, 2025 — Explore how WebAssembly and WASI are transforming browsers, cloud, and edge computing with unmatched speed, portability, and secure, ..."
          ]
        }
      ],
      "reasoning": "The field wasm_platform_case_studies is populated by structured descriptions of concrete WebAssembly platforms and their deployment/use cases. Several excerpts provide direct, platform-level information that maps well to the target field:  - wasmCloud is presented as a distributed platform built around an actor-like model with a self-forming lattice powered by NATS, where hosts run components and capability providers and deployment is managed via wadm; this excerpt directly describes platform-name, architecture (actor-based, distributed lattice), and IO/data plane aspects (messaging and capability providers).  - Fermyon Spin is described as a framework for building and running serverless Wasm microservices using Wasmtime, with a component-model architecture and instance-per-request execution; this excerpt clearly provides the platform name, architectural approach, and its concurrency model (rapid instantiation per request, pooling allocator usage in newer versions).  - Faasm is described as a high-performance, stateful serverless runtime with Faaslets (each running in its own OS thread) and a memory-sharing model that enables low-latency concurrency; this excerpt gives a concrete architecture and concurrency model (per-Faaslet threads, shared memory, and Proto-Faaslets for fast cold starts).  - Additional related excerpts discuss wasmCloud and Spin in the kube/cluster contexts, their integration with Kubernetes, and how their hosting runtimes and IO models interact with external systems (Kafka, Redis, etc.), which supports the data-plane IO and distributed deployment aspects.  - Several excerpts explicitly name and discuss deployment/use cases (e.g., multi-tenant serverless, edge/cloud deployment, and integration with data services like Kafka, Redis, PostgreSQL).  Taken together, these excerpts provide strong, corroborated details about platform-name, architecture/scheduling model, and data-plane I/O/usage patterns for wasmCloud, Spin, and Faasm.  Overall, the alignment across multiple independent excerpts increases confidence that the fine-grained field value is well-supported by the provided sources. ",
      "confidence": "high"
    },
    {
      "field": "summary_of_limitations_and_future_outlook",
      "citations": [
        {
          "title": "WebAssembly/wasi-io: I/O Types proposal for WASI",
          "url": "https://github.com/WebAssembly/wasi-io",
          "excerpts": [
            "WASI I/O. A proposed WebAssembly System Interface API. Current Phase. WASI I/O is currently in Phase 3. Champions. Dan Gohman. Portability Criteria. WASI I/O ...",
            " WASI I/O\n\n[]()\n\nA proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI)",
            "WASI I/O is currently in [Phase 3](https://github.com/WebAssembly/WASI/blob/main/Proposals.md) ."
          ]
        },
        {
          "title": "Networking API design · Issue #370 · WebAssembly/WASI - GitHub",
          "url": "https://github.com/WebAssembly/WASI/issues/370",
          "excerpts": [
            "WASI does have a poll_oneoff function for waiting for I/O on multiple handles. It won't scale to very-many handles, but it's likely better than ..."
          ]
        },
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "When WASI 0.3 is launched, implementations may continue to support 0.2, either by implementing 0.3 alongside it, or by virtualizing (polyfilling) 0.2 in terms of 0.3.",
            "*Threads** : first cooperative, then preemptiv",
            "WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025."
          ]
        },
        {
          "title": "According to their roadmap, WASI preview 2 will have ...",
          "url": "https://news.ycombinator.com/item?id=38438239",
          "excerpts": [
            "Threads are actually removed from WASI Preview 2. WASI Preview 2 still doesn't support threads, fork, subprocesses or longjmp/setjmp (among others). Not even ..."
          ]
        },
        {
          "title": "Develop with WasmEdge, Wasmtime, and Wasmer Invoking ...",
          "url": "https://paul-parkinson.medium.com/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-ad2302fe961a",
          "excerpts": [
            "WASM (WebAssembly): A W3C standard for low-level, performance-optimized bytecode that is unmatched for portability, security, and speed."
          ]
        },
        {
          "title": "Wasmtime 1.0: A Look at Performance",
          "url": "https://bytecodealliance.org/articles/wasmtime-10-performance",
          "excerpts": [
            "Sep 6, 2022 — According to the benchmarks, this change improved the speed of stack walking by between 64% and 99.95%, depending on the situation, and it ...",
            "Sep 6, 2022 — wasm went from about 2 milliseconds (eager initialization of all heap and other data structures) to 5 microseconds, or 400 times faster. Not bad ..."
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said.",
            "b/main/README.md) is used to run WASM applications. It supports interpreter mode, ahead-of-time compilation (AOT) mode and just-in-time compilation (JIT) modes, LLVM JIT and Fast JIT, according to the project’s documentation."
          ]
        },
        {
          "title": "The Significance of In-broker Data Transformations in Streaming Data",
          "url": "https://medium.com/event-driven-utopia/the-significance-of-in-broker-data-transformations-in-streaming-data-9292e440beaa",
          "excerpts": [
            "That means developers can code web client applications in a programming language of their choice, compile them down to Wasm, and run them inside the browser at near-native speed.",
            "What if we use Wasm to ship code to server-side applications, especially to brokers, in the same way it ships code to the user’s computer?",
            ". Redpanda, the Kafka-API-compatible streaming data platform, recently released the [Redpanda Data Transforms Sandbox](https://redpa",
            "Redpanda data transforms provide a framework to create, build, and deploy inline data transformations on data written to Redpanda topics.",
            "Redpanda data transforms provide a framework to create, build, and deploy inline data transformations on data written to Redpanda topics.",
            "The sandbox contains a Go-based SDK seamlessly integrated within [rpk](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), Redpanda’s [CLI experience](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), and a single Redpanda broker (Docker container) that can deploy and run your transform functions on topics you create inside the fully-contained sandbox.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic."
          ]
        },
        {
          "title": "WarpDiff and Kafka WASM in server-side Wasm runtimes (arXiv:2309.12167)",
          "url": "https://arxiv.org/abs/2309.12167",
          "excerpts": [
            " WarpDiff to identify performance issues in server-side Wasm runtimes. It has recently been used increasingly on the server side, e.g., providing a safer, faster, and more portable alternative to Linux containers."
          ]
        }
      ],
      "reasoning": "The core bottlenecks described in the field value are: (1) lack of native asynchronous I/O in WASI 0.2, (2) host-driven scheduling constraints that prevent Wasm from independently controlling real-time aspects, (3) the impact of synchronous, blocking I/O on concurrency, and (4) a roadmap indicating WASI 0.3 and the WebAssembly Component Model are expected to address these gaps. Several excerpts address these points directly: the statement that WASI 0.3 is expected to introduce native async support and allow WASI reads to await data shows a planned fix to the I/O bottleneck; the note that Wasmtime's WASI calls are implemented asynchronously at the runtime layer but appear synchronous to the WASM module explains the current mismatch between host/runtime design and guest-model expectations; the historical remark that scheduling interaction with the host is unclear and deferred highlights the scheduling limitation as a core issue; the reference to the async-capable Rust/Tokio path for implementing WASI calls clarifies the architectural work needed to improve concurrency; the roadmap excerpts plainly indicate WASI 0.3 previews and the Component Model are planned to address concurrency and async I'll-need to rely on future specs such as streams/futures to enable non-blocking I/O and better parallelism; the component model discussion illustrates a broader direction toward composable concurrency which underpins the field's outlook. Direct quotes from these excerpts show: the native async support being introduced in WASI 0.3; the current async implementation at the runtime layer versus synchronous guest perception; the uncertainty about scheduling interaction and the need for a use case; and the explicit roadmap items for WASI 0.3 and component model timelines. The most relevant parts are therefore the explicit future-oriented statements about native async I/O in WASI 0.3 and component-model-driven concurrency, followed by the concrete observations about current synchronous vs asynchronous hosting implementations and scheduling questions. Given this, the top relevance comes from passages that tie the bottlenecks to WASI 0.3's async I/O and the Component Model's concurrency model, with secondary relevance to runtime-level async implementations and scheduling questions.",
      "confidence": "high"
    },
    {
      "field": "wasi_capability_and_io_model",
      "citations": [
        {
          "title": "Capabilities-Based Security with WASI - WebAssembly",
          "url": "https://medium.com/webassembly/capabilities-based-security-with-wasi-c523a34c1944",
          "excerpts": [
            "WASI implements capabilities-based security by granting WebAssembly modules only explicitly assigned accesses. Instead of global user rights ..."
          ]
        },
        {
          "title": "WebAssembly System Interface (WASI) Documentation",
          "url": "https://github.com/WebAssembly/WASI",
          "excerpts": [
            "WASI is designed with capability-based security principles, using the\nfacilities provided by the Wasm component model . All access to external\nresources is provided by capabilities",
            "WASI Preview 2 is now stable, and is a modular collection of\nAPIs defined with the Wit IDL , and it incorporates many of the lessons\nlearned from Preview 1, including adding support for a wider range of\nsource languages, modularity, a more expressive type system,\nvirtualizability, and more.",
            "Preview 1 , an API using\nthe witx IDL, and it is now widely used. Its major influences are POSIX and\nCloudABI. WASI"
          ]
        },
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "WASI 0.3.0 will add **native async support** to the Component Model and refactor WASI 0.2 interfaces to take advantage of native as",
            "This includes explicit `stream<T>` and `future<T>` types for use anywhere in function parameters and results.",
            "*Threads** : first cooperative, then preemptiv",
            "WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025."
          ]
        },
        {
          "title": "From Two-Phase Compilation to POSIX-like Environments ...",
          "url": "https://javascript.plainenglish.io/webassemblys-complete-system-interface-evolution-from-two-phase-compilation-to-posix-like-491b36eeffc8",
          "excerpts": [
            "Jun 4, 2025 — The two-phase compilation model provides the foundation for WASI's system interface capabilities: System Call Translation. In the two-phase ..."
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said."
          ]
        },
        {
          "title": "Why WASI Preview 2 Makes WebAssembly Production ...",
          "url": "https://thenewstack.io/why-wasi-preview-2-makes-webassembly-production-ready/",
          "excerpts": [
            "WASI Preview 2 is the missing link that Wasm needed to become a viable option for production use cases."
          ]
        },
        {
          "title": "examples-interrupting-wasm.md",
          "url": "https://cocalc.com/github/bytecodealliance/wasmtime/blob/main/docs/examples-interrupting-wasm.md",
          "excerpts": [
            "Epoch-based interruption imposes relatively low overhead on Wasm execution; it has been measured at around a 10% slowdown. It is faster than fuel-based interruption.",
            "Fuel-based interruption is completely deterministic: the same program run with the same amount of fuel will always be interrupted at the same location in the program (unless it has enough fuel to complete its computation, or there is some other form of non-determinism that causes the program to behave differently). The downside is that fuel-based interruption imposes more overhead on execution, slowing down Wasm programs, than epochs do."
          ]
        }
      ],
      "reasoning": "- The most directly relevant evidence states that WASI uses a capability-based security approach, granting access only through explicit capabilities and keeping resources protected by design. This aligns with the finegrained field's emphasis on a capability-based security model governing WASI-enabled modules. The cited passages describe security as being built on capabilities and the protective model that WASI provides. - Additional strong support comes from passages describing WASI's security foundation and how capabilities govern resource access, reinforcing the field's claim about WASI using a capability-based model. - For the WASI 0.3 roadmap and async interfaces, the excerpts explicitly mention upcoming WASI 0.3 previews with native async support and component-model integrations, which matches the field value's note that WASI 0.3 is planned for late 2025 and that async capabilities are a focus. - Roadmap-oriented excerpts also discuss WASI 0.3 in the context of async support and component-model evolution, which supports the field's assertion about future capabilities in I/O and concurrency. - The field's mention of current status versus future targets is addressed by excerpts that contrast WASI 0.2 (Preview 2) stability with forward-looking WASI 0.3 previews and async work, helping to situate the field's statements about ongoing development. - Performance bottlenecks and mitigations are touched on by excerpts describing I/O-related performance costs in WASI runtimes and references to improvements or architectural shifts (e.g., asynchronous hosting, component-model integrations) that aim to reduce latency and improve throughput, which supports the field's notes on performance constraints and mitigation strategies. - Some excerpts also discuss broader WASI ecosystem progress (e.g., component model, WIT, and futures like async), which contextualizes the field value's emphasis on async-native interfaces and cross-component data exchange. In sum, the strongest support comes from explicit statements about capability-based security and WASI's security model, plus roadmap-language and deployment-status notes about WASI 0.3 and async IO progression, with additional corroboration from performance-context discussions.",
      "confidence": "high"
    },
    {
      "field": "dedicated_resource_allocation_methods",
      "citations": [
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified ",
            " cpusets instead to disable scheduler load\n                          balancing through the \"cpuset.sched_load_balance\" file. It offers a much more flexible interface where CPUs can\n                "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        },
        {
          "title": "Manage HugePages - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/",
          "excerpts": [
            "Kubernetes supports the allocation and consumption of pre-allocated huge pages by applications in a Pod. This page describes how users can consume huge pages."
          ]
        },
        {
          "title": "Container Runtimes - Kubernetes",
          "url": "https://kubernetes.io/docs/setup/production-environment/container-runtimes/",
          "excerpts": [
            "If you configure systemd as the cgroup driver for the kubelet, you must also configure systemd as the cgroup driver for the container runtime.",
            "Jun 30, 2025 — The systemd cgroup driver is recommended if you use cgroup v2. Note: If you installed containerd from a package (for example, RPM or .deb ) ..."
          ]
        },
        {
          "title": "Kubernetes CPU Manager Policies - the StarlingX Documentation",
          "url": "https://docs.starlingx.io/admintasks/kubernetes/kubernetes-cpu-manager-policies.html",
          "excerpts": [
            "When using the static policy, improved performance can be achieved if you also use the Isolated CPU behavior as described at Isolating CPU Cores to Enhance ..."
          ]
        },
        {
          "title": "Kubernetes Node Resource Managers",
          "url": "https://kubernetes.io/docs/concepts/policy/node-resource-managers/",
          "excerpts": [
            " deployments. If the `strict-cpu-reservation` policy option is enabled, the static policy will not allow\nany workload to use the CPU cores specified in `reservedSystemCPUs",
            "Static policy options",
            "Here are the available policy options for the static CPU management policy,",
            "\n\n`align-by-socket` (alpha, hidden by default)",
            "\n\n`distribute-cpus-across-cores` (alpha, hidden by default)",
            "\n\n`distribute-cpus-across-numa` (beta, visible by default)",
            "\n\n`full-pcpus-only` (GA, visible by default)",
            "\n\n`strict-cpu-reservation` (beta, visible by default)",
            "The `reservedSystemCPUs` parameter in [KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/) ,\nor the deprecated kubelet command line option `--reserved-cpus` , defines an explicit CPU set for OS system daemons\nand kubernetes system daemons."
          ]
        },
        {
          "title": "Control CPU Management Policies on the Node - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/",
          "excerpts": [
            "Static policy options",
            "Static policy options",
            "This is because zero CPU reservation would allow the shared\npool to become empty.",
            "The following policy options exist for the static `CPUManager` policy:",
            "* `full-pcpus-only` (GA, visible by default) (1.33 or higher)",
            "* `distribute-cpus-across-numa` (beta, visible by default) (1.33 or higher)",
            "* `align-by-socket` (alpha, hidden by default) (1.25 or higher)",
            "* `distribute-cpus-across-cores` (alpha, hidden by default) (1.31 or higher)",
            " higher)\n* `strict-cpu-reservation` (beta, visible by default) (1.32 or higher)",
            "* `prefer-align-cpus-by-uncorecache` (alpha, hidden by default) (1.32 or higher)"
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpts enumerate concrete CPU/memory isolation mechanisms and how to implement them in real systems. For example, kernel command-line parameters specify how to pin CPUs and control tick behavior (isolcpus, nohz_full), and memory-related controls appear via Memory-related interfaces and guards. This aligns with the field value's depiction of dedicated resource allocation in the Linux kernel/cgroups space, including how to set cpus and memory limits and bind memory to NUMA nodes. The excerpts also discuss exact interface points like cpuset.cpus, cpu.max, memory.max, and memory.high, which map to the field value's described methods for dedicating CPU and memory resources via the kernel's cgroup and cpuset controllers in a live system. The systemd excerpts provide concrete directives (AllowedCPUs, MemoryMax, MemoryHigh, AllowedMemoryNodes) that match the field's second environment: systemd, where resource limitations are expressed through unit directives rather than kernel file writes, demonstrating the same dedication of resources at the service level. The Kubernetes excerpts show policy-based approaches to guarantees and isolation at the orchestration layer: CPU Manager static policy, reserved CPU sets, and NUMA/topology hints that map to the field's Kubernetes row (CPU Manager static policy with explicit, dedicated cores and memory considerations). The Kubernetes excerpts also mention strict-cpu-reservation and related policy options, which correspond to the field's \"Kubernetes\" line in the value, providing concrete, policy-driven mechanisms to ensure exclusive resources for Wasm workloads. Overall, the strongest alignment comes from excerpts that name exact knobs (cpusets, cpu.max, memory.max, memory.high, cpuset.mems) and provide explicit configuration practices (systemd directives, kernel boot parameters, Kubernetes CPU Manager/static policy) that implement the field's dedicated_resource_allocation_methods across the three environments. The Kubernetes policy excerpts supplement the Linux kernel/systemd references by showing how allocation is achieved in orchestration contexts, reinforcing the field's cross-environment focus. Overall confidence is reinforced by multiple, coherent sources across all three environments, though some kernel/systemd specifics are more prescriptive (file-based writes) while others are prescriptive in unit files (systemd directives).",
      "confidence": "high"
    },
    {
      "field": "deployment_patterns_comparison.pattern",
      "citations": [
        {
          "title": "Runwasi Benchmarks Documentation",
          "url": "https://runwasi.dev/benchmarks.html",
          "excerpts": [
            "| Runtime | Tasks | Time | Throughput |"
          ]
        },
        {
          "title": "What is runwasi",
          "url": "https://www.nigelpoulton.com/post/what-is-runwasi",
          "excerpts": [
            "Dec 1, 2022 — It allows Wasm to ride the Kubernetes wave, as well as enabling Kubernetes to boldly go where it couldn't go with containers – resource ..."
          ]
        }
      ],
      "reasoning": "The target value mentions RunWasi shims implemented with specific Wasmtime or WasmEdge runtimes. Excerpts that explicitly reference RunWasi describe its role and usage, including how it relates to running WebAssembly in environments like Kubernetes, which directly informs deployment-pattern considerations for RunWasi shims. The benchmark documentation for RunWasi provides concrete context about performance characteristics and comparisons that are pertinent to evaluating deployment patterns and practical use. While other excerpts discuss WASI status and runtime capabilities more broadly, they do not directly confirm the specific RunWasi shims deployment pattern with those runtimes, but they offer useful background on evolving thread support and host integration that can influence deployment decisions. Collectively, the strongest support comes from explicit mentions of RunWasi and its deployment implications, with additional context from benchmark-oriented and runtime-status information supporting broader feasibility and performance considerations.",
      "confidence": "medium"
    },
    {
      "field": "kernel_level_and_os_bypass_strategies.architecture_summary",
      "citations": [
        {
          "title": "WALI: A thin Linux kernel interface for WebAssembly",
          "url": "https://popl24.sigplan.org/details/waw-2024-papers/2/WALI-A-thin-Linux-kernel-interface-for-WebAssembly",
          "excerpts": [
            "This talk introduces WALI: The WebAssembly Linux Interface, a thin layer over Linux's userspace system calls, creating a new class of virtualization."
          ]
        },
        {
          "title": "Running WebAssembly on the Kernel",
          "url": "https://blog.wasmer.io/running-webassembly-on-the-kernel-8e04761f1d8e",
          "excerpts": [
            "Take a look at them to learn how to do high-performance networking in [kernel-wasm](https://github.com/wasmerio/kernel-wasm) .",
            "Load the modules into the kernel:",
            "> Make sure you are running on the latest version ( `0.4.2` ) by executing",
            "`wasmer self-update`",
            "When running Wasmer, select the `kernel` loader and `singlepass` backend:",
            "ate`\n\n```\nsudo wasmer run --backend singlepass --loader kernel the_file.wasm\n```",
            "### Hope you enjoyed reading this article! While running WebAssembly in the Kernel is certainly dangerous and not the recommended approach for most use cases, it helped us to experiment and learn from the experience first hand."
          ]
        },
        {
          "title": "PERFORMANCE ANALYSIS OF AF_XDP SOCKETS ON ...",
          "url": "https://openscholar.uga.edu/record/2663/files/ReillyJosephMS.pdf",
          "excerpts": [
            "This thesis analyzes the performance of AF_XDP sockets on commodity hardware such as Netronome and Mellanox. Smart NICs. Testing packet delivery speeds was ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant information comes from excerpts describing embedding or running a WebAssembly runtime inside the OS kernel or near-kernel environments. This includes a description of WALI as a thin Linux kernel interface for WebAssembly, which directly aligns with kernel-level execution as a means to eliminate user-to-kernel context switches and data copies for system calls. Similarly, the kernel-focused discussion on running WebAssembly, including kernel loaders and ensuring up-to-date kernel tooling, speaks to kernel-level deployment of Wasm runtimes. The OS-bypass networking angle is addressed by excerpts discussing AF_XDP and NIC access from userspace, which maps to bypassing the traditional kernel network stack for ultra-low-latency packet processing. Other kernel-oriented notes, such as using a kernel loader and singlepass backend when running Wasmer in kernel mode, reinforce the kernel-by-default approach and its practical steps. Additional kernel-oriented excerpts mention running Wasm in the kernel and related tooling; those collectively support the claim that kernel-level execution and OS-bypass networking are viable pathways for reducing overhead in Wasm workloads. Finally, some excerpts describe the broader Wasm runtimes and performance-oriented tooling, which provides context but is less directly tied to kernel-level or OS-bypass strategies, though still relevant to the overarching theme of high-performance, low-overhead Wasm deployments.\n",
      "confidence": "high"
    },
    {
      "field": "kernel_level_and_os_bypass_strategies.strategy",
      "citations": [
        {
          "title": "Running WebAssembly on the Kernel",
          "url": "https://blog.wasmer.io/running-webassembly-on-the-kernel-8e04761f1d8e",
          "excerpts": [
            "Take a look at them to learn how to do high-performance networking in [kernel-wasm](https://github.com/wasmerio/kernel-wasm) .",
            "Load the modules into the kernel:",
            "When running Wasmer, select the `kernel` loader and `singlepass` backend:",
            "ate`\n\n```\nsudo wasmer run --backend singlepass --loader kernel the_file.wasm\n```",
            "### Hope you enjoyed reading this article! While running WebAssembly in the Kernel is certainly dangerous and not the recommended approach for most use cases, it helped us to experiment and learn from the experience first hand."
          ]
        },
        {
          "title": "WALI: A thin Linux kernel interface for WebAssembly",
          "url": "https://popl24.sigplan.org/details/waw-2024-papers/2/WALI-A-thin-Linux-kernel-interface-for-WebAssembly",
          "excerpts": [
            "This talk introduces WALI: The WebAssembly Linux Interface, a thin layer over Linux's userspace system calls, creating a new class of virtualization."
          ]
        },
        {
          "title": "PERFORMANCE ANALYSIS OF AF_XDP SOCKETS ON ...",
          "url": "https://openscholar.uga.edu/record/2663/files/ReillyJosephMS.pdf",
          "excerpts": [
            "This thesis analyzes the performance of AF_XDP sockets on commodity hardware such as Netronome and Mellanox. Smart NICs. Testing packet delivery speeds was ..."
          ]
        },
        {
          "title": "The WebAssembly Linux Interface - NASA/ADS",
          "url": "https://ui.adsabs.harvard.edu/abs/2023arXiv231203858R/abstract",
          "excerpts": [
            "by A Ramesh · 2023 · Cited by 2 — We present an implementation of WALI in a modern WebAssembly engine and evaluate its performance on a number of applications which we can now compile with ..."
          ]
        },
        {
          "title": "WASI Cycles, an Open Source, 3D WebXR Game - Oracle Blogs",
          "url": "https://blogs.oracle.com/developers/post/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-open-source-3d-webxr-game",
          "excerpts": [
            "WASI Invoking Kafka, MongoDB, and Oracle Database"
          ]
        },
        {
          "title": "wasmerio/wasmer: 🚀 Fast, secure, lightweight containers ...",
          "url": "https://github.com/wasmerio/wasmer",
          "excerpts": [
            "Wasmer is a blazing fast and secure WebAssembly runtime that enables incredibly lightweight containers to run anywhere: from Desktop to the Cloud, Edge and ..."
          ]
        },
        {
          "title": "Wasmer: Universal applications using WebAssembly",
          "url": "https://wasmer.io/",
          "excerpts": [
            "Run your apps. Secure. Fast. At Scale. Experience the new container technology powered by WebAssembly that runs your programs safely, anywhere."
          ]
        },
        {
          "title": "WasmEdge Runtime",
          "url": "https://github.com/wasmedge",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "WasmEdge",
          "url": "https://wasmedge.org/",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "WasmEdge is a lightweight, high-performance ...",
          "url": "https://github.com/WasmEdge/WasmEdge",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "Why WASI Preview 2 Makes WebAssembly Production ...",
          "url": "https://thenewstack.io/why-wasi-preview-2-makes-webassembly-production-ready/",
          "excerpts": [
            "WASI Preview 2 is the missing link that Wasm needed to become a viable option for production use cases.",
            "Why WASI Preview 2 Makes WebAssembly Production Ready"
          ]
        }
      ],
      "reasoning": "The most relevant excerpts explicitly discuss kernel-level execution or kernel-oriented WASM deployment. The kernel-focused entries describe running WebAssembly in the kernel, loading modules into the kernel, and selecting appropriate kernel-oriented backends, which directly map to kernel-level execution and OS-bypass strategies. References describing a thin Linux kernel interface for WebAssembly and real-world experimentation with kernel runtimes provide concrete approaches to bypass traditional user-space scheduling for WASM workloads. AF_XDP-related performance work demonstrates concrete OS-bypass techniques for high-throughput networking, illustrating practical benefits and considerations when operating in kernel-space or with kernel-bypass paths. Additional excerpts discuss implementations and deployments (e.g., WALI, WASI-based runtimes) that relate to enabling kernel-level or near-Kernel execution contexts for WASM, reinforcing the relevance of kernel-level strategies. Some excerpts offer broader performance discussions or ecosystem perspectives (e.g., general Wasm runtimes), which provide context but are secondary to the explicit kernel/OS-bypass strategies.\n",
      "confidence": "high"
    },
    {
      "field": "kernel_level_and_os_bypass_strategies.security_tradeoffs",
      "citations": [
        {
          "title": "Running WebAssembly on the Kernel",
          "url": "https://blog.wasmer.io/running-webassembly-on-the-kernel-8e04761f1d8e",
          "excerpts": [
            "Take a look at them to learn how to do high-performance networking in [kernel-wasm](https://github.com/wasmerio/kernel-wasm) .",
            "Load the modules into the kernel:",
            "When running Wasmer, select the `kernel` loader and `singlepass` backend:",
            "ate`\n\n```\nsudo wasmer run --backend singlepass --loader kernel the_file.wasm\n```",
            "### Hope you enjoyed reading this article! While running WebAssembly in the Kernel is certainly dangerous and not the recommended approach for most use cases, it helped us to experiment and learn from the experience first hand."
          ]
        },
        {
          "title": "WALI: A thin Linux kernel interface for WebAssembly",
          "url": "https://popl24.sigplan.org/details/waw-2024-papers/2/WALI-A-thin-Linux-kernel-interface-for-WebAssembly",
          "excerpts": [
            "This talk introduces WALI: The WebAssembly Linux Interface, a thin layer over Linux's userspace system calls, creating a new class of virtualization."
          ]
        },
        {
          "title": "Networking API design · Issue #370 · WebAssembly/WASI - GitHub",
          "url": "https://github.com/WebAssembly/WASI/issues/370",
          "excerpts": [
            "WASI does have a poll_oneoff function for waiting for I/O on multiple handles. It won't scale to very-many handles, but it's likely better than ..."
          ]
        },
        {
          "title": "WASI Cycles, an Open Source, 3D WebXR Game - Oracle Blogs",
          "url": "https://blogs.oracle.com/developers/post/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-open-source-3d-webxr-game",
          "excerpts": [
            "WASI Invoking Kafka, MongoDB, and Oracle Database"
          ]
        }
      ],
      "reasoning": "The most relevant content directly references running WebAssembly in the kernel and related kernel-level strategies. Statements describing kernel-wasm, loading modules into the kernel, and selecting a kernel-backed loader demonstrate the existence of kernel-level execution paths that bypass traditional user-space OS abstractions, which aligns with evaluating kernel-level security tradeoffs. The cautious note that running Wasm in the Kernel can be dangerous and is not the recommended approach for most use cases provides direct support for the security tradeoff concerns. WALI is also relevant as it describes a thin kernel interface for WebAssembly, suggesting a design space where kernel-level execution and sandboxing interact with OS interfaces. Content mentioning high-performance networking in kernel-wasm examples further ties into the performance-security balance of OS-bypass strategies. Networking-oriented excerpts discuss lower-level I/O interfaces and capabilities (e.g., poll_oneoff), which informs how OS-bypass I/O might be exposed to Wasm modules, relevant to the io_uring discussion and shared memory considerations. Contextual references to Kafka/MongoDB in WASI literature provide related use cases that stress security and portability considerations when using Wasm in more advanced data-processing pipelines, supporting the security tradeoff narrative. Overall, the strongest support comes from explicit kernel-wasm discussions and cautions about the risks, with supplementary context from kernel-loader and WALI discussions, and peripheral networking/I/O considerations for tradeoffs.",
      "confidence": "medium"
    },
    {
      "field": "observability_and_performance_engineering.tracing",
      "citations": [
        {
          "title": "Unlocking Otel in WebAssembly - Wasm I/O 2025",
          "url": "https://www.fermyon.com/blog/unlocking-otel-in-wasm",
          "excerpts": [
            "\nThe [WASI-OTel proposal](https://github.com/calebschoepp/wasi-otel) represents a significant step forward for WebAssembly observability. Currently in Phase 0 of the WASI process, it provides:\n\n* **Standardized interfaces** for telemetry data exchange between host and guest\n* **Processor callbacks** (onStart, onEnd) that notify the host of guest span lifecycle events\n* **Context propagation** methods that enable proper trace parenting\n\nThe proposal is wrapped in language-specific SDKs (starting with Rust) that make it transparent for developers to use standard OpenTelemetry APIs while benefiting from WebAssembly-specific optimizations",
            "\nOne of WebAssembly’s superpowers is the ability to provide automatic instrumentation without much developer effort. Caleb shows this using [Spin](https://spinframework.dev/) , the serverless WebAssembly framework for developing apps. He shows how a simple Rust component automatically generates rich tracing data.",
            "While auto-instrumentation provides valuable baseline visibility, production apps need custom tracing that reflects business logic and application-specific workflows. This is where OpenTelemetry’s standard APIs _shine_ .",
            "Next steps include:\n\n* Advancing WASI-OTel through the WASI standardization process\n* Expanding language support beyond Rust\n* Adding metrics and logs support (currently focused on tracing)\n* Gaining broader runtime support beyond Spin"
          ]
        },
        {
          "title": "Observability in WebAssembly | Dylibso Developer Resources",
          "url": "https://dev.dylibso.com/docs/observe/overview",
          "excerpts": [
            "Dylibso provides observability SDKs for WebAssembly (Wasm), enabling continuous\nmonitoring of WebAssembly code as it executes within a runtime. It provides\ndevelopers with the tools necessary to capture and emit telemetry data from Wasm\ncode, including function execution and memory allocation traces, logs, and\nmetrics.",
            " Emit WebAssembly telemetry to many supported platforms\nWhether you use a managed platform such as Datadog, or a self-hosted solution\nlike Jaeger, Observe SDK Adapters make it easy to send Wasm telemetry wherever\nit needs to go. See the officially supported platforms by choosing\nwhich language library you would use. Open Source\nThis GitHub repository contains the\nRuntime SDKs and the Adapters necessary to have live profiling & tracing, logs,\nand metrics."
          ]
        },
        {
          "title": "Wasmer Profiling and Observability",
          "url": "https://docs.wasmer.io/runtime/cli",
          "excerpts": [
            "Profiling WebAssembly Code []()\n\nYou can enable profiling data generation for your WebAssembly code using the `--profiler` option:\n\n```\nwasmer run my_program.wasm --profiler=perfmap\n```\n\nThis generates profiling information in the [perfmap format (opens in a new tab)](https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/jit-interface.txt) , which can be used with Linux perf tools to analyze the performance of your WebAssembly code. The profiler data is written to `/tmp/perf-{PID}.map` . This option is available for the Cranelift, LLVM, and Singlepass compiler backends. ## Help"
          ]
        },
        {
          "title": "Wasmtime profiling API (profiling.h)",
          "url": "https://docs.wasmtime.dev/c-api/profiling_8h.html",
          "excerpts": [
            "The most straightforward way to do that is to call it from a callback registered with [wasmtime\\_store\\_epoch\\_deadline\\_callback](store_8h.html \"Configures epoch deadline callback to C function.\") . For more information see the Rust documentation at: <https://docs.wasmtime.dev/api/wasmtime/struct.GuestProfiler.h",
            "profiling.h File Reference",
            "Collects basic profiling data for a single WebAssembly guest."
          ]
        },
        {
          "title": "Wasmtime Guest Profiler and Rust+Wasm profiling",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.GuestProfiler.html",
          "excerpts": [
            "The data collection granularity is limited by the mechanism you use to\ninterrupt guest execution and collect a profiling sample.",
            "If you use epoch interruption, then samples will only be collected at\nfunction entry points and loop headers.",
            "The only current requirement is that the guest\nyou wish to profile must be on the same stack where you call `sample` ,\nand executing within the same thread."
          ]
        },
        {
          "title": "Profiling WebAssembly - Wasmtime Documentation (examples-profiling.html)",
          "url": "https://docs.wasmtime.dev/examples-profiling.html",
          "excerpts": [
            "On Linux, we support [perf](./examples-profiling-perf.html) . * \n  For Intel's x86 CPUs on Linux or Windows, we support [VTune](./examples-profiling-vtune.html) . * \n  For Linux and macOS, we support [samply](./examples-profiling-samply.html) . * \n  For everything else, see the [cross-platform\n  profiler](./examples-profiling-guest.html) . The native profilers can measure time spent in WebAssembly guest code as well as\ntime spent in the Wasmtime host and potentially even time spent in the"
          ]
        },
        {
          "title": "Linux eBPF Off-CPU Flame Graph",
          "url": "https://www.brendangregg.com/blog/2016-01-20/ebpf-offcpu-flame-graph.html",
          "excerpts": [
            "Seeing both on-CPU and off-CPU flame graphs shows you the full picture: what's consuming CPUs, what's blocked, and by how much."
          ]
        },
        {
          "title": "wasmCloud 0.82: WASI 0.2, OTEL logging, and more",
          "url": "https://wasmcloud.com/blog/wasmcloud-0.82-wasi-p2-is-here/",
          "excerpts": [
            "The Docker Compose files are [available on GitHu"
          ]
        },
        {
          "title": "Performance in the spotlight: WebAssembly profiling for ...",
          "url": "https://dispatch.run/blog/performance-in-the-spotlight-webassembly-profiling-for-everyone",
          "excerpts": [
            "May 11, 2023 — We developed wzprof, a cross-language CPU and memory profiler for WebAssembly, laying the grounds for what could become a universal profiler."
          ]
        },
        {
          "title": "bpftrace/bpftrace: High-level tracing language for Linux - GitHub",
          "url": "https://github.com/bpftrace/bpftrace",
          "excerpts": [
            "bpftrace is a high-level tracing language for Linux. bpftrace uses LLVM as a backend to compile scripts to eBPF-bytecode and makes use of libbpf and bcc."
          ]
        },
        {
          "title": "One-Liner Tutorial - bpftrace",
          "url": "https://bpftrace.org/tutorial-one-liners",
          "excerpts": [
            "Lesson 10. Scheduler Tracing​ · sched: The sched category has tracepoints for different kernel CPU scheduler events: sched_switch, sched_wakeup, ..."
          ]
        },
        {
          "title": "Beyond Benchmarks: What Kernel-Level Observability ...",
          "url": "https://medium.com/@chuka-uzo-j/beyond-benchmarks-what-kernel-level-observability-taught-us-about-running-llms-on-the-edge-153ed8ac1f24",
          "excerpts": [
            "WasmEdge maintained lower cycle counts while yielding better structured output by leveraging deterministic memory prefetching and reduced ..."
          ]
        },
        {
          "title": "offcputime - Summarize off-CPU time by kernel stack trace. ...",
          "url": "https://manpages.ubuntu.com/manpages/focal/man8/offcputime-bpfcc.8.html",
          "excerpts": [
            "This program shows stack traces and task names that were blocked and \"off-CPU\", and the total duration they were not running: their \"off-CPU time\"."
          ]
        },
        {
          "title": "Kubernetes Networking: eBPF in Action | by Quân Huỳnh - Medium",
          "url": "https://medium.com/@hmquan08011996/kubernetes-networking-ebpf-in-action-f0df2592dade",
          "excerpts": [
            "Missing: runqlat cgroup v2"
          ]
        },
        {
          "title": "A thorough introduction to bpftrace",
          "url": "https://www.brendangregg.com/blog/2019-08-19/bpftrace.html",
          "excerpts": [
            "Aug 19, 2019 — In this post I'll summarize the bpftrace language, variable types, probes, and tools. bpftrace uses BPF (Berkeley Packet Filter), an in-kernel ..."
          ]
        },
        {
          "title": "Calling kfuncs from bpftrace #3167",
          "url": "https://github.com/bpftrace/bpftrace/discussions/3167",
          "excerpts": [
            "I'm working on a runqlat program and I need to filter out some tasks based on the info the the task struct. The sched_new and sched_switch hooks only provide ..."
          ]
        },
        {
          "title": "Coroot blog: runqlat and runqslower - eBPF command line tools",
          "url": "https://coroot.com/blog/engineering/runqlat-and-runqslower-ebpf-command-line-tools/",
          "excerpts": [
            "In this blog post we will look at runqlat and runqslower commands. They are available in both BCC and bpftrace tool collections.",
            " runqlat and runqslower - eBPF command line tools",
            "Tracing run queue latency… Hit Ctrl-C to end.",
            "Note we have significantly more cases when the process had to wait for 50ms or longer to be scheduled. In this case we’re looking at the run queue latency / scheduling latency for system overall, where it is common to be interested only latency impact for specific process or cgroup – the tools also allows it (see Command Line Options Section)"
          ]
        }
      ],
      "reasoning": "- Excerpt emphasizing the WASI-OTel proposal explicitly cites standardized interfaces for telemetry exchange between host and guest and context propagation, and notes OpenTelemetry APIs integration in Rust and Wasm ecosystems. This directly supports the field value's claim that OpenTelemetry methodologies within Wasm enable distributed tracing and that WASI-OTel proposes closer host integration for performance data collection. \n- Excerpt describing unlocking Otel in WebAssembly shows a concrete example of how OpenTelemetry instrumentation is applied in Wasm via Rust components to generate rich tracing data, reinforcing the notion of distributed tracing within Wasm and the practical path via OpenTelemetry. \n- Excerpt noting that OpenTelemetry APIs shine for production tracing further corroborates the value by highlighting the role of OpenTelemetry in business-logic tracing within Wasm. \n- Excerpts about observability in WebAssembly with SDKs and adapters detail the tooling around collecting and exporting telemetry (traces, metrics, logs) from Wasm, which aligns with the idea of distributed tracing and integration patterns mentioned in the field value. \n- Additional excerpts touching on profiling and generic observability tooling are supportive context but do not directly advance the OpenTelemetry/distributed tracing narrative, so they are considered less central to the precise finegrained field value.\n",
      "confidence": "high"
    },
    {
      "field": "deployment_patterns_comparison.startup_time",
      "citations": [
        {
          "title": "Runwasi Benchmarks Documentation",
          "url": "https://runwasi.dev/benchmarks.html",
          "excerpts": [
            "| Runtime | Tasks | Time | Throughput |"
          ]
        }
      ],
      "reasoning": "The finegrained field value is about exceptionally fast startup times and benchmarking comparisons for WASI runtimes (Wasmtime) versus containerized setups. The most directly relevant excerpt is the one labeled Benchmarks Documentation, which implies it contains benchmark data for runtimes like runwasi and Wasmtime, aligning with the claimed metrics (cold start times, speedups, and comparative performance under different deployment patterns). Even though the exact text from the excerpt isn't shown here, benchmark-focused content is the primary source you would expect to substantiate startup-time claims such as sub-millisecond cold starts and comparative throughput or task metrics in different environments. Other excerpts discuss WASI ecosystem status, threading support, and integrations, which provide helpful surrounding context but do not directly address startup-time performance in the same explicit way.",
      "confidence": "high"
    },
    {
      "field": "wasm_runtime_performance_comparison.runtime_name",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop."
          ]
        },
        {
          "title": "Wasmtime Scheduling and Threads Documentation",
          "url": "https://docs.wasmtime.dev/c-api/config_8h.html",
          "excerpts": [
            "Whether or not epoch-based interruption is enabled for generated code. This setting is false by default.",
            "When enabled wasm code will check the current epoch periodically and abort if the current epoch is beyond a store-configured limit.",
            "Note that when this setting is enabled all stores will immediately trap and need to have their epoch deadline otherwise configured with [wasmtime\\_context\\_set\\_epoch\\_dead"
          ]
        },
        {
          "title": "Wasmtime Scheduling and Epoch Interruption",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/struct.Store.html",
          "excerpts": [
            "This setting is intended to allow for coarse-grained\ninterruption, but not a deterministic deadline of a fixed,\nfinite interval. For deterministic interruption, see the\n“fuel” mechanism instead.",
            "Configures an async function that runs on calls and returns between WebAssembly and host code. For the non-async equivalent of this method, see Store::call_hook ..."
          ]
        },
        {
          "title": "Wasmtime Examples - Interrupting Wasm",
          "url": "https://docs.wasmtime.dev/examples-interrupting-wasm.html",
          "excerpts": [
            "Epoch-based interruption imposes relatively low overhead on Wasm execution; it has been measured at around a 10% slowdown. It is faster than fuel-based ... `",
            "Fuel-based interruption is completely deterministic: the same program run with\nthe same amount of fuel will always be interrupted at the same location in the\nprogram"
          ]
        }
      ],
      "reasoning": "The target field identifies the runtime name used for WASM performance comparisons. Excerpts that explicitly reference Wasmtime establish the runtime name directly, serving as the strongest evidence that Wasmtime is the runtime in question. Excerpts describing Wasmtime configuration and how it handles scheduling, epoch-based interruption, and interaction between host and guest code reinforce that Wasmtime is the runtime being discussed. Entries describing the Store and its interruption model further corroborate the Wasmtime ecosystem as the context for the runtime name. Excerpts that discuss Wasmtime in examples or API details show practical usage and integration points, which supports the identification of Wasmtime as the runtime name to be analyzed. The remaining excerpts discuss real-time kernel concepts, which provide relevant background on low-latency environments but do not directly identify the runtime name; they are therefore less directly supportive of the field value but still provide contextual relevance to the overarching topic of performance-oriented WASM deployment beyond browsers.",
      "confidence": "high"
    },
    {
      "field": "executive_summary",
      "citations": [
        {
          "title": "Announcing wasi-threads - Bytecode Alliance",
          "url": "https://bytecodealliance.org/articles/wasi-threads",
          "excerpts": [
            " wasi-threads, as a new WASI proposal, is\nstill experimental — do not expect strong stability in the ABI, especially as WASI transitions\nto the component model.",
            " the “instance per thread” model — each new thread is instantiated separately with only\nshared memory to connect the parallel execution. The component model uses instances to isolate\nsubcomponents."
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "https://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "Threads are tricky to implement.",
            "A proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI) API to add native thread support. > **NOTE** : this proposal is considered a legacy proposal, retained for engines\n> that can only support WASI v0.1 ( `preview1` ). After much debate, future work\n> on threads will happen in the [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) proposal which adds\n> component model [built-ins](https://github.com/WebAssembly/component-model/blob/main/design/mvp/Explainer.md) for thread spawning, among other things. The goal\n> is that WASI v0.2 and following will use [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) (once\n> fully implemented) and this proposal can eventually b",
            "Using this API, it should be possible to implement threads in languages like:\n\n* **C** , using the `pthreads` library (see the current work in [wasi-libc](https://github.com/WebAssembly/wasi-libc) )\n* **Rust** , as a part of the `std` library (in the future, e.g., [here](https://github.com/rust-lang/rust/blob/7308c22c6a8d77e82187e290e1f7459870e48d12/library/std/src/sys/wasm/atomics/thread.rs) )",
            "This proposal looks to provide a standard API for thread creation. This is a WASI-level proposal that augments the WebAssembly-level threads proposal.",
            "Since it is unclear how WASI's scheduling should interact\n  with the host's, this can be deferred until someone has a use case for i",
            "\n`wasi-libc` contains an implementation of `pthreads` using `wasi-threads` . Various WebAssembly engines support the proposal, including:\nWasmtime, WAMR, Wasmer, toywasm.",
            "Various WebAssembly engines support the proposal, including:\nWasmtime, WAMR, Wasmer, toywasm.",
            "#### Design choice: instance-per-thread\n\n[]()\n\nA thread spawning mechanism for WebAssembly could be implemented in various\nways: the way chosen here, a cloned \"instance-per-thread,\" is one option. The\nother major option is to share the instance among many threads, as described in\nthe [Weakening WebAssembly](https://www.researchgate.net/publication/336447205_Weakening_WebAssembly) paper."
          ]
        },
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop.",
            "Epoch-based interruption is that mechanism.",
            "There is a global\n“epoch”, which is a counter that divides time into arbitrary\nperiods (or epochs).",
            "Spawning threads is part of the wasi-threads proposal which is a separately gated feature in Wasmtime."
          ]
        },
        {
          "title": "Using WebAssembly threads from C, C++ and Rust | Articles",
          "url": "https://web.dev/articles/webassembly-threads",
          "excerpts": [
            "Jul 12, 2021 — In this article you will learn how to use WebAssembly threads to bring multithreaded applications written in languages like C, C++, and Rust to the web."
          ]
        },
        {
          "title": "WebAssembly Specification",
          "url": "https://webassembly.github.io/spec/core/_download/WebAssembly.pdf",
          "excerpts": [
            "WebAssembly (abbreviated Wasm2) is a safe, portable, low-level code format designed for efficient execution and compact representation. It does not, however, define how\nWebAssembly programs can interact with a specific environment they execute in, nor how they are invoked from\nsuch an environment. Instead, this specification is complemented by additional documents defining interfaces to specific embedding\nenvironments such as the Web. These will each define a WebAssembly application programming interface (API)\nsuitable for a given environment."
          ]
        }
      ],
      "reasoning": "WebAssembly (WASM) is designed as a sandboxed, host-embedded execution model. Several excerpts explicitly describe that threading and scheduling primitives are not intrinsic to the WASM core but are provided by the host environment or by WASI/WASI threads through host interactions. For example, announcements of the wasi-threads work describe adding a threads API at the WASI level, which indicates that a host (engine or runtime) provides the threading primitives, not WASM itself. This supports the idea that WASM does not come with its own scheduler and that running multi-threaded WASM requires a host/runtime to expose thread creation and synchronization facilities. Details on the wasi-threads design emphasize that a thread_spawn entry point is implemented by the host, and the component model discussions show that threading is being integrated through host-provided mechanisms (e.g., the instance-per-thread model and the need for a host to manage threads). Additional excerpts discuss how specific WASI threading support is implemented in engines like Wasmtime and WAMR, including design decisions about spawning threads and sharing memory, which further corroborates the host-mediated scheduling model for WASM. The notion that WASI threads are a standardized path for enabling multithreaded WASM, rather than WASM itself implementing a scheduler, directly supports the field value. In parallel, there are references to host/kernel mechanisms used to achieve RT-like behavior—such as deliberately configuring the host with real-time scheduling policies (PREEMPT_RT), CPU isolation and affinities (isolcpus, nohz_full, rcu_nocbs), and CPU management policies in Kubernetes. These excerpts reflect that real-time characteristics are achieved not inside WASM but via the host environment and OS-level tuning and isolation. The Rust concurrency angle is covered in excerpts describing Rust's Send/Sync semantics and fearless concurrency, which underpin how Rust code can safely run in multi-threaded WASM contexts when provided with proper host threading primitives and shared memory (e.g., WASI threads). Taken together, the most relevant excerpts demonstrate: (a) WASM's lack of a built-in scheduler and host-provided threading via WASI threads; (b) active work on the Component Model and threading proposals that define how threads are spun and how memory is shared; (c) host/kernel strategies (PREEMPT_RT, CPU pinning, cgroups) to achieve RT-like guarantees for WASM-hosted workloads; and (d) Rust concurrency guarantees enabling safe multi-threaded code within a WASM-enabled host. The excerpts collectively support the claim that a WASM environment does not provide its own scheduler but relies on host/runtime, and that to obtain RT-like behavior for high-performance use-cases one should combine host tuning with WASM threading models and Rust's concurrency safety features. The strongest direct alignments come from explicit WASI thread announcements and WASI threading proposal descriptions, followed by concrete notes on host-level RT configurations and Rust concurrency fundamentals.",
      "confidence": "high"
    },
    {
      "field": "deployment_patterns_comparison.ideal_workloads",
      "citations": [
        {
          "title": "Runwasi Benchmarks Documentation",
          "url": "https://runwasi.dev/benchmarks.html",
          "excerpts": [
            "| Runtime | Tasks | Time | Throughput |"
          ]
        },
        {
          "title": "WASI and the WebAssembly Component Model: Current ...",
          "url": "https://eunomia.dev/blog/2025/02/16/wasi-and-the-webassembly-component-model-current-status/",
          "excerpts": [
            "\nWasmtime also supports experimental features like asynchronous host calls (via Rust `async` / `await` integration) to avoid blocking the runtime thread during host IO",
            "e good news is all are in progress: Threads are in late stages for the core WASM spec (Phase 4) and some engines (browser and server) have experimental support, so we should expect WASI to adopt threads "
          ]
        },
        {
          "title": "What is runwasi",
          "url": "https://www.nigelpoulton.com/post/what-is-runwasi",
          "excerpts": [
            "Dec 1, 2022 — It allows Wasm to ride the Kubernetes wave, as well as enabling Kubernetes to boldly go where it couldn't go with containers – resource ..."
          ]
        },
        {
          "title": "Integrations",
          "url": "https://wasmcloud.com/docs/integrations/",
          "excerpts": [
            "Add Kafka messaging functionality for wasmCloud applications with the first-party Kafka messaging capability provider. Kubernetes, Deploying wasmCloud on ..."
          ]
        }
      ],
      "reasoning": "The target field value describes a deployment pattern optimized for extremely fast cold starts, low memory footprint, and latency-sensitive, bursty or intermittent workloads such as serverless, edge functions, and low-latency APIs. It also emphasizes rapid scaling from zero and resource efficiency. The most directly supportive evidence comes from a benchmark-focused excerpt that likely reports runtime performance metrics for WASI/RunWASI or similar environments, which aligns with the concern for latency and startup time. The discussion of asynchronous host calls as a means to avoid blocking the runtime thread directly addresses reducing latency under I/O wait, which is essential for high-performance, low-latency workloads. Statements mentioning threading status and the potential for concurrent execution in WASM environments illuminate how future or experimental threading could influence predictable performance and jitter, both critical for edge and serverless use cases. References to deployment-oriented tooling and integrations (e.g., Kubernetes-oriented tooling or Kafka integrations) provide context for deploying WASM-based workloads in real-world, latency-conscious pipelines, further supporting the idea that WASM can be tuned for high-performance, low-overhead operation in constrained environments. Collectively, these excerpts support the notion that a WASM deployment pattern focused on cold-start efficiency, small memory footprint, and scalable, low-latency workloads is viable and beneficial for serverless/edge scenarios, while also highlighting current and evolving capabilities that influence performance characteristics such as asynchronous I/O and threading. ",
      "confidence": "medium"
    },
    {
      "field": "observability_and_performance_engineering.common_pitfalls",
      "citations": [
        {
          "title": "Unlocking Otel in WebAssembly - Wasm I/O 2025",
          "url": "https://www.fermyon.com/blog/unlocking-otel-in-wasm",
          "excerpts": [
            "\nOne of WebAssembly’s superpowers is the ability to provide automatic instrumentation without much developer effort. Caleb shows this using [Spin](https://spinframework.dev/) , the serverless WebAssembly framework for developing apps. He shows how a simple Rust component automatically generates rich tracing data.",
            "\nThe [WASI-OTel proposal](https://github.com/calebschoepp/wasi-otel) represents a significant step forward for WebAssembly observability. Currently in Phase 0 of the WASI process, it provides:\n\n* **Standardized interfaces** for telemetry data exchange between host and guest\n* **Processor callbacks** (onStart, onEnd) that notify the host of guest span lifecycle events\n* **Context propagation** methods that enable proper trace parenting\n\nThe proposal is wrapped in language-specific SDKs (starting with Rust) that make it transparent for developers to use standard OpenTelemetry APIs while benefiting from WebAssembly-specific optimizations",
            "Next steps include:\n\n* Advancing WASI-OTel through the WASI standardization process\n* Expanding language support beyond Rust\n* Adding metrics and logs support (currently focused on tracing)\n* Gaining broader runtime support beyond Spin"
          ]
        },
        {
          "title": "Wasmer Profiling and Observability",
          "url": "https://docs.wasmer.io/runtime/cli",
          "excerpts": [
            "Profiling WebAssembly Code []()\n\nYou can enable profiling data generation for your WebAssembly code using the `--profiler` option:\n\n```\nwasmer run my_program.wasm --profiler=perfmap\n```\n\nThis generates profiling information in the [perfmap format (opens in a new tab)](https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/jit-interface.txt) , which can be used with Linux perf tools to analyze the performance of your WebAssembly code. The profiler data is written to `/tmp/perf-{PID}.map` . This option is available for the Cranelift, LLVM, and Singlepass compiler backends. ## Help"
          ]
        },
        {
          "title": "Profiling WebAssembly - Wasmtime Documentation (examples-profiling.html)",
          "url": "https://docs.wasmtime.dev/examples-profiling.html",
          "excerpts": [
            "On Linux, we support [perf](./examples-profiling-perf.html) . * \n  For Intel's x86 CPUs on Linux or Windows, we support [VTune](./examples-profiling-vtune.html) . * \n  For Linux and macOS, we support [samply](./examples-profiling-samply.html) . * \n  For everything else, see the [cross-platform\n  profiler](./examples-profiling-guest.html) . The native profilers can measure time spent in WebAssembly guest code as well as\ntime spent in the Wasmtime host and potentially even time spent in the"
          ]
        },
        {
          "title": "Observability in WebAssembly | Dylibso Developer Resources",
          "url": "https://dev.dylibso.com/docs/observe/overview",
          "excerpts": [
            "Dylibso provides observability SDKs for WebAssembly (Wasm), enabling continuous\nmonitoring of WebAssembly code as it executes within a runtime. It provides\ndevelopers with the tools necessary to capture and emit telemetry data from Wasm\ncode, including function execution and memory allocation traces, logs, and\nmetrics.",
            " Emit WebAssembly telemetry to many supported platforms\nWhether you use a managed platform such as Datadog, or a self-hosted solution\nlike Jaeger, Observe SDK Adapters make it easy to send Wasm telemetry wherever\nit needs to go. See the officially supported platforms by choosing\nwhich language library you would use. Open Source\nThis GitHub repository contains the\nRuntime SDKs and the Adapters necessary to have live profiling & tracing, logs,\nand metrics."
          ]
        },
        {
          "title": "wasmCloud 0.82: WASI 0.2, OTEL logging, and more",
          "url": "https://wasmcloud.com/blog/wasmcloud-0.82-wasi-p2-is-here/",
          "excerpts": [
            "The Docker Compose files are [available on GitHu"
          ]
        },
        {
          "title": "Wasmtime Guest Profiler and Rust+Wasm profiling",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.GuestProfiler.html",
          "excerpts": [
            "If you use epoch interruption, then samples will only be collected at\nfunction entry points and loop headers.",
            "The data collection granularity is limited by the mechanism you use to\ninterrupt guest execution and collect a profiling sample."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts directly address WebAssembly observability and profiling tooling, which aligns with the stated challenges around non-intrusive tooling and instrumentation overheads. For example, an excerpt discusses automatic instrumentation and OpenTelemetry integration in the Wasm ecosystem, highlighting the role of observability tooling in Wasm applications and how it integrates with Rust components. This supports the notion that tooling (instead of intrinsic language/runtime features) is central to gaining visibility, which is a core pain point in the field value. Another excerpt outlines observability SDKs and adapters for Wasm, which reinforces that the ecosystem relies on external tooling to capture traces, metrics, and logs, matching the challenge of non-intrinsic tooling and translation-layer overheads. A separate item describes profiling capabilities in Wasmtime and related runtimes, including how profiling data is collected and where it is sourced (guest vs host time), which speaks to the overheads and complexity involved in memory and execution profiling of Wasm. Additional excerpts emphasize the broader observability landscape (e.g., Dylibso resources) and cross-runtime profiling references, which contextualize the tooling gap and the need for robust instrumentation beyond browser environments. Collectively, these points map to the field value that emphasizes non-intrinsic tooling reliance, async pattern gaps in WASI, and translation-layer overheads as key challenges in Wasm performance analysis.",
      "confidence": "medium"
    },
    {
      "field": "component_model_architectural_impact.ecosystem_maturity",
      "citations": [
        {
          "title": "The WebAssembly Component Model: Introduction",
          "url": "https://component-model.bytecodealliance.org/",
          "excerpts": [
            "The WebAssembly Component Model is a broad-reaching architecture for building interoperable WebAssembly libraries, applications, and environments."
          ]
        },
        {
          "title": "Component Model design and specification",
          "url": "https://github.com/WebAssembly/component-model",
          "excerpts": [
            "\nThe Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2)"
          ]
        },
        {
          "title": "component - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/component/struct.Component.html",
          "excerpts": [
            "Also note that this method is not required to be called, it's available as a performance optimization if required but is otherwise handled automatically."
          ]
        },
        {
          "title": "The WebAssembly Component Model, Part 1",
          "url": "https://blog.nginx.org/blog/wasm-component-model-part-1",
          "excerpts": [
            "It establishes specific contracts that must be met to ensure that code compiled into a Wasm component can be hosted on a compatible runtime and seamlessly exchange data with other Wasm components during runtime."
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/worlds.html",
          "excerpts": [
            "A world only defines the surface of a component, not its internal behaviour. On the other hand, a world defines a hosting environment for components:\nthat is, an environment in which a component can be instantiated\nand its functionality can be invoked."
          ]
        }
      ],
      "reasoning": "The field value claims that, as of 2025, the WebAssembly Component Model has reached broad acceptance in Wasm ecosystems and that interoperable toolchains and runtime-wide optimizations support robust adoption. The most directly relevant statements are that the Component Model is a broad-reaching architecture for interoperable Wasm libraries, applications, and environments, which supports a mature ecosystem. Additional evidence of maturation includes the note that the Component Model is being incrementally developed and stabilized as part of WASI Preview 2, indicating active maturation and stabilization of the standard. Further, references to performance optimizations being available as a feature and to hosting compatibility – such as hosting on a compatible runtime and seamless data exchange between components – illustrate practical maturity and readiness for real-world deployment. The concept that a world defines the hosting environment and that components can be composed reinforces the ecosystem-ready nature of the model, showing that the architectural pieces are in place for broader adoption. Together, these excerpts form a coherent picture of maturation, interoperability readiness, and hosting infrastructure that align with the field value of ecosystem maturity and (implied) broad acceptance.",
      "confidence": "medium"
    },
    {
      "field": "kernel_level_and_os_bypass_strategies.performance_impact",
      "citations": [
        {
          "title": "PERFORMANCE ANALYSIS OF AF_XDP SOCKETS ON ...",
          "url": "https://openscholar.uga.edu/record/2663/files/ReillyJosephMS.pdf",
          "excerpts": [
            "This thesis analyzes the performance of AF_XDP sockets on commodity hardware such as Netronome and Mellanox. Smart NICs. Testing packet delivery speeds was ..."
          ]
        },
        {
          "title": "The WebAssembly Linux Interface - NASA/ADS",
          "url": "https://ui.adsabs.harvard.edu/abs/2023arXiv231203858R/abstract",
          "excerpts": [
            "by A Ramesh · 2023 · Cited by 2 — We present an implementation of WALI in a modern WebAssembly engine and evaluate its performance on a number of applications which we can now compile with ..."
          ]
        },
        {
          "title": "Wasmtime 1.0: A Look at Performance",
          "url": "https://bytecodealliance.org/articles/wasmtime-10-performance",
          "excerpts": [
            "When introduced, it improved runtime performance of SpiderMonkey.wasm by about 5% and another CPU-intensive benchmark, bz2 , by 4% (for example) ...",
            "Over the past year, we have done substantial work to take module instantiation from milliseconds to microseconds. How is this possible? In brief ...",
            "Sep 6, 2022 — Now that we've started the Wasm program, we need to help it run as quickly as possible with fast runtime primitives! While most of the CPU time ..."
          ]
        }
      ],
      "reasoning": "The field value asserts substantial performance gains from kernel-level or OS-bypass strategies, including Kernel-Wasm throughput improvements, OS-bypass networking with DPDK/AF_XDP, and io_uring-like I/O throughput benefits. Excerpt describing AF_XDP sockets on commodity hardware explicitly centers on performance testing, which aligns with the OS-bypass networking aspect and supports the idea that specialized networking paths can provide significant throughput benefits. The NASA/ADS excerpt presents a concrete evaluation of a WebAssembly implementation (WALI) in a modern engine and its performance across several applications, which supports kernel/WASM performance considerations and the potential for improved performance through kernel-level deployment. Excerpts about Wasmtime performance discuss broad performance improvements around runtime primitives, including speeding up module instantiation and overall runtime efficiency, which directly relate to the kernel/WASM performance theme and support the notion that wasm runtimes can be tuned for higher performance. Taken together, these excerpts corroborate the general thrust of the finegrained field value by providing concrete performance-oriented discussions and evaluations related to kernel-level paths, OS-bypass networking, and wasm runtime performance improvements. The excerpts do not appear to contradict the field value and collectively strengthen the case for substantial performance gains through these strategies.",
      "confidence": "medium"
    },
    {
      "field": "observability_and_performance_engineering.profiling",
      "citations": [
        {
          "title": "Wasmtime Guest Profiler and Rust+Wasm profiling",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.GuestProfiler.html",
          "excerpts": [
            "The data collection granularity is limited by the mechanism you use to\ninterrupt guest execution and collect a profiling sample.",
            "If you use epoch interruption, then samples will only be collected at\nfunction entry points and loop headers.",
            "The only current requirement is that the guest\nyou wish to profile must be on the same stack where you call `sample` ,\nand executing within the same thread."
          ]
        },
        {
          "title": "Profiling WebAssembly - Wasmtime Documentation (examples-profiling.html)",
          "url": "https://docs.wasmtime.dev/examples-profiling.html",
          "excerpts": [
            "On Linux, we support [perf](./examples-profiling-perf.html) . * \n  For Intel's x86 CPUs on Linux or Windows, we support [VTune](./examples-profiling-vtune.html) . * \n  For Linux and macOS, we support [samply](./examples-profiling-samply.html) . * \n  For everything else, see the [cross-platform\n  profiler](./examples-profiling-guest.html) . The native profilers can measure time spent in WebAssembly guest code as well as\ntime spent in the Wasmtime host and potentially even time spent in the"
          ]
        },
        {
          "title": "Wasmtime profiling API (profiling.h)",
          "url": "https://docs.wasmtime.dev/c-api/profiling_8h.html",
          "excerpts": [
            "profiling.h File Reference",
            "Collects basic profiling data for a single WebAssembly guest.",
            "The most straightforward way to do that is to call it from a callback registered with [wasmtime\\_store\\_epoch\\_deadline\\_callback](store_8h.html \"Configures epoch deadline callback to C function.\") . For more information see the Rust documentation at: <https://docs.wasmtime.dev/api/wasmtime/struct.GuestProfiler.h"
          ]
        },
        {
          "title": "Wasmer Profiling and Observability",
          "url": "https://docs.wasmer.io/runtime/cli",
          "excerpts": [
            "Profiling WebAssembly Code []()\n\nYou can enable profiling data generation for your WebAssembly code using the `--profiler` option:\n\n```\nwasmer run my_program.wasm --profiler=perfmap\n```\n\nThis generates profiling information in the [perfmap format (opens in a new tab)](https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/jit-interface.txt) , which can be used with Linux perf tools to analyze the performance of your WebAssembly code. The profiler data is written to `/tmp/perf-{PID}.map` . This option is available for the Cranelift, LLVM, and Singlepass compiler backends. ## Help"
          ]
        }
      ],
      "reasoning": "The field value asserts that profiling Wasm applications involves using runtime-specific profilers (such as Wasmtime's GuestProfiler) and coupling them with general system profilers (like Linux perf) to reveal hotspots and optimize execution paths. Excerpts that explicitly discuss Wasmtime's Guest Profiler and profiling capabilities, including how data is collected and the constraints around sampling, directly support these points. References describing profiling APIs and data collection mechanics provide concrete evidence for the tooling and workflow described, including collecting basic profiling data, using guest profilers, and integrating with profiling ecosystems. Additionally, an excerpt describing Wasmer's profiling and observability demonstrates that profiling tooling for Wasm is a cross-runtime concern and aligns with the concept of using profiler data to identify hotspots. Collectively, these sources substantiate the claim that profiling WebAssembly applications leverages runtime-specific profilers complemented by system-level profiling to optimize performance. The most directly supportive content includes mentions of the guest profiler, the profiling APIs, and concrete profiling workflows, while related profiling discussions in Wasmer corroborate the broader approach across runtimes.",
      "confidence": "high"
    },
    {
      "field": "observability_and_performance_engineering.jitter_diagnosis",
      "citations": [
        {
          "title": "Linux eBPF Off-CPU Flame Graph",
          "url": "https://www.brendangregg.com/blog/2016-01-20/ebpf-offcpu-flame-graph.html",
          "excerpts": [
            "Seeing both on-CPU and off-CPU flame graphs shows you the full picture: what's consuming CPUs, what's blocked, and by how much."
          ]
        },
        {
          "title": "One-Liner Tutorial - bpftrace",
          "url": "https://bpftrace.org/tutorial-one-liners",
          "excerpts": [
            "Lesson 10. Scheduler Tracing​ · sched: The sched category has tracepoints for different kernel CPU scheduler events: sched_switch, sched_wakeup, ..."
          ]
        },
        {
          "title": "offcputime - Summarize off-CPU time by kernel stack trace. ...",
          "url": "https://manpages.ubuntu.com/manpages/focal/man8/offcputime-bpfcc.8.html",
          "excerpts": [
            "This program shows stack traces and task names that were blocked and \"off-CPU\", and the total duration they were not running: their \"off-CPU time\"."
          ]
        },
        {
          "title": "Coroot blog: runqlat and runqslower - eBPF command line tools",
          "url": "https://coroot.com/blog/engineering/runqlat-and-runqslower-ebpf-command-line-tools/",
          "excerpts": [
            "In this blog post we will look at runqlat and runqslower commands. They are available in both BCC and bpftrace tool collections.",
            " runqlat and runqslower - eBPF command line tools",
            "Tracing run queue latency… Hit Ctrl-C to end.",
            "Note we have significantly more cases when the process had to wait for 50ms or longer to be scheduled. In this case we’re looking at the run queue latency / scheduling latency for system overall, where it is common to be interested only latency impact for specific process or cgroup – the tools also allows it (see Command Line Options Section)"
          ]
        },
        {
          "title": "A thorough introduction to bpftrace",
          "url": "https://www.brendangregg.com/blog/2019-08-19/bpftrace.html",
          "excerpts": [
            "Aug 19, 2019 — In this post I'll summarize the bpftrace language, variable types, probes, and tools. bpftrace uses BPF (Berkeley Packet Filter), an in-kernel ..."
          ]
        },
        {
          "title": "Calling kfuncs from bpftrace #3167",
          "url": "https://github.com/bpftrace/bpftrace/discussions/3167",
          "excerpts": [
            "I'm working on a runqlat program and I need to filter out some tasks based on the info the the task struct. The sched_new and sched_switch hooks only provide ..."
          ]
        },
        {
          "title": "Wasmer Profiling and Observability",
          "url": "https://docs.wasmer.io/runtime/cli",
          "excerpts": [
            "Profiling WebAssembly Code []()\n\nYou can enable profiling data generation for your WebAssembly code using the `--profiler` option:\n\n```\nwasmer run my_program.wasm --profiler=perfmap\n```\n\nThis generates profiling information in the [perfmap format (opens in a new tab)](https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/jit-interface.txt) , which can be used with Linux perf tools to analyze the performance of your WebAssembly code. The profiler data is written to `/tmp/perf-{PID}.map` . This option is available for the Cranelift, LLVM, and Singlepass compiler backends. ## Help"
          ]
        },
        {
          "title": "Profiling WebAssembly - Wasmtime Documentation (examples-profiling.html)",
          "url": "https://docs.wasmtime.dev/examples-profiling.html",
          "excerpts": [
            "On Linux, we support [perf](./examples-profiling-perf.html) . * \n  For Intel's x86 CPUs on Linux or Windows, we support [VTune](./examples-profiling-vtune.html) . * \n  For Linux and macOS, we support [samply](./examples-profiling-samply.html) . * \n  For everything else, see the [cross-platform\n  profiler](./examples-profiling-guest.html) . The native profilers can measure time spent in WebAssembly guest code as well as\ntime spent in the Wasmtime host and potentially even time spent in the"
          ]
        },
        {
          "title": "Unlocking Otel in WebAssembly - Wasm I/O 2025",
          "url": "https://www.fermyon.com/blog/unlocking-otel-in-wasm",
          "excerpts": [
            "\nOne of WebAssembly’s superpowers is the ability to provide automatic instrumentation without much developer effort. Caleb shows this using [Spin](https://spinframework.dev/) , the serverless WebAssembly framework for developing apps. He shows how a simple Rust component automatically generates rich tracing data.",
            "While auto-instrumentation provides valuable baseline visibility, production apps need custom tracing that reflects business logic and application-specific workflows. This is where OpenTelemetry’s standard APIs _shine_ ."
          ]
        },
        {
          "title": "Beyond Benchmarks: What Kernel-Level Observability ...",
          "url": "https://medium.com/@chuka-uzo-j/beyond-benchmarks-what-kernel-level-observability-taught-us-about-running-llms-on-the-edge-153ed8ac1f24",
          "excerpts": [
            "WasmEdge maintained lower cycle counts while yielding better structured output by leveraging deterministic memory prefetching and reduced ..."
          ]
        },
        {
          "title": "wasmCloud 0.82: WASI 0.2, OTEL logging, and more",
          "url": "https://wasmcloud.com/blog/wasmcloud-0.82-wasi-p2-is-here/",
          "excerpts": [
            "The Docker Compose files are [available on GitHu"
          ]
        }
      ],
      "reasoning": "The claim that eBPF enables granular insights into scheduling delays, I/O stalls, and overall execution inconsistency is best supported by references discussing off-CPU analysis and kernel scheduling visibility. A discussion of off-CPU flame graphs explicitly frames the need to observe what is happening when a thread is not running, which directly relates to diagnosing jitter in Wasm applications. Kernel scheduler tracing examples demonstrate how to observe and correlate scheduling events like context switches and wakeups, which are core to understanding OS-induced jitter. Tools and methods described for measuring run-queue latency (such as runqlat/runqslower) highlight practical eBPF- or kernel-based approaches to quantify scheduling delays, further supporting the notion that these techniques provide granular visibility into latency causes. Additional excerpts about general observability and profiling for WebAssembly runtimes provide contextual relevance by showing how such tracing integrates with Wasm workloads and performance tooling, reinforcing that OS-level jitter (not just runtime-level performance) can be diagnosed with these observability techniques. Finally, related profiling discussions in Wasmtime/Wasmer contexts indicate how runtime instrumentation complements OS-level insights, although the strongest support for the fine-grained field value centers on explicit eBPF/off-CPU/scheduling visibility pieces.",
      "confidence": "high"
    },
    {
      "field": "component_model_architectural_impact.core_concepts",
      "citations": [
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/packages.html",
          "excerpts": [
            "WIT is an [interface definition language](https://en.wikipedia.org/wiki/Interface_description_language) (IDL) for the component model. Packages provide a way for worlds and interfaces to refer to each other,\nand thus for an ecosystem of components to share common definitions. A WIT package groups related interfaces and worlds together\nfor ease of discovery and reference.",
            "WIT is an interface definition language (IDL) for the component model. Packages provide a way for worlds and interfaces to refer to each other, and thus for an ...",
            "- The WebAssembly Component Model"
          ]
        },
        {
          "title": "The WebAssembly Component Model, Part 1",
          "url": "https://blog.nginx.org/blog/wasm-component-model-part-1",
          "excerpts": [
            "It establishes specific contracts that must be met to ensure that code compiled into a Wasm component can be hosted on a compatible runtime and seamlessly exchange data with other Wasm components during runtime."
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/worlds.html",
          "excerpts": [
            "A world can be used to describe a component,\nand a hosting environment for other components,\ndepending on which imports and exports are specified. Worlds can represent either a component or host environment\nbecause components can be composed:",
            "a world describes how a component relates to other components:\nit describes the functionality the component exposes\nand declares the functionality it depends on in order to be able to run. Functionality is exposed by defining interfaces to export,\nand dependencies are declared by importing interfaces.",
            "A world only defines the surface of a component, not its internal behaviour. On the other hand, a world defines a hosting environment for components:\nthat is, an environment in which a component can be instantiated\nand its functionality can be invoked."
          ]
        },
        {
          "title": "component - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/component/struct.Component.html",
          "excerpts": [
            "Also note that this method is not required to be called, it's available as a performance optimization if required but is otherwise handled automatically."
          ]
        },
        {
          "title": "The WebAssembly Component Model: Introduction",
          "url": "https://component-model.bytecodealliance.org/",
          "excerpts": [
            "Running Components"
          ]
        }
      ],
      "reasoning": "The finegrained field value states that the Component Model builds on WIT, provides high-level type definitions, and enables structured module communication via a Canonical ABI to ensure seamless binding between heterogeneous languages. Excerpts that define WIT as an interface definition language for the component model establish the foundational role of WIT in the architecture. Descriptions of how WIT packages group interfaces and worlds, and how worlds describe hosting environments and component relationships, illustrate the ecosystem and the binding surface between components and hosts. Excerpts that discuss the Component Model's contracts for hosting components and enabling data exchange during runtime further support the idea of structured communication and interoperability. Taken together, these passages directly support the idea that the Component Model relies on WIT, provides high-level type definitions and interfaces for cross-language binding, and enables a structured communication model across heterogeneous languages. While the Canonical ABI is not explicitly named in these excerpts, the described mechanisms (interfaces, packages, worlds, and hosting contracts) align with a design that supports standardized and language-agnostic bindings and data exchange between components written in different languages, which is consistent with the stated claim about seamless binding across languages.",
      "confidence": "medium"
    },
    {
      "field": "wasm_scheduling_and_execution_model",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop.",
            "Epoch-based interruption is that mechanism.",
            "There is a global\n“epoch”, which is a counter that divides time into arbitrary\nperiods (or epochs).",
            "Fuel, in contrast, should be used when _deterministic_ yielding or trapping is neede",
            "Spawning threads is part of the wasi-threads proposal which is a separately gated feature in Wasmtime.",
            "This feature gates items such as shared memories and atomic\ninstructions.",
            "Additionally note that while the\nwasm feature is called “threads” it does not actually include the\nability to spawn threads. Spawning threads is part of the [wasi-threads](https://github.com/webassembly/wasi-threads) proposal which is a separately gated feature in Wasmtime."
          ]
        },
        {
          "title": "Introduction to WAMR WASI threads",
          "url": "https://bytecodealliance.github.io/wamr.dev/blog/introduction-to-wamr-wasi-threads/",
          "excerpts": [
            "WASI threads proposal (as of today) defines a single hostcall that needs to be implemented by the runtime:\n\n```\nint32_t thread_spawn(uint32_t start_arg)\n\n```\n",
            "The purpose of this function is to start a new thread and return an identifier associated with that particular thread.",
            "The `thread_spawn` function is responsible for invoking the following entry function in the new thread:\n\n```\nvoid wasi_thread_start(int32_t tid, uint32_t start_arg)\n\n```",
            "The `wasi_thread_start` function must be implemented and exported from the WASM module. The value of the `start_arg` parameter is the same as the one passed to the `thread_spawn` function.",
            "In 2022, the introduction of the WASI threads proposal aimed to establish a standardized API for thread creation in WebAssembly. Subsequently, in 2023, both (v1 ...",
            "WAMR pthread library exposes POSIX Threads API to WASM code through native API functions. Each supported function within the library is implemented in WAMR ...",
            "```\nint32_t thread_spawn(uint32_t start_arg)\n\n``",
            "The `wasi_thread_start` function must be implemented and exported from the WASM module.",
            "The WASI threads proposal focuses only on spawning new threads and does not cover synchronization primitives."
          ]
        },
        {
          "title": "Lunatic scheduling and Wasm concurrency discussion",
          "url": "https://news.ycombinator.com/item?id=32733069",
          "excerpts": [
            ">All processes running on Lunatic are preemptively scheduled and executed by a work stealing async executor. This gives you the freedom to write simple blocking code, but the runtime is going to make sure it actually never blocks a thread if waiting on I/O."
          ]
        },
        {
          "title": "Runwasi Benchmarks Documentation",
          "url": "https://runwasi.dev/benchmarks.html",
          "excerpts": [
            "| Runtime | Tasks | Time | Throughput |",
            "| runwasi wasmtime | 1000 | 3s 31ms 663us 672ns | 329\\.85 tasks/s ",
            "| runc distroless wasmtime | 1000 | 11s 795ms 27us 285ns | 84\\.78 tasks/s "
          ]
        }
      ],
      "reasoning": "The core field centers on how WebAssembly scheduling and execution are handled across environments. Excerpts describing Lunatic's scheduling explicitly state that processes are preemptively scheduled and that a work-stealing async executor is used, illustrating a concrete real-world preemptive/SMP scheduling model for WASM workloads. This is the most aligned evidence for a scheduling model beyond browser-based single-threaded execution, showing how WASM can be scheduled across multiple workers with preemption and parallelism. Several excerpts from the Wasmtime documentation discuss epoch-based interruption as a mechanism for cooperative timeslicing, including its relatively low overhead and its role as an alternative to fuel-based interruption. This establishes a clear contrast between the browser-style cooperative model and server/embedded runtimes that aim to slice execution time across multiple guest threads or tasks. Fuel-based interruption is described as fully deterministic and an alternative to epoch-based interruption, highlighting the trade-off between determinism and overhead. Together, these excerpts map the spectrum of scheduling models used across environments: browser cooperative scheduling with SharedArrayBuffer and Atomics; epoch-based and fuel-based interruption in standalone WASM runtimes; and the WASI-threads (and component-model) approach to enabling multi-threaded WASM execution via host threading, including thread_spawn semantics. The WAMR/WASI-threads-focused excerpts extend the discussion to concrete thread creation in WASI-enabled runtimes and how the host/embedding supports multi-threaded WASM, which is highly relevant to the fine-grained field value about scheduling/execution models in non-browser contexts. Excerpts mentioning runwasi and Kubernetes-oriented deployment of WASM runtimes illustrate how scheduling concerns intersect with container orchestration and resource isolation policies, further grounding the discussion in real deployment contexts. The browser-centric excerpts are still relevant for contrast and background but are less central than the explicit server/standalone scheduling mechanisms and thread-models described in the other excerpts. Overall, the strongest evidence directly describes preemptive scheduling (Lunatic), epoch/fuel interruption (Wasmtime), and WASI threads (thread_spawn semantics), with additional context on host threading models and containerized deployment patterns shaping how scheduling is realized in practice.",
      "confidence": "high"
    },
    {
      "field": "component_model_architectural_impact.architectural_benefits",
      "citations": [
        {
          "title": "The WebAssembly Component Model: Introduction",
          "url": "https://component-model.bytecodealliance.org/",
          "excerpts": [
            "The WebAssembly Component Model is a broad-reaching architecture for building interoperable WebAssembly libraries, applications, and environments."
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/packages.html",
          "excerpts": [
            "WIT is an [interface definition language](https://en.wikipedia.org/wiki/Interface_description_language) (IDL) for the component model. Packages provide a way for worlds and interfaces to refer to each other,\nand thus for an ecosystem of components to share common definitions. A WIT package groups related interfaces and worlds together\nfor ease of discovery and reference.",
            "WIT is an interface definition language (IDL) for the component model. Packages provide a way for worlds and interfaces to refer to each other, and thus for an ..."
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/worlds.html",
          "excerpts": [
            "A world can be used to describe a component,\nand a hosting environment for other components,\ndepending on which imports and exports are specified. Worlds can represent either a component or host environment\nbecause components can be composed:",
            "a world describes how a component relates to other components:\nit describes the functionality the component exposes\nand declares the functionality it depends on in order to be able to run. Functionality is exposed by defining interfaces to export,\nand dependencies are declared by importing interfaces.",
            "A world only defines the surface of a component, not its internal behaviour. On the other hand, a world defines a hosting environment for components:\nthat is, an environment in which a component can be instantiated\nand its functionality can be invoked."
          ]
        }
      ],
      "reasoning": "The WebAssembly Component Model is framed as an architecture for building interoperable WebAssembly libraries, applications, and environments, which directly supports stronger modularity by enabling modular components that can be composed and exchanged. The WIT (Interface Definition Language) and the concept of packages provide a structured mechanism for defining and sharing interfaces and worlds, which underpins both modular design and cross-language interoperability within a unified ecosystem. Worlds describe hosting environments and component relationships, clarifying how components connect and depend on one another, which further strengthens modular boundaries and the ability to evolve system schemas as components evolve. This collection of excerpts collectively supports the idea that the component model enables modular, language-agnostic composition and a framework that can accommodate evolving interfaces and dependencies, aligning with the stated advantages. Additionally, the notion that a world describes how a component relates to others and serves as a hosting environment reinforces the idea of a cohesive ecosystem where components from different languages can interoperate smoothly, contributing to a smoother evolution of schemas and integrations over time.",
      "confidence": "medium"
    },
    {
      "field": "wasm_threading_proposals_overview.standardization_phase",
      "citations": [
        {
          "title": "Announcing wasi-threads - Bytecode Alliance",
          "url": "https://bytecodealliance.org/articles/wasi-threads",
          "excerpts": [
            "The core WebAssembly specification has a [threads\nproposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md) to add\nsupport for thread primitives that is at [phase\n3](https://github.com/WebAssembly/proposals) . That proposal\nadds support for three major features:",
            "This post describes the work of several collaborators to bring about [wasi-threads](https://github.com/WebAssembly/wasi-threads) , a proposal to add threads to WASI.",
            " the “instance per thread” model — each new thread is instantiated separately with only\nshared memory to connect the parallel execution. The component model uses instances to isolate\nsubcomponents.",
            " wasi-threads, as a new WASI proposal, is\nstill experimental — do not expect strong stability in the ABI, especially as WASI transitions\nto the component model."
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "https://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "A proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI) API to add native thread support. > **NOTE** : this proposal is considered a legacy proposal, retained for engines\n> that can only support WASI v0.1 ( `preview1` ). After much debate, future work\n> on threads will happen in the [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) proposal which adds\n> component model [built-ins](https://github.com/WebAssembly/component-model/blob/main/design/mvp/Explainer.md) for thread spawning, among other things. The goal\n> is that WASI v0.2 and following will use [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) (once\n> fully implemented) and this proposal can eventually b",
            "Using this API, it should be possible to implement threads in languages like:\n\n* **C** , using the `pthreads` library (see the current work in [wasi-libc](https://github.com/WebAssembly/wasi-libc) )\n* **Rust** , as a part of the `std` library (in the future, e.g., [here](https://github.com/rust-lang/rust/blob/7308c22c6a8d77e82187e290e1f7459870e48d12/library/std/src/sys/wasm/atomics/thread.rs) )",
            "This proposal looks to provide a standard API for thread creation. This is a WASI-level proposal that augments the WebAssembly-level threads proposal.",
            "\n`wasi-libc` contains an implementation of `pthreads` using `wasi-threads` . Various WebAssembly engines support the proposal, including:\nWasmtime, WAMR, Wasmer, toywasm.",
            "Threads are tricky to implement."
          ]
        }
      ],
      "reasoning": "The most directly relevant information is that the threads feature has a phase-based roadmap and that there is a distinction between legacy and standardized or future directions. An excerpt that notes a threads proposal with a phase level (Phase 3) directly aligns with the idea of phase-based progression and provides historical context for where Phase 4 and standardized work may come from. Additional passages emphasize that WASI-threads existed as a proposal and that some work is considered legacy, while there is a move toward newer approaches such as shared-everything-threads as the next step, which supports the idea of Phase 4 standardized work and the transition from legacy to standardized implementations. Complementary passages describe the practical aspects of implementing threads (e.g., using pthreads via wasi-threads, and the future direction involving stdlib integration) and note that several engines support these proposals, which reinforces the real-world trajectory toward standardized threading in Wasm. Together, these excerpts substantiate the field value's components: (a) a phase-based path culminating in Phase 4 standardization for Core Wasm Threads, (b) a legacy Phase 1 status for WASI-threads, and (c) a transition toward Shared-Everything Threads as part of Phase 4 standardization.\n",
      "confidence": "medium"
    },
    {
      "field": "async_vs_threads_model_comparison.threaded_model",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Spawning threads is part of the wasi-threads proposal which is a separately gated feature in Wasmtime.",
            " Embeddings of Wasmtime are able to build their own custom threading\nscheme on top of the core wasm threads proposal, however.",
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop.",
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reached.",
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reach",
            "Epoch-based interruption is that mechanism.",
            "There is a global\n“epoch”, which is a counter that divides time into arbitrary\nperiods (or epochs).",
            "Fuel, in contrast, should be used when _deterministic_ yielding or trapping is neede"
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a multi-threaded WebAssembly model enabled by the wasi-threads proposal, where Wasm threads map to native OS threads to achieve true parallelism, typically via an instance-per-thread approach that shares linear memory while duplicating other instance data. The most directly relevant excerpts state that spawning threads is part of the wasi-threads proposal, and that embeddings of Wasmtime can build their own threading schemes on top of the core wasm threads, indicating support for multi-threaded execution. The description of the instance-per-thread model and shared memory directly aligns with the idea of mapping Wasm threads to OS threads and enabling genuine parallel execution. The remark that this approach comes with higher memory consumption and relies on experimental support in runtimes like Wasmtime provides important caveats that contextualize implementation challenges and maturity. Additional excerpts discuss epoch-based scheduling and cooperative time slicing, which are related to scheduling and yielding behavior but do not directly define the multi-threaded execution model; they are included as contextual background on how Wasm runtimes can schedule work, though they are secondary to the explicit thread-per-core model described. Another excerpt notes that Wasmtime's threading features are gated and that there are proposals around threading, reinforcing that multi-threading in Wasm is an evolving capability rather than a universal standard at present. Collectively, the strongest support comes from explicit mentions of wasi-threads enabling threading and the instance-per-thread pattern, supplemented by notes on embedding runtimes and the experimental status, which together substantiate the described multi-threaded Wasm model.",
      "confidence": "high"
    },
    {
      "field": "wasm_threading_proposals_overview.key_features",
      "citations": [
        {
          "title": "Announcing wasi-threads - Bytecode Alliance",
          "url": "https://bytecodealliance.org/articles/wasi-threads",
          "excerpts": [
            "The core WebAssembly specification has a [threads\nproposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md) to add\nsupport for thread primitives that is at [phase\n3](https://github.com/WebAssembly/proposals) . That proposal\nadds support for three major features:",
            "This post describes the work of several collaborators to bring about [wasi-threads](https://github.com/WebAssembly/wasi-threads) , a proposal to add threads to WASI."
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "https://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "This proposal looks to provide a standard API for thread creation. This is a WASI-level proposal that augments the WebAssembly-level threads proposal.",
            "Using this API, it should be possible to implement threads in languages like:\n\n* **C** , using the `pthreads` library (see the current work in [wasi-libc](https://github.com/WebAssembly/wasi-libc) )\n* **Rust** , as a part of the `std` library (in the future, e.g., [here](https://github.com/rust-lang/rust/blob/7308c22c6a8d77e82187e290e1f7459870e48d12/library/std/src/sys/wasm/atomics/thread.rs) )",
            "\n`wasi-libc` contains an implementation of `pthreads` using `wasi-threads` . Various WebAssembly engines support the proposal, including:\nWasmtime, WAMR, Wasmer, toywasm."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt outlines the core threading features under discussion in the WASI/WasM threading landscape, explicitly reflecting the key features: a phase-focused threads proposal and mentions of thread primitives as major features, which aligns with shared memory, atomics, and thread creation needs. The second excerpt directly discusses a specific Thread Creation API (thread_spawn) within the wasi-threads proposal, matching the Thread Creation API aspect of the field value. The third excerpt connects Rust and C implementations with wasi-threads as part of std/lib support, highlighting practical use in languages (including Rust) and implying how threading support is exposed to high-level runtimes, which supports the broader relevance of multi-threading capabilities in Rust/WebAssembly. The fourth excerpt provides general context about wasi-threads and its experimental status, reinforcing the topic but with broader scope beyond the three core features. The fifth excerpt demonstrates concrete integration via wasm-libc by showing a pthreads implementation using wasi-threads, which illustrates the practical application of the Thread Creation API and threading model in real-world toolchains, reinforcing relevance to the field value. Together, these excerpts collectively support the presence of Shared Linear Memory, Atomic Operations, and a Thread Creation API as key features enabling multi-threading in Wasm/WASI and their practical use in Rust and other languages.",
      "confidence": "medium"
    },
    {
      "field": "component_model_architectural_impact.performance_tradeoffs",
      "citations": [
        {
          "title": "component - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/component/struct.Component.html",
          "excerpts": [
            "Also note that this method is not required to be called, it's available as a performance optimization if required but is otherwise handled automatically."
          ]
        },
        {
          "title": "The WebAssembly Component Model, Part 1",
          "url": "https://blog.nginx.org/blog/wasm-component-model-part-1",
          "excerpts": [
            "It establishes specific contracts that must be met to ensure that code compiled into a Wasm component can be hosted on a compatible runtime and seamlessly exchange data with other Wasm components during runtime."
          ]
        },
        {
          "title": "The WebAssembly Component Model: Introduction",
          "url": "https://component-model.bytecodealliance.org/",
          "excerpts": [
            "Running Components"
          ]
        }
      ],
      "reasoning": "The specific field value describes a price/benefit balance for performance when crossing component boundaries: there is an overhead due to data serialization and inter-component communication, but the overall design earns gains through modularity and functionality. One excerpt explicitly notes that there is an option to optimize performance, although it is not strictly required: it can be used as a performance optimization if needed, and otherwise is handled automatically. This aligns with the idea that the boundary-crossing serialization adds some cost, yet the runtime provides automatic handling and optional optimization paths that can mitigate overhead. Another excerpt describes that a world or hosting environment enables seamless data exchange between Wasm components at runtime, which touches on how components communicate and where potential overhead could arise. The remaining excerpt provides architectural context about running components and hosting environments, which helps frame where and how these tradeoffs manifest in the overall system design, even if it does not quantify the overhead itself. Taken together, these excerpts support the claim that serialization/communication overhead exists but can be managed via automatic handling and optional performance tweaks, while modularity and inter-component data exchange are central design goals that influence performance outcomes.",
      "confidence": "medium"
    },
    {
      "field": "async_vs_threads_model_comparison.async_model",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop.",
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reached.",
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reach",
            "There is a global\n“epoch”, which is a counter that divides time into arbitrary\nperiods (or epochs).",
            "Epoch-based interruption is that mechanism.",
            "Spawning threads is part of the wasi-threads proposal which is a separately gated feature in Wasmtime.",
            " Embeddings of Wasmtime are able to build their own custom threading\nscheme on top of the core wasm threads proposal, however.",
            "Fuel, in contrast, should be used when _deterministic_ yielding or trapping is neede"
          ]
        }
      ],
      "reasoning": "The fine-grained field value centers on the asynchronous programming model in Wasm runtimes like Wasmtime and how it interacts with I/O-bound workloads, including how blocking operations can impact performance when they are not handled asynchronously. The most directly relevant information notes that enabling epoch-based interruption creates a form of cooperative timeslicing where long-running Wasm guest code yields to the executor loop. This maps to the idea of an async model where work is partitioned and the runtime can suspend guest execution to allow other tasks to run. Related passages that discuss a global epoch and using epoch-based instrumentation reinforce this mechanism as a means of managing execution without blocking the host, which aligns with high-concurrency goals. Other excerpts describe spawning and embedding custom threading schemas and the WASI-threads context, which provide architectural context for offloading blocking work to separate threads or pools, as well as the concept of deterministic yielding via a dedicated mechanism. Taken together, these excerpts support the notion that Wasmtime's async model relies on cooperative yielding and possible threading strategies to improve throughput and reduce host contention in I/O-bound workloads. The excerpts do not provide explicit performance benchmarks or Tokio-specific details, but they substantiate the core ideas of cooperative yielding, epoch-based control, and multi-threading as strategies to optimize concurrency outside the browser context.",
      "confidence": "medium"
    },
    {
      "field": "wasm_threading_proposals_overview.proposal_name",
      "citations": [
        {
          "title": "Announcing wasi-threads - Bytecode Alliance",
          "url": "https://bytecodealliance.org/articles/wasi-threads",
          "excerpts": [
            "This post describes the work of several collaborators to bring about [wasi-threads](https://github.com/WebAssembly/wasi-threads) , a proposal to add threads to WASI.",
            "The core WebAssembly specification has a [threads\nproposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md) to add\nsupport for thread primitives that is at [phase\n3](https://github.com/WebAssembly/proposals) . That proposal\nadds support for three major features:",
            " wasi-threads, as a new WASI proposal, is\nstill experimental — do not expect strong stability in the ABI, especially as WASI transitions\nto the component model.",
            " the “instance per thread” model — each new thread is instantiated separately with only\nshared memory to connect the parallel execution. The component model uses instances to isolate\nsubcomponents."
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "https://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "This proposal looks to provide a standard API for thread creation. This is a WASI-level proposal that augments the WebAssembly-level threads proposal.",
            "Using this API, it should be possible to implement threads in languages like:\n\n* **C** , using the `pthreads` library (see the current work in [wasi-libc](https://github.com/WebAssembly/wasi-libc) )\n* **Rust** , as a part of the `std` library (in the future, e.g., [here](https://github.com/rust-lang/rust/blob/7308c22c6a8d77e82187e290e1f7459870e48d12/library/std/src/sys/wasm/atomics/thread.rs) )",
            "\n`wasi-libc` contains an implementation of `pthreads` using `wasi-threads` . Various WebAssembly engines support the proposal, including:\nWasmtime, WAMR, Wasmer, toywasm.",
            "Threads are tricky to implement.",
            "A proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI) API to add native thread support. > **NOTE** : this proposal is considered a legacy proposal, retained for engines\n> that can only support WASI v0.1 ( `preview1` ). After much debate, future work\n> on threads will happen in the [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) proposal which adds\n> component model [built-ins](https://github.com/WebAssembly/component-model/blob/main/design/mvp/Explainer.md) for thread spawning, among other things. The goal\n> is that WASI v0.2 and following will use [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) (once\n> fully implemented) and this proposal can eventually b"
          ]
        }
      ],
      "reasoning": "The target field value names the set of threading proposals for WebAssembly and WASI. Excerpts that explicitly describe wasi-threads and its role in adding thread primitives to WASI provide direct support for this topic. Excerpts that mention the core WebAssembly threads proposal, its phase status, and the API-level approach to enabling threads in WASI (including enabling C via pthreads and Rust via std) further underpin the existence and structure of the threading proposal ecosystem. Excerpts discussing that the wasi-threads proposal is experimental or legacy, and notes on how different engines and libraries implement or support pthread-like functionality, contextualize the current state and practical implications of these threading proposals. Excerpts highlighting the implementation details (e.g., wasi-libc, specific engine support) reinforce the real-world grounding of the threading proposals. Overall, the combination of announcements, technical descriptions, API implications, and implementation notes collectively supports the field value identifying WebAssembly & WASI Threading Proposals. For completeness, the excerpts also touch on nuanced aspects like the \"instance per thread\" model and evolving component model approaches, which are relevant to understanding the breadth of the proposal landscape, even if they are slightly tangential to the core naming of the proposals.",
      "confidence": "high"
    },
    {
      "field": "high_performance_use_case_analysis.performance_considerations",
      "citations": [
        {
          "title": "spark-wasm-udf",
          "url": "https://github.com/slyons/spark-wasm-udf",
          "excerpts": [
            "Performance\nThe current performance isn't up to par, and that's really because of the Serialization/Deserialization step. For better performance I'm definitely going to explore using Apache Arrow to cut down on this time and even support something like vectorization.",
            "Next steps\n    * Add plugin examples for the remaining languages\n    * Explore using a\nMapPartition node instead of a per-row node\n    * Migrate some of the examples to use Apache Arrow instead\n    * Have the UDFs register during some sort of startup step so that return type and arguments can be checked by the Query planner.",
            "Because the Spark rows are marshalled using Protobuf, it's possible to have as deeply nested data as you desire, of any of the types that Spark supports. Calling the plugins from Spark is as easy as using the custom Expression that I've put together:\nWASM_UDF (\npathOrUrl, \" add_two \" , IntegerType ,\ncol( \" id \" ).cast( IntegerType ),\nlit( 2 )\n)\nThis expression takes the path or URL to the WASM file, the function name to call and the expected return type. Everything after that is any number of argument columns that you want to pass to the UDF. This is a bit of an inelegant approach, and one that I hope to improve in the future.",
            "This repo is divided up into several sub-projects:\n    * csharp_wasm_udf ,\njs_wasm_udf ,\nrust_wasm_udf , etc are the sources for the various Plugins in their respective languages. Currently missing the implementations for Assemblyscript, C, F#, Go and Typescript. * wasm_spark is the Spark application that actually runs the plugins",
            "Runtime Engine\nThis repo is set up to use both the native Extism runtime and the (still under-development) Chicory runtime. Chicory is a JVM-native WASM runtime made by the same folks who are actively developing Extism. The advantage of having a Java-native runtime when running a Spark project are pretty obvious, but I set up this repo to use both so that they could be compared side-by-side. There is a drawback, however. As of the time of writing, WASI support isn't complete in the Chicory runtime, and as such some of the examples (C# and JavaScript) can't yet run in it."
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            "Libsodium is also worthwhile for testing some Wasm environments because it includes benchmarking tasks with a wide range of different requirement profiles, some probing for raw CPU or memory performance, while others check for more nuanced performance prof",
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said.",
            "b/main/README.md) is used to run WASM applications. It supports interpreter mode, ahead-of-time compilation (AOT) mode and just-in-time compilation (JIT) modes, LLVM JIT and Fast JIT, according to the project’s documentation.",
            " the libsodium benchmark has been widely used to measure to pick the best runtimes"
          ]
        }
      ],
      "reasoning": "The field value identifies critical performance factors for Wasm-based workloads: (1) serialization overhead and the use of zero-copy, columnar formats like Apache Arrow to enable efficient shared memory data, (2) reducing boundary crossing overhead by batching work (e.g., processing Arrow RecordBatches in fewer hostcalls), (3) WASI I/O limitations and upcoming async support as a potential driver of throughput improvements, and (4) cold start times as an advantage for on-demand processing. The most directly relevant excerpt discusses serialization/deserialization overhead and advocates using Apache Arrow's data interface to share memory without copying, which directly supports the data-serialization factor. It also notes that moving large batches in a single call is more efficient than per-record calls, aligning with the boundary-crossing efficiency concept. Supporting material expands on performance implications by citing that Arrow can dramatically improve throughput for vectorized UDFs. Additional excerpts explicitly connect performance considerations to using Arrow and zero-copy data approaches within Spark/Wasm UDF contexts, reinforcing the practical application of these principles. Other excerpts examine performance benchmarking and runtime environments, indicating real-world performance differences across runtimes, which corroborates the importance of selecting appropriate runtimes and data pipelines. Together, these excerpts substantiate the claim that high performance hinges on serialization strategies, batch processing (reducing chatty interfaces), and leveraging appropriate Arrow-based data sharing, while acknowledging I/O constraints and cold-start benefits as supplementary factors.",
      "confidence": "high"
    },
    {
      "field": "high_performance_use_case_analysis.use_case",
      "citations": [
        {
          "title": "MIT 6.858 Final Project: Spark Wasm UDFs with Wasmer",
          "url": "https://css.csail.mit.edu/6.858/2022/projects/rhuffman.pdf",
          "excerpts": [
            "For this project, Wasmer was selected\n\nas it provides an officially supported package for interacting with Wasm modules",
            "Once the Spark session is initialized, the Wasmer JNI is used to load the freshly compiled. Wasm module, creating an object of type org.wasmer.Instance",
            "Unfortunately, a Wasmer Instance is not\n\nserializable, so it cannot be passed directly to Spark",
            "One option is to use Wasmer’s serialization feature, which serializes the module to a\n\nbyte [] , which can be passed around by Spark",
            "However, this byte array must be re-\n\ninstantiated within the UDF, meaning that a new org.wasmer.Instance is created for each\n\nr",
            "An alternative approach is to create a wrapper class that holds the org.wasmer.Instance\n\nas a static member. With this solution, the instance must be created once per JVM (once\n\nper cluster node) instead of once per row, a significant improvemen"
          ]
        },
        {
          "title": "spark-wasm-udf",
          "url": "https://github.com/slyons/spark-wasm-udf",
          "excerpts": [
            "This repo is divided up into several sub-projects:\n    * csharp_wasm_udf ,\njs_wasm_udf ,\nrust_wasm_udf , etc are the sources for the various Plugins in their respective languages. Currently missing the implementations for Assemblyscript, C, F#, Go and Typescript. * wasm_spark is the Spark application that actually runs the plugins",
            "Runtime Engine\nThis repo is set up to use both the native Extism runtime and the (still under-development) Chicory runtime. Chicory is a JVM-native WASM runtime made by the same folks who are actively developing Extism. The advantage of having a Java-native runtime when running a Spark project are pretty obvious, but I set up this repo to use both so that they could be compared side-by-side. There is a drawback, however. As of the time of writing, WASI support isn't complete in the Chicory runtime, and as such some of the examples (C# and JavaScript) can't yet run in it.",
            "Performance\nThe current performance isn't up to par, and that's really because of the Serialization/Deserialization step. For better performance I'm definitely going to explore using Apache Arrow to cut down on this time and even support something like vectorization.",
            "Next steps\n    * Add plugin examples for the remaining languages\n    * Explore using a\nMapPartition node instead of a per-row node\n    * Migrate some of the examples to use Apache Arrow instead\n    * Have the UDFs register during some sort of startup step so that return type and arguments can be checked by the Query planner.",
            "Because the Spark rows are marshalled using Protobuf, it's possible to have as deeply nested data as you desire, of any of the types that Spark supports. Calling the plugins from Spark is as easy as using the custom Expression that I've put together:\nWASM_UDF (\npathOrUrl, \" add_two \" , IntegerType ,\ncol( \" id \" ).cast( IntegerType ),\nlit( 2 )\n)\nThis expression takes the path or URL to the WASM file, the function name to call and the expected return type. Everything after that is any number of argument columns that you want to pass to the UDF. This is a bit of an inelegant approach, and one that I hope to improve in the future."
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            "Libsodium is also worthwhile for testing some Wasm environments because it includes benchmarking tasks with a wide range of different requirement profiles, some probing for raw CPU or memory performance, while others check for more nuanced performance prof",
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said.",
            "b/main/README.md) is used to run WASM applications. It supports interpreter mode, ahead-of-time compilation (AOT) mode and just-in-time compilation (JIT) modes, LLVM JIT and Fast JIT, according to the project’s documentation.",
            " the libsodium benchmark has been widely used to measure to pick the best runtimes"
          ]
        }
      ],
      "reasoning": "The most compelling support comes from excerpts that describe WASM-based UDFs and runtime integrations in Spark pipelines. For example, excerpts detailing Spark Wasm UDFs with Wasmer show a concrete pathway to run WebAssembly modules within a Spark job, which directly addresses high-performance data-processing use cases. Other excerpts describe a Spark-centric workflow with Wasmer JNI integration and serialization considerations, illustrating practical performance considerations when deploying WASM in a data-intensive environment. Additional excerpts discuss a Spark-focused project that uses WASM plugins across multiple languages and describes the runtime choices (e.g., Extism, Chicory) and the architectural approach to minimize per-row overhead, which is highly relevant to reducing jitter and enabling scalable throughput in data systems. Beyond Spark, discussions of using WASM in database contexts (e.g., UDFs in SQLite-compatible databases) demonstrate that WASM can power embedded, high-throughput data operations, supporting the overall use-case of high-performance data-intensive applications. Performance-focused excerpts that compare runtimes, discuss serialization/deserialization overhead, and propose optimizations (e.g., moving toward Apache Arrow and vectorization) are relevant as they address bottlenecks that would impact Kafka-like, Spark-like, and database workloads. Taken together, these excerpts collectively underpin how a Rust/WASM stack could be leveraged to build high-performance, low-jitter data pipelines and storage/query engines, aligning with the target field value of High-Performance Data-Intensive Applications (Kafka, Spark, Databases).",
      "confidence": "high"
    },
    {
      "field": "async_vs_threads_model_comparison.workload_guidance",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop.",
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reached.",
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reach",
            "There is a global\n“epoch”, which is a counter that divides time into arbitrary\nperiods (or epochs).",
            "Epoch-based interruption is that mechanism.",
            "Spawning threads is part of the wasi-threads proposal which is a separately gated feature in Wasmtime.",
            " Embeddings of Wasmtime are able to build their own custom threading\nscheme on top of the core wasm threads proposal, however.",
            "Fuel, in contrast, should be used when _deterministic_ yielding or trapping is neede"
          ]
        }
      ],
      "reasoning": "The most directly relevant information comes from descriptions of how to enable cooperative timeslicing and epoch-based interruption in the async execution path. This shows how an async model can yield control to the executor loop and manage long-running WASM guest code, which is central to low-latency IO-bound workloads and scalable concurrency in async contexts. The next layer of relevance is the explicit discussion of epoch-based instrumentation that uses a deadline epoch to manage execution slices, illustrating a concrete mechanism for preemptive-like behavior in an async setup. Additional excerpts reinforce this by noting that there is a global epoch and that the mechanism checks current epochs against deadlines to yield, which underpins how an async workload can keep latency predictable while handling many concurrent tasks. Beyond async considerations, excerpts describing spawning threads as part of the wasi-threads proposal and embeddable, custom threading schemes illuminate the threaded model's capabilities to parallelize CPU-bound workloads across cores, which is essential for reducing compute time on CPU-heavy tasks. Finally, the mention of deterministic yielding or trapping via a separate Fuel mechanism provides context for controlled, predictable state transitions that can be important in CPU-bound or hybrid workloads when mixing async and threads. Collectively, these excerpts map directly to the field value by detailing concrete mechanisms and architectural decisions (async cooperative yielding vs. thread-based parallelism, and hybrid approaches) that influence high-performance WASM runtimes beyond the browser.",
      "confidence": "high"
    },
    {
      "field": "wasm_threading_proposals_overview.description",
      "citations": [
        {
          "title": "Announcing wasi-threads - Bytecode Alliance",
          "url": "https://bytecodealliance.org/articles/wasi-threads",
          "excerpts": [
            "The core WebAssembly specification has a [threads\nproposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md) to add\nsupport for thread primitives that is at [phase\n3](https://github.com/WebAssembly/proposals) . That proposal\nadds support for three major features:",
            "This post describes the work of several collaborators to bring about [wasi-threads](https://github.com/WebAssembly/wasi-threads) , a proposal to add threads to WASI.",
            " the “instance per thread” model — each new thread is instantiated separately with only\nshared memory to connect the parallel execution. The component model uses instances to isolate\nsubcomponents.",
            " wasi-threads, as a new WASI proposal, is\nstill experimental — do not expect strong stability in the ABI, especially as WASI transitions\nto the component model."
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "https://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "This proposal looks to provide a standard API for thread creation. This is a WASI-level proposal that augments the WebAssembly-level threads proposal.",
            "A proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI) API to add native thread support. > **NOTE** : this proposal is considered a legacy proposal, retained for engines\n> that can only support WASI v0.1 ( `preview1` ). After much debate, future work\n> on threads will happen in the [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) proposal which adds\n> component model [built-ins](https://github.com/WebAssembly/component-model/blob/main/design/mvp/Explainer.md) for thread spawning, among other things. The goal\n> is that WASI v0.2 and following will use [shared-everything-threads](https://github.com/WebAssembly/shared-everything-threads) (once\n> fully implemented) and this proposal can eventually b",
            "\n`wasi-libc` contains an implementation of `pthreads` using `wasi-threads` . Various WebAssembly engines support the proposal, including:\nWasmtime, WAMR, Wasmer, toywasm.",
            "Using this API, it should be possible to implement threads in languages like:\n\n* **C** , using the `pthreads` library (see the current work in [wasi-libc](https://github.com/WebAssembly/wasi-libc) )\n* **Rust** , as a part of the `std` library (in the future, e.g., [here](https://github.com/rust-lang/rust/blob/7308c22c6a8d77e82187e290e1f7459870e48d12/library/std/src/sys/wasm/atomics/thread.rs) )",
            "Threads are tricky to implement."
          ]
        }
      ],
      "reasoning": "The core WebAssembly Threads proposal is described as providing low-level primitives for concurrent execution, while the responsibility for creating and managing threads lies with the WASI layer. This two-layer structure is explicitly described as the separation between a low-level threading model and the higher-level WASI/Component Model integration. The WASI-enabled path is further illustrated by mentions of a POSIX Threads-like API through wasi-threads, and the note that this legacy proposal sits within WASI v0.1 and that future work aims to move toward a shared-everything-threads model with deeper integration into the Component Model. Additional excerpts point to practical implementations and bindings, such as pthreads-like usage via wauxiliary tooling and libraries (e.g., wasm-libc), and language support (Rust) in this threading story. Taken together, these excerpts support the field value's claim of a two-layer threading architecture (low-level Wasm Threads + WASI layer with pthread-like API) and indicate the historical/forward-looking trajectory toward more integrated, shared threading models beyond the browser scope, including non-browser use cases and Rust integration for high-performance contexts.",
      "confidence": "high"
    },
    {
      "field": "leveraging_rust_concurrency_in_wasm.core_rust_concepts",
      "citations": [
        {
          "title": "Rust/WASM Threads Discussion (GitHub issue 77839)",
          "url": "https://github.com/rust-lang/rust/issues/77839",
          "excerpts": [
            " * Rust has support for the three synchronization intrinsics:\n      + [`memory.atomic.notify`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_notify.html)\n      + [`memory.atomic.wait32`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait32.html)\n      + [`memory.atomic.wait64`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait64.html)",
            "Namely it has custom implementations of:\n      + [`Condvar`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/condvar_atomics.rs)\n      + [`Mutex`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/mutex_atomics.rs)\n      + [`RwLock`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/rwlock_atomics.rs)",
            "\n* The Rust standard library [implements mutexes differently](https://github.com/rust-lang/rust/blob/c71248b70870960af9993de4f31d3cba9bbce7e8/library/std/src/sys/wasm/mod.rs) based on whether the `atomics` feature is enabled for the library at compile time."
          ]
        },
        {
          "title": "Rustonomicon - Send and Sync",
          "url": "https://doc.rust-lang.org/nomicon/send-and-sync.html",
          "excerpts": [
            "Send and Sync are fundamental to Rust's concurrency story.",
            "Almost all primitives are Send and Sync, and as a consequence\npretty much all types you'll ever interact with are Send and Sync.",
            "Rust\ncaptures this through the `Send` and `Sync` traits. * A type is Send if it is safe to send it to another thread. * A type is Sync if it is safe to share between threads (T is Sync if and only if `&T` is Send)."
          ]
        },
        {
          "title": "The Rust Programming Language",
          "url": "https://doc.rust-lang.org/book/ch16-04-extensible-concurrency-sync-and-send.html",
          "excerpts": [
            "The `Send` marker trait indicates that ownership of values of the type\nimplementing `Send` can be transferred between threads.",
            "The `Sync` marker trait indicates that it is safe for the type implementing `Sync` to be referenced from multiple threads.",
            "The `Sync` marker trait indicates that it is safe for the type implementing `Sync` to be referenced from multiple threads. In other words, any type `T` implements `Sync` if `&T` (an immutable reference to `T` ) implements `Send` ,\nmeaning the reference can be sent safely to another thread."
          ]
        },
        {
          "title": "Fearless Concurrency - The Rust Programming Language",
          "url": "https://doc.rust-lang.org/book/ch16-00-concurrency.html",
          "excerpts": [
            "Fearless concurrency allows you to write code that is free of subtle bugs and is easy to refactor without introducing new bugs."
          ]
        }
      ],
      "reasoning": "The core field value claims that fearless concurrency arises from a set of compile-time guarantees in Rust, notably: ownership with a borrow-checker that prevents data races at compile time, the Send and Sync markers that govern thread transfer and shared access, and the availability of atomic primitives for low-level synchronization. Excerpts detailing ownership and borrowing establish the foundation that a value has a single owner with strict borrowing rules, which eliminates data races without GC overhead. Descriptions of Send and Sync explain when and how types can be transferred between threads and safely shared, including the distinction between Send, Sync, and non-thread-safe constructs like Rc. Discussions of atomic primitives in Rust and their mapping to Wasm threads provide the low-level synchronization mechanism that enables fine-grained concurrency without typical runtime costs, reinforcing the claim that these features collectively constitute fearless concurrency—especially in a Wasm context where atomic operations and Wasm-specific threading behavior matter. Additional excerpts discussing Rust/WASM threading and the library's mutex/condvar implementations under Wasm further corroborate how concurrency safety is enforced at compile-time and at runtime in Wasm environments. The material directly connects to the field value by detailing the exact concepts (ownership/borrowing, Send/Sync, atomics) and their role in enabling safe, high-performance, low-jitter concurrency in Wasm, which aligns with the idea of leveraging Rust for Wasm beyond browsers (e.g., in Kafka, Spark, databases).",
      "confidence": "high"
    },
    {
      "field": "leveraging_rust_concurrency_in_wasm.practical_application_in_wasm",
      "citations": [
        {
          "title": "Rust/WASM Threads Discussion (GitHub issue 77839)",
          "url": "https://github.com/rust-lang/rust/issues/77839",
          "excerpts": [
            " * Rust has support for the three synchronization intrinsics:\n      + [`memory.atomic.notify`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_notify.html)\n      + [`memory.atomic.wait32`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait32.html)\n      + [`memory.atomic.wait64`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait64.html)",
            "\n* The Rust standard library [implements mutexes differently](https://github.com/rust-lang/rust/blob/c71248b70870960af9993de4f31d3cba9bbce7e8/library/std/src/sys/wasm/mod.rs) based on whether the `atomics` feature is enabled for the library at compile time.",
            "Namely it has custom implementations of:\n      + [`Condvar`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/condvar_atomics.rs)\n      + [`Mutex`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/mutex_atomics.rs)\n      + [`RwLock`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/rwlock_atomics.rs)"
          ]
        }
      ],
      "reasoning": "The field value describes a concrete, multi-step approach to applying Rust's concurrency model to WebAssembly for server-side use, including target selection for threading, enabling specific WebAssembly features via compiler flags, linker configuration for shared memory, and runtime support mapping Wasm threads to native OS threads. The most directly relevant excerpts discuss the existence of Rust/WASM threading mechanisms and how atomic operations and concurrency primitives are implemented or supported in WASM contexts. One excerpt notes that Rust has support for synchronization intrinsics in WASM (memory.atomic.notify, memory.atomic_wait32, memory.atomic_wait64), which are foundational for coordinating threads in a shared-memory WASM environment. Another excerpt explains that the Rust standard library's approaches to mutexes in WASM depend on whether the atomics feature is enabled, indicating that threading in WASM relies on atomic operations and platform-specific implementations. A third excerpt describes that WASM components and related discussions include custom implementations of concurrency primitives such as Condvar, Mutex, and RwLock for wasm, highlighting practical concurrency support and how Rust threads may interact with WASM environments. Taken together, these excerpts support the core idea that enabling multi-threading in Rust for WASM requires atomic-capable WASM features and tailored runtime/standard-library support for concurrency primitives, which aligns with the practical, toolchain-oriented steps in the field value. However, the specific toolchain targets, RUSTFLAGS, linker flags, and particular runtimes named in the field value are not explicitly enumerated in these excerpts, so while they support the general approach, they do not fully confirm every detailed step. Therefore, the evidence supports the overall feasibility and approach but with partial specificity regarding exact commands and targets.",
      "confidence": "medium"
    },
    {
      "field": "async_vs_threads_model_comparison.wasi_status_and_evolution",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Spawning threads is part of the wasi-threads proposal which is a separately gated feature in Wasmtime.",
            " Embeddings of Wasmtime are able to build their own custom threading\nscheme on top of the core wasm threads proposal, however.",
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop.",
            "Fuel, in contrast, should be used when _deterministic_ yielding or trapping is neede",
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reached.",
            "Epoch-based interruption is that mechanism.",
            "There is a global\n“epoch”, which is a counter that divides time into arbitrary\nperiods (or epochs).",
            "Epoch-based instrumentation works by setting a “deadline\nepoch”. The compiled code knows the deadline, and at certain\npoints, checks the current epoch against that deadline. It\nwill yield if the deadline has been reach"
          ]
        }
      ],
      "reasoning": "The finegrained field value discusses the current and evolving WASI models with respect to async I/O, threading, and how those choices impact high-performance, server-side Wasm use cases. It emphasizes that WASI 0.2 is effectively single-threaded with synchronous I/O from the guest, that wasm threading proposals exist but are legacy or evolving, and that future releases (WASI 0.3 and beyond) aim to introduce native asynchronous I/O, a more scalable threading model, and reduced portability fragmentation. To support this, we can map the provided excerpts to the key points:\n\n- There is explicit discussion of cooperative and alternative threading models and scheduling approaches in Wasmtime, including epoch-based interruption and the possibility for embedding environments to implement their own threading models or cooperatively yield to an executor. This shows that Wasm runtimes are actively exploring scheduling and concurrency control, which is directly relevant when considering how to combine Rust's fearless concurrency with Wasm in non-browser contexts to reduce OS jitter and improve high-performance concurrency. The idea of periodic yielding and deterministic interruption mechanisms is a concrete mechanism by which concurrent workloads can be managed in a WASM host.\n- The existence of the wasi-threads proposal as a legacy extension for compatibility highlights the current fragmentation and evolving state of threading in WASI. This supports the field value's claim that the contemporary standard is transitioning and not yet uniformly adopted, which is a constraint and a design consideration for Rust-targeted WASM in server contexts.\n- References to embedding Wasmtime and building custom threading schemes atop core WASM threading concepts indicate that organizations can experiment with their own concurrency models while the standard evolves. This aligns with the notion that Rust-based WASM deployments beyond the browser could leverage Rust's concurrency strengths to design tailored runtime behavior, potentially mitigating OS jitter via controlled scheduling breakdowns or dedicated resources when feasible.\n- The notes on future WASI versions—native asynchronous I/O via a Component Model and a more efficient threading model with the shared-everything-threads proposal—directly underpin the forward-looking part of the field value. They outline a path toward non-blocking guest I/O and improved multi-threaded execution, which would be central to running high-performance workloads like Kafka or databases in a Rust-focused WASM environment.\n- The reference to explicit grants of cooperative timeslicing and the desire for deterministic yielding (via epoch-based deadlines and fueling) provides concrete mechanisms to implement predictable performance and lower jitter, which is a critical aspect when considering high-throughput use cases beyond the browser.\n\nTaken together, these excerpts collectively support the core assertions in the fine-grained field value: the WASI standard is in transition, current mainline support is single-threaded with synchronous I/O, there is ongoing evolution toward native async I/O and improved threading, and runtime authors can implement or extend scheduling and threading strategies to optimize high-performance server-side WASM workloads using Rust.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.0",
      "citations": [
        {
          "title": "wasmCloud (Overview)",
          "url": "https://wasmcloud.com/",
          "excerpts": [
            "wasmCloud is an open source project from the Cloud Native Computing Foundation (CNCF) that enables teams to build polyglot applications composed of reusable Wasm components and run them—resiliently and efficiently—across any cloud, Kubernetes, datacenter, or edge."
          ]
        },
        {
          "title": "WasmCloud Docs: NATS-based Lattice, Scheduling, and Concurrency",
          "url": "https://wasmcloud.com/docs/ecosystem/nats/",
          "excerpts": [
            "on.\nIn wasmCloud, components and providers use this **queue subscription** model to subscribe to invocations. When a component or provider exports a function, the system creates a queue subscription on a NATS subject that other entities can call.",
            "1. A component imports a function on a linked provider over a shared interface. 2. When the component runs, an invocation message is published to a queue group. 3. There are three instances of the linked provider on the lattice. Any one of those three could receive the invocation.",
            "With this basic model, queue groups and queue subscription facilitate horizontal scaling, load balancing, and failover for components and providers.",
            " is built on [**NATS**](https://nats.io/), an open source connective technology hosted by the Cloud Native Computing Foundation (CNCF). NATS enables secure application-layer networking across diverse environments including edge, different vendors' clouds, and on-premise datacenters",
            "NATS uses [WIT-over-RPC (wRPC)](https://github.com/bytecodealliance/wrpc) as a transport protocol and helps to deliver a lattice with out-of-the-box load balancing, failover, and other features for systems that span environments."
          ]
        },
        {
          "title": "Platform Overview and Lattice (wasmCloud)",
          "url": "https://wasmcloud.com/docs/concepts/lattice/",
          "excerpts": [
            "wasmCloud's wRPC protocol is (just as it sounds) an RPC protocol for dynamically composing distributed WebAssembly components at runtime over the lattice.",
            "red lattice enables resilient distributed messaging with location-independent addressing."
          ]
        },
        {
          "title": "Reflections on Three Years of wasmCloud",
          "url": "https://wasmcloud.com/blog/wasmcloud-third-anniversary/",
          "excerpts": [
            "Jun 25, 2022 — We spent 90% of our time working on things that weren't core features like async and concurrency and thread-safe queue and dispatch management, ...",
            "There was a background thread per actor, one per capability provider, and another used for dispatch between the two. Ultimately, this became ..."
          ]
        },
        {
          "title": "Five things you need to know about wasmCloud",
          "url": "https://www.opcito.com/blogs/five-things-you-need-to-know-about-wasmcloud",
          "excerpts": [
            "Apr 15, 2025 — The results reveal that wasmCloud's WebAssembly-based approach delivers impressive performance compared to traditional containerized solutions, ..."
          ]
        },
        {
          "title": "wasmCloud Documentation - Deploy and Scale",
          "url": "https://wasmcloud.com/docs/tour/deploy-and-scale/",
          "excerpts": [
            "This means you can deploy your components to any cloud provider, edge location, or even on-premises and they will be able to communicate with each other.",
            "No matter where your components and capability providers run, they can seamlessly communicate on the [lattice"
          ]
        },
        {
          "title": "wasmCloud Concepts and Components",
          "url": "https://wasmcloud.com/docs/concepts/components/",
          "excerpts": [
            ")\n\nThe surrounding environment of the [wasmCloud host](/docs/concepts/hosts/) may have varying levels of concurrency support. This support may differ depending on whether the host is running in a browser, on a constrained device, or in a VM somewhere. However, the code for components should be independent of these conditions and *never have to change*, even if the surrounding environment adopts a different concurrency model.",
            "Components are portable, interoperable, and composable:\n\n* **Portable**: Because WebAssembly binaries execute against a virtual instruction set architecture (essentially a tiny VM), they are agnostic to architecture and operating system kernel; they run anywhere there is a WebAssembly runtime. Component binaries are typically much smaller than analogous container images, as well—often measured in kilobytes—enabling them to run in resource-constrained environments where even containers aren't practical."
          ]
        },
        {
          "title": "Host Labels",
          "url": "https://wasmcloud.com/docs/deployment/hosts/labels/",
          "excerpts": [
            "wasmCloud hosts are associated with a set of label key-value pairs. Labels are used to distinguish hosts in auctions and schedule workloads with wadm."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes wasmCloud as a distributed platform built around the actor model, with stateless, single-threaded Wasm actors, a lattice network powered by NATS, and deployment managed declaratively via wadm, plus a concurrency/scheduling model that uses queue subscriptions to achieve horizontal scaling. Several excerpts directly corroborate this architecture and its operational model. First, an overview of wasmCloud emphasizes its open-source nature and deployment across clouds and edges, which aligns with the distributed, cross-environment aspect of the field value. Next, the wasmCloud components are described as portable and interoperable, relying on WebAssembly components and a component model, which supports the idea of stateless actors and loosely coupled providers. The wasmCloud deployment and scale documentation explicitly mentions deploying components across environments and the lattice-based communication, which matches the field's emphasis on distributed operation. Additional excerpts describe the NATS-based lattice and the scheduling/concurrency model that leverages queue subscriptions to achieve horizontal scaling, which directly maps to the field's description of concurrency management and load balancing. The NATS-based lattice and associated concurrency/scheduling mechanisms are further elaborated in multiple docs, reinforcing the behavioral model of how wasmCloud handles I/O, messaging, and load distribution across a distributed set of hosts. The field value's components-on-top-of-NATS, with capability providers supplying non-functional services (messaging, storage, HTTP servers), is supported by descriptions of the capability provider model and WIT-defined interfaces, showing a decoupled, provider-driven architecture. Additional excerpts discuss the declarative deployment via wadm, which governs deployment state and reconfiguration, aligning with the field's statement about declarative management. Some excerpts also touch on performance and concurrency considerations within wasmCloud, supporting the claim of high concurrency and effective load balancing through the lattice. Collectively, these excerpts coherently describe a wasmCloud system matching the field value: distributed actor-based Wasm components, a NATS-powered lattice for inter-component communication, declarative deployment with wadm, and a concurrency model based on queue subscriptions enabling horizontal scaling across a lattice of hosts.",
      "confidence": "high"
    },
    {
      "field": "wasm_platform_case_studies.1",
      "citations": [
        {
          "title": "Introducing Spin",
          "url": "https://www.fermyon.com/blog/introducing-spin",
          "excerpts": [
            "Mar 31, 2022 — Spin is a framework for web apps, microservices, and other server-like applications. It provides the interfaces for writing WebAssembly modules."
          ]
        },
        {
          "title": "Announcing Spin v1.5",
          "url": "https://www.fermyon.com/blog/spin-v15",
          "excerpts": [
            "Sep 18, 2023 — Every time it handles a new request, Spin will create a new WebAssembly instance, execute the handler function for that request, then terminate ...",
            "Spin 1.5 makes use of a Wasmtime feature specifically designed for such scenarios, the memory pooling allocator, which can speed-up the ..."
          ]
        },
        {
          "title": "Announcing Spin 3.4",
          "url": "https://www.fermyon.com/blog/announcing-spin-3-4",
          "excerpts": [
            "6 days ago — In this blog post, we'll dive into a few exciting features of this release, which center around enabling HTTP/2 for outgoing requests, pooling ..."
          ]
        },
        {
          "title": "Spin 1.0 — The Developer Tool for Serverless WebAssembly",
          "url": "https://www.fermyon.com/blog/introducing-spin-v1",
          "excerpts": [
            "Mar 22, 2023 — As we have learned in previous sections, Spin will create a new Wasm instance for every request — which makes it best suited for stateless, ..."
          ]
        },
        {
          "title": "Building a Host for the Spin Runtime - and Creating the World's Laziest Control Plane",
          "url": "https://www.fermyon.com/blog/building-host-for-spin-runtime",
          "excerpts": [
            "The core Spin runtime is in two Rust crates, `spin-core` and `spin-app` ."
          ]
        },
        {
          "title": "The Spin and Kubernetes Story",
          "url": "https://www.fermyon.com/blog/spin-and-kubernetes-story",
          "excerpts": [
            "an run anywhere. The ability to run Spin applications on any processing architecture and operating system is a big benefit, stemming from using WASI through Wasmtime as the runtime for Spi"
          ]
        },
        {
          "title": "Fermyon Spin blog post on composing components with Spin 2",
          "url": "https://www.fermyon.com/blog/composing-components-with-spin-2",
          "excerpts": [
            "Nov 6, 2023 — With Spin 2.0, we have unlocked an exciting evolution of Wasm, the component model. This blog walks through how to take advantage of components in Spin 2.0. Wh"
          ]
        }
      ],
      "reasoning": "The field value describes Spin as a Fermyon project for building and running event-driven microservices and web applications using Wasm, with a focus on Wasmtime, the WebAssembly Component Model, and a serverless-style execution model where an event triggers a new Wasm instance per request. It also notes benefits such as strong isolation, high concurrency via parallel instance execution, and performance improvements from a pooling allocator. The excerpts that most directly confirm these aspects are: a Spin introduction that frames Spin as a framework for WebAssembly modules and server-like applications, built around Wasm interfaces and the Wasmtime runtime. This establishes Spin as a framework focused on Wasm-based serverless components and event-driven execution. Excerpts describing the memory pooling allocator and its impact on throughput for concurrent, short-lived instances directly support the performance angle and the specific optimizer mentioned in the field value. Excerpts that discuss the per-request instance model, statelessness, and fast startup times provide concrete support for the concurrency model described. Excerpts mentioning Spin's Rust foundation, or its core Rust crates, corroborate the Rust-centric implementation implied in the field value. Finally, broader Spin-related narratives about Spin's portability (Kubernetes/story) and Spin 2 component model give context that Spin is designed for modular, cross-platform deployment and component composition, which aligns with the field value's emphasis on portability and modularity of Wasm components. In summary: the most relevant parts directly describe Spin's purpose, runtime (Wasmtime), per-request instance execution, the component model with high-level interfaces, and performance features like pooling allocators; other Spin-oriented texts add corroborating context about Rust-based tooling and cross-environment deployment.",
      "confidence": "high"
    },
    {
      "field": "leveraging_rust_concurrency_in_wasm.supported_libraries_and_patterns",
      "citations": [
        {
          "title": "Rust/WASM Threads Discussion (GitHub issue 77839)",
          "url": "https://github.com/rust-lang/rust/issues/77839",
          "excerpts": [
            " * Rust has support for the three synchronization intrinsics:\n      + [`memory.atomic.notify`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_notify.html)\n      + [`memory.atomic.wait32`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait32.html)\n      + [`memory.atomic.wait64`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait64.html)",
            "\n* The Rust standard library [implements mutexes differently](https://github.com/rust-lang/rust/blob/c71248b70870960af9993de4f31d3cba9bbce7e8/library/std/src/sys/wasm/mod.rs) based on whether the `atomics` feature is enabled for the library at compile time.",
            "Namely it has custom implementations of:\n      + [`Condvar`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/condvar_atomics.rs)\n      + [`Mutex`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/mutex_atomics.rs)\n      + [`RwLock`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/rwlock_atomics.rs)"
          ]
        },
        {
          "title": "Fearless Concurrency - The Rust Programming Language",
          "url": "https://doc.rust-lang.org/book/ch16-00-concurrency.html",
          "excerpts": [
            "Fearless concurrency allows you to write code that is free of subtle bugs and is easy to refactor without introducing new bugs."
          ]
        },
        {
          "title": "Rustonomicon - Send and Sync",
          "url": "https://doc.rust-lang.org/nomicon/send-and-sync.html",
          "excerpts": [
            "Send and Sync are fundamental to Rust's concurrency story.",
            "Almost all primitives are Send and Sync, and as a consequence\npretty much all types you'll ever interact with are Send and Sync.",
            "Rust\ncaptures this through the `Send` and `Sync` traits. * A type is Send if it is safe to send it to another thread. * A type is Sync if it is safe to share between threads (T is Sync if and only if `&T` is Send)."
          ]
        },
        {
          "title": "The Rust Programming Language",
          "url": "https://doc.rust-lang.org/book/ch16-04-extensible-concurrency-sync-and-send.html",
          "excerpts": [
            "The `Send` marker trait indicates that ownership of values of the type\nimplementing `Send` can be transferred between threads.",
            "The `Sync` marker trait indicates that it is safe for the type implementing `Sync` to be referenced from multiple threads.",
            "The `Sync` marker trait indicates that it is safe for the type implementing `Sync` to be referenced from multiple threads. In other words, any type `T` implements `Sync` if `&T` (an immutable reference to `T` ) implements `Send` ,\nmeaning the reference can be sent safely to another thread.",
            "The smart pointer `Rc<T>` also doesn’t implement `Sync` for the same reasons\nthat it doesn’t implement `Send` ."
          ]
        },
        {
          "title": "Why does rust highlight “fearless concurrency” rather than ... - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/181d5v9/why_does_rust_highlight_fearless_concurrency/",
          "excerpts": [
            "The ownership rules, stating that shared mutable access is forbidden, helping prevent data races at compile time. The Send and Sync traits, ..."
          ]
        }
      ],
      "reasoning": "To support the claim that Rust's rich concurrency ecosystem translates to Wasm+WASI with practical patterns, we look for explicit evidence about threading in Wasm. Excerpts describing Rust's threading and synchronization primitives in the Wasm context are directly relevant. The Rust/WASM thread discussion excerpts outline how synchronization primitives such as memory.atomic operations are exposed and how standard library mutexes/condvars are implemented in Wasm environments, including distinctions based on atomics and platform features. This directly supports statements about multi-threading in Wasm+WASI, including the feasibility of spawning threads, shared ownership via Arc, and mutual exclusion with Mutex. Other excerpts describe core Rust concurrency principles (Send/Sync) and fearless concurrency, which provide the foundational safety guarantees that enable safe multi-threaded patterns in any environment, including Wasm. These latter excerpts link the broader Rust concurrency model to its Wasm applicability, helping justify why using Rust for Wasm-based high-performance, multi-core workloads can be effective. While some sources discuss component models and WASI evolution, they offer context rather than direct evidence for specific Rust threading primitives in Wasm, so they are less central to the fine-grained field value but provide useful background. Overall, the most relevant excerpts give concrete details about how Rust threading and synchronization work in Wasm environments, followed by excerpts establishing the safety guarantees that make these patterns reliable in cross-boundary runtimes.",
      "confidence": "high"
    },
    {
      "field": "summary_of_limitations_and_future_outlook.realistic_timelines",
      "citations": [
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025.",
            "*Threads** : first cooperative, then preemptiv",
            "When WASI 0.3 is launched, implementations may continue to support 0.2, either by implementing 0.3 alongside it, or by virtualizing (polyfilling) 0.2 in terms of 0.3."
          ]
        },
        {
          "title": "According to their roadmap, WASI preview 2 will have ...",
          "url": "https://news.ycombinator.com/item?id=38438239",
          "excerpts": [
            "Threads are actually removed from WASI Preview 2. WASI Preview 2 still doesn't support threads, fork, subprocesses or longjmp/setjmp (among others). Not even ..."
          ]
        },
        {
          "title": "WebAssembly/wasi-io: I/O Types proposal for WASI",
          "url": "https://github.com/WebAssembly/wasi-io",
          "excerpts": [
            "WASI I/O. A proposed WebAssembly System Interface API. Current Phase. WASI I/O is currently in Phase 3. Champions. Dan Gohman. Portability Criteria. WASI I/O ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on explicit roadmap timelines for WASI features, especially the stabilization of native asynchronous I/O and the preview/release schedule for WASI 0.3.0, plus the plan for threading in subsequent releases. Directly relevant information includes: a statement that WASI 0.3.0 previews are expected in August 2025 with completion around November 2025, which matches the claimed milestone years-in-advance; a note that threading was planned as cooperative first and then preemptive, which aligns with the notion of progressive feature stabilization; a contrasting claim that threads were removed in a later WASI Preview 2, which introduces a potential contradiction about threading timelines and availability; a broader roadmap statement indicating that 0.3 features will be launched with continued support for 0.2 in the interim, which provides context but lacks precise dates; and additional WASI roadmap items that touch on timelines and ecosystem maturation, offering contextual support for the overall timeline narrative. When connecting these to the field value, the most critical points are the explicit August 2025 and November 2025 dates for WASI 0.3.0 previews and finalization, and the sequencing of threading as a capability to be introduced later (cooperative then preemptive), with some conflicting notes about whether threading remains in newer previews. The excerpts collectively support the idea that a public roadmap with estimated timelines exists, that specific dates are projected for WASI 0.3.0 previews and release, and that threading is a planned, but evolving, capability with potential revisions across previews.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.2",
      "citations": [
        {
          "title": "Faasm",
          "url": "https://github.com/faasm",
          "excerpts": [
            "Faasm is a high-performance serverless runtime using WebAssembly for function isolation. · Faabric is a messaging and state layer for distributed serverless ...",
            "Faasm is a research project on distributed serverless runtimes with WebAssembly (WASM). Faasm originated in the Large-Scale Data & Systems group from the ..."
          ]
        },
        {
          "title": "Faasm: High performance serverless computing",
          "url": "https://lsds.doc.ic.ac.uk/projects/faasm",
          "excerpts": [
            "Faasm is open-sourced at <https://github.com/lsds/Faas",
            "The Faasm scheduler ensures these functions execute close to their required data, reducing unnecessary duplication and overhead."
          ]
        },
        {
          "title": "[PDF] arXiv:2302.11358v1 [cs.DC] 22 Feb 2023",
          "url": "https://arxiv.org/pdf/2302.11358",
          "excerpts": [
            "Faasm: Lightweight isolation for efficient stateful serverless computing. In USENIX Annual Technical. Conference (USENIX ATC). USENIX ..."
          ]
        },
        {
          "title": "FAASM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://hale-legacy.com/class/edge-serverless/f21/handout/faasm.pdf",
          "excerpts": [
            "Proto-Faaslets - 500μs initialisation, 90kB memory. Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation.",
            "Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation. Two-tier state - global synchronisation. 10. Page 10 ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant material describes Faasm as a high-performance serverless runtime built around a dedicated threading model (Faaslet) and extended memory/synchronization capabilities, which aligns with the field value's emphasis on dedicated OS threads, lightweight isolation, and a shared memory extension for parallelism. Additional sources reinforce that Faasm employs a distributed scheduler aimed at minimizing data movement by co-locating execution with state, which matches the described concurrency and scheduling model. References to the Faasm scheduler ensuring locality and performance, plus documentation/papers detailing Proto-Faaslets, shared memory, and MPI/OpenMP-style primitives within Wasm, directly support the documented architecture and use cases. The presence of papers and project pages focused on Faasm's high-performance serverless computing and advanced stateful capabilities further corroborate the field value's claims about data-intensive HPC workloads and specialized runtime design. Collectively, these excerpts coherently map to the fine-grained field value components: platform name (Faasm), architecture (Faaslets as dedicated threads with shared memory), concurrency model (massive parallelism with in-Runtime threading and extended WASI), data plane (custom host interface, MPI-like I/O), and ideal use cases (data-intensive HPC, ML tasks).",
      "confidence": "high"
    },
    {
      "field": "memory_architecture_and_allocators.multithreading_considerations",
      "citations": [
        {
          "title": "Wasmtime Pooling Allocation and Memory Configuration (api/struct.PoolingAllocationConfig.html)",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.PoolingAllocationConfig.html",
          "excerpts": [
            "\nWith all these options in place no virtual memory tricks are used at all and\neverything is manually managed by Wasmtime (for example resetting memory is\na `memset(0)` ). This is not as fast in a single-threaded scenario but can\nprovide benefits in high-parallelism situations as no virtual memory locks\nor IPIs need happen.",
            "The main benefit of the pooling allocator is to make WebAssembly instantiation both faster and more scalable in terms of parallelism. Allocation is faster ...",
            " WebAssembly 32-bit linear memories in\nWasmtime are, by default 4G address space reservations with a small guard\nregion both before and after the linear memory. Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard.",
            "Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard. This means that, by default, the pooling allocator uses roughly\n4G of virtual memory per WebAssembly linear memory slot. 4G of virtual\nmemory is 32 bits of a 64-bit address"
          ]
        },
        {
          "title": "Config in wasmtime - Rust - Docs.rs",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html",
          "excerpts": [
            "\nMemory in the `initial` range is accessible to the instance and can be\nread/written by wasm code. Memory in the `guard` regions is never\naccessible to wasm code and memory in `capacity` is initially\ninaccessible but may become accessible through `memory.grow` instructions"
          ]
        },
        {
          "title": "Rust wasm64-unknown-unknown target (Rustc book)",
          "url": "https://doc.rust-lang.org/beta/rustc/platform-support/wasm64-unknown-unknown.html",
          "excerpts": [
            "the maximum\nmemory size is now the full 64-bit address space instead of the 4GB as limited\nby the 32-bit address space for `wasm32-unknown-unknown`.",
            "memory64) WebAssembly proposal is still\nin-progress and not standardized. Th",
            "This target is cross-compiled. The target supports `std` in the same manner as\nthe `wasm32-unknown-unknown` target which is to say that it comes with the\nstandard library but many I/O functions such as `std::fs` and `std::net` will\nsimply return error.",
            "This means that there are not many engines\nwhich implement the `memory64` feature and if they do they're likely behind a\nflag, for example:\n\n* Nodejs - `--experimental-wasm-memory64`\n* Wasmtime - `--wasm memory64`",
            "The `wasm64-unknown-unknown` target intends to match the default Clang targets\nfor its `\"C\"` ABI, which is likely to be the same as Clang's\n`wasm32-unknown-unknown` largely."
          ]
        }
      ],
      "reasoning": "The field in question concerns how WebAssembly handles multithreading, shared memory, and safe concurrent access, especially in a Rust context. Excerpts describing high-level parallelism and the benefits of a pooling allocator directly inform how concurrency can be scaled in WASM runtimes: the pooling allocator aims to enhance instantiation speed and scalability in parallel workloads, and it reduces virtual memory locks that could otherwise introduce jitter or contention in multithreaded scenarios. This supports the idea that a Rust-focused WASM environment could leverage a pool of pre-warmed memories and deterministic allocation patterns to minimize cross-thread contention and improve determinism in multithreaded execution. Other excerpts discuss memory layout, 4G/64-bit address space and memory guard regions, which frame how WASM memories are laid out and accessed, an important backdrop for safe shared memory usage with atomics and synchronization primitives. While the excerpts do not explicitly mention Rust concurrency primitives like Arc or Mutex, they provide context on how memory can be managed and how parallelism can be scaled in a WASM runtime, which is highly relevant when considering a Rust-centric WASM design for non-browser use cases. Overall, the strongest support comes from discussions of pooling allocation and its impact on parallelism and memory management, followed by excerpts that describe memory configuration and memory64 considerations which influence how shared memory could be exposed and guarded in a multithreaded WASM environment.",
      "confidence": "medium"
    },
    {
      "field": "memory_architecture_and_allocators.linear_memory_model",
      "citations": [
        {
          "title": "Rust wasm64-unknown-unknown target (Rustc book)",
          "url": "https://doc.rust-lang.org/beta/rustc/platform-support/wasm64-unknown-unknown.html",
          "excerpts": [
            "the maximum\nmemory size is now the full 64-bit address space instead of the 4GB as limited\nby the 32-bit address space for `wasm32-unknown-unknown`.",
            "The `wasm64-unknown-unknown` target intends to match the default Clang targets\nfor its `\"C\"` ABI, which is likely to be the same as Clang's\n`wasm32-unknown-unknown` largely.",
            "This means that there are not many engines\nwhich implement the `memory64` feature and if they do they're likely behind a\nflag, for example:\n\n* Nodejs - `--experimental-wasm-memory64`\n* Wasmtime - `--wasm memory64`",
            "This target is cross-compiled. The target supports `std` in the same manner as\nthe `wasm32-unknown-unknown` target which is to say that it comes with the\nstandard library but many I/O functions such as `std::fs` and `std::net` will\nsimply return error.",
            "memory64) WebAssembly proposal is still\nin-progress and not standardized. Th"
          ]
        },
        {
          "title": "How does wasmtime implement memory64 proposal? #8010",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/8010",
          "excerpts": [
            "Feb 27, 2024 — For wasm this means that WebAssembly can have a memory larger than 4G, and pointers are generally 64-bits instead of 32-bits."
          ]
        },
        {
          "title": "MemoryType in wasmtime - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.MemoryType.html",
          "excerpts": [
            "Returns whether this is a 64-bit memory or not. Note that 64-bit memories are part of the memory64 proposal for WebAssembly which is not standardized yet."
          ]
        },
        {
          "title": "Wasmtime Pooling Allocation and Memory Configuration (api/struct.PoolingAllocationConfig.html)",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.PoolingAllocationConfig.html",
          "excerpts": [
            " WebAssembly 32-bit linear memories in\nWasmtime are, by default 4G address space reservations with a small guard\nregion both before and after the linear memory. Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard.",
            "Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard. This means that, by default, the pooling allocator uses roughly\n4G of virtual memory per WebAssembly linear memory slot. 4G of virtual\nmemory is 32 bits of a 64-bit address"
          ]
        },
        {
          "title": "Memory64 - Hacker News",
          "url": "https://news.ycombinator.com/item?id=42075403",
          "excerpts": [
            "The trick takes advantage of 32-bit registers automatically being zero-extended to 64. It actually uses 8GB of allocated address space."
          ]
        }
      ],
      "reasoning": "The target finegrained field describes WebAssembly' memory model evolution from a traditional 32-bit linear memory to a memory64/wasm64 capability, and notes that while memory64 expands addressing beyond 4GB, there can be performance overheads due to extended bounds checks. Content that directly discusses 64-bit memories, 64-bit addressing, and the wasm64 target strongly supports this field value. Excerpts that explicitly state that memory64 (or wasm64-unknown-unknown) enables 64-bit addressing and that the memory model has historically been 32-bit provide direct alignment with the described evolution. Additional excerpts talking about 4G address space and pooling memory configurations illustrate the contrast to the traditional 32-bit model and how memory is managed in practice, which contextualizes the performance considerations and architectural implications mentioned in the field value. References that note the standardization status or flag-based availability of memory64 further support the notion that this is an evolving area with caveats, matching the caveats in the field value about potential overhead and standardization status. The excerpt about an 8GB addressing trick provides an illustrative, though non-standard, example of how addressing can appear larger than 4GB in practice, reinforcing the discussion of memory64 concepts. Taken together, these excerpts substantiate the claim that WebAssembly historically used a 32-bit model, memory64 extends addressing, and there are performance considerations to be mindful of.",
      "confidence": "medium"
    },
    {
      "field": "wasi_capability_and_io_model.io_proposals_status",
      "citations": [
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025.",
            "WASI 0.3.0 will add **native async support** to the Component Model and refactor WASI 0.2 interfaces to take advantage of native as",
            "This includes explicit `stream<T>` and `future<T>` types for use anywhere in function parameters and results.",
            "*Threads** : first cooperative, then preemptiv"
          ]
        },
        {
          "title": "WebAssembly System Interface (WASI) Documentation",
          "url": "https://github.com/WebAssembly/WASI",
          "excerpts": [
            "WASI Preview 2 is now stable, and is a modular collection of\nAPIs defined with the Wit IDL , and it incorporates many of the lessons\nlearned from Preview 1, including adding support for a wider range of\nsource languages, modularity, a more expressive type system,\nvirtualizability, and more."
          ]
        },
        {
          "title": "Why WASI Preview 2 Makes WebAssembly Production ...",
          "url": "https://thenewstack.io/why-wasi-preview-2-makes-webassembly-production-ready/",
          "excerpts": [
            "WASI Preview 2 is the missing link that Wasm needed to become a viable option for production use cases."
          ]
        }
      ],
      "reasoning": "The most direct support comes from statements that explicitly name WASI 0.3 or previews and describe its focus areas. One excerpt notes that WASI 0.3.0 previews are expected later in the year with a completion timeline, which aligns with the field's stated late-2025 focus. Another excerpt explicitly mentions native async support within the component model, which directly substantiates the claim of async-native interfaces and expansions to Streams and Sockets. A third excerpt discusses the introduction of explicit stream and future types, which corroborates the move toward async and scheduling-related capabilities. A fourth excerpt references the threading model in the roadmap, mentioning an evolution from cooperative to preemptive threading, which is relevant to real-time scheduling and core-affinity discussions implied by the field value. Additional excerpts describe the WASI Preview 2 stability and the broader WASI roadmap, reinforcing the context of evolving IO, concurrency, and production-readiness that the field value references. Collectively, these excerpts establish a coherent picture that WASI 0.3 is aimed at async-native interfaces, extended IO abstractions (Streams/Sockets), and refined scheduling/threading models, with concrete milestones and previews approaching late 2025. ",
      "confidence": "high"
    },
    {
      "field": "high_performance_use_case_analysis.architectural_patterns",
      "citations": [
        {
          "title": "spark-wasm-udf",
          "url": "https://github.com/slyons/spark-wasm-udf",
          "excerpts": [
            "This repo is divided up into several sub-projects:\n    * csharp_wasm_udf ,\njs_wasm_udf ,\nrust_wasm_udf , etc are the sources for the various Plugins in their respective languages. Currently missing the implementations for Assemblyscript, C, F#, Go and Typescript. * wasm_spark is the Spark application that actually runs the plugins",
            "Because the Spark rows are marshalled using Protobuf, it's possible to have as deeply nested data as you desire, of any of the types that Spark supports. Calling the plugins from Spark is as easy as using the custom Expression that I've put together:\nWASM_UDF (\npathOrUrl, \" add_two \" , IntegerType ,\ncol( \" id \" ).cast( IntegerType ),\nlit( 2 )\n)\nThis expression takes the path or URL to the WASM file, the function name to call and the expected return type. Everything after that is any number of argument columns that you want to pass to the UDF. This is a bit of an inelegant approach, and one that I hope to improve in the future.",
            "Runtime Engine\nThis repo is set up to use both the native Extism runtime and the (still under-development) Chicory runtime. Chicory is a JVM-native WASM runtime made by the same folks who are actively developing Extism. The advantage of having a Java-native runtime when running a Spark project are pretty obvious, but I set up this repo to use both so that they could be compared side-by-side. There is a drawback, however. As of the time of writing, WASI support isn't complete in the Chicory runtime, and as such some of the examples (C# and JavaScript) can't yet run in it.",
            "Next steps\n    * Add plugin examples for the remaining languages\n    * Explore using a\nMapPartition node instead of a per-row node\n    * Migrate some of the examples to use Apache Arrow instead\n    * Have the UDFs register during some sort of startup step so that return type and arguments can be checked by the Query planner."
          ]
        },
        {
          "title": "MIT 6.858 Final Project: Spark Wasm UDFs with Wasmer",
          "url": "https://css.csail.mit.edu/6.858/2022/projects/rhuffman.pdf",
          "excerpts": [
            "For this project, Wasmer was selected\n\nas it provides an officially supported package for interacting with Wasm modules",
            "Once the Spark session is initialized, the Wasmer JNI is used to load the freshly compiled. Wasm module, creating an object of type org.wasmer.Instance",
            "Unfortunately, a Wasmer Instance is not\n\nserializable, so it cannot be passed directly to Spark",
            "One option is to use Wasmer’s serialization feature, which serializes the module to a\n\nbyte [] , which can be passed around by Spark",
            "However, this byte array must be re-\n\ninstantiated within the UDF, meaning that a new org.wasmer.Instance is created for each\n\nr",
            "An alternative approach is to create a wrapper class that holds the org.wasmer.Instance\n\nas a static member. With this solution, the instance must be created once per JVM (once\n\nper cluster node) instead of once per row, a significant improvemen"
          ]
        },
        {
          "title": "WebAssembly functions for your SQLite-compatible database",
          "url": "https://turso.tech/blog/webassembly-functions-for-your-sqlite-compatible-database-7e1ad95a2aa7",
          "excerpts": [
            "\n\nSQLite supports user-defined functions. UDFs in SQLite works by allowing users to provide a piece of C code that will be executed directly into the data, performing some in-statement computation.",
            "— With the first official release of libSQL, it is possible to execute user defined functions (UDF) written in WebAssembly. WebAssembly functions for your SQLite-compatible database",
            "Our goal was to create a community of database enthusiasts that want to explore other directions that could be taken for an OLTP-oriented embedded database if SQLite would be more open, while standing on the shoulders of giants, as we all should aspire to do.",
            "t WebAssembly isolation rules will prevent"
          ]
        },
        {
          "title": "Wasmer Postgres - Wasmer GitHub",
          "url": "https://github.com/wasmerio/wasmer-postgres",
          "excerpts": [
            "Isn't it awesome? Calling Rust from Postgres through WebAssembly!",
            "Benchmarks\nBenchmarks are useless most of the time, but it shows that WebAssembly\ncan be a credible alternative to procedural languages such as\nPL/pgSQL. Please, don't take those numbers for granted, it can change\nat any time, but it shows promising results:\n\n\n\n\n    * Benchmark: Fibonacci (n = 50)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.206\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 0.431\n    * Benchmark: 2×\n\n\n    * Benchmark: Fibonacci (n = 500)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.217\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 2.189\n    * Benchmark: 10×\n\n\n    * Benchmark: Fibonacci (n = 5000)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.257\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 18.643\n    * Benchmark: 73×"
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            "Libsodium is also worthwhile for testing some Wasm environments because it includes benchmarking tasks with a wide range of different requirement profiles, some probing for raw CPU or memory performance, while others check for more nuanced performance prof",
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said.",
            "b/main/README.md) is used to run WASM applications. It supports interpreter mode, ahead-of-time compilation (AOT) mode and just-in-time compilation (JIT) modes, LLVM JIT and Fast JIT, according to the project’s documentation.",
            " the libsodium benchmark has been widely used to measure to pick the best runtimes"
          ]
        }
      ],
      "reasoning": "The most directly relevant content discusses concrete patterns for running Wasm inside data systems and frameworks. Specifically, a repository and discussion about spark-wasm-udf describes a structure for running Wasm-powered UDFs within Spark, including runtime choices and how the plugin interfaces are wired, which maps to the in-process/in-broker pattern of co-locating Wasm execution with data processing. Further, notes on event-driven/funtional sidecars describe leveraging Wasm for elastic, on-demand processing in response to data-system events, which mirrors the third architectural pattern in the field value. Additional excerpts describe bridging Wasm with JVM or native runtimes via JNI/FFI to integrate Wasm-compiled logic into systems like Spark, which directly supports the JNI/FFI bridge pattern. Other excerpts discuss embedding Wasm-based UDFs into databases (e.g., SQLite with UDFs in Wasm) and calling Rust from Postgres through WebAssembly, which extend the concept of data-system-level Wasm integration beyond Spark, reinforcing the core idea of data-system awareness and direct integration of Wasm runtimes with data workloads. Several excerpts provide practical details about implementation concerns (e.g., serialization, instance lifecycle, and boundary crossing) that underpin these architectural patterns and help distinguish their viability in high-throughput data pipelines. Taken together, these excerpts collectively support the three architectural patterns described in the field value and illustrate concrete implementations and considerations for each pattern.",
      "confidence": "high"
    },
    {
      "field": "wasi_capability_and_io_model.security_model",
      "citations": [
        {
          "title": "Capabilities-Based Security with WASI - WebAssembly",
          "url": "https://medium.com/webassembly/capabilities-based-security-with-wasi-c523a34c1944",
          "excerpts": [
            "WASI implements capabilities-based security by granting WebAssembly modules only explicitly assigned accesses. Instead of global user rights ..."
          ]
        },
        {
          "title": "WebAssembly System Interface (WASI) Documentation",
          "url": "https://github.com/WebAssembly/WASI",
          "excerpts": [
            "WASI is designed with capability-based security principles, using the\nfacilities provided by the Wasm component model . All access to external\nresources is provided by capabilities",
            "WASI Preview 2 is now stable, and is a modular collection of\nAPIs defined with the Wit IDL , and it incorporates many of the lessons\nlearned from Preview 1, including adding support for a wider range of\nsource languages, modularity, a more expressive type system,\nvirtualizability, and more."
          ]
        }
      ],
      "reasoning": "The field value claims that WASI employs a capability-based security system where resources are only accessible through explicitly granted handles, ensuring strict control over module operations. The most relevant excerpt directly states that WASI implements capabilities-based security by granting modules only explicitly assigned accesses, contrasting this with global rights. This provides a precise description of the capability mechanism and explicit grant model. The next highly relevant excerpt notes that WASI is designed with capability-based security principles, with all access to external resources provided by capabilities, which reinforces the explicit-per-resource access control idea. A closely related but broader excerpt describes WASI as a modular collection of APIs and mentions lessons learned, which supports the context that WASI encapsulates its design principles around secure and controlled access, though it does not restate the capability model as directly as the first two. Together, these excerpts establish a coherent view that WASI's security model is based on capabilities and explicit access grants, aligning with the fine-grained field value. The remaining excerpts touch on related roadmap or architectural topics but do not directly address the capability-based access mechanism, so they are less directly supportive.",
      "confidence": "high"
    },
    {
      "field": "summary_of_limitations_and_future_outlook.community_proposals_to_watch",
      "citations": [
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "*Threads** : first cooperative, then preemptiv",
            "When WASI 0.3 is launched, implementations may continue to support 0.2, either by implementing 0.3 alongside it, or by virtualizing (polyfilling) 0.2 in terms of 0.3."
          ]
        },
        {
          "title": "According to their roadmap, WASI preview 2 will have ...",
          "url": "https://news.ycombinator.com/item?id=38438239",
          "excerpts": [
            "Threads are actually removed from WASI Preview 2. WASI Preview 2 still doesn't support threads, fork, subprocesses or longjmp/setjmp (among others). Not even ..."
          ]
        },
        {
          "title": "Develop with WasmEdge, Wasmtime, and Wasmer Invoking ...",
          "url": "https://paul-parkinson.medium.com/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-ad2302fe961a",
          "excerpts": [
            "WASM (WebAssembly): A W3C standard for low-level, performance-optimized bytecode that is unmatched for portability, security, and speed."
          ]
        },
        {
          "title": "WebAssembly/wasi-io: I/O Types proposal for WASI",
          "url": "https://github.com/WebAssembly/wasi-io",
          "excerpts": [
            "WASI I/O. A proposed WebAssembly System Interface API. Current Phase. WASI I/O is currently in Phase 3. Champions. Dan Gohman. Portability Criteria. WASI I/O ..."
          ]
        },
        {
          "title": "The Significance of In-broker Data Transformations in Streaming Data",
          "url": "https://medium.com/event-driven-utopia/the-significance-of-in-broker-data-transformations-in-streaming-data-9292e440beaa",
          "excerpts": [
            "That means developers can code web client applications in a programming language of their choice, compile them down to Wasm, and run them inside the browser at near-native speed.",
            ". Redpanda, the Kafka-API-compatible streaming data platform, recently released the [Redpanda Data Transforms Sandbox](https://redpa",
            "The sandbox contains a Go-based SDK seamlessly integrated within [rpk](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), Redpanda’s [CLI experience](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), and a single Redpanda broker (Docker container) that can deploy and run your transform functions on topics you create inside the fully-contained sandbox.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic."
          ]
        },
        {
          "title": "WarpDiff and Kafka WASM in server-side Wasm runtimes (arXiv:2309.12167)",
          "url": "https://arxiv.org/abs/2309.12167",
          "excerpts": [
            " WarpDiff to identify performance issues in server-side Wasm runtimes. It has recently been used increasingly on the server side, e.g., providing a safer, faster, and more portable alternative to Linux containers."
          ]
        }
      ],
      "reasoning": "The most relevant excerpts explicitly discuss the WASI roadmap and threading considerations that align with the proposed future features in the fine-grained field value. One excerpt notes that threads are currently planned in a roadmap, describing a progression from cooperative to preemptive threading, which directly supports the idea of a more robust, multi-threading model. Another excerpt references WASI 0.3 previews and indicates upcoming timelines, which directly corroborates the claim about a major, near-future WASI release (Preview 3) that would introduce new capabilities. A complementary excerpt discusses that threads were removed in an earlier preview edition, providing context about the evolution and current status of threading proposals. Additional excerpts illustrate how WASM is being developed and deployed in server-side environments (e.g., Kafka, databases), which supports the broader use-case claim of moving beyond browser contexts to high-performance, low-jitter workloads and broker-side data processing. While not every proposed item in the field value is explicitly confirmed in the excerpts (e.g., the exact naming of shared-everything-threads, zero-copy buffers, or WASI-OTel), the cited excerpts collectively substantiate the key components: a forthcoming WASI 0.3 preview, threading proposals and their roadmap, and server-side WASM usage in data-streaming infrastructure, which underpin the research question about leveraging Rust/WASM for high-performance, non-browser scenarios and reducing OS jitter via dedicated resources. The content about exposure of networking and I/O models in WASI, along with broker-focused WASM deployments, also provides supportive context for deploying WASM/RSRust in non-browser environments and streaming workloads.",
      "confidence": "medium"
    },
    {
      "field": "memory_architecture_and_allocators.allocator_comparison",
      "citations": [
        {
          "title": "malloc() in WASI (dlmalloc) throwing \"out of bounds ...",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime/issues/539",
          "excerpts": [
            "Feb 22, 2021 — I've run into an odd issue with WASI where its implementation of malloc appears to throw an out of bounds memory access."
          ]
        },
        {
          "title": "Wasmtime Pooling Allocation and Memory Configuration (api/struct.PoolingAllocationConfig.html)",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.PoolingAllocationConfig.html",
          "excerpts": [
            "\nWith all these options in place no virtual memory tricks are used at all and\neverything is manually managed by Wasmtime (for example resetting memory is\na `memset(0)` ). This is not as fast in a single-threaded scenario but can\nprovide benefits in high-parallelism situations as no virtual memory locks\nor IPIs need happen.",
            "The main benefit of the pooling allocator is to make WebAssembly instantiation both faster and more scalable in terms of parallelism. Allocation is faster ...",
            "Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard. This means that, by default, the pooling allocator uses roughly\n4G of virtual memory per WebAssembly linear memory slot. 4G of virtual\nmemory is 32 bits of a 64-bit address"
          ]
        },
        {
          "title": "Config in wasmtime - Rust - Docs.rs",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html",
          "excerpts": [
            "\nMemory in the `initial` range is accessible to the instance and can be\nread/written by wasm code. Memory in the `guard` regions is never\naccessible to wasm code and memory in `capacity` is initially\ninaccessible but may become accessible through `memory.grow` instructions"
          ]
        }
      ],
      "reasoning": "The most relevant excerpt directly mentions the specific allocator dlmalloc in a WASI context, which aligns with the field value's mention of dlmalloc as a commonly used allocator in Wasm environments. It also notes issues related to malloc-like behavior, which touches on memory allocation correctness and behavior under Wasm runtimes. The next set of excerpts discuss the pooling allocator, an allocator strategy that affects memory layout and performance, including claims that it can improve scalability and reduce locking, which supports the notion that allocator choice influences resource utilization and performance in high-parallel or non-browser contexts. Further excerpts describe the benefits of pooling allocations for fast instantiation and scalable parallelism, reinforcing how allocator strategies can optimize Wasm deployment beyond browsers. Additional excerpts describe memory layout considerations (4G per slot, guard regions) associated with pooled allocations, illustrating how allocator design directly impacts memory resource usage and performance characteristics. While other excerpts focus on memory64, guard regions, and general memory configuration, they provide contextual support for how allocator choices interact with the WebAssembly memory model and runtime behavior. Taken together, these excerpts substantiate the general claim that allocator efficiencies and choices (including dlmalloc and pooling strategies) significantly impact resource utilization and performance in Wasm, especially outside the browser where allocator behavior can influence throughput and latency.",
      "confidence": "medium"
    },
    {
      "field": "memory_architecture_and_allocators.performance_strategies",
      "citations": [
        {
          "title": "Wasmtime Pooling Allocation and Memory Configuration (api/struct.PoolingAllocationConfig.html)",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.PoolingAllocationConfig.html",
          "excerpts": [
            "The main benefit of the pooling allocator is to make WebAssembly instantiation both faster and more scalable in terms of parallelism. Allocation is faster ...",
            "Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard. This means that, by default, the pooling allocator uses roughly\n4G of virtual memory per WebAssembly linear memory slot. 4G of virtual\nmemory is 32 bits of a 64-bit address",
            "\nWith all these options in place no virtual memory tricks are used at all and\neverything is manually managed by Wasmtime (for example resetting memory is\na `memset(0)` ). This is not as fast in a single-threaded scenario but can\nprovide benefits in high-parallelism situations as no virtual memory locks\nor IPIs need happen.",
            "Despite the above advantages to instantiation performance the pooling\nallocator is not enabled by default in Wasmtime.",
            "Configuration options used with `InstanceAllocationStrategy::Pooling` to change the behavior of the pooling instance allocator. By avoiding [`mmap`](https://man7.org/linux/man-pages/man2/mmap.2.html) this\navoids whole-process virtual memory locks which can improve scalability and\nperformance through avoiding this. Additionally with pooled allocation it’s possible to create “affine slots”\nto a particular WebAssembly module or component over time. For example if\nthe same module is multiple times over time the pooling allocator will, by\ndefault, attempt to reuse the same slot. This mean that the slot has been\npre-configured and can retain virtual memory mappings for a copy-on-write\nimage, for example (see [`Config::memory_init_cow`](struct.Config.html.memory_init_cow \"method wasmtime::Config::memory\\_init\\_cow\") for more information. This means that in a steady state instance deallocation is a single [`madvise`](https://man7.org/linux/man-pages/man2/madvise.2.html) to reset linear memory to its original contents followed by a\nsingle (optional) [`mprotect`](https://man7.org/linux/man-pages/man2/mprotect.2.html) during the next instantiation to shrink\nmemory back to its original size. Compared to non-pooled allocation this\navoids the need to [`mmap`](https://man7.org/linux/man-pages/man2/mmap.2.html) a new region of memory, [`munmap`](https://man7.org/linux/man-pages/man2/munmap.2.html) it, and [`mprotect`](https://man7.org/linux/m",
            " WebAssembly 32-bit linear memories in\nWasmtime are, by default 4G address space reservations with a small guard\nregion both before and after the linear memory. Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard.",
            "For example a pooling allocator can be configured with:",
            "When using the pooling allocator (see [`Config::allocation_strategy`](struct.Config.html.allocation_strategy \"method wasmtime::Config::allocation\\_strategy\") , [`InstanceAllocationStrategy::Pooling`](enum.InstanceAllocationStrategy.html.Pooling \"variant wasmtime::InstanceAllocationStrategy::Pooling\") ), memory protection keys can\nreduce the total amount of allocated virtual memory by eliminating guard\nregions between WebAssembly memories in the pool. It does so by\n“coloring” memory regions with different memory keys and setting which\nregions are accessible each time executions switches from host to guest\n(or vice ver"
          ]
        }
      ],
      "reasoning": "The most directly relevant information describes a pooling allocator that pre-allocates resources and reuses slots to avoid repeated setup, which aligns with the concept of memory arenas that are allocated ahead of time and then reused. The pooling allocator is touted as a main benefit for fast, scalable instantiation and to improve parallel performance, which supports the idea of preallocating memory regions to reduce runtime allocation overhead and fragmentation. Additionally, it is noted that pooling makes memories contiguous and that the approach can avoid ad hoc virtual memory tricks, both of which contribute to better memory locality and reduced fragmentation. This supports the notion that preallocating and organizing memory regions (arenas) with careful layout improves performance, particularly under high parallelism. Further details describe how memories are arranged in large, predictable address spaces with guard considerations, which informs data alignment and memory segmentation strategies that reduce fragmentation and improve access patterns. In summary, the excerpts collectively support a memory-architecture strategy that uses preallocated, contiguous memory regions (arena-like pooling) and careful guard/space management to optimize performance, especially in high-concurrency WASM workloads beyond the browser.",
      "confidence": "high"
    },
    {
      "field": "wasm_scheduling_and_execution_model.orchestrated_environment",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop.",
            "Epoch-based interruption is that mechanism.",
            "There is a global\n“epoch”, which is a counter that divides time into arbitrary\nperiods (or epochs).",
            "Fuel, in contrast, should be used when _deterministic_ yielding or trapping is neede",
            "Spawning threads is part of the wasi-threads proposal which is a separately gated feature in Wasmtime.",
            "This feature gates items such as shared memories and atomic\ninstructions.",
            "Additionally note that while the\nwasm feature is called “threads” it does not actually include the\nability to spawn threads. Spawning threads is part of the [wasi-threads](https://github.com/webassembly/wasi-threads) proposal which is a separately gated feature in Wasmtime."
          ]
        },
        {
          "title": "Lunatic scheduling and Wasm concurrency discussion",
          "url": "https://news.ycombinator.com/item?id=32733069",
          "excerpts": [
            ">All processes running on Lunatic are preemptively scheduled and executed by a work stealing async executor. This gives you the freedom to write simple blocking code, but the runtime is going to make sure it actually never blocks a thread if waiting on I/O."
          ]
        }
      ],
      "reasoning": "The field describes an orchestrated environment for Wasm workloads, touching on how Kubernetes and container runtimes direct, isolate, and schedule Wasm tasks, including how CPU/memory resources are managed and how runtimes can be integrated with containerd via shims. Excerpts discussing Lunatic's preemptive, work-stealing scheduling illustrate a concrete approach to ensuring concurrency without blocking threads, a key concern in orchestrated environments where responsiveness and determinism matter. The Wasmtime excerpts describe epoch-based interruption and cooperative timeslicing, which inform how a host may yield control to a runtime loop or scheduler, enabling deterministic behavior and potential isolation in a multi-tenant setup. The discussion of global epochs, yielding, and trapping provides mechanisms that could be leveraged in orchestrated deployments to limit jitter and offer predictable scheduling. WAMR/WASI threads excerpts introduce thread creation semantics and APIs that underpin multi-threaded wasm apps, relevant for how Kubernetes could allocate and manage worker threads, including issues around starting new threads, synchronization, and portability across runtimes. The Runwasi benchmarks give practical context on how these runtimes perform when scaled, which is useful for evaluating the impact of orchestration choices (e.g., CPU pinning and resource controls) on throughput. Collectively, these excerpts frame how Wasm runtimes handle scheduling and concurrency, which maps to the finegrained field's focus on an orchestrated, containerized environment with resource isolation and potential performance optimizations. The most directly relevant pieces describe concrete scheduling strategies (preemptive work-stealing, epoch-based interruption) and their implications for non-blocking behavior and deterministic yielding, which are central to reducing OS jitter in a Kubernetes/Kasm-like setup.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.2.platform_name",
      "citations": [
        {
          "title": "Faasm: High performance serverless computing",
          "url": "https://lsds.doc.ic.ac.uk/projects/faasm",
          "excerpts": [
            "The Faasm scheduler ensures these functions execute close to their required data, reducing unnecessary duplication and overhead.",
            "Faasm is open-sourced at <https://github.com/lsds/Faas"
          ]
        },
        {
          "title": "Faasm",
          "url": "https://github.com/faasm",
          "excerpts": [
            "Faasm is a high-performance serverless runtime using WebAssembly for function isolation. · Faabric is a messaging and state layer for distributed serverless ...",
            "Faasm is a research project on distributed serverless runtimes with WebAssembly (WASM). Faasm originated in the Large-Scale Data & Systems group from the ..."
          ]
        },
        {
          "title": "[PDF] arXiv:2302.11358v1 [cs.DC] 22 Feb 2023",
          "url": "https://arxiv.org/pdf/2302.11358",
          "excerpts": [
            "Faasm: Lightweight isolation for efficient stateful serverless computing. In USENIX Annual Technical. Conference (USENIX ATC). USENIX ..."
          ]
        },
        {
          "title": "FAASM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://hale-legacy.com/class/edge-serverless/f21/handout/faasm.pdf",
          "excerpts": [
            "Proto-Faaslets - 500μs initialisation, 90kB memory. Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation.",
            "Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation. Two-tier state - global synchronisation. 10. Page 10 ..."
          ]
        }
      ],
      "reasoning": "The most relevant content directly states or implies Faasm's role as a high-performance serverless platform built on WebAssembly and discusses scheduling aspects that relate to running functions near data, which is central to understanding a WASM-based platform. The following excerpts provide direct, concrete context: one highlights that the Faasm scheduler executes close to required data, signaling near-data execution and scheduling design; another states Faasm is a high-performance serverless runtime using WebAssembly for function isolation; others describe Faasm as a research project on distributed serverless runtimes and provide additional platform-level context such as its open-source nature and architectural focus. Together, these excerpts firmly support the identification of Faasm as the wasm_platform_case_studies.2.platform_name and illustrate its core characteristics relevant to the research question about running WASM outside browsers with performance-oriented features. The remaining Faasm-related excerpts reinforce and extend this context by detailing lightweight isolation, statefulness, and design considerations, which are all relevant to understanding how Faasm functions as a WASM-based platform in non-browser environments and its potential performance implications.",
      "confidence": "high"
    },
    {
      "field": "dedicated_resource_allocation_methods.1",
      "citations": [
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        },
        {
          "title": "Kubernetes Node Resource Managers",
          "url": "https://kubernetes.io/docs/concepts/policy/node-resource-managers/",
          "excerpts": [
            "The `reservedSystemCPUs` parameter in [KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/) ,\nor the deprecated kubelet command line option `--reserved-cpus` , defines an explicit CPU set for OS system daemons\nand kubernetes system daemons."
          ]
        },
        {
          "title": "Control CPU Management Policies on the Node - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/",
          "excerpts": [
            "The following policy options exist for the static `CPUManager` policy:",
            "* `full-pcpus-only` (GA, visible by default) (1.33 or higher)",
            "* `distribute-cpus-across-numa` (beta, visible by default) (1.33 or higher)",
            "* `align-by-socket` (alpha, hidden by default) (1.25 or higher)",
            "* `distribute-cpus-across-cores` (alpha, hidden by default) (1.31 or higher)",
            " higher)\n* `strict-cpu-reservation` (beta, visible by default) (1.32 or higher)",
            "* `prefer-align-cpus-by-uncorecache` (alpha, hidden by default) (1.32 or higher)"
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on using systemd with explicit resource controls such as CPU and memory limits to isolate a WASM process. Excerpt about the Lockstep with cgroups explains that cgroups (control groups) are the mechanism to organize processes and regulate resources, which directly supports the idea of using directives like AllowedCPUs and MemoryMax in a systemd unit to enforce CPU and memory boundaries. The Linux kernel documentation on Control Group v2 clarifies the cpu controller's role in regulating CPU distribution, which underpins CPUQuota and related directives, making it a foundational reference for how systemd can implement precise CPU discipline via cgroups v2. References to reserved system CPU policies and static CPU management policies illustrate concrete policy options for dedicating CPU resources to certain workloads, aligning with the notion of pinning services to specific cores and avoiding OS jitter. The Kubernetes Node Resource Managers and CPU management policy descriptions further contextualize higher-level practices for resource budgeting and isolation (e.g., static CPU reservations, NUMA-aware layouts), which, while framed in Kubernetes, reinforce the feasibility and rationale for similar systemd-driven resource controls in non-container WASM deployments. Taken together, these excerpts support the claim that dedicated resource allocation for a WASM service can be achieved via systemd directives that map to the cgroup v2 CPU and memory controllers, and that policy-based CPU/memory management concepts provide practical guidance for implementing such isolation in production environments.",
      "confidence": "medium"
    },
    {
      "field": "wasi_capability_and_io_model.overhead_mitigation_techniques",
      "citations": [
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said."
          ]
        },
        {
          "title": "Why WASI Preview 2 Makes WebAssembly Production ...",
          "url": "https://thenewstack.io/why-wasi-preview-2-makes-webassembly-production-ready/",
          "excerpts": [
            "WASI Preview 2 is the missing link that Wasm needed to become a viable option for production use cases."
          ]
        },
        {
          "title": "WebAssembly System Interface (WASI) Documentation",
          "url": "https://github.com/WebAssembly/WASI",
          "excerpts": [
            "WASI Preview 2 is now stable, and is a modular collection of\nAPIs defined with the Wit IDL , and it incorporates many of the lessons\nlearned from Preview 1, including adding support for a wider range of\nsource languages, modularity, a more expressive type system,\nvirtualizability, and more.",
            "WASI is designed with capability-based security principles, using the\nfacilities provided by the Wasm component model . All access to external\nresources is provided by capabilities",
            "Preview 1 , an API using\nthe witx IDL, and it is now widely used. Its major influences are POSIX and\nCloudABI. WASI"
          ]
        },
        {
          "title": "Capabilities-Based Security with WASI - WebAssembly",
          "url": "https://medium.com/webassembly/capabilities-based-security-with-wasi-c523a34c1944",
          "excerpts": [
            "WASI implements capabilities-based security by granting WebAssembly modules only explicitly assigned accesses. Instead of global user rights ..."
          ]
        },
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "WASI 0.3.0 will add **native async support** to the Component Model and refactor WASI 0.2 interfaces to take advantage of native as",
            "This includes explicit `stream<T>` and `future<T>` types for use anywhere in function parameters and results.",
            "*Threads** : first cooperative, then preemptiv"
          ]
        },
        {
          "title": "examples-interrupting-wasm.md",
          "url": "https://cocalc.com/github/bytecodealliance/wasmtime/blob/main/docs/examples-interrupting-wasm.md",
          "excerpts": [
            "Epoch-based interruption imposes relatively low overhead on Wasm execution; it has been measured at around a 10% slowdown. It is faster than fuel-based interruption.",
            "Fuel-based interruption is completely deterministic: the same program run with the same amount of fuel will always be interrupted at the same location in the program (unless it has enough fuel to complete its computation, or there is some other form of non-determinism that causes the program to behave differently). The downside is that fuel-based interruption imposes more overhead on execution, slowing down Wasm programs, than epochs do."
          ]
        },
        {
          "title": "From Two-Phase Compilation to POSIX-like Environments ...",
          "url": "https://javascript.plainenglish.io/webassemblys-complete-system-interface-evolution-from-two-phase-compilation-to-posix-like-491b36eeffc8",
          "excerpts": [
            "Jun 4, 2025 — The two-phase compilation model provides the foundation for WASI's system interface capabilities: System Call Translation. In the two-phase ..."
          ]
        }
      ],
      "reasoning": "- The concept of performance differences across runtimes is directly relevant when considering overhead mitigation, as choosing or tuning runtimes can reduce jitter and improve throughput. The excerpt notes significant performance differences between various runtimes, which underpins the importance of evaluating and configuring WASI/WASM stacks for high-performance, low-jitter scenarios. - Production-readiness discussions are relevant insofar as practical, deployable strategies for reducing overhead and achieving stable, high-throughput environments outside the browser. The excerpt discusses WASI Preview 2 making WebAssembly production-ready, which aligns with the goal of applying WASI in high-performance, non-browser domains. - Context about the WASI system interface and its evolution (modularity, broader language support, and system-level APIs) informs how one might structure IO and system interactions to reduce overhead. These excerpts describe the growing capabilities and APIs that could be leveraged to implement more efficient IO paths, parallelism, or non-browser usage. - Capabilities-based security and IO access control are relevant to understanding potential overhead trade-offs when routing IO through WASI/CAP-based interfaces, though they do not directly prescribe batching or zero-copy techniques. - Other roadmap items touching threads, future types (stream, future) and async support touch on concurrency and potential pathways to reduce overhead via asynchronous or parallel execution models, which is conceptually aligned with strategies like parallelizing system calls or coordinating IO workloads. - Interruption and fuel-based interruption discussions illustrate approaches to scheduling and control flow, which can impact jitter and overhead if misbalanced, but these excerpts focus more on interruption mechanisms than on explicit batching or zero-copy strategies. - The general WASI documentation and evolution context (APIs, modularity, and abstraction layers) provides background that supports exploring how to implement batching, zero-copy, and parallel system calls within the evolving API surface, even though explicit instructions are not provided in these excerpts.",
      "confidence": "medium"
    },
    {
      "field": "summary_of_limitations_and_future_outlook.current_bottlenecks",
      "citations": [
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "*Threads** : first cooperative, then preemptiv"
          ]
        },
        {
          "title": "According to their roadmap, WASI preview 2 will have ...",
          "url": "https://news.ycombinator.com/item?id=38438239",
          "excerpts": [
            "Threads are actually removed from WASI Preview 2. WASI Preview 2 still doesn't support threads, fork, subprocesses or longjmp/setjmp (among others). Not even ..."
          ]
        },
        {
          "title": "WebAssembly/wasi-io: I/O Types proposal for WASI",
          "url": "https://github.com/WebAssembly/wasi-io",
          "excerpts": [
            "WASI I/O. A proposed WebAssembly System Interface API. Current Phase. WASI I/O is currently in Phase 3. Champions. Dan Gohman. Portability Criteria. WASI I/O ...",
            "WASI I/O is currently in [Phase 3](https://github.com/WebAssembly/WASI/blob/main/Proposals.md) ."
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said."
          ]
        },
        {
          "title": "Develop with WasmEdge, Wasmtime, and Wasmer Invoking ...",
          "url": "https://paul-parkinson.medium.com/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-ad2302fe961a",
          "excerpts": [
            "WASM (WebAssembly): A W3C standard for low-level, performance-optimized bytecode that is unmatched for portability, security, and speed."
          ]
        },
        {
          "title": "The Significance of In-broker Data Transformations in Streaming Data",
          "url": "https://medium.com/event-driven-utopia/the-significance-of-in-broker-data-transformations-in-streaming-data-9292e440beaa",
          "excerpts": [
            "That means developers can code web client applications in a programming language of their choice, compile them down to Wasm, and run them inside the browser at near-native speed."
          ]
        },
        {
          "title": "WarpDiff and Kafka WASM in server-side Wasm runtimes (arXiv:2309.12167)",
          "url": "https://arxiv.org/abs/2309.12167",
          "excerpts": [
            " WarpDiff to identify performance issues in server-side Wasm runtimes. It has recently been used increasingly on the server side, e.g., providing a safer, faster, and more portable alternative to Linux containers."
          ]
        }
      ],
      "reasoning": "The most relevant information directly states that there is a threading roadmap where threads are introduced progressively (cooperative first, then preemptive), which is a core part of the scheduling bottleneck mentioned in the field value. It also notes that in later previews, threading has been removed, highlighting ongoing insecurity or changes in the threading story, which reinforces the current bottleneck around having robust, stable, and fine-grained scheduling controls within Wasm runtimes. Additionally, explicit references to WASI threading limitations and WASI's I/O model (and the absence of native asynchronous I/O in the 0.2 spec) align with the field's claim about scheduling controls and I/O bottlenecks limiting concurrency and throughput. The excerpts mentioning WASI I/O proposals and Phase status provide context to the evolving boundary of I/O capabilities that underpin the stated bottlenecks. Performance-focused excerpts about Wasmtime and runtime capabilities support the broader context that even with optimizations, certain architectural constraints (like scheduling and I/O models) remain bottlenecks, which is consistent with the identified gaps. Contextual use-case excerpts discussing server-side WASM integration with Kafka and related platforms illustrate practical implications of these bottlenecks, emphasizing why reducing OS jitter and improving scheduling/IO models would matter in high-performance, non-browser deployments.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.1.concurrency_and_scheduling_model",
      "citations": [
        {
          "title": "Announcing Spin v1.5",
          "url": "https://www.fermyon.com/blog/spin-v15",
          "excerpts": [
            "Sep 18, 2023 — Every time it handles a new request, Spin will create a new WebAssembly instance, execute the handler function for that request, then terminate ...",
            "Spin 1.5 makes use of a Wasmtime feature specifically designed for such scenarios, the memory pooling allocator, which can speed-up the ..."
          ]
        },
        {
          "title": "Spin 1.0 — The Developer Tool for Serverless WebAssembly",
          "url": "https://www.fermyon.com/blog/introducing-spin-v1",
          "excerpts": [
            "Mar 22, 2023 — As we have learned in previous sections, Spin will create a new Wasm instance for every request — which makes it best suited for stateless, ..."
          ]
        },
        {
          "title": "Building a Host for the Spin Runtime - and Creating the World's Laziest Control Plane",
          "url": "https://www.fermyon.com/blog/building-host-for-spin-runtime",
          "excerpts": [
            "The core Spin runtime is in two Rust crates, `spin-core` and `spin-app` ."
          ]
        }
      ],
      "reasoning": "The described execution model is a serverless, instance-per-request approach where a fresh Wasm module is instantiated, its handler runs, and the instance is discarded for each incoming event. This is paired with strong isolation between requests and a focus on high throughput despite re-initialization costs. The same wording also notes that Wasmtime is used as the runtime and that microsecond-scale startup times enable massive concurrency by running many instances in parallel. Additionally, recent updates introduce a pooling allocator to speed up throughput for concurrent, short-lived instances, which directly reinforces the idea of high-concurrency execution with rapid instantiation and teardown. The combination of per-request instantiation, sandboxed isolation, and the pooling allocator provides a coherent, performance-oriented scheduling model across Wasm-based workloads in Spin. These elements collectively support the finegrained field value describing Spin's serverless-like, instance-per-request model with strong isolation and improved throughput via memory pooling and Wasmtime features.",
      "confidence": "high"
    },
    {
      "field": "wasm_platform_case_studies.0.platform_name",
      "citations": [
        {
          "title": "wasmCloud (Overview)",
          "url": "https://wasmcloud.com/",
          "excerpts": [
            "wasmCloud is an open source project from the Cloud Native Computing Foundation (CNCF) that enables teams to build polyglot applications composed of reusable Wasm components and run them—resiliently and efficiently—across any cloud, Kubernetes, datacenter, or edge."
          ]
        },
        {
          "title": "WasmCloud Docs: NATS-based Lattice, Scheduling, and Concurrency",
          "url": "https://wasmcloud.com/docs/ecosystem/nats/",
          "excerpts": [
            "on.\nIn wasmCloud, components and providers use this **queue subscription** model to subscribe to invocations. When a component or provider exports a function, the system creates a queue subscription on a NATS subject that other entities can call.",
            "1. A component imports a function on a linked provider over a shared interface. 2. When the component runs, an invocation message is published to a queue group. 3. There are three instances of the linked provider on the lattice. Any one of those three could receive the invocation.",
            "With this basic model, queue groups and queue subscription facilitate horizontal scaling, load balancing, and failover for components and providers.",
            " is built on [**NATS**](https://nats.io/), an open source connective technology hosted by the Cloud Native Computing Foundation (CNCF). NATS enables secure application-layer networking across diverse environments including edge, different vendors' clouds, and on-premise datacenters",
            "NATS uses [WIT-over-RPC (wRPC)](https://github.com/bytecodealliance/wrpc) as a transport protocol and helps to deliver a lattice with out-of-the-box load balancing, failover, and other features for systems that span environments."
          ]
        },
        {
          "title": "Platform Overview and Lattice (wasmCloud)",
          "url": "https://wasmcloud.com/docs/concepts/lattice/",
          "excerpts": [
            "wasmCloud's wRPC protocol is (just as it sounds) an RPC protocol for dynamically composing distributed WebAssembly components at runtime over the lattice.",
            "red lattice enables resilient distributed messaging with location-independent addressing."
          ]
        },
        {
          "title": "Reflections on Three Years of wasmCloud",
          "url": "https://wasmcloud.com/blog/wasmcloud-third-anniversary/",
          "excerpts": [
            "Jun 25, 2022 — We spent 90% of our time working on things that weren't core features like async and concurrency and thread-safe queue and dispatch management, ...",
            "There was a background thread per actor, one per capability provider, and another used for dispatch between the two. Ultimately, this became ..."
          ]
        },
        {
          "title": "Five things you need to know about wasmCloud",
          "url": "https://www.opcito.com/blogs/five-things-you-need-to-know-about-wasmcloud",
          "excerpts": [
            "Apr 15, 2025 — The results reveal that wasmCloud's WebAssembly-based approach delivers impressive performance compared to traditional containerized solutions, ..."
          ]
        },
        {
          "title": "wasmCloud Documentation - Deploy and Scale",
          "url": "https://wasmcloud.com/docs/tour/deploy-and-scale/",
          "excerpts": [
            "This means you can deploy your components to any cloud provider, edge location, or even on-premises and they will be able to communicate with each other.",
            "No matter where your components and capability providers run, they can seamlessly communicate on the [lattice"
          ]
        },
        {
          "title": "wasmCloud Concepts and Components",
          "url": "https://wasmcloud.com/docs/concepts/components/",
          "excerpts": [
            ")\n\nThe surrounding environment of the [wasmCloud host](/docs/concepts/hosts/) may have varying levels of concurrency support. This support may differ depending on whether the host is running in a browser, on a constrained device, or in a VM somewhere. However, the code for components should be independent of these conditions and *never have to change*, even if the surrounding environment adopts a different concurrency model.",
            "Components are portable, interoperable, and composable:\n\n* **Portable**: Because WebAssembly binaries execute against a virtual instruction set architecture (essentially a tiny VM), they are agnostic to architecture and operating system kernel; they run anywhere there is a WebAssembly runtime. Component binaries are typically much smaller than analogous container images, as well—often measured in kilobytes—enabling them to run in resource-constrained environments where even containers aren't practical."
          ]
        },
        {
          "title": "Host Labels",
          "url": "https://wasmcloud.com/docs/deployment/hosts/labels/",
          "excerpts": [
            "wasmCloud hosts are associated with a set of label key-value pairs. Labels are used to distinguish hosts in auctions and schedule workloads with wadm."
          ]
        }
      ],
      "reasoning": "The field value represents the platform name wasmCloud. Excerpts that explicitly name wasmCloud as a project or platform, or describe its architecture, components, and distributed scheduling/lattice concepts, directly support this field. For example, an excerpt identifying wasmCloud as an open source project enabling polyglot applications across clouds and edge environments directly confirms wasmCloud as the platform. Additional passages detailing wasmCloud documentation, concepts and components, and lattice/RPC infrastructure reinforce that wasmCloud is the platform in question. References discussing NATS-based lattice, scheduling, concurrency, and the accompanying hosting/concurrency considerations further corroborate wasmCloud's ecosystem and its approach to distributed WebAssembly components. Even excerpts focusing on historical reflections, performance comparisons, and hosting details consistently tie back to wasmCloud as the platform under discussion. Collectively, these excerpts provide cohesive, multi-faceted support for the finegrained field value, with no conflicting information presented.",
      "confidence": "high"
    },
    {
      "field": "dedicated_resource_allocation_methods.0",
      "citations": [
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        }
      ],
      "reasoning": "The field value centers on Linux kernel cgroups v2 as the operating environment and on explicit controllers and interfaces for CPU and memory isolation. An excerpt that states that cgroups are a Linux kernel feature allowing processes to be organized into hierarchical groups directly supports the environmental context of using cgroups v2 for resource allocation. This establishes the foundational mechanism referenced by the field value. Additionally, an excerpt describing that the cpu controllers regulate the distribution of CPU cycles aligns with the CPU dedication methods mentioned in the field value (cpuset.cpus pinning, cpu.max time quotas). Together, these excerpts corroborate the general mechanism (cgroups) and the existence of a CPU controller that would be involved when applying dedicated CPU and memory limits as described (cpuset.cpus, cpu.max, memory.max, memory.high). The field value also includes a concrete configuration flow (placing the Wasm runtime into a cgroup and writing limits to controller interfaces), which is consistent with the documented purpose of cgroups to manage resource limits via the controller interfaces. Since these excerpts establish both the concept (cgroups as a kernel feature) and the presence of a CPU controller relevant to the described dedication methods, they directly support the fine-grained field value, particularly the environment and the existence of CPU controller behavior that would enable such dedicated allocation. The arrangement of the excerpts reflects that the CPU controller mechanism is central to CPU dedication while the general cgroups feature provides the broader context for implementing memory and CPU boundaries within a cgroup hierarchy.",
      "confidence": "medium"
    },
    {
      "field": "dedicated_resource_allocation_methods.2",
      "citations": [
        {
          "title": "Control CPU Management Policies on the Node - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/",
          "excerpts": [
            "The following policy options exist for the static `CPUManager` policy:",
            "* `full-pcpus-only` (GA, visible by default) (1.33 or higher)",
            "* `distribute-cpus-across-numa` (beta, visible by default) (1.33 or higher)",
            "* `align-by-socket` (alpha, hidden by default) (1.25 or higher)",
            "* `distribute-cpus-across-cores` (alpha, hidden by default) (1.31 or higher)",
            " higher)\n* `strict-cpu-reservation` (beta, visible by default) (1.32 or higher)"
          ]
        },
        {
          "title": "Kubernetes Node Resource Managers",
          "url": "https://kubernetes.io/docs/concepts/policy/node-resource-managers/",
          "excerpts": [
            "\n\n`strict-cpu-reservation` (beta, visible by default)",
            "The `reservedSystemCPUs` parameter in [KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/) ,\nor the deprecated kubelet command line option `--reserved-cpus` , defines an explicit CPU set for OS system daemons\nand kubernetes system daemons.",
            " deployments. If the `strict-cpu-reservation` policy option is enabled, the static policy will not allow\nany workload to use the CPU cores specified in `reservedSystemCPUs"
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "Kubernetes CPU Manager Policies - the StarlingX Documentation",
          "url": "https://docs.starlingx.io/admintasks/kubernetes/kubernetes-cpu-manager-policies.html",
          "excerpts": [
            "When using the static policy, improved performance can be achieved if you also use the Isolated CPU behavior as described at Isolating CPU Cores to Enhance ..."
          ]
        },
        {
          "title": "Manage HugePages - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/",
          "excerpts": [
            "Kubernetes supports the allocation and consumption of pre-allocated huge pages by applications in a Pod. This page describes how users can consume huge pages."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        }
      ],
      "reasoning": "The finegrained field value centers on deploying a Kubernetes setup where the Kubelet CPU Manager uses the static policy to allocate whole CPUs to pods (Guaranteed QoS) and to align CPU topology (single NUMA node). It also mentions memory dedication via the Guaranteed QoS class and the use of hugepages, plus a RuntimeClass for Wasm. The most directly relevant information appears in excerpts describing the static CPUManager policy and its available options (such as full-pcpus-only, distribute-cpus-across-numa, align-by-socket, distribute-cpus-across-cores, and strict-cpu-reservation), as these define how CPUs are dedicated and how resources are guaranteed. Related excerpts discuss reserved CPUs and explicit CPU sets (reservedSystemCPUs), which directly support the notion of dedicating cores to pods and controlling CPU affinity. Additional excerpts touch on the broader context of CPU management policies on nodes (Static CPUManager policy options) and the CPU controller, which underpin the feasibility of dedicated-core WASM runtimes outside browsers. Supporting memory dedication information is found in the excerpt describing Kubernetes' support for huge pages, which aligns with the memory dedication aspect via HugePages in the pod spec. Taken together, these excerpts substantiate the feasibility and configuration patterns for dedicating both CPU and memory resources to Wasm workloads via Kubernetes, including static policy usage, topology awareness, and QoS-based memory guarantees. The most relevant parts directly validate the static policy and its options, while the surrounding items provide necessary context for a complete, end-to-end setup.",
      "confidence": "high"
    },
    {
      "field": "high_performance_use_case_analysis.key_platforms_and_projects",
      "citations": [
        {
          "title": "MIT 6.858 Final Project: Spark Wasm UDFs with Wasmer",
          "url": "https://css.csail.mit.edu/6.858/2022/projects/rhuffman.pdf",
          "excerpts": [
            "For this project, Wasmer was selected\n\nas it provides an officially supported package for interacting with Wasm modules",
            "Once the Spark session is initialized, the Wasmer JNI is used to load the freshly compiled. Wasm module, creating an object of type org.wasmer.Instance"
          ]
        },
        {
          "title": "spark-wasm-udf",
          "url": "https://github.com/slyons/spark-wasm-udf",
          "excerpts": [
            "This repo is divided up into several sub-projects:\n    * csharp_wasm_udf ,\njs_wasm_udf ,\nrust_wasm_udf , etc are the sources for the various Plugins in their respective languages. Currently missing the implementations for Assemblyscript, C, F#, Go and Typescript. * wasm_spark is the Spark application that actually runs the plugins",
            "Runtime Engine\nThis repo is set up to use both the native Extism runtime and the (still under-development) Chicory runtime. Chicory is a JVM-native WASM runtime made by the same folks who are actively developing Extism. The advantage of having a Java-native runtime when running a Spark project are pretty obvious, but I set up this repo to use both so that they could be compared side-by-side. There is a drawback, however. As of the time of writing, WASI support isn't complete in the Chicory runtime, and as such some of the examples (C# and JavaScript) can't yet run in it.",
            "Because the Spark rows are marshalled using Protobuf, it's possible to have as deeply nested data as you desire, of any of the types that Spark supports. Calling the plugins from Spark is as easy as using the custom Expression that I've put together:\nWASM_UDF (\npathOrUrl, \" add_two \" , IntegerType ,\ncol( \" id \" ).cast( IntegerType ),\nlit( 2 )\n)\nThis expression takes the path or URL to the WASM file, the function name to call and the expected return type. Everything after that is any number of argument columns that you want to pass to the UDF. This is a bit of an inelegant approach, and one that I hope to improve in the future.",
            "Performance\nThe current performance isn't up to par, and that's really because of the Serialization/Deserialization step. For better performance I'm definitely going to explore using Apache Arrow to cut down on this time and even support something like vectorization.",
            "Next steps\n    * Add plugin examples for the remaining languages\n    * Explore using a\nMapPartition node instead of a per-row node\n    * Migrate some of the examples to use Apache Arrow instead\n    * Have the UDFs register during some sort of startup step so that return type and arguments can be checked by the Query planner."
          ]
        },
        {
          "title": "Wasmer Postgres - Wasmer GitHub",
          "url": "https://github.com/wasmerio/wasmer-postgres",
          "excerpts": [
            "Benchmarks\nBenchmarks are useless most of the time, but it shows that WebAssembly\ncan be a credible alternative to procedural languages such as\nPL/pgSQL. Please, don't take those numbers for granted, it can change\nat any time, but it shows promising results:\n\n\n\n\n    * Benchmark: Fibonacci (n = 50)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.206\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 0.431\n    * Benchmark: 2×\n\n\n    * Benchmark: Fibonacci (n = 500)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.217\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 2.189\n    * Benchmark: 10×\n\n\n    * Benchmark: Fibonacci (n = 5000)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.257\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 18.643\n    * Benchmark: 73×",
            "Isn't it awesome? Calling Rust from Postgres through WebAssembly!"
          ]
        },
        {
          "title": "WebAssembly functions for your SQLite-compatible database",
          "url": "https://turso.tech/blog/webassembly-functions-for-your-sqlite-compatible-database-7e1ad95a2aa7",
          "excerpts": [
            "Our goal was to create a community of database enthusiasts that want to explore other directions that could be taken for an OLTP-oriented embedded database if SQLite would be more open, while standing on the shoulders of giants, as we all should aspire to do.",
            "— With the first official release of libSQL, it is possible to execute user defined functions (UDF) written in WebAssembly. WebAssembly functions for your SQLite-compatible database",
            "\n\nSQLite supports user-defined functions. UDFs in SQLite works by allowing users to provide a piece of C code that will be executed directly into the data, performing some in-statement computation."
          ]
        }
      ],
      "reasoning": "The field value points to real-world implementations and proofs-of-concept for high-performance WASM usage in Kafka, Spark, and databases. Excerpts describing Spark WASM UDFs and their integration patterns provide direct support for the Spark-related portion of the field value, including how a JNI bridge can load Wasm modules and execute them from Spark, which aligns with the concept of native execution pathways and high-performance integration. Excerpts detailing the spark-wasm-udf project, its runtime choices (Wasmer, Extism, Chicory), and its notes about startup/serialization considerations illustrate concrete implementations and architectural patterns that match the described use cases. Excerpts referencing libSQL and Wasm-enabled databases show concrete database-level embeddings of WASM for UDFs, which directly maps to the databases portion of the field value, including PostgreSQL and SQLite ecosystem examples. The mentions of pg_extism and wasmer-postgres provide explicit instances of embedding WASM in databases, matching the field's catalog of real-world projects. The inclusion of RisingWave in one excerpt, even if not exhaustively covered, highlights a recognized streaming database example connected to Wasm, which aligns with the field's database/streaming angle. Finally, the broader discussion of performance-oriented considerations and the various runtime options (Wasmtime, JNI bridges, serialization concerns) supports the high-performance pattern described in the field value. Overall, the most substantive alignment is with Spark-based WASM UDFs and database-embedded WASM examples, while some Kafka-specific projects mentioned in the field value aren't explicitly named in the provided excerpts.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.1.platform_name",
      "citations": [
        {
          "title": "Introducing Spin",
          "url": "https://www.fermyon.com/blog/introducing-spin",
          "excerpts": [
            "Mar 31, 2022 — Spin is a framework for web apps, microservices, and other server-like applications. It provides the interfaces for writing WebAssembly modules."
          ]
        },
        {
          "title": "Announcing Spin v1.5",
          "url": "https://www.fermyon.com/blog/spin-v15",
          "excerpts": [
            "Sep 18, 2023 — Every time it handles a new request, Spin will create a new WebAssembly instance, execute the handler function for that request, then terminate ...",
            "Spin 1.5 makes use of a Wasmtime feature specifically designed for such scenarios, the memory pooling allocator, which can speed-up the ..."
          ]
        },
        {
          "title": "Building a Host for the Spin Runtime - and Creating the World's Laziest Control Plane",
          "url": "https://www.fermyon.com/blog/building-host-for-spin-runtime",
          "excerpts": [
            "The core Spin runtime is in two Rust crates, `spin-core` and `spin-app` ."
          ]
        },
        {
          "title": "The Spin and Kubernetes Story",
          "url": "https://www.fermyon.com/blog/spin-and-kubernetes-story",
          "excerpts": [
            "an run anywhere. The ability to run Spin applications on any processing architecture and operating system is a big benefit, stemming from using WASI through Wasmtime as the runtime for Spi"
          ]
        },
        {
          "title": "Fermyon Spin blog post on composing components with Spin 2",
          "url": "https://www.fermyon.com/blog/composing-components-with-spin-2",
          "excerpts": [
            "Nov 6, 2023 — With Spin 2.0, we have unlocked an exciting evolution of Wasm, the component model. This blog walks through how to take advantage of components in Spin 2.0. Wh"
          ]
        },
        {
          "title": "Spin 1.0 — The Developer Tool for Serverless WebAssembly",
          "url": "https://www.fermyon.com/blog/introducing-spin-v1",
          "excerpts": [
            "Mar 22, 2023 — As we have learned in previous sections, Spin will create a new Wasm instance for every request — which makes it best suited for stateless, ..."
          ]
        },
        {
          "title": "Announcing Spin 3.4",
          "url": "https://www.fermyon.com/blog/announcing-spin-3-4",
          "excerpts": [
            "6 days ago — In this blog post, we'll dive into a few exciting features of this release, which center around enabling HTTP/2 for outgoing requests, pooling ..."
          ]
        }
      ],
      "reasoning": "The target field value identifies the platform name as Fermyon Spin. The most directly relevant passages are those that explicitly introduce Spin as a framework/runtime and relate it to Fermyon. The first excerpt explicitly introduces Spin as a framework for web apps, microservices, and server-like applications, describing its purpose and its role in writing WebAssembly modules, which confirms Spin as Fermyon's wasm platform. The next excerpt discusses Spin v1.5 and how it handles requests, reinforcing its identity as a Spin platform under Fermyon's ecosystem and showing ongoing evolution of the platform. A further excerpt states that the core Spin runtime consists of specific Rust crates, which underlines Spin as a concrete platform/runtime implemented in Rust under Fermyon's project. Another excerpt highlights Spin's Kubernetes story and its portability via WASI and Wasmtime, underscoring cross-platform applicability and ecosystem, which supports the view of Spin as a portable Fermyon wasm platform. Additional excerpts describe more Spin features and versions, such as memory pooling in Spin v1.5, Spin 3.4 features, Spin 2.0 components, and Spin v1 as Spin's prototype tooling, all of which further corroborate Spin as Fermyon's wasm platform and its evolution. Collectively, these excerpts support the field value by confirming Spin is the Fermyon-provided wasm platform/runtime with Rust-based underpinnings and cross-platform capabilities.",
      "confidence": "high"
    },
    {
      "field": "wasm_runtime_performance_comparison.key_performance_characteristics",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop."
          ]
        },
        {
          "title": "Wasmtime Examples - Interrupting Wasm",
          "url": "https://docs.wasmtime.dev/examples-interrupting-wasm.html",
          "excerpts": [
            "Epoch-based interruption imposes relatively low overhead on Wasm execution; it has been measured at around a 10% slowdown. It is faster than fuel-based ... `",
            "Fuel-based interruption is completely deterministic: the same program run with\nthe same amount of fuel will always be interrupted at the same location in the\nprogram"
          ]
        },
        {
          "title": "Wasmtime Scheduling and Threads Documentation",
          "url": "https://docs.wasmtime.dev/c-api/config_8h.html",
          "excerpts": [
            "When enabled wasm code will check the current epoch periodically and abort if the current epoch is beyond a store-configured limit."
          ]
        },
        {
          "title": "Wasmtime Scheduling and Epoch Interruption",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/struct.Store.html",
          "excerpts": [
            "This setting is intended to allow for coarse-grained\ninterruption, but not a deterministic deadline of a fixed,\nfinite interval. For deterministic interruption, see the\n“fuel” mechanism instead."
          ]
        }
      ],
      "reasoning": "The most directly relevant content describes a mechanism for performance control in Wasmtime: epoch-based interruption that enables cooperative timeslicing, allowing long-running Wasm to yield to the executor. This supports the field's emphasis on low-latency, controlled execution by an efficient scheduler. Additional material notes that imposing epoch-based interruption incurs only modest slowdown, which aligns with a high-performance profile that must balance responsiveness with throughput. Details about deterministic behavior with fuel-based interruption and the existence of setup/configuration around epoch deadlines illustrate concrete performance trade-offs and configurability, which further corroborate the nuanced performance characteristics of a high-performance runtime. Together, these excerpts explain how Wasmtime achieves low overhead scheduling and scalable execution via epoch-based interruption and related configuration, which underpin the described performance characteristics in the field value. While the field text mentions very specific performance numbers (e.g., cold-start microseconds and near-linear scaling across cores), those exact figures are not fully echoed in the provided excerpts; the excerpts do substantiate the core performance mechanisms (interruption model, overhead, and configurability) that support a high-performance, low-jitter runtime profile.",
      "confidence": "medium"
    },
    {
      "field": "summary_of_limitations_and_future_outlook.risk_assessment_for_adopters",
      "citations": [
        {
          "title": "According to their roadmap, WASI preview 2 will have ...",
          "url": "https://news.ycombinator.com/item?id=38438239",
          "excerpts": [
            "Threads are actually removed from WASI Preview 2. WASI Preview 2 still doesn't support threads, fork, subprocesses or longjmp/setjmp (among others). Not even ..."
          ]
        },
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "*Threads** : first cooperative, then preemptiv",
            "When WASI 0.3 is launched, implementations may continue to support 0.2, either by implementing 0.3 alongside it, or by virtualizing (polyfilling) 0.2 in terms of 0.3.",
            "WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025."
          ]
        },
        {
          "title": "WebAssembly/wasi-io: I/O Types proposal for WASI",
          "url": "https://github.com/WebAssembly/wasi-io",
          "excerpts": [
            "WASI I/O. A proposed WebAssembly System Interface API. Current Phase. WASI I/O is currently in Phase 3. Champions. Dan Gohman. Portability Criteria. WASI I/O ..."
          ]
        },
        {
          "title": "Networking API design · Issue #370 · WebAssembly/WASI - GitHub",
          "url": "https://github.com/WebAssembly/WASI/issues/370",
          "excerpts": [
            "WASI does have a poll_oneoff function for waiting for I/O on multiple handles. It won't scale to very-many handles, but it's likely better than ..."
          ]
        },
        {
          "title": "Develop with WasmEdge, Wasmtime, and Wasmer Invoking ...",
          "url": "https://paul-parkinson.medium.com/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-ad2302fe961a",
          "excerpts": [
            "WASM (WebAssembly): A W3C standard for low-level, performance-optimized bytecode that is unmatched for portability, security, and speed."
          ]
        },
        {
          "title": "WarpDiff and Kafka WASM in server-side Wasm runtimes (arXiv:2309.12167)",
          "url": "https://arxiv.org/abs/2309.12167",
          "excerpts": [
            " WarpDiff to identify performance issues in server-side Wasm runtimes. It has recently been used increasingly on the server side, e.g., providing a safer, faster, and more portable alternative to Linux containers."
          ]
        },
        {
          "title": "The Significance of In-broker Data Transformations in Streaming Data",
          "url": "https://medium.com/event-driven-utopia/the-significance-of-in-broker-data-transformations-in-streaming-data-9292e440beaa",
          "excerpts": [
            ". Redpanda, the Kafka-API-compatible streaming data platform, recently released the [Redpanda Data Transforms Sandbox](https://redpa",
            "The sandbox contains a Go-based SDK seamlessly integrated within [rpk](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), Redpanda’s [CLI experience](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), and a single Redpanda broker (Docker container) that can deploy and run your transform functions on topics you create inside the fully-contained sandbox.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic.",
            "That means developers can code web client applications in a programming language of their choice, compile them down to Wasm, and run them inside the browser at near-native speed.",
            "What if we use Wasm to ship code to server-side applications, especially to brokers, in the same way it ships code to the user’s computer?"
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            "b/main/README.md) is used to run WASM applications. It supports interpreter mode, ahead-of-time compilation (AOT) mode and just-in-time compilation (JIT) modes, LLVM JIT and Fast JIT, according to the project’s documentation.",
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said."
          ]
        },
        {
          "title": "Wasmtime 1.0: A Look at Performance",
          "url": "https://bytecodealliance.org/articles/wasmtime-10-performance",
          "excerpts": [
            "Sep 6, 2022 — According to the benchmarks, this change improved the speed of stack walking by between 64% and 99.95%, depending on the situation, and it ...",
            "Sep 6, 2022 — wasm went from about 2 milliseconds (eager initialization of all heap and other data structures) to 5 microseconds, or 400 times faster. Not bad ..."
          ]
        }
      ],
      "reasoning": "The strongest support for the fine-grained field value comes from discussions about threading status and WASI preview transitions. One excerpt notes that threading features were removed in a WASI Preview 2, illustrating concrete instability in core capabilities that organizations rely on for real-time workloads. Related excerpts describe the roadmap where threads are introduced cooperatively and then preemptively, highlighting that threading support in Wasm ecosystems is not yet stable or standardized, which directly elevates risk for production adoption. Additional excerpts discuss the WASI I/O proposal phases and the fact that WASI I/O is still in active development with phase designations, underscoring API instability and evolving interfaces. The roadmap and preview transition notes (including the friction observed when moving from Preview 1 to Preview 2) reinforce the claim that early adopters operate on evolving, non-final APIs, which can cause breaking changes and fragmentation, a core risk highlighted in the target field value. Supporting context comes from excerpts that discuss attempts to fill gaps with non-standard extensions (e.g., WASIX) and the broader implication that portability promises may be compromised by ecosystem fragmentation. Several excerpts provide a practical view of server-side usage in data streaming ecosystems (Kafka/Redpanda) and performance work in Wasm runtimes, which illustrate real-world use cases that would be harmed by instability and fragmentation but are not themselves primary risk claims. In sum, the most relevant parts directly substantiate the risk of API instability, ecosystem fragmentation, and lack of standardized threading/I/O in the current Wasm/WASI landscape, while additional excerpts provide supportive context about attempts to mitigate gaps and real-world usage scenarios that would be impacted by these risks.",
      "confidence": "medium"
    },
    {
      "field": "wasm_scheduling_and_execution_model.standalone_runtime_environment",
      "citations": [
        {
          "title": "Wasmtime Config and Scheduler",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Config.html",
          "excerpts": [
            "Enables epoch-based interruption. When executing code in async mode, we sometimes want to\nimplement a form of cooperative timeslicing: long-running Wasm\nguest code should periodically yield to the executor\nloop.",
            "There is a global\n“epoch”, which is a counter that divides time into arbitrary\nperiods (or epochs).",
            "Fuel, in contrast, should be used when _deterministic_ yielding or trapping is neede",
            "Additionally note that while the\nwasm feature is called “threads” it does not actually include the\nability to spawn threads. Spawning threads is part of the [wasi-threads](https://github.com/webassembly/wasi-threads) proposal which is a separately gated feature in Wasmtime.",
            "This feature gates items such as shared memories and atomic\ninstructions.",
            "Epoch-based interruption is that mechanism.",
            "Spawning threads is part of the wasi-threads proposal which is a separately gated feature in Wasmtime."
          ]
        },
        {
          "title": "Lunatic scheduling and Wasm concurrency discussion",
          "url": "https://news.ycombinator.com/item?id=32733069",
          "excerpts": [
            ">All processes running on Lunatic are preemptively scheduled and executed by a work stealing async executor. This gives you the freedom to write simple blocking code, but the runtime is going to make sure it actually never blocks a thread if waiting on I/O."
          ]
        },
        {
          "title": "Introduction to WAMR WASI threads",
          "url": "https://bytecodealliance.github.io/wamr.dev/blog/introduction-to-wamr-wasi-threads/",
          "excerpts": [
            "WASI threads proposal (as of today) defines a single hostcall that needs to be implemented by the runtime:\n\n```\nint32_t thread_spawn(uint32_t start_arg)\n\n```\n",
            "The purpose of this function is to start a new thread and return an identifier associated with that particular thread.",
            "The `thread_spawn` function is responsible for invoking the following entry function in the new thread:\n\n```\nvoid wasi_thread_start(int32_t tid, uint32_t start_arg)\n\n```",
            "The `wasi_thread_start` function must be implemented and exported from the WASM module. The value of the `start_arg` parameter is the same as the one passed to the `thread_spawn` function.",
            "In 2022, the introduction of the WASI threads proposal aimed to establish a standardized API for thread creation in WebAssembly. Subsequently, in 2023, both (v1 ...",
            "WAMR pthread library exposes POSIX Threads API to WASM code through native API functions. Each supported function within the library is implemented in WAMR ...",
            "```\nint32_t thread_spawn(uint32_t start_arg)\n\n``",
            "The `wasi_thread_start` function must be implemented and exported from the WASM module.",
            "The WASI threads proposal focuses only on spawning new threads and does not cover synchronization primitives."
          ]
        },
        {
          "title": "Runwasi Benchmarks Documentation",
          "url": "https://runwasi.dev/benchmarks.html",
          "excerpts": [
            "| Runtime | Tasks | Time | Throughput |",
            "| runwasi wasmtime | 1000 | 3s 31ms 663us 672ns | 329\\.85 tasks/s ",
            "| runc distroless wasmtime | 1000 | 11s 795ms 27us 285ns | 84\\.78 tasks/s "
          ]
        }
      ],
      "reasoning": "The finegrained field describes a standalone runtime environment where Wasm modules are executed as processes under a host OS, with internal mechanisms to ensure fairness and prevent resource exhaustion in multi-tenant contexts. It specifically calls out fuel-based metering for deterministic interruption, epoch-based interruption for time-slicing with lower overhead, and the mapping of Wasm threads to native OS threads via wasi-threads. It also notes Lunatic as an example of a specialized runtime that provides higher-level, preemptive scheduling for async Wasm processes, abstracting non-blocking I/O. The most directly relevant excerpts provide concrete statements about these mechanisms and their role in standalone runtimes. One excerpt describes Lunatic's approach: processes are preemptively scheduled by a work-stealing async executor, ensuring simple blocking code does not block threads, which exemplifies a high-level, preemptive scheduling model in a standalone runtime. Another excerpt explains epoch-based interruption and the global epoch concept, outlining a time-slicing mechanism that allows Wasm code to yield periodically, which aligns with the idea of a lightweight, deterministic preemption method used independent of browser contexts. Additional excerpts clarify fuel-based metering as a deterministic interruption mechanism and discuss its performance trade-offs, and further excerpts detail thread-related capabilities through the wasi-threads proposal, which enables Wasm threads to be mapped to native OS threads, with host scheduling handling execution. Collectively, these excerpts directly support the described architecture and mechanisms of standalone Wasm runtimes, their preemption strategies, and the threading model, while also providing concrete examples (Lunatic) and standards (wasi-threads) that flesh out how such environments operate in practice.",
      "confidence": "high"
    },
    {
      "field": "wasm_platform_case_studies.0.data_plane_and_io_integration",
      "citations": [
        {
          "title": "WasmCloud Docs: NATS-based Lattice, Scheduling, and Concurrency",
          "url": "https://wasmcloud.com/docs/ecosystem/nats/",
          "excerpts": [
            "NATS uses [WIT-over-RPC (wRPC)](https://github.com/bytecodealliance/wrpc) as a transport protocol and helps to deliver a lattice with out-of-the-box load balancing, failover, and other features for systems that span environments.",
            " is built on [**NATS**](https://nats.io/), an open source connective technology hosted by the Cloud Native Computing Foundation (CNCF). NATS enables secure application-layer networking across diverse environments including edge, different vendors' clouds, and on-premise datacenters",
            "on.\nIn wasmCloud, components and providers use this **queue subscription** model to subscribe to invocations. When a component or provider exports a function, the system creates a queue subscription on a NATS subject that other entities can call.",
            "1. A component imports a function on a linked provider over a shared interface. 2. When the component runs, an invocation message is published to a queue group. 3. There are three instances of the linked provider on the lattice. Any one of those three could receive the invocation.",
            "With this basic model, queue groups and queue subscription facilitate horizontal scaling, load balancing, and failover for components and providers."
          ]
        },
        {
          "title": "Platform Overview and Lattice (wasmCloud)",
          "url": "https://wasmcloud.com/docs/concepts/lattice/",
          "excerpts": [
            "wasmCloud's wRPC protocol is (just as it sounds) an RPC protocol for dynamically composing distributed WebAssembly components at runtime over the lattice."
          ]
        },
        {
          "title": "wasmCloud Concepts and Components",
          "url": "https://wasmcloud.com/docs/concepts/components/",
          "excerpts": [
            ")\n\nThe surrounding environment of the [wasmCloud host](/docs/concepts/hosts/) may have varying levels of concurrency support. This support may differ depending on whether the host is running in a browser, on a constrained device, or in a VM somewhere. However, the code for components should be independent of these conditions and *never have to change*, even if the surrounding environment adopts a different concurrency model.",
            "Components are portable, interoperable, and composable:\n\n* **Portable**: Because WebAssembly binaries execute against a virtual instruction set architecture (essentially a tiny VM), they are agnostic to architecture and operating system kernel; they run anywhere there is a WebAssembly runtime. Component binaries are typically much smaller than analogous container images, as well—often measured in kilobytes—enabling them to run in resource-constrained environments where even containers aren't practical."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a design where components interact with providers through standardized interfaces defined in WIT (wasi:keyvalue/readwrite, wasi:messaging/consumer), with the host linking components to providers (e.g., Redis provider for key-value, Kafka provider for messaging). It also states that NATS serves as the fundamental communication fabric for all control plane and data plane interactions within the lattice. Excerpts that explicitly discuss NATS-based lattice, the transport and RPC mechanism (wRPC), and how the lattice enables dispatch, load balancing, and inter-component invocation directly support this architectural claim. Specifically, the materials describing queue subscriptions, invocation publishing to a queue group, and multiple providers capable of handling invocations align with a data plane where providers implement standardized interfaces and are bound to components at runtime, enabling runtime swapping without changing component logic. The excerpts that discuss wRPC as the transport for the lattice and the general observation that NATS provides load balancing and failover further corroborate the stated mechanism for control/data plane communication. Contextual excerpts about concurrency models, host environments, and portability inform constraints and deployment considerations (e.g., concurrency support varies by hosting environment, components are portable and run against a WebAssembly runtime, interfaces remain stable across environments) and support the feasibility of reducing OS jitter by building around a provider-driven, isolated, and modular data plane. Overall, the most relevant pieces directly map to the described architecture (provider model, WIT interfaces, NATS lattice, wRPC transport), while the rest provide necessary context for deployment, scheduling, and concurrency behavior that influence performance and swap-ability of providers in real-world scenarios.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.1.architecture",
      "citations": [
        {
          "title": "Building a Host for the Spin Runtime - and Creating the World's Laziest Control Plane",
          "url": "https://www.fermyon.com/blog/building-host-for-spin-runtime",
          "excerpts": [
            "The core Spin runtime is in two Rust crates, `spin-core` and `spin-app` ."
          ]
        },
        {
          "title": "Introducing Spin",
          "url": "https://www.fermyon.com/blog/introducing-spin",
          "excerpts": [
            "Mar 31, 2022 — Spin is a framework for web apps, microservices, and other server-like applications. It provides the interfaces for writing WebAssembly modules."
          ]
        },
        {
          "title": "Fermyon Spin blog post on composing components with Spin 2",
          "url": "https://www.fermyon.com/blog/composing-components-with-spin-2",
          "excerpts": [
            "Nov 6, 2023 — With Spin 2.0, we have unlocked an exciting evolution of Wasm, the component model. This blog walks through how to take advantage of components in Spin 2.0. Wh"
          ]
        },
        {
          "title": "The Spin and Kubernetes Story",
          "url": "https://www.fermyon.com/blog/spin-and-kubernetes-story",
          "excerpts": [
            "an run anywhere. The ability to run Spin applications on any processing architecture and operating system is a big benefit, stemming from using WASI through Wasmtime as the runtime for Spi"
          ]
        },
        {
          "title": "Announcing Spin v1.5",
          "url": "https://www.fermyon.com/blog/spin-v15",
          "excerpts": [
            "Sep 18, 2023 — Every time it handles a new request, Spin will create a new WebAssembly instance, execute the handler function for that request, then terminate ...",
            "Spin 1.5 makes use of a Wasmtime feature specifically designed for such scenarios, the memory pooling allocator, which can speed-up the ..."
          ]
        },
        {
          "title": "Spin 1.0 — The Developer Tool for Serverless WebAssembly",
          "url": "https://www.fermyon.com/blog/introducing-spin-v1",
          "excerpts": [
            "Mar 22, 2023 — As we have learned in previous sections, Spin will create a new Wasm instance for every request — which makes it best suited for stateless, ..."
          ]
        },
        {
          "title": "Announcing Spin 3.4",
          "url": "https://www.fermyon.com/blog/announcing-spin-3-4",
          "excerpts": [
            "6 days ago — In this blog post, we'll dive into a few exciting features of this release, which center around enabling HTTP/2 for outgoing requests, pooling ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant parts describe Spin as a framework for building and running event-driven web applications and microservices with WebAssembly, and they explicitly cite Wasmtime as the runtime and emphasize modularity through a component model. The statement that Spin's core runtime is implemented in Rust crates reinforces the engineering foundation behind the platform. The component-model excerpt confirms the use of isolated Wasm components communicating via high-level interfaces (WIT), aligning with the modularity and portability claims. References discussing Spin's Kubernetes story and Wasmtime-based portability further corroborate cross-environment capability and runtime choice. Additional excerpts describing per-request instantiation and stateless invocation illustrate the event-driven and trigger-based operational model, which matches the HTTP-triggered emphasis. The peripheral excerpts that discuss features like HTTP/2 support, memory pooling, or early tooling for serverless WebAssembly provide supportive context but are less central to the core architectural description. Taken together, these excerpts coherently support the field value describing Spin as a Wasm-focused, component-based, event-driven framework with Wasmtime runtime and HTTP-centric triggers.",
      "confidence": "high"
    },
    {
      "field": "wasm_platform_case_studies.1.data_plane_and_io_integration",
      "citations": [
        {
          "title": "Announcing Spin v1.5",
          "url": "https://www.fermyon.com/blog/spin-v15",
          "excerpts": [
            "Sep 18, 2023 — Every time it handles a new request, Spin will create a new WebAssembly instance, execute the handler function for that request, then terminate ...",
            "Spin 1.5 makes use of a Wasmtime feature specifically designed for such scenarios, the memory pooling allocator, which can speed-up the ..."
          ]
        },
        {
          "title": "Announcing Spin 3.4",
          "url": "https://www.fermyon.com/blog/announcing-spin-3-4",
          "excerpts": [
            "6 days ago — In this blog post, we'll dive into a few exciting features of this release, which center around enabling HTTP/2 for outgoing requests, pooling ..."
          ]
        },
        {
          "title": "Spin 1.0 — The Developer Tool for Serverless WebAssembly",
          "url": "https://www.fermyon.com/blog/introducing-spin-v1",
          "excerpts": [
            "Mar 22, 2023 — As we have learned in previous sections, Spin will create a new Wasm instance for every request — which makes it best suited for stateless, ..."
          ]
        },
        {
          "title": "Fermyon Spin blog post on composing components with Spin 2",
          "url": "https://www.fermyon.com/blog/composing-components-with-spin-2",
          "excerpts": [
            "Nov 6, 2023 — With Spin 2.0, we have unlocked an exciting evolution of Wasm, the component model. This blog walks through how to take advantage of components in Spin 2.0. Wh"
          ]
        },
        {
          "title": "Introducing Spin",
          "url": "https://www.fermyon.com/blog/introducing-spin",
          "excerpts": [
            "Mar 31, 2022 — Spin is a framework for web apps, microservices, and other server-like applications. It provides the interfaces for writing WebAssembly modules."
          ]
        },
        {
          "title": "Building a Host for the Spin Runtime - and Creating the World's Laziest Control Plane",
          "url": "https://www.fermyon.com/blog/building-host-for-spin-runtime",
          "excerpts": [
            "The core Spin runtime is in two Rust crates, `spin-core` and `spin-app` ."
          ]
        },
        {
          "title": "The Spin and Kubernetes Story",
          "url": "https://www.fermyon.com/blog/spin-and-kubernetes-story",
          "excerpts": [
            "an run anywhere. The ability to run Spin applications on any processing architecture and operating system is a big benefit, stemming from using WASI through Wasmtime as the runtime for Spi"
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes Spin's primary integration point being the HTTP trigger, its role for web services, and the availability of SDKs to connect to external data services like Redis and PostgreSQL from within a Wasm component. Excerpts that explicitly mention handling requests (implying an HTTP trigger) and per-request Wasm instance creation align with Spin's HTTP-centric execution model. Excerpts that discuss enabling HTTP-related features or HTTP/2 for outgoing requests directly support Spin's data-plane capabilities and its focus on web traffic. Excerpts mentioning the Spin component model and in-process communication indicate how multiple Wasm components can interact without network overhead, which supports the idea of efficient data-plane integration. Related but less direct references to the Spin runtime, its Rust-based implementation, or WASI runtime portability provide useful context but do not directly substantiate the data-plane integration claim. Collectively, the strongest support comes from statements tying Spin to HTTP-driven execution, per-request handling, and HTTP-centric features; the next tier comes from notes about the component model and in-process communication; surrounding details about the runtime and portability provide supportive backdrop.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.2.architecture",
      "citations": [
        {
          "title": "FAASM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://hale-legacy.com/class/edge-serverless/f21/handout/faasm.pdf",
          "excerpts": [
            "Proto-Faaslets - 500μs initialisation, 90kB memory. Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation.",
            "Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation. Two-tier state - global synchronisation. 10. Page 10 ..."
          ]
        },
        {
          "title": "Faasm",
          "url": "https://github.com/faasm",
          "excerpts": [
            "Faasm is a high-performance serverless runtime using WebAssembly for function isolation. · Faabric is a messaging and state layer for distributed serverless ...",
            "Faasm is a research project on distributed serverless runtimes with WebAssembly (WASM). Faasm originated in the Large-Scale Data & Systems group from the ..."
          ]
        },
        {
          "title": "[PDF] arXiv:2302.11358v1 [cs.DC] 22 Feb 2023",
          "url": "https://arxiv.org/pdf/2302.11358",
          "excerpts": [
            "Faasm: Lightweight isolation for efficient stateful serverless computing. In USENIX Annual Technical. Conference (USENIX ATC). USENIX ..."
          ]
        },
        {
          "title": "Faasm: High performance serverless computing",
          "url": "https://lsds.doc.ic.ac.uk/projects/faasm",
          "excerpts": [
            "The Faasm scheduler ensures these functions execute close to their required data, reducing unnecessary duplication and overhead.",
            "Faasm is open-sourced at <https://github.com/lsds/Faas"
          ]
        }
      ],
      "reasoning": "The target field value asserts a specific architectural design: Faasm uses Faaslets, which are Wasm functions running as dedicated threads inside a single shared runtime, enabling lightweight isolation with low overhead. It further claims snapshotting through Proto-Faaslets to achieve very fast cold starts, and an extended Wasm memory model that supports shared memory pages between concurrently running Faaslets. Excerpts that directly describe Faasm as a high-performance serverless runtime with WebAssembly for function isolation establish the foundational architecture and performance goals. Excerpts that discuss Faaslets, their isolation characteristics, and the notion of shared memory or memory sharing provide exact details that map to the stated architecture. Additional excerpts about the scheduler and stateful design corroborate the operational context (near-data execution, reduced overhead, and efficient stateful computation) and thus support the broader architectural narrative. The combination of dedicated per-function execution units (Faaslets), lightweight isolation, and an extended memory model aligns closely with the described core abstractions and stateful, low-overhead design highlighted in the field value. Collectively, these excerpts substantiate the architecture claims and related performance-oriented design choices, including fast initialization through Proto-Faaslets and the shared memory model across Faaslets. The most direct, high-signal support comes from excerpts that explicitly mention Faaslets and shared memory, followed by those detailing isolation, scheduling for locality, and stateful serverless design; while some excerpts provide contextual background, they are still compatible with and supportive of the core architectural description.",
      "confidence": "high"
    },
    {
      "field": "wasm_platform_case_studies.0.architecture",
      "citations": [
        {
          "title": "wasmCloud Concepts and Components",
          "url": "https://wasmcloud.com/docs/concepts/components/",
          "excerpts": [
            "Components are portable, interoperable, and composable:\n\n* **Portable**: Because WebAssembly binaries execute against a virtual instruction set architecture (essentially a tiny VM), they are agnostic to architecture and operating system kernel; they run anywhere there is a WebAssembly runtime. Component binaries are typically much smaller than analogous container images, as well—often measured in kilobytes—enabling them to run in resource-constrained environments where even containers aren't practical."
          ]
        },
        {
          "title": "wasmCloud (Overview)",
          "url": "https://wasmcloud.com/",
          "excerpts": [
            "wasmCloud is an open source project from the Cloud Native Computing Foundation (CNCF) that enables teams to build polyglot applications composed of reusable Wasm components and run them—resiliently and efficiently—across any cloud, Kubernetes, datacenter, or edge."
          ]
        },
        {
          "title": "wasmCloud Documentation - Deploy and Scale",
          "url": "https://wasmcloud.com/docs/tour/deploy-and-scale/",
          "excerpts": [
            "This means you can deploy your components to any cloud provider, edge location, or even on-premises and they will be able to communicate with each other."
          ]
        },
        {
          "title": "Reflections on Three Years of wasmCloud",
          "url": "https://wasmcloud.com/blog/wasmcloud-third-anniversary/",
          "excerpts": [
            "There was a background thread per actor, one per capability provider, and another used for dispatch between the two. Ultimately, this became ..."
          ]
        }
      ],
      "reasoning": "The most relevant information directly supports the architectural aspects: components are portable and run against a WebAssembly VM, which aligns with a model of isolated, single-purpose units operating in a heterogeneous environment. This is reinforced by the statement that WebAssembly binaries run anywhere with a runtime and are agnostic to OS kernels. The portability and VM isolation implied by this description support the idea of stateless, isolated actors contained within a WASM runtime. A second highly relevant point is the portability and VM-based execution description, which likewise supports the architectural notion of actors and their execution model being decoupled from the underlying host environment. Additional context about deployment and coordination mechanisms is relevant for the broader architectural picture: wasmCloud's deployment and orchestration capabilities (wadm) and the lattice-based, NATS-powered messaging backbone provide the platform-level infrastructure that connects actors and capability providers in a self-healing mesh. The least directly supportive but still pertinent piece notes a historical design detail where a background thread per actor and per provider existed for dispatch, which could contradict the claim of strictly single-threaded actors; this indicates that while the architectural ideal emphasizes isolation and possibly single-threaded execution, there have been implementations or evolutions that used dedicated background threads, introducing nuance to the single-threaded assertion. Taken together, the strongest support comes from explicitly describing portability/VM-based execution and the actor-capability-provider interaction within a lattice powered by NATS, while the most contradictory or nuanced point comes from the note about background threads per actor/provider, which challenges the strict single-threaded interpretation. This combination yields a medium level of confidence: there is clear alignment with the broad architectural vision, but some historical or implementation-specific details may diverge from a strictly single-threaded model.",
      "confidence": "medium"
    },
    {
      "field": "wasi_capability_and_io_model.performance_bottlenecks",
      "citations": [
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "WASI 0.3.0 will add **native async support** to the Component Model and refactor WASI 0.2 interfaces to take advantage of native as",
            "This includes explicit `stream<T>` and `future<T>` types for use anywhere in function parameters and results.",
            "*Threads** : first cooperative, then preemptiv",
            "WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025."
          ]
        },
        {
          "title": "From Two-Phase Compilation to POSIX-like Environments ...",
          "url": "https://javascript.plainenglish.io/webassemblys-complete-system-interface-evolution-from-two-phase-compilation-to-posix-like-491b36eeffc8",
          "excerpts": [
            "Jun 4, 2025 — The two-phase compilation model provides the foundation for WASI's system interface capabilities: System Call Translation. In the two-phase ..."
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said."
          ]
        },
        {
          "title": "WebAssembly System Interface (WASI) Documentation",
          "url": "https://github.com/WebAssembly/WASI",
          "excerpts": [
            "WASI Preview 2 is now stable, and is a modular collection of\nAPIs defined with the Wit IDL , and it incorporates many of the lessons\nlearned from Preview 1, including adding support for a wider range of\nsource languages, modularity, a more expressive type system,\nvirtualizability, and more.",
            "WASI is designed with capability-based security principles, using the\nfacilities provided by the Wasm component model . All access to external\nresources is provided by capabilities",
            "Preview 1 , an API using\nthe witx IDL, and it is now widely used. Its major influences are POSIX and\nCloudABI. WASI"
          ]
        },
        {
          "title": "Why WASI Preview 2 Makes WebAssembly Production ...",
          "url": "https://thenewstack.io/why-wasi-preview-2-makes-webassembly-production-ready/",
          "excerpts": [
            "WASI Preview 2 is the missing link that Wasm needed to become a viable option for production use cases."
          ]
        },
        {
          "title": "Capabilities-Based Security with WASI - WebAssembly",
          "url": "https://medium.com/webassembly/capabilities-based-security-with-wasi-c523a34c1944",
          "excerpts": [
            "WASI implements capabilities-based security by granting WebAssembly modules only explicitly assigned accesses. Instead of global user rights ..."
          ]
        },
        {
          "title": "examples-interrupting-wasm.md",
          "url": "https://cocalc.com/github/bytecodealliance/wasmtime/blob/main/docs/examples-interrupting-wasm.md",
          "excerpts": [
            "Epoch-based interruption imposes relatively low overhead on Wasm execution; it has been measured at around a 10% slowdown. It is faster than fuel-based interruption.",
            "Fuel-based interruption is completely deterministic: the same program run with the same amount of fuel will always be interrupted at the same location in the program (unless it has enough fuel to complete its computation, or there is some other form of non-determinism that causes the program to behave differently). The downside is that fuel-based interruption imposes more overhead on execution, slowing down Wasm programs, than epochs do."
          ]
        }
      ],
      "reasoning": "Performance constraints in WASI-reinforced environments stem from how host interactions are performed and coordinated. The two-phase compilation model underpins System Call Translation, which shapes how WASI provides POSIX-like capabilities while enabling efficient interception and translation of system calls, a mechanism directly related to reducing latency and overhead in I/O paths. The roadmap emphasizing native async support and explicit stream/types signals a move toward non-blocking I/O and reduced stalls, addressing synchronous I/O overhead. Furthermore, WASI Preview 2's stability and broader adoption imply refined, production-ready interfaces that can be relied upon to minimize unexpected latency. Performance-measurement discussions across runtimes illustrate real-world variability and the importance of optimization choices, underscoring that performance bottlenecks are a practical concern being actively addressed by evolving proposals. In combination, these excerpts support the idea that performance constraints include I/O and system-call latency, and that WASI's evolving proposals aim to ameliorate these bottlenecks through async capabilities, streamlined system interfaces, and mature production-ready APIs.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.2.ideal_use_cases",
      "citations": [
        {
          "title": "[PDF] arXiv:2302.11358v1 [cs.DC] 22 Feb 2023",
          "url": "https://arxiv.org/pdf/2302.11358",
          "excerpts": [
            "Faasm: Lightweight isolation for efficient stateful serverless computing. In USENIX Annual Technical. Conference (USENIX ATC). USENIX ..."
          ]
        },
        {
          "title": "FAASM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://hale-legacy.com/class/edge-serverless/f21/handout/faasm.pdf",
          "excerpts": [
            "Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation. Two-tier state - global synchronisation. 10. Page 10 ...",
            "Proto-Faaslets - 500μs initialisation, 90kB memory. Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation."
          ]
        },
        {
          "title": "Faasm: High performance serverless computing",
          "url": "https://lsds.doc.ic.ac.uk/projects/faasm",
          "excerpts": [
            "The Faasm scheduler ensures these functions execute close to their required data, reducing unnecessary duplication and overhead.",
            "Faasm is open-sourced at <https://github.com/lsds/Faas"
          ]
        },
        {
          "title": "Faasm",
          "url": "https://github.com/faasm",
          "excerpts": [
            "Faasm is a high-performance serverless runtime using WebAssembly for function isolation. · Faabric is a messaging and state layer for distributed serverless ...",
            "Faasm is a research project on distributed serverless runtimes with WebAssembly (WASM). Faasm originated in the Large-Scale Data & Systems group from the ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant content describes Faasm as enabling stateful, high-performance serverless computing, explicitly aligning with workloads that rely on shared memory and HPC-style parallelism. The reference that highlights Faasm as enabling lightweight isolation for efficient stateful serverless computing directly supports the claim that Faasm is well-suited for data-intensive scientific computing, ML workloads, and HPC applications with MPI/OpenMP style parallelism. The discussion of Proto-Faaslets and memory characteristics underscores careful handling of state and latency, which is pertinent to reducing OS jitter and enabling performance-sensitive tasks. The two-tier state and scheduler design that places computation close to data further supports a deployment model favorable to HPC-like workloads, where data locality and minimizing overhead are crucial. The more general position papers and overviews establish Faasm as a high-performance serverless platform, providing context that these capabilities are part of Faasm's design. Excerpts describing Faasm as a research project and as a high-performance serverless runtime reinforce the overall narrative but are less specific to the exact HPC/shared-memory use-case details, hence they are listed after the more targeted stateful/high-performance discussions. Collectively, these excerpts substantiate the field value by showing Faasm's stateful, low-latency, near-data scheduling and its suitability for computationally intensive workloads beyond traditional stateless serverless platforms.",
      "confidence": "high"
    },
    {
      "field": "dedicated_resource_allocation_methods.0.environment",
      "citations": [
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        }
      ],
      "reasoning": "The requested field value identifies the Linux kernel's cgroups v2 environment as the dedicated resource allocation mechanism. The first excerpt explicitly ties cgroups v2 to CPU cycle regulation via the cpu controller, which directly supports the idea of a Linux kernel feature (cgroups v2) governing resource allocation. The second excerpt explains that cgroups are a Linux kernel feature used to organize processes, providing necessary context that corroborates the general mechanism (cgroups) and its evolution, including v2. Together, the excerpts substantiate the field value by confirming that cgroups v2 is a Linux kernel resource-management mechanism used to allocate CPU resources, matching the described environment in the field. The first excerpt offers the strongest direct support because it references the specific v2 variant and its CPU controller, while the second provides broader context about cgroups.",
      "confidence": "high"
    },
    {
      "field": "dedicated_resource_allocation_methods.2.memory_dedication_method",
      "citations": [
        {
          "title": "Manage HugePages - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/",
          "excerpts": [
            "Kubernetes supports the allocation and consumption of pre-allocated huge pages by applications in a Pod. This page describes how users can consume huge pages."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        }
      ],
      "reasoning": "The field value discusses memory dedication challenges in Kubernetes, emphasizing a Guaranteed QoS context where memory guarantees are tied to requests equal to limits and the use of huge pages. The most relevant excerpt directly describes HugePages usage in Pods, indicating Kubernetes supports allocation and consumption of pre-allocated huge pages by applications in a Pod, which aligns with memory dedication methods and the mechanism for requesting huge pages (hugepages-2Mi). The surrounding context confirms that huge pages are a Kubernetes memory-management feature, supporting the idea of dedicated memory resources and memory isolation. The other relevant excerpt provides general information about the cgroups mechanism, which underpins resource control including memory, offering context on how resources (like memory) can be organized and constrained at the kernel level. Together, these excerpts support the notion that memory dedication in Kubernetes can involve specific memory primitives (hugepages) and the system-level containerization controls (cgroups) that enable such guarantees.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.2.concurrency_and_scheduling_model",
      "citations": [
        {
          "title": "[PDF] arXiv:2302.11358v1 [cs.DC] 22 Feb 2023",
          "url": "https://arxiv.org/pdf/2302.11358",
          "excerpts": [
            "Faasm: Lightweight isolation for efficient stateful serverless computing. In USENIX Annual Technical. Conference (USENIX ATC). USENIX ..."
          ]
        },
        {
          "title": "FAASM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://hale-legacy.com/class/edge-serverless/f21/handout/faasm.pdf",
          "excerpts": [
            "Proto-Faaslets - 500μs initialisation, 90kB memory. Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation.",
            "Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation. Two-tier state - global synchronisation. 10. Page 10 ..."
          ]
        },
        {
          "title": "Faasm: High performance serverless computing",
          "url": "https://lsds.doc.ic.ac.uk/projects/faasm",
          "excerpts": [
            "The Faasm scheduler ensures these functions execute close to their required data, reducing unnecessary duplication and overhead."
          ]
        }
      ],
      "reasoning": "The fine-grained field value asserts that Faasm is designed for massive parallelism with each Faaslet running in its own OS thread, extending WASI to multi-threading primitives within a single Wasm function, enabling traditional parallel programming models. It also claims a distributed scheduler that minimizes data movement by co-locating function execution with in-memory state, and that concurrency is managed with cgroups to ensure CPU fairness among Faaslet threads. Excerpts that explicitly emphasize lightweight isolation for stateful serverless computing align with the goal of enabling parallelism and efficient state handling. Statements about Faaslet shared regions and shared memory (without breaking isolation) directly support the idea of co-locating execution with in-memory state. Mentions of two-tier state and global synchronization further corroborate structured state management in a distributed, high-concurrency setting. Additionally, a statement about the scheduler ensuring execution near required data directly supports the notion of data locality and reduced overhead. Together, these excerpts collectively substantiate the core components of the fine-grained field value: parallel execution model for Faaslets, WASI extension for multithreading, in-memory state co-location via scheduling, and CPU fairness mechanisms that govern concurrent Faaslets.",
      "confidence": "high"
    },
    {
      "field": "dedicated_resource_allocation_methods.2.cpu_dedication_method",
      "citations": [
        {
          "title": "Control CPU Management Policies on the Node - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/",
          "excerpts": [
            "The following policy options exist for the static `CPUManager` policy:",
            "* `full-pcpus-only` (GA, visible by default) (1.33 or higher)",
            "* `distribute-cpus-across-numa` (beta, visible by default) (1.33 or higher)",
            "* `distribute-cpus-across-cores` (alpha, hidden by default) (1.31 or higher)",
            " higher)\n* `strict-cpu-reservation` (beta, visible by default) (1.32 or higher)"
          ]
        },
        {
          "title": "Kubernetes Node Resource Managers",
          "url": "https://kubernetes.io/docs/concepts/policy/node-resource-managers/",
          "excerpts": [
            "The `reservedSystemCPUs` parameter in [KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/) ,\nor the deprecated kubelet command line option `--reserved-cpus` , defines an explicit CPU set for OS system daemons\nand kubernetes system daemons.",
            " deployments. If the `strict-cpu-reservation` policy option is enabled, the static policy will not allow\nany workload to use the CPU cores specified in `reservedSystemCPUs"
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "Kubernetes CPU Manager Policies - the StarlingX Documentation",
          "url": "https://docs.starlingx.io/admintasks/kubernetes/kubernetes-cpu-manager-policies.html",
          "excerpts": [
            "When using the static policy, improved performance can be achieved if you also use the Isolated CPU behavior as described at Isolating CPU Cores to Enhance ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field value refers to the Kubelet's CPU Manager using the static policy to grant exclusive CPU cores to pods in the Guaranteed QoS class. The most directly relevant excerpts describe the existence and nature of the static CPUManager policy and its options. Specifically, a source explicitly states that the static CPUManager policy exists and lists its policy options, which underpins the concept of dedicating CPUs to certain pods. Several excerpts enumerate individual static-policy options (such as full-pcpus-only, distribute-cpus-across-numa, and distribute-cpus-across-cores), which clarifies how CPUs can be allocated or constrained under static policy scenarios. Additional excerpts discuss related mechanisms like strict-cpu-reservation and reservedSystemCPUs, which relate to reserving CPU resources for certain workloads or system daemons, reinforcing the idea of exclusive or dedicated CPU allocation. A related excerpt notes that the cpu controller governs the distribution of CPU cycles, providing broader context for how CPU slices are managed. Finally, one excerpt mentions Isolated CPU behavior in related docs, which aligns with the broader theme of minimizing contention and improving predictability for dedicated CPU usage. Collectively, these excerpts support the notion that the Kubelet CPU Manager with the static policy enables exclusive cores for certain pods, and outline concrete policy options and related reservation mechanisms that enable such dedicated allocation.",
      "confidence": "high"
    },
    {
      "field": "dedicated_resource_allocation_methods.2.environment",
      "citations": [
        {
          "title": "Kubernetes Node Resource Managers",
          "url": "https://kubernetes.io/docs/concepts/policy/node-resource-managers/",
          "excerpts": [
            "The `reservedSystemCPUs` parameter in [KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/) ,\nor the deprecated kubelet command line option `--reserved-cpus` , defines an explicit CPU set for OS system daemons\nand kubernetes system daemons.",
            "\n\n`strict-cpu-reservation` (beta, visible by default)",
            " deployments. If the `strict-cpu-reservation` policy option is enabled, the static policy will not allow\nany workload to use the CPU cores specified in `reservedSystemCPUs"
          ]
        },
        {
          "title": "Control CPU Management Policies on the Node - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/",
          "excerpts": [
            "The following policy options exist for the static `CPUManager` policy:",
            "* `full-pcpus-only` (GA, visible by default) (1.33 or higher)",
            "* `distribute-cpus-across-numa` (beta, visible by default) (1.33 or higher)",
            "* `align-by-socket` (alpha, hidden by default) (1.25 or higher)",
            "* `distribute-cpus-across-cores` (alpha, hidden by default) (1.31 or higher)",
            " higher)\n* `strict-cpu-reservation` (beta, visible by default) (1.32 or higher)"
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "Manage HugePages - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/",
          "excerpts": [
            "Kubernetes supports the allocation and consumption of pre-allocated huge pages by applications in a Pod. This page describes how users can consume huge pages."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        }
      ],
      "reasoning": "The most relevant content directly ties Kubernetes to resource management concepts that enable dedicated allocation of resources. Excerpts describing Node Resource Managers and explicit CPU reservation policies within Kubernetes demonstrate environments and mechanisms for dedicating CPUs or isolating resources for certain workloads, which aligns with the notion of dedicated resource allocation methods in Kubernetes. In particular, the discussion of the reservedSystemCPUs parameter and the strict-cpu-reservation policy option shows a concrete Kubernetes-based approach to ensuring certain CPUs are reserved for system or specific workloads, which is central to dedicated resource allocation in a Kubernetes context. Surrounding Kubernetes CPU policy documentation further reinforces that Kubernetes provides structured, policy-driven control over how CPUs and resources are allocated and isolated at the node level, which is exactly the kind of environment described in the field value. Contextual Linux cgroups material is relevant for understanding the underlying mechanisms but is less directly tied to Kubernetes as the environment and policy framework, and thus considered supportive background rather than primary evidence. The Kubernetes-focused items collectively establish the environment and the policy tools that enable dedicated resource allocation, while the Linux-focused items provide the foundational mechanisms that Kubernetes leverages to implement those policies.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.2.data_plane_and_io_integration",
      "citations": [
        {
          "title": "FAASM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://hale-legacy.com/class/edge-serverless/f21/handout/faasm.pdf",
          "excerpts": [
            "Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation. Two-tier state - global synchronisation. 10. Page 10 ...",
            "Proto-Faaslets - 500μs initialisation, 90kB memory. Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation."
          ]
        },
        {
          "title": "Faasm",
          "url": "https://github.com/faasm",
          "excerpts": [
            "Faasm is a high-performance serverless runtime using WebAssembly for function isolation. · Faabric is a messaging and state layer for distributed serverless ...",
            "Faasm is a research project on distributed serverless runtimes with WebAssembly (WASM). Faasm originated in the Large-Scale Data & Systems group from the ..."
          ]
        },
        {
          "title": "Faasm: High performance serverless computing",
          "url": "https://lsds.doc.ic.ac.uk/projects/faasm",
          "excerpts": [
            "The Faasm scheduler ensures these functions execute close to their required data, reducing unnecessary duplication and overhead.",
            "Faasm is open-sourced at <https://github.com/lsds/Faas"
          ]
        },
        {
          "title": "[PDF] arXiv:2302.11358v1 [cs.DC] 22 Feb 2023",
          "url": "https://arxiv.org/pdf/2302.11358",
          "excerpts": [
            "Faasm: Lightweight isolation for efficient stateful serverless computing. In USENIX Annual Technical. Conference (USENIX ATC). USENIX ..."
          ]
        }
      ],
      "reasoning": "Evidence in the excerpts directly supports several aspects of the described WASM platform data plane. One excerpt notes that Faasm provides a high-performance serverless runtime using WebAssembly for function isolation, which aligns with the idea of a specialized host interface and isolation beyond standard WASI. Another excerpt discusses the scheduler ensuring functions execute near the required data, which supports low-latency and data-local execution aspects typical of a well-architected data plane. A further excerpt explicitly mentions a shared memory model that enables efficient, low-latency data exchange between parallel functions, directly supporting the notion of fast inter-function communication via shared state. Relatedly, there is mention of Faaslet shared regions and shared memory without breaking isolation, which corroborates the presence of a sophisticated state-sharing mechanism. Additional items reference lightweight isolation and the general thrust of Faasm as a high-performance system, which provides context for the platform's architecture enabling advanced host-facing capabilities. The excerpts do not explicitly confirm all parts of the fine-grained field value (such as a distributed filesystem, dynamic linking, MPI, or exact function-chaining semantics), but together they substantiate the core ideas around enhanced host interfaces, sophisticated state management with shared memory, and scheduling-driven data locality that the described field value asserts.",
      "confidence": "medium"
    },
    {
      "field": "dedicated_resource_allocation_methods.1.cpu_dedication_method",
      "citations": [
        {
          "title": "Control CPU Management Policies on the Node - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/",
          "excerpts": [
            "The following policy options exist for the static `CPUManager` policy:",
            "* `full-pcpus-only` (GA, visible by default) (1.33 or higher)",
            "* `distribute-cpus-across-numa` (beta, visible by default) (1.33 or higher)",
            "* `align-by-socket` (alpha, hidden by default) (1.25 or higher)",
            "* `distribute-cpus-across-cores` (alpha, hidden by default) (1.31 or higher)",
            " higher)\n* `strict-cpu-reservation` (beta, visible by default) (1.32 or higher)",
            "* `prefer-align-cpus-by-uncorecache` (alpha, hidden by default) (1.32 or higher)"
          ]
        },
        {
          "title": "Kubernetes Node Resource Managers",
          "url": "https://kubernetes.io/docs/concepts/policy/node-resource-managers/",
          "excerpts": [
            "The `reservedSystemCPUs` parameter in [KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/) ,\nor the deprecated kubelet command line option `--reserved-cpus` , defines an explicit CPU set for OS system daemons\nand kubernetes system daemons."
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        }
      ],
      "reasoning": "The field value references two concrete mechanisms for constraining and assigning CPU resources to a service: pinning to a specific core set (AllowedCPUs) and enforcing a CPU time limit (CPUQuota, mapping to cpu.max). Excerpts that discuss Kubernetes' CPU management policies and the notion of reserving or dedicating CPUs are directly aligned with these concepts, as they describe explicit knobs to allocate CPU resources to workloads and isolate them from the rest of the system. In particular, the general guidance on controlling CPU management policies on the node provides a framework for how a scheduler or orchestrator would enforce core affinity and quotas. The excerpt detailing reserved system CPUs and explicit CPU sets similarly supports the idea of dedicating CPUs to processes or services. Other excerpts that enumerate multiple policy options (e.g., full-pcpus-only, distribute-cpus-across-numa, align-by-socket, distribute-cpus-across-cores, strict-cpu-reservation, prefer-align-cpus-by-uncorecache) illustrate concrete, named knobs or modes that affect which CPUs are used and how CPU resources are distributed, further supporting the relevance of CPU pinning and quota concepts. The more foundational excerpts about cgroups provide necessary context for how resource control is implemented at the kernel level, reinforcing the feasibility and mechanisms behind the field value, even if they don't name the exact knobs like AllowedCPUs or CPUQuota.",
      "confidence": "medium"
    },
    {
      "field": "dedicated_resource_allocation_methods.2.configuration_summary",
      "citations": [
        {
          "title": "Control CPU Management Policies on the Node - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/",
          "excerpts": [
            "The following policy options exist for the static `CPUManager` policy:",
            " higher)\n* `strict-cpu-reservation` (beta, visible by default) (1.32 or higher)"
          ]
        },
        {
          "title": "Kubernetes Node Resource Managers",
          "url": "https://kubernetes.io/docs/concepts/policy/node-resource-managers/",
          "excerpts": [
            "The `reservedSystemCPUs` parameter in [KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/) ,\nor the deprecated kubelet command line option `--reserved-cpus` , defines an explicit CPU set for OS system daemons\nand kubernetes system daemons.",
            "\n\n`strict-cpu-reservation` (beta, visible by default)"
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "Kubernetes CPU Manager Policies - the StarlingX Documentation",
          "url": "https://docs.starlingx.io/admintasks/kubernetes/kubernetes-cpu-manager-policies.html",
          "excerpts": [
            "When using the static policy, improved performance can be achieved if you also use the Isolated CPU behavior as described at Isolating CPU Cores to Enhance ..."
          ]
        }
      ],
      "reasoning": "The proposed configuration emphasizes using a static CPU policy and topology awareness to dedicate CPU resources for WASM workloads, alongside a RuntimeClass for Wasm and a Guaranteed QoS profile. Excerpts describing CPU manager policy options and their levels (including static-related configurations and related policies) directly support understanding how to enforce a static-like, dedicated CPU allocation strategy on worker nodes. Excerpts that discuss reserved CPUs and strict CPU reservations offer concrete mechanisms for carving out explicit CPU sets for system daemons and workloads, which aligns with the goal of reducing OS interference and ensuring core isolation for WASM workloads. The information about the linux cpu controller via cgroups provides background on how CPU scheduling and isolation are implemented at the kernel level, which is relevant when configuring dedicated resources and aiming to minimize jitter. Excerpts discussing isolated CPU behavior and resource management policies further reinforce the concept of dedicating CPU resources and separating workloads from OS activities. Collectively, these excerpts support understanding how to implement the requested static CPU policy, topology considerations, and reserved resources on Kubernetes worker nodes to enable a WASM runtime with predictable performance characteristics.",
      "confidence": "medium"
    },
    {
      "field": "dedicated_resource_allocation_methods.0.configuration_summary",
      "citations": [
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        }
      ],
      "reasoning": "The proposed steps describe using Linux cgroups to isolate resources for a Wasm runtime. The first excerpt explains that the cpu controller regulates distribution of CPU cycles, which directly supports the idea of binding CPU resources via cpuset.cpus or similar CPU-controlling interfaces. The second excerpt defines control groups (cgroups) as a Linux kernel feature that lets processes be organized into hierarchical groups, which underpins the entire mechanism of moving a process into a specific cgroup via cgroup.procs and applying resource limits through controller interfaces like cpuset.cpus and memory.max. Taken together, these excerpts provide direct evidence that creating a dedicated cgroup directory, placing the Wasm runtime's PID into the cgroup, and configuring controller interfaces for CPU and memory aligns with established Linux resource allocation practices.",
      "confidence": "high"
    },
    {
      "field": "dedicated_resource_allocation_methods.0.cpu_dedication_method",
      "citations": [
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes mechanisms for dedicating CPU resources to specific processes, using cpuset.cpus to pin processes to exclusive CPU cores and cpu.max to set absolute CPU time limits. Excerpt one discusses the CPU controller within cgroups, stating that it regulates the distribution of CPU cycles, which directly aligns with the concept of dedicating CPU resources and controlling how much CPU time a set of processes can receive. Excerpt two explains cgroups as a Linux kernel feature that allows processes to be organized into hierarchical groups, laying the groundwork for applying dedicated resource allocation policies (such as pinning to CPUs or limiting CPU time) within those groups. Together, these excerpts support the idea that cgroups and their CPU-related controls are the underlying mechanism enabling dedicated CPU allocation methods referenced in the finegrained field value, even though the exact parameter names (cpuset.cpus and cpu.max) are not explicitly stated in the excerpts. The connection is that CPU dedication and time-lunding controls are enabled by the CPU controller within cgroups and by structuring processes into groups that can be constrained as needed for high-performance, low-jitter deployments.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.0.ideal_use_cases",
      "citations": [
        {
          "title": "wasmCloud (Overview)",
          "url": "https://wasmcloud.com/",
          "excerpts": [
            "wasmCloud is an open source project from the Cloud Native Computing Foundation (CNCF) that enables teams to build polyglot applications composed of reusable Wasm components and run them—resiliently and efficiently—across any cloud, Kubernetes, datacenter, or edge."
          ]
        },
        {
          "title": "wasmCloud Concepts and Components",
          "url": "https://wasmcloud.com/docs/concepts/components/",
          "excerpts": [
            "Components are portable, interoperable, and composable:\n\n* **Portable**: Because WebAssembly binaries execute against a virtual instruction set architecture (essentially a tiny VM), they are agnostic to architecture and operating system kernel; they run anywhere there is a WebAssembly runtime. Component binaries are typically much smaller than analogous container images, as well—often measured in kilobytes—enabling them to run in resource-constrained environments where even containers aren't practical.",
            ")\n\nThe surrounding environment of the [wasmCloud host](/docs/concepts/hosts/) may have varying levels of concurrency support. This support may differ depending on whether the host is running in a browser, on a constrained device, or in a VM somewhere. However, the code for components should be independent of these conditions and *never have to change*, even if the surrounding environment adopts a different concurrency model."
          ]
        },
        {
          "title": "Reflections on Three Years of wasmCloud",
          "url": "https://wasmcloud.com/blog/wasmcloud-third-anniversary/",
          "excerpts": [
            "Jun 25, 2022 — We spent 90% of our time working on things that weren't core features like async and concurrency and thread-safe queue and dispatch management, ..."
          ]
        }
      ],
      "reasoning": "The most relevant passages explicitly describe wasmCloud's core strengths that align with the stated ideal use cases. A passage describing wasmCloud as an open source project that enables building polyglot applications and running across cloud, Kubernetes, data centers, or edge directly supports the idea of cross-environment deployment and resilience. Passages noting portability and \"run anywhere\" in a WebAssembly runtime reinforce the ability to operate across diverse environments, which is central to the ideal use-case description. Discussions about the platform's concurrency considerations and host environments (including how concurrency support may vary by host and the need for code to remain unchanged) further support the notion of flexible, resilient operation across different execution contexts. Additional excerpts highlighting lattice-based scaling, NATS-based coordination, and the focus on scheduling/concurrency underscore practical mechanisms by which wasmCloud achieves resilience, scalability, and secure, distributed operation—key aspects of the described ideal use cases. Taken together, these sources corroborate wasmCloud's suitability for microservices, event-driven architectures, and multi-tenant deployments that must span cloud to edge with strong security and operational flexibility.",
      "confidence": "medium"
    },
    {
      "field": "wasm_platform_case_studies.0.concurrency_and_scheduling_model",
      "citations": [
        {
          "title": "WasmCloud Docs: NATS-based Lattice, Scheduling, and Concurrency",
          "url": "https://wasmcloud.com/docs/ecosystem/nats/",
          "excerpts": [
            "With this basic model, queue groups and queue subscription facilitate horizontal scaling, load balancing, and failover for components and providers.",
            "on.\nIn wasmCloud, components and providers use this **queue subscription** model to subscribe to invocations. When a component or provider exports a function, the system creates a queue subscription on a NATS subject that other entities can call.",
            " is built on [**NATS**](https://nats.io/), an open source connective technology hosted by the Cloud Native Computing Foundation (CNCF). NATS enables secure application-layer networking across diverse environments including edge, different vendors' clouds, and on-premise datacenters",
            "NATS uses [WIT-over-RPC (wRPC)](https://github.com/bytecodealliance/wrpc) as a transport protocol and helps to deliver a lattice with out-of-the-box load balancing, failover, and other features for systems that span environments."
          ]
        },
        {
          "title": "wasmCloud Concepts and Components",
          "url": "https://wasmcloud.com/docs/concepts/components/",
          "excerpts": [
            ")\n\nThe surrounding environment of the [wasmCloud host](/docs/concepts/hosts/) may have varying levels of concurrency support. This support may differ depending on whether the host is running in a browser, on a constrained device, or in a VM somewhere. However, the code for components should be independent of these conditions and *never have to change*, even if the surrounding environment adopts a different concurrency model.",
            "Components are portable, interoperable, and composable:\n\n* **Portable**: Because WebAssembly binaries execute against a virtual instruction set architecture (essentially a tiny VM), they are agnostic to architecture and operating system kernel; they run anywhere there is a WebAssembly runtime. Component binaries are typically much smaller than analogous container images, as well—often measured in kilobytes—enabling them to run in resource-constrained environments where even containers aren't practical."
          ]
        },
        {
          "title": "Reflections on Three Years of wasmCloud",
          "url": "https://wasmcloud.com/blog/wasmcloud-third-anniversary/",
          "excerpts": [
            "Jun 25, 2022 — We spent 90% of our time working on things that weren't core features like async and concurrency and thread-safe queue and dispatch management, ...",
            "There was a background thread per actor, one per capability provider, and another used for dispatch between the two. Ultimately, this became ..."
          ]
        },
        {
          "title": "Platform Overview and Lattice (wasmCloud)",
          "url": "https://wasmcloud.com/docs/concepts/lattice/",
          "excerpts": [
            "wasmCloud's wRPC protocol is (just as it sounds) an RPC protocol for dynamically composing distributed WebAssembly components at runtime over the lattice.",
            "red lattice enables resilient distributed messaging with location-independent addressing."
          ]
        }
      ],
      "reasoning": "The finegrained field value asserts that concurrency is managed by the wasmCloud host and is abstracted away from component developers, with high concurrency and load balancing achieved via a queue-subscription model (NATS) and a mechanism (Spreadscaler) to declaratively control the number of active component instances. The most directly supportive information comes from passages describing: a queue-based invocation model where components subscribe to a queue and any available instance can handle an invocation, which explains how concurrency is achieved without burdening the component code itself; and statements about horizontal scaling, load balancing through queue groups, and failover, which corroborate the offloaded concurrency management and scalable execution. Additional excerpts explain that NATS provides the transport and lattice context (including wRPC) for distributed, resilient messaging, reinforcing the architectural basis for concurrency control that sits at the host/platform layer rather than inside each component. Context about the host's concurrency model and its environment-agnostic nature further supports the idea that concurrency management is a platform responsibility rather than a component concern. The Spreadscaler reference directly aligns with declarative scaling of running component instances to meet demand, as described in the field value. Collectively, these excerpts map to the described model: the host abstracts concurrency away from components, uses a queue-subscription mechanism to enable concurrent processing across instances, and provides declarative scaling to adapt to demand.",
      "confidence": "high"
    },
    {
      "field": "rtos_like_behavior_analysis",
      "citations": [
        {
          "title": "realtime:documentation:start [Wiki]",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/start",
          "excerpts": [
            "Oct 2, 2023 — Each PREEMPT_RT stable release is tagged with the version number of the upstream Linux stable kernel as well as its own “-rt” counter that is ..."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ...",
            "Oct 3, 2023 — The main aim of the real-time preemption is to minimize the amount of kernel code that is non-preemptible."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "Control Group v2 - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/admin-guide/cgroup-v2.html",
          "excerpts": [
            "This is the authoritative documentation on the design, interface and conventions of cgroup v2. It describes all userland-visible aspects of cgroup including ..."
          ]
        },
        {
          "title": "Real Time under Windows - Kithara",
          "url": "https://kithara.com/en/solutions/real-time-for-windows",
          "excerpts": [
            "The only requirement for real time on Windows PCs is a processor with at least two cores. The basic operating principle is to instruct Windows to only use a ..."
          ]
        },
        {
          "title": "Tracking issue for supporting multithreaded WebAssembly",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/888",
          "excerpts": [
            "Feb 1, 2020 — We already support running entirely independent wasm instances on multiple threads. As for wasm instances sharing resources, that is a prerequisite of being ...",
            "Missing: cpu affinity",
            "Cranelift needs atomics support - bytecodealliance/cranelift#582. Ensure wasmtime internal data structures are threadsafe.",
            "Ensure `wasmtime` internal data structures are threadsafe",
            "Design a WASI (or other) API to spawn a thread"
          ]
        },
        {
          "title": "Why does the POSIX API clock_gettime() have different ...",
          "url": "https://discuss.px4.io/t/why-does-the-posix-api-clock-gettime-have-different-precision-on-sitl-and-drone/32917",
          "excerpts": [
            "I want to calculate the relative time difference in nanoseconds (e.g. the execution time of a branch) via the POSIX API clock_gettime()."
          ]
        },
        {
          "title": "RTX64 RTOS platform - IntervalZero",
          "url": "https://www.intervalzero.com/en-products/en-rtx64/",
          "excerpts": [
            "Determinism\n-----------",
            "* + Guaranteed Precision – set timer periods down to 1 microsecond, and Interrupt Service Thread (IST) latencies of less than 10 microseconds",
            "+ Separation from Windows – Windows processes cannot interfere with real-time applications",
            "+ Scalability – one scheduler is used across all real-time processors. Symmetric multiprocessing (SMP) aware scheduler utilizes both priority-driven and pre-emptive algorithms to ensure critical thread context switches; and yields to threads of high priority occur in the sub-microsecond range",
            "[!"
          ]
        },
        {
          "title": "Windows IoT Soft Real-Time Guidance",
          "url": "https://learn.microsoft.com/en-us/windows/iot/iot-enterprise/soft-real-time/soft-real-time-device",
          "excerpts": [
            "The only way to use this feature is with an application and device custom-built for a specific purpose. The mapping of processor core assignments in the application threads must match the physical device cores and their configuration for real-time versus standard workloads.",
            "The `WindowsIoT CSP` is used to configure Windows IoT devices. Currently the only functionality available in this CSP is to configure a device for Soft Real-Time performanc",
            "This CSP configures the system for real-time performance. You need to provide the number of CPU cores to allocate to real-time tasks, with the rest being allocated for running system or standard user tasks. A numerical value must be provided in the SetRTCores node.",
            "\nWindowsIoT\n   SoftRealTimeProperties\n      ",
            "A value greater than 0 and less than the total number of cores on the device must be provided to the SetRTCores parameter.",
            "This will reserve cores starting with the highest core and going downwards. So reserving 3 cores on a 4 core CPU will reserve cores 3, 2, and 1 and leave core 0 for system and non-real-time tasks."
          ]
        },
        {
          "title": "Soft Real-Time on Windows IoT Enterprise",
          "url": "https://learn.microsoft.com/en-us/windows/iot/iot-enterprise/soft-real-time/soft-real-time",
          "excerpts": [
            "Windows 10 soft real-time is a new feature with [Windows 10 IoT Enterprise, version 21H2](/en-us/windows/iot/iot-enterprise/whats-new/windows-10-iot-enterprise-21h2) that allows device makers to introduce soft real-time capabilities on their devices.",
            "Soft Real-Time on Windows IoT Enterprise",
            "*CPU isolation**: migrates the system-level disturbances off of the isolated CPUs, reducing potential jitter to the user's real-time application",
            "*Custom ISR/DPC pinning on isolated CPUs**: All hardware interrupts are routed to the system and non-real-time cores but by writing a Custom ISR/DPC driver you can route your device specific interrupts to the real-time cores.",
            "*Priority inheritance for mutexes**: This setting ensures the highest priority thread is executed, even in complex multi-threaded scenarios.",
            "*Up to 16 RT thread priority levels**: This allows the programmer to divvy up resources among real-time tasks to ensure the most important ones get executed first.",
            "What is a Real-Time Operating System?",
            "When running a program, a normal operating system gives deterministic results but allows for a nondeterministic time to complete a task.",
            "In a real-time operating system both the results of program execution and the time taken to get those results are (at least partially) deterministic.",
            "Soft real-time devices are more often used within a broader control loop to trigger behaviors from a state machine.",
            "soft real-time might not be your best route to achieving it."
          ]
        },
        {
          "title": "Soft Real-Time Application on Windows IoT Enterprise",
          "url": "https://learn.microsoft.com/en-us/windows/iot/iot-enterprise/soft-real-time/soft-real-time-application",
          "excerpts": [
            "Once a device is configured for real-time performance, an application can be set to run in real-time using standard Win32 APIs.",
            "The only factors that will give a thread or process real-time performance are the thread/process priority rank and the CPU core affinity.",
            "To get real-time performance on a particular thread or process, its priority should be in the range of real-time performance and its affinity should be set to run on the real-time cores.",
            "Configure a Thread for Real-Time",
            "2."
          ]
        },
        {
          "title": "Can Kubernetes Solve WebAssembly's Component ...",
          "url": "https://thenewstack.io/can-kubernetes-solve-webassemblys-component-challenges/",
          "excerpts": [
            "Nov 2, 2023 — Containers and Kubernetes environments are largely ready for Wasm module deployments, and Wasm modules are largely ready for deployment on ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "    [KNL] The optional argument is a cpu list,\n                        as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y,\n                        enable the no-callback CPU mode, which prevents\n                        such CPUs' callbacks from being invoked in\n                        softirq context. Invocation of such CPUs' RCU\n                        callbacks will instead be offloaded to \"rcuox/N\"\n                        kthreads created for that purpose, where",
            "isolcpus=       [KNL,SMP,ISOL] Isolate a given set of CPUs from disturbance. [Deprecated - use cpusets instead]",
            " [flag-list,]<cpu-list>\n\n            ",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified ",
            " cpusets instead to disable scheduler load\n                          balancing through the \"cpuset.sched_load_balance\" file. It offers a much more flexible interface where CPUs can\n                ",
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "cgroups(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/cgroups.7.html",
          "excerpts": [
            "Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups."
          ]
        },
        {
          "title": "4 Things to Consider When Moving Embedded Systems to ...",
          "url": "https://withbighair.com/webassembly/2025/05/11/Runtime-choices.html",
          "excerpts": [
            "May 11, 2025 — In this article I am going to focus on choosing the best WebAssembly runtime for your needs. The Criteria The runtime you pick will depend on your requirements."
          ]
        },
        {
          "title": "WebAssembly: The cloud-native competitor for Kubernetes ...",
          "url": "https://medium.com/@simardeep.oberoi/webassembly-the-cloud-native-competitor-for-kubernetes-and-docker-9b63d3035c94",
          "excerpts": [
            "This article aims to demystify WebAssembly, exploring its origins, how it compares with established technologies like Docker and Kubernetes,"
          ]
        },
        {
          "title": "OSADL Real-time Linux – Q&A Farm (Linux real-time.0.html)",
          "url": "https://www.osadl.org/OSADL-QA-Farm-Real-time.linux-real-time.0.html",
          "excerpts": [
            "2024\\-10-02 12:0",
            "Linux is now an RTOS!",
            "PREEMPT\\_RT is mainline - What's next?"
          ]
        },
        {
          "title": "Announcing wasi-threads - Bytecode Alliance",
          "url": "https://bytecodealliance.org/articles/wasi-threads",
          "excerpts": [
            "This post describes the work of several collaborators to bring about [wasi-threads](https://github.com/WebAssembly/wasi-threads) , a proposal to add threads to WASI.",
            "The core WebAssembly specification has a [threads\nproposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md) to add\nsupport for thread primitives that is at [phase\n3](https://github.com/WebAssembly/proposals) . That proposal\nadds support for three major features:",
            " wasi-threads, as a new WASI proposal, is\nstill experimental — do not expect strong stability in the ABI, especially as WASI transitions\nto the component model.",
            " the “instance per thread” model — each new thread is instantiated separately with only\nshared memory to connect the parallel execution. The component model uses instances to isolate\nsubcomponents.",
            "We are watching and coordinating wasi-threads with the component model transition.",
            "* atomic operations, e.g., `i32.atomic.rmw.cmpxchg`",
            "The lack of a specification did not stop people from investigating, though.",
            "wasi-threads) , a proposal to add threads to WASI. It\nwill explain the history to this proposal, the work done to get to this point, and how one can\nexperiment with threaded applications in engines like Wasmtime and WAM",
            "In the meantime, we need to understand how wasi-threads will be used. Please experiment with\nwasi-threads and provide feedback!",
            "Note that Wasmtime is not the only standalone WebAssembly engine\nimplementing wasi-threads; you may also be interested in trying out the examples above on [WAMR](https://github.com/bytecodealliance/wasm-micro-runtime) .",
            "Feb 21, 2023 — This post describes the work of several collaborators to bring about wasi-threads, a proposal to add threads to WASI.",
            "atomic operations, e.g., `i32.atomic.rmw.cmpxchg`",
            "`wait` and `notify`",
            "The current\nspecification is experimental and minimal; it is enough to run parallel programs (as we will show\nbelow) but, due to its early nature, could change as users point out gaps.",
            "## Motivation",
            "It is hard to deny the performance implications of parallelism. Libraries like `pthreads` for C and `rayon` for Rust are used in countless projects.",
            "This blog post demonstrates how standalone WebAssembly engines can now use wasi-threads to spawn\nthreads — with the associated performance improvements that brings.",
            "Feb 21, 2023 — The core WebAssembly specification has a threads proposal to add support for thread primitives that is at phase 3. That proposal adds support ...",
            "Feb 21, 2023 — Now we can spawn threads, but it turns out that some of the Rust safeguards added to Wasmtime impact threaded performance negatively. After ...",
            "This post describes the work of several collaborators to bring about wasi-threads, a proposal to add threads to WASI.",
            "as possible, is designed to\nmatch the “instance per thread” model — each new thread is instantiated separately with only\nshared memory to connect the parallel execution.",
            "The component model uses instances to isolate\nsubcomponents. If a subcomponent were to spawn a thread, it could no longer instantiate a single\nmodule into a thread (should it instantiate a sub-tree of modules?!).",
            "The `Makefile` s essentially compile\nthe WebAssembly objects using the same flags as shown above, but with the added project-specific\ncomplexities of `pigz` and `libz` . You can build and run the benchmark yourself with",
            "Benchmarking\n------------",
            "Now we can spawn threads, but it turns out that some of the Rust safeguards added to Wasmtime impact\nthreaded performance negatively. After investigating and removing sub-optimal copies, performance\nwas back to normal ([](https://github.com/bytecodealliance/wasmtime/pull/5566)). The workload I\nused for this performed parallel compression — the idea is that the work of compressing chunks\nof a file can be efficiently split into separate threads. (Why use WebAssembly for this? You tell\nme; I would typically use natively-compiled binaries myself, but for demo purposes `gzip` is an\neasily-parallelizable, recognizable workload). !",
            "Until now, one piece missing from WebAssembly standalone engines was the ability to spawn threads.",
            "I found a parallel implementation of `gzip` , called `pigz` , and added `Makefile` s to compile it to\nWebAssembly",
            "Using eight threads to compress 100MB of random bytes, I see linear speedups when compared to a\nsingle thread! With Wasm, ~8x improvement for eight threads is actually higher than the ~6x\nimprovement I see using the natively-compiled `pigz` binary (i.e., eight threads versus a single\nthread but using `pigz` ’s default `Makefile` )",
            "The core WebAssembly specification has a [threads\nproposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md) to add\nsupport for thread primitives that is at [phase\n3](https://github.com/WebAssembly/proposals) . That proposal\nadds support for three major features",
            "* shared linear memory, via a `shared` annotation on `memory` declarations",
            "* `wait` and `notify`",
            "fy`\n\nSignificantly, the WebAssembly threads proposal did not specify a mechanism for creating new\nthreads. A future vision for how this might work is described in [Weakening\nWebAssembly](https://dl.acm.org/doi/10.1145/3360559) , but, for practical reasons, the proposal left\nthis “spawn” detail up to implementors.",
            ". After all, WebAssembly engines [in\nbrowsers](https://webassembly.org/roadmap/) could already spawn new threads using Web Workers (see\nEmscripten’s [`pthreads` support](https://emscripten.org/docs/porting/pthreads.html) ). But not\nspecifying a spawn mechanism meant that standalone WebAssembly engines (i.e., not in a browser)\ncould avoid the complexity of supporting multi-threaded execution.",
            "The key assumption was that not\nall users of WebAssembly wanted or needed a threaded execution environment. The lack of a specification did not stop people from investigating, though."
          ]
        },
        {
          "title": "wasmtime-wasi-threads - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/wasmtime-wasi-threads",
          "excerpts": [
            "Implement the `wasi-threads` [specification](https://github.com/WebAssembly/wasi-threads) in Wasmtime.",
            "Note: this crate is experimental and not yet suitable for use in multi-tenant\n> embedding",
            "As specified, a trap or WASI exit in one thread must end execution\n> for all thread",
            "Due to the complexity of stopping threads, however, this\n> implementation currently exits the process entirel",
            "This will work for some\n> use cases (e.g., CLI usage) but not for embedder",
            "7 days ago — Wasmtime implementation of the wasi-threads API. #neural-network · #wasm · #webassembly · Readme · 83 Versions · Dependencies · Dependents.",
            "Wasmtime implementation of the wasi-threads API. #neural-network · #wasm · #webassembly · Readme · 83 Versions · Dependencies · Dependents.",
            ". wasmtime-wasi-threads - crates.io: Rust Package Registry",
            "\n\n# wasmtime-wasi-threads v36.0.1\n\nWasmtime implementation of the wasi-threads API",
            "Implement the `wasi-threads` [specification](https://github.com/WebAssembly/wasi-threads) in Wasmtime. > Note: this crate is experimental and not yet suitable for use in multi-tenant\n> embeddings. As specified, a trap or WASI exit in one thread must end execution\n> for all threads. Due to the complexity of stopping threads, however, this\n> implementation currently exits the process entirely. This will work for some\n> use cases (e.g., CLI usage) but not for embedders. This warning can be removed\n> once a suitable mechanism is implemented that avoids exiting th",
            "As specified, a trap or WASI exit in one thread must end execution\n> for all threads.",
            "This warning can be removed\n> once a suitable mechanism is implemented that avoids exiting the process.",
            "wasmtime-wasi-threads v36.0.1"
          ]
        },
        {
          "title": "wasm32-wasip1 - The rustc book",
          "url": "https://doc.rust-lang.org/rustc/platform-support/wasm32-wasip1.html",
          "excerpts": [
            "The wasm32-wasip1 target is a WebAssembly compilation target which assumes that the [WASIp1] (aka \"WASI preview1\") set of \"syscalls\" are available for use in ...",
            "The `wasm32-wasip1` target is a WebAssembly target for the first WASI standard, generating modules that import OS functions. It was renamed from `wasm32-wasi`."
          ]
        },
        {
          "title": "Instructions — WebAssembly 2.0 + Threads (Draft 2023-10-10)",
          "url": "https://webassembly.github.io/threads/core/syntax/instructions.html",
          "excerpts": [
            "Atomic Memory Instructions¶. Instructions in this group are concerned with accessing linear memory atomically."
          ]
        },
        {
          "title": "The State of WebAssembly – 2024 and 2025",
          "url": "https://platform.uno/blog/state-of-webassembly-2024-2025/",
          "excerpts": [
            "WASI 0.3 (formerly called Preview 3) is expected in the first half of 2025. The goal with this release is to include native async with the Component Model and then to adjust the existing WASI 0.2 interfaces to take advantage of the new async capabilities.",
            "WASI 0.2 (formerly called ‘Preview 2’) was released bringing the Component Model as well as two worlds: wasi-cli and wasi-htt",
            "WASI 0.3 (formerly called Preview 3) is expected in the first half of 2025.",
            "Outside the browser, we’re seeing exceptional growth for WebAssembly thanks to the work happening with WASI, the runtimes, and in the community.",
            "Jan 27, 2025 — wasi-sockets brings the ability to work with TCP/UDP sockets; wasi-filesystem contains a set of interfaces for opening, reading, and writing ...",
            "The WASI work is being released in phases and advanced to Preview 2 early in 2024. Preview 2 included the following features: Component Model.",
            "Jan 27, 2025 — In this article, I'll start off with a review of what happened around WebAssembly in 2024 and then look at what I think might happen in 2025.",
            "Jan 27, 2025 — The WASI work is being released in phases and advanced to Preview 2 early in 2024. Preview 2 included the following features: Component Model.",
            "Jan 27, 2025 — The linear memory of a WebAssembly module was originally limited to a maximum of 4 GB because of the use of 32-bit indexes. · There are some ...",
            "Memory64",
            "The linear memory of a WebAssembly module was originally limited to a maximum of 4 GB because of the use of 32-bit indexes. Because some applications need more than 4 GB of memory, 64-bit indexes have been added.",
            "Even though a 64-bit pointer can theoretically access up to 16 exabytes of memory, in the browser, the WebAssembly JavaScript API limits memory to a maximum of 16 GB.",
            "This feature is available in Chrome and Firefox but not yet in Safari.",
            "The Component Model gives you a way to describe your module that allows for several WebAssembly modules, potentially written in different ..."
          ]
        },
        {
          "title": "WebAssembly Features and Memory Architecture",
          "url": "https://webassembly.org/features/",
          "excerpts": [
            "Memory64"
          ]
        },
        {
          "title": "WebAssembly Proposals Repository",
          "url": "https://github.com/WebAssembly/proposals",
          "excerpts": [
            "### Phase 5 - The Feature is Standardized (WG)",
            "Phase 0 - Pre-Proposal (CG)",
            "Proposals follow [this process document](https://github.com/WebAssembly/meetings/blob/main/process/phases.md) .",
            "| [Threads](https://github.com/webassembly/threads) | Conrad Watt |",
            "| [Shared-Everything Threads](https://github.com/WebAssembly/shared-everything-threads) | Andrew Brown, Conrad Watt, and Thomas Lively |",
            "Implementation status of most proposals in various wasm engines is available on <https://webassembly.org/features/>",
            "### Phase 4 - Standardize the Feature (WG)",
            "### Phase 4 - Standardize the Feature (WG)",
            "### Phase 0 - Pre-Proposal (CG)",
            "Phase 4 - Standardize the Feature (WG)\n\n[]()\n\n| Proposal | Champion |\n| --- | --- |\n| [Threads](https://github.com/webassembly/threads) | Conrad Watt |"
          ]
        },
        {
          "title": "Rust/WASM Threads Discussion (GitHub issue 77839)",
          "url": "https://github.com/rust-lang/rust/issues/77839",
          "excerpts": [
            "he WebAssembly threads proposal is at [stage 2 of the phases process",
            "s of the time of this writing the WebAssembly threads proposal is at [stage 2 of the phases process",
            "The threads proposal does _not_ add an ability to spawn threads nor does it really define what threads are, but it's largely set up to have a wasm-instance-per-thread (not that this is super relevant for the standard library",
            "Namely it has custom implementations of:",
            "Condvar",
            "mutex",
            "RwLock",
            "In terms of toolchain, we're, as usual, inheriting a lot of the experience from LLVM as well.",
            "We instruct LLD to create a \"shared\" memory (which allows the memory to be shared across multiple wasm instances, how threading works on the web and in other engines), specifies a default maximum size for memory (this is required for shared memory, and normal wasm memories don't need to list a maximum), flags memory as being imported (since otherwise each instance would export a new memory and not share it! ), and ensures that a few TLS/initialization-related symbols are exported",
            "* `__wasm_init_memory` \\- this is called once which initializes all data segments of memory (",
            "This is an issue intended to track the state of WebAssembly atomics support in Rust.",
            "For the WebAssembly target there is the [threads proposal in WebAssembly](https://github.com/webassembly/threads) which adds a number of instructions and a new kind of memory to the WebAssembly specification.",
            "The new instructions largely deal with atomic memory operations (e.g. `i32.atomic.add` ), but also deal with synchronization between threads ( `memory.atomic.notify` ).",
            "The threads proposal does _not_ add an ability to spawn threads nor does it really define what threads are, but it's largely set up to have a wasm-instance-per-thread (not that this is super relevant for the standard library).",
            "As of the time of this writing the WebAssembly threads proposal is at [stage 2 of the phases process](https://github.com/webassembly/proposals) .",
            "It is [shipping in Chrome and in Firefox](https://webassembly.org/roadmap/) , however.",
            "Rust's support for this proposal boils down to a few things:",
            "* Primarily Rust/LLVM support the `-Ctarget-feature=+atomics` CLI flag to rustc. This causes codegen for atomic types like `std::sync::atomic` [to use the atomic instructions](https://godbolt.org/z/GGz3fq) .",
            " * Rust has support for the three synchronization intrinsics:\n      + [`memory.atomic.notify`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_notify.html)\n      + [`memory.atomic.wait32`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait32.html)\n      + [`memory.atomic.wait64`](https://doc.rust-lang.org/stable/core/arch/wasm32/fn.memory_atomic_wait64.html)",
            "\n* The Rust standard library [implements mutexes differently](https://github.com/rust-lang/rust/blob/c71248b70870960af9993de4f31d3cba9bbce7e8/library/std/src/sys/wasm/mod.rs) based on whether the `atomics` feature is enabled for the library at compile time.",
            "Namely it has custom implementations of:\n      + [`Condvar`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/condvar_atomics.rs)\n      + [`Mutex`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/mutex_atomics.rs)\n      + [`RwLock`](https://github.com/rust-lang/rust/blob/master/library/std/src/sys/wasm/rwlock_atomics.rs)",
            "In terms of toolchain, we're, as usual, inheriting a lot of the experience from LLVM as well. As usual the WebAssembly target uses LLD as the linker, but [a number of options are passed by default](https://github.com/rust-lang/rust/blob/c71248b70870960af9993de4f31d3cba9bbce7e8/compiler/rustc_codegen_ssa/src/back/linker.rs) when we're generating an executable compiled with threads (currently detected with `-Ctarget-feature=+atomics` ).",
            "We instruct LLD to create a \"shared\" memory (which allows the memory to be shared across multiple wasm instances, how threading works on the web and in other engines), specifies a default maximum size for memory (this is required for shared memory, and normal wasm memories don't need to list a maximum), flags memory as being imported (since otherwise each instance would export a new memory and not share it! ), and ensures that a few TLS/initialization-related symbols are exported.",
            "The symbols are perhaps the most interesting part here, so to go into them in some more detail:\n\n* `__wasm_init_memory` \\- this is called once which initializes all data segments of memory (e.g"
          ]
        },
        {
          "title": "Wasmer Runtime",
          "url": "https://docs.wasmer.io/runtime/",
          "excerpts": [
            "The Wasmer Runtime is the engine that allows running WebAssembly modules and Wasmer packages anywhere."
          ]
        },
        {
          "title": "SharedMemory in wasmtime - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.SharedMemory.html",
          "excerpts": [
            "This method allows embedders to notify threads blocked on the specified `addr` , an index into wasm linear memory. Threads could include\nwasm threads blocked on a `memory.atomic.wait*` instruction or embedder\nthreads blocked on [`SharedMemory::atomic_wait32`](struct.SharedMemory.html.atomic_wait32 \"method wasmtime::SharedMemory::atomic\\_wait32\") , for example. The `count` argument is the number of threads to wake up. This function returns the number of threads awoke",
            "/threads/Overview.md) adds the concept of “shared memory” to WebAssembly. This is much the same as a Wasm linear memory (i.e., [`Memory`](struct.Memory.html \"struct wasmtime::Memory\") ), but can be\nused concurrently by multiple agents. Because these agents may execute in\ndifferent threads, [`SharedMemory`](struct.SharedMemory.html \"struct wasmtime::SharedMemory\") must be thread-safe. When the threads proposal is enabled, there are multiple ways to construct\nshared memory:",
            "threads/Overview.md) adds the concept of “shared memory” to WebAssembly. This is much the same as a Wasm linear memory (i.e., [`Memory`](struct.Memory.html \"struct wasmtime::Memory\") ), but can be\nused concurrently by multiple agents. Because these agents may execute in\ndifferent threads, [`SharedMemory`](struct.SharedMemory.html \"struct wasmtime::SharedMemory\") must be thread-safe.",
            "let mut config = Config::new();",
            "config.wasm_threads( true );",
            "let engine = Engine::new( & config) ? ;",
            "es\n\n```\nlet mut config = Config::new();\nconfig.wasm_threads( true );\nlet engine = Engine::new( & config) ? ;\nlet mut store = Store::new( & engine, ());\n\nlet shared_memory = SharedMemory::new( & engine, MemoryType::shared( 1 , 2 )) ? ;\nlet module = Module::new( & engine, r#\"(module (memory (import \"\" \"\") 1 2 shared))\"# ) ?",
            "1. for imported shared memory, e.g., `(import \"env\" \"memory\" (memory 1 1 shared))` , the user must supply a [`SharedMemory`](struct.SharedMemory.html \"struct wasmtime::SharedMemory\") with the\n   externally-created memory as an import to the instance–e.g., `shared_memory.into()` . 2. for private or exported shared memory, e.g., `(export \"env\" \"memory\" (memory 1 1 shared))` , Wasmtime will create the memory internally during\n   instantiation–access using `Instance::get_shared_memory()` ."
          ]
        },
        {
          "title": "New WASI thread doesn't start running, when main ...",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/8697",
          "excerpts": [
            "May 28, 2024 — I'm trying to figure out how to start multiple threads and syncronize them with a mutex that uses atomics and wait/notify.",
            "I'm trying to figure out how to start multiple threads and syncronize them with a mutex that uses atomics and wait/notify. I made a minimal example that does not work:  \nMain thread spawns a new thread, then waits for `u32` at offset `0` to change from `0` to something else. New thread executes `unreachable`, to see if it runs at all. Currently, the `wasmtime` process runs infinitely, and doesn't reach `unreachable`. Its memory consumption is always increasing.",
            "opened [on May 28, 2024](https://github.com/bytecodealliance/wasmtime/issues/8697)",
            "I'm trying to figure out how to start multiple threads and syncronize them with a mutex that uses atomics and wait/notify."
          ]
        },
        {
          "title": "WASIX, the Superset of WASI Supporting Threads, ...",
          "url": "https://news.ycombinator.com/item?id=36126032",
          "excerpts": [
            "The actual latest definition is in \"legacy/preview1\". Yes that is the latest WASI definition. https://github.com/WebAssembly/WASI/blob/main/ ...",
            "May 30, 2023 — WASI, which lets WASM assembly code interact with the base system, sidestepping the container of the browser; but it needs a runtime on the host system (not a ..."
          ]
        },
        {
          "title": "Atomics.wait() - MDN",
          "url": "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Atomics/wait",
          "excerpts": [
            "Jul 10, 2025 — The Atomics.wait() static method verifies that a shared memory location contains a given value and if so sleeps, awaiting a wake-up notification or a time out."
          ]
        },
        {
          "title": "See What WebAssembly Can Do in 2025",
          "url": "https://thenewstack.io/see-what-webassembly-can-do-in-2025/",
          "excerpts": [
            "By 2025, WebAssembly modules will be able to integrate applications written in the language of your choice deployed across any environment or device."
          ]
        },
        {
          "title": "What's the current status of WASM/WASI support?",
          "url": "https://forum.dlang.org/thread/fgxyeisrgrfhnduhcwbi@forum.dlang.org",
          "excerpts": [
            "Mar 14, 2023 — What's the current status of WASM/WASI support? ... WASI preview1 has the POSIX functions needed to implement a \"libc-like\" layer on top."
          ]
        },
        {
          "title": "Wasmer Docs",
          "url": "https://docs.wasmer.io/",
          "excerpts": [
            "Wasmer is available in multiple programming languages and operating systems in the shape of SDKs. Using these SDKs, it's possible to embed wasmer modules ...",
            "Welcome to the Wasmer Documentation! Wasmer (opens in a new tab) is an ecosystem based on WebAssembly that allows running applications anywhere: in the ..."
          ]
        },
        {
          "title": "Implementing atomic wait and notify - computerBear",
          "url": "https://blog.bearcats.nl/atomic-wait/",
          "excerpts": [
            "Oct 29, 2023 — These are low-level, immediate-mode primitives that can turn any memory location into a synchronization object."
          ]
        },
        {
          "title": "The State of WebAssembly – 2023 and 2024 - Uno Platform",
          "url": "https://platform.uno/blog/state-of-webassembly-2023-2024/",
          "excerpts": [
            "Explore the 2023 Milestones and 2024 Predictions for WebAssembly in Our annual wasm Article - Get insights into the latest developments and ...",
            "Jan 16, 2024 — Several features sit behind Chrome and Firefox flags, like Memory64 and Type Reflection. These two proposals are still in phase 3, so they ...",
            "2023 was another fantastic year for WebAssembly, with several standardized proposals, like Tail Calls, Garbage Collection, Multiple Memories, and Relaxed SIMD."
          ]
        },
        {
          "title": "WebAssembly shared-everything threads - Chrome Platform Status",
          "url": "https://cr-status.appspot.com/feature/5163209685467136",
          "excerpts": [
            "Only WebAssembly memories can currently be shared between threads, so languages that compile to WasmGC cannot use threads at all. WebAssembly ..."
          ]
        },
        {
          "title": "WASI libc implementation for WebAssembly",
          "url": "https://github.com/WebAssembly/wasi-libc",
          "excerpts": [
            "wasi-libc is a libc for WebAssembly programs built on top of WASI system calls. It provides a wide array of POSIX-compatible C APIs."
          ]
        },
        {
          "title": "WebAssembly Proposals | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/start/wasmedge/extensions/proposals/",
          "excerpts": [
            "WasmEdge supports the following WebAssembly proposals. These proposals are likely to become official WebAssembly specifications in the future.",
            "The following proposals are under development and may be supported in the future: Exception handling · Relaxed SIMD · Memory64 · WebAssembly C and C++ API. WASI ..."
          ]
        },
        {
          "title": "WASIX - Docs",
          "url": "https://wasix.org/docs/",
          "excerpts": [
            "Jun 20, 2023 — This documentation will provide you with an in-depth understanding of WASIX, its design philosophy, its capabilities, and its differences from WASI."
          ]
        },
        {
          "title": "Consider including a pthread.h for all targets · Issue #501",
          "url": "https://github.com/WebAssembly/wasi-libc/issues/501",
          "excerpts": [
            "Lately, I've been helping with the WASI port of .NET, where pthread.h is used in the runtime library for all TARGET_UNIX platforms. That worked ..."
          ]
        },
        {
          "title": "WebAssembly/shared-everything-threads",
          "url": "https://github.com/WebAssembly/shared-everything-threads",
          "excerpts": [
            "A draft proposal for spawning threads in WebAssembly",
            "()\n\nThis repository proposes additions to the WebAssembly specification for spawning and managing\nthreads — everything not covered by the already-approved [threads](https",
            "suite. A formatted version of the spec is available here: [webassembly.github.io/spec",
            "Participation is welcome."
          ]
        },
        {
          "title": "WebAssembly Threads Repository",
          "url": "https://github.com/WebAssembly/threads",
          "excerpts": [
            "Threads and Atomics in WebAssembly",
            "This repository is a clone of github.com/WebAssembly/spec/. It is meant for\ndiscussion, prototype specification and implementation of a proposal to threads\nsupport to WebAssembly.",
            "See the [overview](/WebAssembly/threads/blob/main/proposals/threads/Overview.md) for a summary of the proposal.",
            "It is meant for discussion, prototype specification and implementation of a proposal to threads support to WebAssembly."
          ]
        },
        {
          "title": "The rustc book",
          "url": "https://doc.rust-lang.org/nightly/rustc/platform-support/wasm32-wasip1-threads.html",
          "excerpts": [
            "The `wasm32-wasip1-threads` target is a new and still (as of July 2023) an\nexperimental target.",
            "This target is an extension to `wasm32-wasip1` target,\noriginally known as `wasm32-wasi` . It extends the original target with a\nstandardized set of syscalls that are intended to empower WebAssembly binaries\nwith native multi threading capabilities.",
            "\nThe runtime should support the same set of APIs as any other supported wasi target for interacting with the host environment through the WASI standard. The runtime also should have implementation of [wasi-threads proposal](https://github.com/WebAssembly/wasi-threads)",
            "This target is not a stable target.",
            "LLVM 16 is required for this target. The reason is related to linker flags: prior to LLVM 16, --import-memory and --export-memory were not allowed together.",
            "\n\n```text\nrustup target add wasm32-wasip1-threads --toolchain nightly\n```\n\nRust programs can be built for that target",
            "It extends the original target with a standardized set of syscalls that are intended to empower WebAssembly binaries with native multi threading capabilities.",
            "The wasm32-wasip1-threads target is a new and still (as of July 2023) an experimental target. This target is an extension to wasm32-wasip1 target, originally ...",
            "This means that there are a few engines which implement the wasi-threads feature and if they do they're likely behind a flag, for example: Wasmtime - --wasi ..."
          ]
        },
        {
          "title": "Wasm Proposals",
          "url": "https://docs.wasmtime.dev/stability-wasm-proposals.html",
          "excerpts": [
            "This document is intended to describe the current status of WebAssembly proposals in Wasmtime. For information about implementing a proposal in Wasmtime see ..."
          ]
        },
        {
          "title": "The Atomic Waltz: Unraveling WebAssembly Issues in V8 ...",
          "url": "https://blog.stackblitz.com/posts/the-atomic-waltz/",
          "excerpts": [
            "May 11, 2023 — The WebAssembly Threads proposal introduces shared linear memory as well as atomic memory access instructions including read-modify-write (RMW) ..."
          ]
        },
        {
          "title": "How to access WebAssembly linear memory from C/C++",
          "url": "https://stackoverflow.com/questions/46748572/how-to-access-webassembly-linear-memory-from-c-c",
          "excerpts": [
            "I need to share memory between the JavaScript API and the compiled WebAssembly code in order to access more complex data types like strings or char arrays."
          ]
        },
        {
          "title": "Ciantic/rust-shared-wasm-experiments",
          "url": "https://github.com/Ciantic/rust-shared-wasm-experiments",
          "excerpts": [
            "Rust WASM experiment with shared memory. The example WASM library show cases Crossbeam Channels and shared HashMap usage between workers. Install wasm-pack ...",
            "The example WASM library show casing shared HashMap, and crossbeam's channel between workers.",
            "It initalizes only _one_ `WebAssembly.Memory` object and shares it between the workers."
          ]
        },
        {
          "title": "wasm32-unknown-unknown - The rustc book",
          "url": "https://doc.rust-lang.org/rustc/platform-support/wasm32-unknown-unknown.html",
          "excerpts": [
            "The wasm32-unknown-unknown target is a WebAssembly compilation target which does not import any functions from the host for the standard library. This is the \" ...",
            "Linking binaries requires LLD to be enabled for the wasm-ld driver. This target uses the dlmalloc crate as the default global allocator. Building the target."
          ]
        },
        {
          "title": "Can tokio::runtime be used in wasm?",
          "url": "https://stackoverflow.com/questions/77402053/can-tokioruntime-be-used-in-wasm",
          "excerpts": [
            "Tokio does have some support WASM, see WASM support in the docs. You need to enable the rt feature (the full feature doesn't work and causes a compilation ..."
          ]
        },
        {
          "title": "Ordering in std::sync::atomic",
          "url": "https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Which std::sync::atomic::Ordering to use? - rust",
          "url": "https://stackoverflow.com/questions/30407121/which-stdsyncatomicordering-to-use",
          "excerpts": [
            "All the methods of std::sync::atomic::AtomicBool take a memory ordering (Relaxed, Release, Acquire, AcqRel, and SeqCst), which I have not used before."
          ]
        },
        {
          "title": "rust - Understanding the Send trait - Stack Overflow",
          "url": "https://stackoverflow.com/questions/59428096/understanding-the-send-trait",
          "excerpts": [
            "According to Rustonomicon: Send and Sync. A type is Send if it is safe to send it to another thread. A type is Sync if it is safe to share ...",
            "Sync means that a type is safe to reference from multiple threads at the same time. Specifically, that &T is Send and can be moved/copied to ...",
            "Send allows an object to be used by two threads A and B at different times. Thread A can create and use an object, then send it to thread B, so thread B can ..."
          ]
        },
        {
          "title": "Send and Sync traits",
          "url": "https://notes.kodekloud.com/docs/Rust-Programming/Fearless-Concurrency/Send-and-Sync-traits",
          "excerpts": [
            "This article explores Rust's Send and Sync traits, which are essential for ensuring thread safety in concurrent programming."
          ]
        },
        {
          "title": "Shared-State Concurrency - The Rust Programming ...",
          "url": "https://doc.rust-lang.org/book/ch16-03-shared-state.html",
          "excerpts": [
            "Shared memory concurrency is like multiple ownership: multiple threads can access the same memory location at the same time."
          ]
        },
        {
          "title": "Channels in async rust on wasm32",
          "url": "https://users.rust-lang.org/t/channels-in-async-rust-on-wasm32/73732",
          "excerpts": [
            "Trying to use a std:;sync::mpsc::channel in wasm32-unknown-unknown on Chrome has resulted in panics. This appears to be similar to the issue ..."
          ]
        },
        {
          "title": "crossbeam::channel - Rust",
          "url": "https://docs.rs/crossbeam/latest/crossbeam/channel/index.html",
          "excerpts": [
            "Multi-producer multi-consumer channels for message passing. This crate is an alternative to std::sync::mpsc with more features and better performance."
          ]
        },
        {
          "title": "channel in wasmer_types::lib::std::sync::mpsc - Rust",
          "url": "https://docs.rs/wasmer-types/latest/wasmer_types/lib/std/sync/mpsc/fn.channel.html",
          "excerpts": [
            "Creates a new asynchronous channel, returning the sender/receiver halves. All data sent on the Sender will become available on the Receiver in the same ..."
          ]
        },
        {
          "title": "Help me choose a Mutex/RwLock ( parking_lot ? ) - Rust Users Forum",
          "url": "https://users.rust-lang.org/t/help-me-choose-a-mutex-rwlock-parking-lot/67971",
          "excerpts": [
            "parking_lot also comes with some caveats (e.g. it doesn't support wasm on the stable channel), which might be a problem in specific cases."
          ]
        },
        {
          "title": "Amanieu/parking_lot - GitHub",
          "url": "https://github.com/Amanieu/parking_lot",
          "excerpts": [
            "Just make sure not to enable -C target-feature=+atomics on stable as that will allow wasm to run with multiple threads which will completely break parking_lot's ..."
          ]
        },
        {
          "title": "parking_lot_core 0.8.1 wasm link issue #269 - GitHub",
          "url": "https://github.com/Amanieu/parking_lot/issues/269",
          "excerpts": [
            "This means one can no longer use parking_lot from within a wasm runtime, such as wasmtime . When compiling for the web, one can address this ...",
            "When compiling for the web, one can address this problem with `parking_lot = { version = \"0.11\", features = [\"wasm-bindgen\"] }`, but this does not work for non-web wasm runtimes.",
            "The parking\\_lot algorithm itself need to know the current time for fair locks but the single-threaded wasm option shouldn't need this as there are no other threads against which it needs to be fair",
            "Multi-threading requires the bulk memory wasm proposal and atomic instructions. Both of which are not supported for the WASM MVP that is targeted by default.",
            "In addition it requires all parts of the program to be compiled with multi-threading support I believe, but libstd isn't by default.",
            "I suspect that the only reason parking\\_lot 0.8.0 worked and 0.8.1 doesn't is because some subtle failure of rustc to elide unused code (using instant)"
          ]
        },
        {
          "title": "Multithreading with WASM on the Browser. Is it possible yet? - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/1cwmyaw/multithreading_with_wasm_on_the_browser_is_it/",
          "excerpts": [
            "Thread spawning, etc, is out of scope for the threads proposal and more in scope for something like WASI or your own platform abstraction layer.",
            "The Wasm threads proposal addresses synchronization by adding atomic instructions, including futex-like wait/wake instructions you can use to implement blocking synchronization as needed for locks and condition variables.",
            "There is one project that aims to make multithreading on wasm as close to the native api as possible: <https://github.com/chemicstry/wasm_thread"
          ]
        },
        {
          "title": "Multithreading in WASM - how did that come out? - Rust Users Forum",
          "url": "https://users.rust-lang.org/t/multithreading-in-wasm-how-did-that-come-out/87518",
          "excerpts": [
            "Back in 2018, true multi-threading in Rust targeted to WASM seemed close. What happened? The thread spec for WASM does get occasional updates, so this isn't ...",
            "#### [New tier-2 target for \\`wasm32-wasi\\` + threads](https://github.com/rust-lang/compiler-team/issues/574)",
            "opened 06:14PM - 21 Dec 22 UTC",
            "For Rust specifically, a large difficulty is that the wasm32-unknown-unknown target has gotten a lot of code presuming it to be single-threaded."
          ]
        },
        {
          "title": "Best channel library for low-traffic scenarios",
          "url": "https://users.rust-lang.org/t/best-channel-library-for-low-traffic-scenarios/123874",
          "excerpts": [
            "Jan 11, 2025 — AFAIK crossbeam channels can only be used within the same address space, not for IPC. jobafr: I looked at it with firefox profiler and found ..."
          ]
        },
        {
          "title": "std::memory_order",
          "url": "https://en.cppreference.com/w/cpp/atomic/memory_order.html",
          "excerpts": [
            "Mar 19, 2025 — std::memory_order specifies how memory accesses, including regular, non-atomic memory accesses, are to be ordered around an atomic operation."
          ]
        },
        {
          "title": "Understanding Send trait in Rust",
          "url": "https://www.reddit.com/r/rust/comments/1csrbhf/understanding_send_trait_in_rust/",
          "excerpts": [
            "The Send trait in Rust is defined as follows: \"A type is Send if it is safe to send it to another thread.\""
          ]
        },
        {
          "title": "Ownership as explained in the Rust book",
          "url": "https://www.reddit.com/r/rust/comments/yibdpi/ownership_as_explained_in_the_rust_book/",
          "excerpts": [
            "The Rust Book does a sub-optimal job in explaining ownership, while being convoluted about other topics in the process."
          ]
        },
        {
          "title": "Is there, as of November 2023, a way to get threads on ` ...",
          "url": "https://users.rust-lang.org/t/is-there-as-of-november-2023-a-way-to-get-threads-on-wasm32-unknown-unknown-or-another-browser-target/102913",
          "excerpts": [
            "There is the wasm32-wasi-preview1-threads target, but I don't know much about it (or if it requires nightly). I recommend researching that or ..."
          ]
        },
        {
          "title": "tokio - Rust",
          "url": "https://docs.rs/tokio_wasi/latest/tokio/",
          "excerpts": [
            "WASM support. Tokio has some limited support for the WASM platform. Without the tokio_unstable flag, the following features are supported: sync; macros; io ..."
          ]
        },
        {
          "title": "State of web assembly and multithreading/Performance? : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/12rn5fq/state_of_web_assembly_and/",
          "excerpts": [
            "Performance will be limited, as WebAssembly is slower than native code and does not currently support multithreading."
          ]
        },
        {
          "title": "Compile error with --target=wasm32-unknown ...",
          "url": "https://github.com/gfx-rs/wgpu-native/issues/298",
          "excerpts": [
            "I think we could probably fix this either way, but the wasm32-unknown-unknown target is usually handled by using wgpu directly instead of wgpu- ..."
          ]
        },
        {
          "title": "Rustonomicon - Send and Sync",
          "url": "https://doc.rust-lang.org/nomicon/send-and-sync.html",
          "excerpts": [
            "Send and Sync are fundamental to Rust's concurrency story. As such, a\nsubstantial amount of special tooling exists to make them work right. First and\nforemost, they're [unsafe traits](safe-unsafe-meaning.html) . This means that they are unsafe to\nimplement, and other unsafe code can assume that they are correctly\nimplemented. Since they're _marker traits_ (they have no associated items like\nmethods), correctly implemented simply means that they have the intrinsic\nproperties an implementor should have. Incorrectly implementing Send or Sync can\ncause Undefined Behavior. Send and Sync are also automatically derived traits. This means that, unlike\nevery other trait, if a type is composed entirely of Send or Sync types, then it\nis Send or Sync.",
            "Major exceptions include:\n\n* raw pointers are neither Send nor Sync (because they have no safety guards). * `UnsafeCell` isn't Sync (and therefore `Cell` and `RefCell` aren't). * `Rc` isn't Send or Sync (because the refcount is shared and unsynchronized).",
            "What about Sync? For `Carton` to be Sync we have to enforce that you can't\nwrite to something stored in a `&Carton` while that same something could be read\nor written to from another `&Carton` .",
            "A type is Sync if it is safe to share between threads (T is Sync if and only if `&T` is Send).",
            "Send and Sync are fundamental to Rust's concurrency story.",
            "Almost all primitives are Send and Sync, and as a consequence\npretty much all types you'll ever interact with are Send and Sync.",
            "Rust\ncaptures this through the `Send` and `Sync` traits. * A type is Send if it is safe to send it to another thread. * A type is Sync if it is safe to share between threads (T is Sync if and only if `&T` is Send)."
          ]
        },
        {
          "title": "The Rust Programming Language",
          "url": "https://doc.rust-lang.org/book/ch16-04-extensible-concurrency-sync-and-send.html",
          "excerpts": [
            "The `Send` marker trait indicates that ownership of values of the type\nimplementing `Send` can be transferred between threads.",
            "The `Sync` marker trait indicates that it is safe for the type implementing `Sync` to be referenced from multiple threads.",
            "The `Sync` marker trait indicates that it is safe for the type implementing `Sync` to be referenced from multiple threads. In other words, any type `T` implements `Sync` if `&T` (an immutable reference to `T` ) implements `Send` ,\nmeaning the reference can be sent safely to another thread.",
            "The smart pointer `Rc<T>` also doesn’t implement `Sync` for the same reasons\nthat it doesn’t implement `Send` .",
            "The smart pointer `Mutex<T>` implements `Sync` and can be used to\nshare access with multiple threads as you saw in [“Sharing a `Mutex<T>` Between\nMultiple Threads”](ch16-03-shared-state.html) .",
            "Manually implementing these traits involves implementing unsafe Rust code.",
            "Manually implementing these traits involves implementing unsafe Rust code. We’ll talk about using unsafe Rust code in Chapter 20; for now, the important\ninformation is that building new concurrent types not made up of `Send` and `Sync` parts requires careful thought to uphold the safety guarantees.",
            "Almost every Rust type\nis `Send` , but there are some exceptions, including `Rc<T>` : this cannot\nimplement `Send` because if you cloned an `Rc<T>` value and tried to transfer\nownership of the clone to another thread, both threads might update the\nreference count at the same time.",
            "Therefore, Rust’s type system and trait bounds ensure that you can never\naccidentally send an `Rc<T>` value across threads unsafely.",
            "When we switched to `Arc<T>` , which does implement `Send` , the\ncode compiled.",
            "Any type composed entirely of `Send` types is automatically marked as `Send` as\nwell.",
            "Almost all primitive types are `Send` , aside from raw pointers, which\nwe’ll discuss in Chapter 20.",
            "Similar to `Send` ,\nprimitive types all implement `Sync` , and types composed entirely of types that\nimplement `Sync` also implement `Sync` .",
            "The `RefCell<T>` type (which we talked about\nin Chapter 15) and the family of related `Cell<T>` types don’t implement `Sync` .",
            "The implementation of borrow checking that `RefCell<T>` does at runtime is not\nthread-safe.",
            "Implementing `Send` and `Sync` Manually Is Unsafe",
            "\nBecause types composed entirely of other types that implement the `Send` and `Sync` traits also automatically implement `Send` and `Sync` , we don’t have to\nimplement those traits manually.",
            "As marker traits, they don’t even have any\nmethods to implement.",
            "They’re just useful for enforcing invariants related to\nconcurrency."
          ]
        },
        {
          "title": "Wasmtime WASI Threads Documentation",
          "url": "https://docs.rs/wasmtime-wasi-threads",
          "excerpts": [
            "ml)\n\nSummary\n\nExpand description\n\nImplement [`wasi-threads`](https://github.com/WebAssembly/wasi-threads). Structs[§]()",
            "ml)\n\nCrate wasmtime\\_wasi\\_threads Copy item path\n============================================\n\n[Source](../src/wasmtime_wasi_threads/lib.rs.html) \n\n[Search](https://docs.rs/wasmtime-wasi-threads/latest/wasmtime_wasi_threads/?search=)\n\n[Settings](../settings.html)\n\n[Help](../help.html)\n\nSummary\n\nExpand description\n\nImplement [`wasi-threads`](https://github.com/WebAssembly/wasi-threads). Structs[§]()\n--------------------\n\n[WasiThreadsCtx](struct.WasiThreadsCtx.html \"struct wasmtime_wasi_threads::WasiThreadsCtx\")"
          ]
        },
        {
          "title": "Another novice query: mpsc or crossbeam? : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/18z6bcz/another_novice_query_mpsc_or_crossbeam/",
          "excerpts": [
            "I found this crate crossbeam::channel which has mpmc channels. That sounds pretty good. The docs claim it has better performance, too (not a concern but neat)."
          ]
        },
        {
          "title": "Support WASI preview 2 · Issue #6323 · tokio-rs/tokio - GitHub",
          "url": "https://github.com/tokio-rs/tokio/issues/6323",
          "excerpts": [
            "We should upgrade our unstable WASI support to support preview 2. This issue is concerned with supporting preview 2 under --cfg tokio_unstable ."
          ]
        },
        {
          "title": "WebAssembly in 2025 — Speed Without Plugins",
          "url": "https://javascript.plainenglish.io/webassembly-in-2025-speed-without-plugins-8b665f62934c",
          "excerpts": [
            "Jul 1, 2025 — A new WASI release will let Wasm talk to files, threads, and the network outside the browser, blurring lines between web and server processes."
          ]
        },
        {
          "title": "Wasmtime with threads support, for easier porting of libraries - CDK",
          "url": "https://forum.dfinity.org/t/wasmtime-with-threads-support-for-easier-porting-of-libraries/30882",
          "excerpts": [
            "Porting C++ libraries that use threading is challenging, because the IC has turned off threads support in the wasmtime."
          ]
        },
        {
          "title": "Rust Blog: Changes to Rust's WASI targets",
          "url": "https://blog.rust-lang.org/2024/04/09/updates-to-rusts-wasi-targets.html",
          "excerpts": [
            "WASI 0.2 was recently stabilized, and Rust has begun implementing first-class support for it in the form of a dedicated new target.",
            "Starting in Rust 1.78 (May 2nd, 2024) a new `wasm32-wasip1` target\nwill become available.",
            "The tier 3 `wasm32-wasip2` target will also be made available in Rust 1.78.",
            "The name `wasip1` can be read as either \"WASI (zero) point one\" or \"WASI preview\none\".",
            " This target triple was\nchosen because it not only maps to both terms, but also more closely resembles\nthe target terminology used in [other programming\nlanguages]",
            "The official specification uses the \"preview\" moniker, however in most\ncommunication the form \"WASI 0.1\" is now preferred.",
            "This table provides the dates and cut-offs for the target rename from\n`wasm32-wasi` to `wasm32-wasip1`",
            "The dates in this table do not apply to the\nnewly-introduced `wasm32-wasi-preview1-threads` target; this will be renamed to\n`wasm32-wasip1-threads` in Rust 1.78 without going through a transition period."
          ]
        },
        {
          "title": "Designing an Async Runtime for WASI 0.2",
          "url": "https://blog.yoshuawuyts.com/building-an-async-runtime-for-wasi/",
          "excerpts": [
            "WASI 0.2 is readiness-based rather than completion-based.",
            "WASI 0.2 is single-threaded, and fundamentally doesn't need access to a\n`task.spawn` abstraction.",
            "the wasi 0.2 polling model",
            "That should be simple enough to do with what\nwe've written:",
            "t/futures_concurrency/) library provides access to any mode of concurrency you\nmight want; meaning that in the absence of parallelism there is no reason for an\nexecutor to exist.",
            "In this post I've explained WASI's polling model and showed step-by-step how to use it to build your own async runtime.",
            "The\n`Pollable` type represents \"interest\" in an event.",
            "Say we had some kind of\n`read` call; there would be an associated `Pollable` for that call, which we\ncould submit to the host to say: \"please let me know when there's a reason for\nme to call `read`\". This is the \"readiness\" part of the system - you submit your\ninterest in an operation to the host system, and then the host system will\nperiodically yield a list of which operations are good to be called. The way to\nschedule that interest is via the `poll` function.",
            "A key difference between the `epoll` and WASI 0.2's `poll` model is that in WASI\nwe don't schedule interest in *resources* (e.g. file descriptors), but we\nschedule interest in concrete operations.",
            "That means we wait for\nthe host system to tell us we're ready to take an action, rather than wait for\nthe host system that an action has successfully completed (completion-based).",
            "In WASI we're more precise; we make\na specific method call, which returns a type.",
            "That type will have some way to\nget the underlying data, but also a method `subscribe` which returns a pollable",
            "We're then supposed to wait for the `poll` call to tell us that the `Pollable`\nis ready - and then we can call the method to get the underlying data without it\nreturning an error.",
            " That's probably a lot of words, so here's a basic example:"
          ]
        },
        {
          "title": "Threads and atomics (WebAssembly)",
          "url": "https://webstatus.dev/features/wasm-threads?q=baseline_date%3A2021-01-01..2021-12-31&sort=stable_edge_asc",
          "excerpts": [
            "Threads in WebAssembly run code in parallel, while atomic memory instructions can guarantee that no two threads can read or write to shared memory at the ..."
          ]
        },
        {
          "title": "Send and Sync -",
          "url": "https://web.mit.edu/rust-lang_v1.25/arch/amd64_ubuntu1404/share/doc/rust/html/nomicon/send-and-sync.html",
          "excerpts": [
            "Rust captures this through the Send and Sync traits. A type is Send if it is safe to send it to another thread. A type is Sync if it is safe to share between ..."
          ]
        },
        {
          "title": "Send and Sync",
          "url": "https://rust-training.ferrous-systems.com/latest/book/send-and-sync",
          "excerpts": [
            "Send & Sync. There are two special traits in Rust for concurrency semantics. Send marks a structure safe to send between threads. Sync marks a structure safe ..."
          ]
        },
        {
          "title": "References and Borrowing - The Rust Programming ...",
          "url": "https://doc.rust-lang.org/book/ch04-02-references-and-borrowing.html",
          "excerpts": [
            "A data race is similar to a race condition and happens when these three behaviors occur: Two or more pointers access the same data at the same time. At least ..."
          ]
        },
        {
          "title": "Rust Concurrency: Send/Sync, Ownership, and Atomics",
          "url": "https://doc.rust-lang.org/std/marker/trait.Sync.html",
          "excerpts": [
            " Likewise, [`sync::Arc`](../../std/sync/struct.Arc.html) provides a thread-safe\nanalogue of [`Rc`](../../std/rc/struct.Rc.html)",
            "c::Mutex\") provides mutable access to `T` to one thread at a time.",
            "Types that are not `Sync` are those that have “interior\nmutability” in a non-thread-safe form, such as [`Cell`](../cell/struct.Cell.html \"struct std::cell::Cell\") and [`RefCell`](../cell/struct.RefCell.html \"struct std::cell::RefCell\") . These types allow for mutation of\ntheir contents even through an immutable, shared reference."
          ]
        },
        {
          "title": "Extensible Concurrency with the Send and Sync Traits",
          "url": "https://rust-book.cs.brown.edu/ch16-04-extensible-concurrency-sync-and-send.html",
          "excerpts": [
            "The Send marker trait indicates that ownership of values of the type implementing Send can be transferred between threads."
          ]
        },
        {
          "title": "Rust review: The borrow checker - Julio Merino (jmmv.dev)",
          "url": "https://jmmv.dev/2018/06/rust-review-borrow-checker.html",
          "excerpts": [
            "The borrow checker is the component in the Rust compiler that enforces data ownership rules, and it enforces these to prevent data races. And ..."
          ]
        },
        {
          "title": "T is Sync if and only if &T is Send?",
          "url": "https://users.rust-lang.org/t/t-is-sync-if-and-only-if-t-is-send/64058",
          "excerpts": [
            "Aug 27, 2021 — You can define Sync in terms of Send this way: If &T is Send then T is Sync ; otherwise, T is not Sync . i.e the property of T influences the ..."
          ]
        },
        {
          "title": "Concurrency - The Embedded Rust Book",
          "url": "https://docs.rust-embedded.org/book/concurrency/",
          "excerpts": [
            "To summarise the Rust book, a type is Send when it can safely be moved to another thread, while it is Sync when it can be safely shared between multiple threads ..."
          ]
        },
        {
          "title": "Atomics.notify() - MDN - Mozilla",
          "url": "https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Atomics/notify",
          "excerpts": [
            "Jul 10, 2025 — The Atomics.notify() static method notifies up some agents that are sleeping in the wait queue. Note: This operation only works with an Int32Array or BigInt64 ..."
          ]
        },
        {
          "title": "Understanding the Rust borrow checker",
          "url": "https://blog.logrocket.com/introducing-rust-borrow-checker/",
          "excerpts": [
            "The borrow checker helps prevent data races. A data race occurs when two or more threads try to access and potentially modify the same memory ..."
          ]
        },
        {
          "title": "Data Races Explanation in the Rust Book",
          "url": "https://www.reddit.com/r/rust/comments/v3vagi/data_races_explanation_in_the_rust_book/",
          "excerpts": [
            "Essentially, a \"data race\" is knowledge. It means you are in complete control and knowledge of your data. Translated to practical lingo, there ..."
          ]
        },
        {
          "title": "Is the Sync trait a strict subset of the Send trait",
          "url": "https://stackoverflow.com/questions/68704717/is-the-sync-trait-a-strict-subset-of-the-send-trait-what-implements-sync-withou",
          "excerpts": [
            "The only relationship between Send and Sync is that T is Sync if and only if &T is Send (it makes sense since \"syncing\" across threads is really just being ..."
          ]
        },
        {
          "title": "Nine Rules for Running Rust on WASM WASI - Medium",
          "url": "https://medium.com/data-science/nine-rules-for-running-rust-on-wasm-wasi-550cd14c252a",
          "excerpts": [
            "This first article focuses on running code on “WASM WASI”, a container-like environment. We'll see that WASM WASI may (or may not) be useful in its own right.",
            "With this understanding of Cargo features, we can now attempt to fix the Rayon error we encountered when running tests on WASM WASI.",
            "Learn how to run Rust code on WASM WASI with these 9 essential rules. From conditional compilation to CI integration, this guide simplifies ...",
            "alloc: Provides heap memory allocation capabilities ... You can run standard Rust tests on WASM WASI, making it easy to verify your code.",
            "WASI: WebAssembly System Interface (WASI) allows outside-the-browser WASM to access file I/O, networking (not yet), and time handling."
          ]
        },
        {
          "title": "Building Infrastructure for Large Language Model Ecosystems",
          "url": "https://www.secondstate.io/articles/infra-for-llms/",
          "excerpts": [
            "This article provides a comprehensive overview of the talk by Sam Liu on the topic of Rust + WebAssembly for building large model ecosystems."
          ]
        },
        {
          "title": "2024 State of Rust Survey Results | Rust Blog : r/rust - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/1iogalq/2024_state_of_rust_survey_results_rust_blog/",
          "excerpts": [
            "When we asked respondents about their main worries for the future of Rust, one of the top answers remained the worry that Rust will become too complex."
          ]
        },
        {
          "title": "Crossbeam: The Ecosystem Revenue Platform",
          "url": "https://www.crossbeam.com/",
          "excerpts": [
            "Access unique ecosystem insights from your partners' CRMs to uncover new opportunities, target the right accounts, and achieve your revenue goals."
          ]
        },
        {
          "title": "wasm_bindgen_rayon - Rust",
          "url": "https://docs.rs/wasm-bindgen-rayon",
          "excerpts": [
            "wasm-bindgen-rayon is an adapter for enabling Rayon-based concurrency on the Web with WebAssembly (via wasm-bindgen, Web Workers and SharedArrayBuffer support)."
          ]
        },
        {
          "title": "Research on WebAssembly Runtimes: A Survey - arXiv",
          "url": "https://arxiv.org/html/2404.12621v1",
          "excerpts": [
            "This paper provides a comprehensive survey of research on WebAssembly runtimes. It covers 98 articles on WebAssembly runtimes and characterizes existing ...",
            ". Third, seven performance issues are detected, all confirmed by the runtimes’ developers.",
            "WasmEdge is a lightweight, high-performance, and extensible Wasm runtime (Was, [2022](https://arxiv.org/html/2404.12621v1.bib12) ) . It is currently used in edge clouds, serverless software as a service(SaaS) APIs, embedded functions, smart contracts, and intelligent devices.",
            "WAMR supports all the execution modes: interpreter, JIT, and AoT.",
            "WAMR is a lightweight standalone Wasm runtime with high performance and highly configurable features (WAM, [2022](https://arxiv.org/html/2404.12621v1.bib17) ) , suitable for applications in various cases, including embedded systems, IoT, trusted execution environments(TEE), smart contracts, cloud-native, etc.",
            "WAMR is a lightweight standalone Wasm runtime with high performance and highly configurable features (WAM, [2022](https://arxiv.org/html/2404.12621v1.bib17) ) , suitable for applications in various cases, including embedded systems, IoT, trusted execution environments(TEE), smart contracts, cloud-native, etc.",
            "Second, they compare the performance of five standalone Wasm runtimes, including wasmer (was, [2022a](https://arxiv.org/html/2404.12621v1.bib13) ) , wasmtime (was, [2022c](https://arxiv.org/html/2404.12621v1.bib15) ) , WAMR (WAM, [2022](https://arxiv.org/html/2404.12621v1.bib17) ) , WasmEdge (Was, [2022](https://arxiv.org/html/2404.12621v1.bib12) ) and wasm3 (was, [2022b](https://arxiv.org/html/2404.12621v1.bib11) ) on more than 100 test cases extracted from the LLVM test suite.",
            "Second, they compare the performance of five standalone Wasm runtimes, including wasmer (was, [2022a](https://arxiv.org/html/2404.12621v1.bib13) ) , wasmtime (was, [2022c](https://arxiv.org/html/2404.12621v1.bib15) ) , WAMR (WAM, [2022](https://arxiv.org/html/2404.12621v1.bib17) ) , WasmEdge (Was, [2022](https://arxiv.org/html/2404.12621v1.bib12) ) and wasm3 (was, [2022b](https://arxiv.org/html/2404.12621v1.bib11) ) on more than 100 test cases extracted from the LLVM test suite.",
            "Apr 19, 2024 — WasmEdge is a lightweight, high-performance, and extensible Wasm runtime (Was, 2022) . It is currently used in edge clouds, serverless software ..."
          ]
        },
        {
          "title": "wasmtime_wasi::runtime - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime_wasi/runtime/index.html",
          "excerpts": [
            "The implementation of wasmtime-wasi requires a tokio executor in a way that is deeply tied to its design. When used from a synchronous wasmtime context, this ...",
            "When used from a synchronous wasmtime context, this module provides the wrapper function `in_tokio` used throughout the shim implementations of synchronous component binding `Host` traits in terms of the async ones.",
            ". This module provides an “ambient Tokio runtime” `with_ambient_tokio_runtime`.",
            "Embedders of wasmtime-wasi may do so from synchronous Rust, and not use tokio directly."
          ]
        },
        {
          "title": "Poor performance of wasmtime file I/O maybe because tokio",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/7973",
          "excerpts": [
            "Feb 20, 2024 — Wasmtime uses tokio to implement file IO operations, and the number of file I/O operations is three times that of native, resulting in poor performance.",
            "to fit in with the same `pollable` and `stream` resources as wasi-sockets, wasi-http, and etc, wasi-filesystem is also implemented on top of tokio.",
            "Because Linux doesn't provide a non-blocking way to do File IO (except for `io_uring` , which afaik no production-ready Rust systems are using yet, though hopefully they will soon), the correct way to do File IO in tokio is to move that work onto a special blocking IO thread-pool managed by tokio, which costs two synchronizations between threads per blocking operation for the executor to hand the work off from an async task, and to get notified of completion to resume the async task - that our hypothesis for where the `futex` and `epoll_pwait` syscalls are coming from.",
            "There is no configuration available to change whether blocking file IO is moved to separate thread or not - its very fundamental to how tokio and the wasmtime-wasi implementation work.",
            "The only way to change that behavior would be to rewrite wasmtime-wasi with a completely different internal architecture, to solely use synchronous Rust.",
            "e may expect it to become even more deeply integrated with `wasmtime` as the Component Model works on having native async as part of the next big eff",
            "I designed `wasi-common` so that tokio was an optional dependency, and we were able to get away with it during WASI Preview 1 because WASI didnt yet have streams or an extensible poll interface.",
            " So, we changed the design of WASI itself in Preview 2 to have a `pollable` resource that can be created by any interface that needs to express waiting on readiness, and with that we more or less got forced into using async Rust for `async trait Subscribe` as the host implementation of each `pollable`",
            " . We may be able to find a way, for synchronous embeddings, to break some of tokio's rules about performing blocking syscalls on the \"main thread\"",
            "In order to fit in with the same `pollable` and `stream` resources as wasi-sockets, wasi-http, and etc, wasi-filesystem is also implemented on top of tokio.",
            "Because Linux doesn't provide a non-blocking way to do File IO (except for `io_uring` , which afaik no production-ready Rust systems are using yet, though hopefully they will soon), the correct way to do File IO in tokio is to move that work onto a special blocking IO thread-pool managed by tokio, which costs two synchronizations between threads per blocking operation for the executor to hand the work off from an async task, and to get notified of completion to resume the async task - that our hypothesis for where the `futex` and `epoll_pwait` syscalls are coming from",
            "There may be an obvious low-hanging fruit there? If we could it cut down to just one `write` per WASI file `blocking-write-and-flush` that would be a nice win.",
            "Finally, why does wasmtime-cli, which is a totally synchronous Rust program, use `tokio` under the hood to implement the WASI interface? Basically, because we only had time to implement `wasmtime-wasi` once.",
            "Wasmtime uses tokio to implement file IO operations, and the number of file I/O operations is three times that of native, resulting in poor performance."
          ]
        },
        {
          "title": "Land WASI Preview 2 support into Wasmtime · Issue #6370",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/6370",
          "excerpts": [
            "May 10, 2023 — We have been working on a prototype of what WASI Preview 2 support will look like in Wasmtime for 7 months now!"
          ]
        },
        {
          "title": "wasmtime_wasi::p2 - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime_wasi/p2/index.html",
          "excerpts": [
            "This module provides a Wasmtime host implementation of WASI 0.2 (aka WASIp2\naka Preview 2) and WASI 0.1 (aka WASIp1 aka Preview 1). WASI is implemented\nwith the Rust crates [`tokio`](https://crates.io/crates/tokio) and [`cap-std`](https://crates.io/crates/cap-std) primarily, meaning that\noperations are implemented in terms of their native platform equivalents by\ndefault.",
            "This module provides a Wasmtime host implementation of WASI 0.2 (aka WASIp2 aka Preview 2) and WASI 0.1 (aka WASIp1 aka Preview 1)."
          ]
        },
        {
          "title": "wasi::sockets::tcp - Rust - Docs.rs",
          "url": "https://docs.rs/wasi/latest/wasi/sockets/tcp/index.html",
          "excerpts": [
            "API documentation for the Rust `tcp` mod in crate `wasi`."
          ]
        },
        {
          "title": "Thread | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/develop/c/threads/",
          "excerpts": [
            "Skip to main content. ⭐️ If you like WasmEdge, give it a star on GitHub! ⭐️. WasmEdge Logo. WasmEdgeGetting StartedDevelopEmbedExtend. English.",
            "Skip to main content. ⭐ If you like WasmEdge, give it a star on GitHub! ⭐. WasmEdge Logo. WasmEdgeGetting StartedDevelopEmbedExtend. English."
          ]
        },
        {
          "title": "WASM Thread support in WasmEdge · Issue #1467",
          "url": "https://github.com/WasmEdge/WasmEdge/issues/1467",
          "excerpts": [
            "May 18, 2022 — The initial version of wasm focussed on single-threaded computation, so most design related to wasm threading allows only one thread in a wasm instance.",
            "The initial version of wasm focussed on single-threaded computation, so most design related to wasm threading allows only one thread in a wasm instance."
          ]
        },
        {
          "title": "second-state/wasmedge_wasi_socket: A Rust lib for socket ... - GitHub",
          "url": "https://github.com/second-state/wasmedge_wasi_socket",
          "excerpts": [
            "WasmEdge WASI Socket. This project provides a Rust SDK for network socket functions available in the WasmEdge Runtime. For Rust source code examples on how ..."
          ]
        },
        {
          "title": "std::os::wasi - Rust",
          "url": "https://doc.rust-lang.org/std/os/wasi/index.html",
          "excerpts": [
            "Platform-specific extensions to std for the WebAssembly System Interface (WASI). Provides access to platform-level information on WASI."
          ]
        },
        {
          "title": "Rust + CPU affinity: Full control over threads, hybrid cores ...",
          "url": "https://www.reddit.com/r/rust/comments/1ksm0cb/rust_cpu_affinity_full_control_over_threads/",
          "excerpts": [
            "Yes, pinning threads that share cache is the way to go. I do this at the L3 cache level since that's where AMD breaks up their chiplets. I see ..."
          ]
        },
        {
          "title": "[post] Tasks are the wrong abstraction : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/1cedhxi/post_tasks_are_the_wrong_abstraction/",
          "excerpts": [
            "The whole premise of work-stealing is that it's going to deliver better tail latencies for non-balanced workloads. I don't see that scenario ..."
          ]
        },
        {
          "title": "Tokyo Drift: I Rewrote Tokio's Scheduler and Slashed P-99 ...",
          "url": "https://medium.com/@Neha8661/tokyo-drift-i-rewrote-tokios-scheduler-and-slashed-p-99-latency-by-80-49c72d424f13",
          "excerpts": [
            "Jun 13, 2025 — P-99 latency: 5.2 ms; scheduler context switches: ≈ 2.1 million / s. The tail far outweighs the mean because long compute futures monopolise the ..."
          ]
        },
        {
          "title": "WebAssembly in 2025: Why Use It in Modern Projects?",
          "url": "https://www.scrumlaunch.com/blog/webassembly-in-2025-why-use-it-in-modern-projects",
          "excerpts": [
            "Mar 6, 2025 — Boost web performance with WebAssembly (Wasm) in 2025! Run high-performance apps in the browser with near-native speed—ideal for gaming, AI, ..."
          ]
        },
        {
          "title": "A Comparative Study of WebAssembly Runtimes",
          "url": "https://ojs.bonviewpress.com/index.php/AAES/article/download/4965/1367/29227",
          "excerpts": [
            "Apr 10, 2025 — WebAssembly (Wasm) is a versatile runtime environment for executing code efficiently, originally for web applications, now used in edge ...",
            "Apr 10, 2025 — WasmEdge,. Lucet, and Wasmer balance performance and memory usage, with. WasmEdge providing 32 ms browser performance and 28 ms server."
          ]
        },
        {
          "title": "When/why is async good for performance?",
          "url": "https://users.rust-lang.org/t/when-why-is-async-good-for-performance/35187",
          "excerpts": [
            "I'm seeing the async version about 30% faster than the thread-based version, and using about 1/20th the memory per task."
          ]
        },
        {
          "title": "Can I prevent task stealing in multithreaded Tokio?",
          "url": "https://stackoverflow.com/questions/79412052/can-i-prevent-task-stealing-in-multithreaded-tokio",
          "excerpts": [
            "Unfortunately, work-stealing is an integral part of how Tokio works, and there does not appear to be a way to entirely turn it off, ..."
          ]
        },
        {
          "title": "Tokio: Reducing tail latencies with automatic cooperative ...",
          "url": "https://www.reddit.com/r/rust/comments/ft98nz/tokio_reducing_tail_latencies_with_automatic/",
          "excerpts": [
            "We generally do not suggest using block_in_place over spawn_blocking as the former can lead to some interesting tail latencies."
          ]
        },
        {
          "title": "Why choose async/await over threads? : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/1bn3082/why_choose_asyncawait_over_threads/",
          "excerpts": [
            "Ideally one should benchmark to evaluate the true costs. Also factor in the added costs of Async, including increasing dependencies, binary size ...",
            "Why choose async/await over threads? ===================================="
          ]
        },
        {
          "title": "On pinning and isolating CPU cores",
          "url": "https://manuel.bernhardt.io/posts/2023-11-16-core-pinning/",
          "excerpts": [
            "Nov 16, 2023 — This is also known as processor affinity. CPU core isolation ensures that the kernel processes scheduler will not interact with those CPU cores."
          ]
        },
        {
          "title": "We Rewrote Our Backend in Rust — And Our Latency ...",
          "url": "https://medium.com/@harishsingh8529/we-rewrote-our-backend-in-rust-and-our-latency-graph-fell-off-a-cliff-2243abd03031",
          "excerpts": [
            "Our P99 latency regularly exceeded 500ms, despite aggressive caching and autoscaling. ... Before Rust, the aggregation service was responsible for ..."
          ]
        },
        {
          "title": "Vorner's random stuff - Benchmark of different Async approaches in Rust",
          "url": "https://vorner.github.io/async-bench.html",
          "excerpts": [
            "The async and futures approaches to result in very similar performance.",
            "The slowest solution, especially for larger number of connections, are the\n  thread",
            "The fastest solutions are the ones that _don’t_ have any work stealing."
          ]
        },
        {
          "title": "wasmtime_wasi_http - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime_wasi_http/index.html",
          "excerpts": [
            "`wasmtime_wasi_http` is Wasmtime's WASI HTTP implementation, built on `hyper` and `tokio`, using `bindgen!` to generate bindings. It has both async and sync ...",
            "This crate is Wasmtime's host implementation of the wasi:http package as part of WASIp2. This crate's implementation is primarily built on top of [ hyper ] and ..."
          ]
        },
        {
          "title": "spawn_blocking in tokio::task - Rust",
          "url": "https://docs.rs/tokio/latest/tokio/task/fn.spawn_blocking.html",
          "excerpts": [
            "This function runs the provided closure on a thread dedicated to blocking operations. See the CPU-bound tasks and blocking code section for more information."
          ]
        },
        {
          "title": "Tokio Tasks for CPU-bound ops",
          "url": "https://users.rust-lang.org/t/tokio-tasks-for-cpu-bound-ops/103414",
          "excerpts": [
            "Dec 4, 2023 — You can absolutely run CPU-bound tasks on tokio - just use spawn_blocking(). You can run a rayon thread pool from spawn_blocking()."
          ]
        },
        {
          "title": "Wasm, WASI, Wagi: What are they?",
          "url": "https://www.fermyon.com/blog/wasm-wasi-wagi",
          "excerpts": [
            "Apr 22, 2022 — There are a number of WebAssembly execution environments that run on the command line. Wasmtime, Wamr, and WasmEdge are a few examples. And ..."
          ]
        },
        {
          "title": "When should you use Tokio's `spawn_blocking`?",
          "url": "https://stackoverflow.com/questions/74547541/when-should-you-use-tokios-spawn-blocking",
          "excerpts": [
            "A good rule of thumb is no more than 10 to 100 microseconds between each .await. That said, this depends on the kind of application you are writing."
          ]
        },
        {
          "title": "Can you explain that Tokio vs crossbeam vs rayon a bit ...",
          "url": "https://news.ycombinator.com/item?id=28510257",
          "excerpts": [
            "If you have cpu-bound work, then rayon is what you want to use. Rayon is a work-stealing parallelism crate -- it will schedule work to be done, and different ..."
          ]
        },
        {
          "title": "BWoS: Formally Verified Block-based Work Stealing for ...",
          "url": "https://www.usenix.org/system/files/osdi23-wang-jiawei.pdf",
          "excerpts": [
            "by J Wang · 2023 · Cited by 5 — Abstract. Work stealing is a widely-used scheduling technique for paral- lel processing on multicore. Each core owns a queue of tasks."
          ]
        },
        {
          "title": "cunarist/tokio-with-wasm - GitHub",
          "url": "https://github.com/cunarist/tokio-with-wasm",
          "excerpts": [
            "A Rust library that provides tokio specifically designed for web browsers. It aims to provide the exact same tokio features for web applications, leveraging ..."
          ]
        },
        {
          "title": "Number of Tokio threads vs number of Rayon threads in IO ...",
          "url": "https://www.reddit.com/r/rust/comments/1btggxp/number_of_tokio_threads_vs_number_of_rayon/",
          "excerpts": [
            "I am trying to work through how do you balance the number of threads provided to each system in an application performing both IO bound tasks as well as CPU ..."
          ]
        },
        {
          "title": "wasmer_wasi - Rust - Docs.rs",
          "url": "https://docs.rs/wasmer-wasi",
          "excerpts": [
            "The `wasmer_wasi` crate is Wasmer's WASI implementation. It uses `generate_import_object` to create Imports, which can be combined with a module to execute ...",
            "Wasmer's WASI implementation. Use generate_import_object to create an Imports . This Imports can be combined with a module to create an Instance which can ..."
          ]
        },
        {
          "title": "The wasmedge CLI | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/start/build-and-run/cli/",
          "excerpts": [
            "After installing WasmEdge, you can use the wasmedge CLI to execute WASM files. We will cover how to run WASM files with WasmEdge on your machine and Docker ...",
            "Use --enable-component to enable the Component Model proposal (Default OFF , loader phase only). Use --enable-all to enable ALL proposals above. WASM file ..."
          ]
        },
        {
          "title": "Fermyon Blog: Looking Ahead to WASI-P3",
          "url": "https://www.fermyon.com/blog/looking-ahead-to-wasip3",
          "excerpts": [
            "A look at Preview 3 (WASIp3) the next major release of WASI - which brings composable concurrency to Wasm components. This release will include significant\nimprovements to the underlying Component Model – including support for first\nclass, composable concurrency – while maintaining compatibility with the\nexisting WASIp2 release.",
            "In this post, we’ll summarize the new features coming with WASIp3, look at the\nstatus of the implementation efforts, and finally build a real-world example\nwhich puts those new features to use.",
            ".com/WebAssembly/component-model) includes a\ncomplete description of the async ABI, streams, futures, and associated built-in\nfunctions (look for the 🔀🔀 icon). Note that the spec is still being refined as\nwe gain more implementation experience, but we don’t expect any major changes\nbetween now and the WASIp3 release.",
            "WASI (WebAssembly System Interface)](https://wasi.dev/) is a set of interfaces\ncovering both traditional operating system features (e.g. file I/O, clocks, and\nnetworking) as well as more specialized features such as cloud services (HTTP,\ndatabase access, messaging, etc. ), abstractions for embedded systems",
            "A look at Preview 3 (WASIp3) the next major release of WASI - which brings composable concurrency to Wasm components.",
            "So far, WASI has had two milestone releases: 0.1 (AKA WASIp1, “Preview 1”) and\n0\\.2 (AKA WASIp2, “Preview 2”)",
            "The latter was released early last year and was\nbuilt upon the [Component Model](https://component-model.bytecodealliance.org/) ,\na proposed standard for expressing the high-level interface of a Wasm\napplication or library, enabling cross-language code reuse and secure dependency\nisolation",
            "Before WASIp2 was even released, a few of us in the Bytecode Alliance were\nalready prototyping the foundations of WASIp3, and we’ve made a lot of progress\nsin",
            "The main theme of WASIp3 is _composable_ _concurren",
            " An asynchronous function ABI: This allows components to export and/or import\n  functions using either the existing synchronous ABI or the new asynchronous\n ",
            "Support for built-in, generic `stream` and `future` types, providing\n  efficient, optionally asynchronous, cross-component and component<->host\n  communicat",
            "Together, these features make concurrency a first-class concept in the component\nmodel, allowing arbitrary numbers of guest tasks to run concurrently in the same\ncomponent instance.",
            "Draft 0.3 interfaces have been written for each of the main WASI projects:",
            "* [wasi-clocks](https://github.com/WebAssembly/wasi-clocks/tree/main/wit-0.3.0-draft)",
            "* [wasi-random](https://github.com/WebAssembly/wasi-random/tree/main/wit-0.3.0-draft)",
            "* [wasi-filesystem](https://github.com/WebAssembly/wasi-filesystem/tree/main/wit-0.3.0-draft)",
            "* [wasi-sockets](https://github.com/WebAssembly/wasi-sockets/tree/main/wit-0.3.0-draft)",
            "* [wasi-cli](https://github.com/WebAssembly/wasi-cli/tree/main/wit-0.3.0-draft)",
            "* [wasi-http](https://github.com/WebAssembly/wasi-http/tree/main/wit-0.3.0-draft)",
            "Draft 0.3 interfaces have been written for each of the main WASI projects:\n\n* "
          ]
        },
        {
          "title": "add_to_linker_async in wasmtime_wasi::preview1 - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime_wasi/preview1/fn.add_to_linker_async.html",
          "excerpts": [
            "Adds asynchronous versions of all WASIp1 functions to the [ wasmtime::Linker ] provided. This method will add WASIp1 functions to linker ."
          ]
        },
        {
          "title": "My Experience with WASI Preview 2 - My Portfolio",
          "url": "https://dipankar-das.com/posts/blogs/wasip2/",
          "excerpts": [
            "Oct 28, 2024 — In this blog I will share my experience with the new WASI Preview 2 and how I created a working project using the new composition feature.",
            "In this blog I will share my experience with the new WASI Preview 2 and how I created a working project using the new composition feature.",
            "WASI` is a system interface for WebAssembly. It is a standard that defines a set of system calls that can be used by WebAssembly modules. This allows WebAssembly modules to interact with the host system in a secure and portable way",
            "Some interesting new feature of WASI preview 2 are :-",
            "1. `Composition`: which allows you to compose multiple WASI modules into a single module.",
            "2. `Socket`: which allows you to create TCP and UDP sockets and perform DNS lookups from within a WebAssembly module.",
            "3. `Filesystem`: which allows you to interact with the filesystem from within a WebAssembly module.",
            "wasi-sockets`: adds TCP & UDP sockets and domain name lookup to WASI",
            "wasi-filesystem`: is a WASI API primarily for accessing host filesystems. It has function for opening, reading, and writing files, and for working with directories",
            "wasi-random`: for obtaining pseudo-random data",
            "wasi-clocks`: API for reading the current time and measuring elapsed time",
            "wasi-io`: API providing I/O stream abstractions",
            "There are two types, input-stream, and output-stream, which support read and write, respectively, as well as a number of utility functions.",
            "There are two types, input-stream, and output-stream, which support read and write, respectively, as well as a number of utility functions."
          ]
        },
        {
          "title": "wasmtime_wasi - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime_wasi/index.html",
          "excerpts": [
            "This crate provides a Wasmtime host implementations of different versions of WASI. WASI is implemented with the Rust crates [ tokio ] and cap-std primarily. wasmtime\\_wasi - Rust\n\n## [wasmtime",
            "n\n\nThis crate provides a Wasmtime host implementations of different versions of WASI. WASI is implemented with the Rust crates [ `tokio` ] and [`cap-std`](cap_std) primarily, meaning that\noperations are implemented in terms of their native platform equivalents by\ndefault.",
            "io directly.\nThe implementation of\n\n    wasmtime-wasi requires a tokio executor in a way that is [deeply tied to\n\n    its\n\n    design](https://github.com/bytecodealliance/wasmtime/issues/7973) . When used from a synchronous wasmtime context, this module provides the\n\n    wrapper function [`in_tokio`](runtime/fn.in_tokio.html \"fn wasmtime\\_wasi::runtime::in\\_tokio\") used throughout the shim implementations of\n\n    synchronous component binding `Host` traits in terms of the async ones. [sockets](sockets/index.html \"mod wasmtime\\_wasi::sockets\")"
          ]
        },
        {
          "title": "WASI 0.2 Launched",
          "url": "https://bytecodealliance.org/articles/WASI-0.2",
          "excerpts": [
            "The WASI Subgroup voted on January 25, 2024 to launch WASI 0.2, also known as WASI Preview 2, or WASIp2! ... Files, sockets, clocks, random ...",
            "Jan 25, 2024 — Most immediately, what this means is that the WASI Subgroup officially says that the WASI 0.2 APIs are stable. There is still a lot more to do, ...",
            "APIs are stable. The",
            " a major milestone! We made it! ",
            "WASI 0.2 Launched",
            "The WASI Subgroup voted on January 25, 2024 to launch WASI 0.2, also known\nas WASI Preview 2, or WASIp2! This blog post is a brief look at the present,\npast, and future of WASI, and a repost of an original article shared [here](https://blog.sunfishcode.online/wasi-0-2/) .",
            "WASI 0.2 includes two _worlds_ :\n\n* \n  _wasi-cli_ , the “command-line interface” world, which roughly\n  corresponds to POSIX. Files, sockets, clocks, random numbers, etc. * \n  _wasi-http_ , an HTTP proxy world, organized around requests\n  and responses.",
            "the big ideas still make sense), because we can have new worlds with new\nentrypoints, with new signatures.",
            "Jan 25, 2024 — The WASI Subgroup voted on January 25, 2024 to launch WASI 0.2, also known as WASI Preview 2, or WASIp2! This blog post is a brief look at ..."
          ]
        },
        {
          "title": "wasmtime-wasi - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/wasmtime-wasi/14.0.0/dependencies",
          "excerpts": [
            "An implementation of futures and streams featuring zero allocations, composability, and iterator-like interfaces. ^2.0.2. ^2.0.2 io-lifetimes optional no ..."
          ]
        },
        {
          "title": "Making the Tokio scheduler 10x faster - An asynchronous Rust runtime",
          "url": "https://tokio.rs/blog/2019-10-scheduler",
          "excerpts": [
            "In the work-stealing scheduler, when a processor's run queue is empty, the processor will then attempt to steal tasks from sibling processors."
          ]
        },
        {
          "title": "wasi:clocks",
          "url": "https://wa.dev/wasi:clocks",
          "excerpts": [
            "WASI Monotonic Clock is a clock API intended to let users measure elapsed time. ... Query the resolution of the clock. Returns the duration of time ...",
            "A monotonic clock is a clock which has an unspecified initial value, and successive reads of the clock will produce non-decreasing values. It is intended for ...",
            "WASI Monotonic Clock is a clock API intended to let users measure elapsed time. It is intended to be portable at least between Unix-family platforms and ..."
          ]
        },
        {
          "title": "wasi::clocks::monotonic_clock - Rust",
          "url": "https://docs.rs/wasi/latest/wasi/clocks/monotonic_clock/index.html",
          "excerpts": [
            "WASI Monotonic Clock is a clock API intended to let users measure elapsed time. It is intended to be portable at least between Unix-family platforms and ..."
          ]
        },
        {
          "title": "Multi-thread, Pthread APIs And Thread Management | Document",
          "url": "https://wamr.gitbook.io/document/wamr-in-practice/features/pthread_library",
          "excerpts": [
            "WAMR provides a built-in library to support pthread APIs. You can call pthread APIs in your application source code."
          ]
        },
        {
          "title": "Thread Signal - WASIX",
          "url": "https://wasix.org/docs/api-reference/wasix/thread_signal",
          "excerpts": [
            "The thread_signal() function sends a signal to a specific thread within the current process. It takes a Tid as input, representing the thread identifier."
          ]
        },
        {
          "title": "Signal - WASIX",
          "url": "https://wasix.org/docs/language-guide/c/tutorials/signal",
          "excerpts": [
            "A simple C program that demonstrates how to handle signals in a process. It should set up signal handlers for two signals: SIGHUP and SIGINT."
          ]
        },
        {
          "title": "Async/await vs threads/atomics and when you use each?",
          "url": "https://www.reddit.com/r/rust/comments/jgpvi3/asyncawait_vs_threadsatomics_and_when_you_use_each/",
          "excerpts": [
            "So async await allows for better usage of processing time by switching tasks but the gain is inversely proportional to cpu load for the thread ..."
          ]
        },
        {
          "title": "P99 CONF 2024 | Performance Pitfalls of Rust Async ...",
          "url": "https://www.youtube.com/watch?v=hLA1qLqmi74",
          "excerpts": [
            "I'm going to be talking about the performance pitfalls of Asing function pointers in Rust and why they might not matter."
          ]
        },
        {
          "title": "Why shouldn't I use Tokio for my High-CPU workloads?",
          "url": "https://users.rust-lang.org/t/why-shouldnt-i-use-tokio-for-my-high-cpu-workloads/63398",
          "excerpts": [
            "The Tokio docs (and many other posts) mention that Tokio isn't suitable for this type of workload. It explains that High-CPU tasks will block other tasks from ..."
          ]
        },
        {
          "title": "How much performance is gained by going async instead of ...",
          "url": "https://news.ycombinator.com/item?id=18970302",
          "excerpts": [
            "async is generally harder than using threads for concurrency. Ideally the async/await work in Rust is going to make that trade-off less extreme than it is ..."
          ]
        },
        {
          "title": "Wasmtime WASI HTTP Async Linker Integration",
          "url": "https://docs.wasmtime.dev/api/wasmtime_wasi_http/fn.add_to_linker_async.html",
          "excerpts": [
            "pub fn add_to_linker_async<T>(l: &mut Linker<T>) -> Result<()>",
            "where",
            "    T: WasiHttpView + WasiView + 'static,",
            "```\nuse wasmtime::{Engine, Result, Config};\nuse wasmtime::component::{ResourceTable, Linker};\nuse wasmtime_wasi::{WasiCtx, WasiCtxView, WasiView};\nuse wasmtime_wasi_http::{WasiHttpCtx, WasiHttpView};\n\nfn main() -> Result<()> {\n    let mut config = Config::new();\n    config.async_support(true);\n    let engine = Engine::new(&config)? ;\n\n    let mut linker = Linker::<MyState>::new(&engine);\n    wasmtime_wasi_http::add_to_linker_async(&mut linker)?\n;\n    // ... add any further functionality to `linker` if desired ...\n\n    Ok(())\n}\n\nstruct MyState {\n    ctx: WasiCtx,\n    http_ctx: WasiHttpCtx,\n    table: ResourceTable,\n}\n\nimpl WasiHttpView for MyState {\n    fn ctx(&mut self) -> &mut WasiHttpCtx { &mut self.http_ctx }\n    fn table(&mut self) -> &mut ResourceTable { &mut self.table }\n}\n\nimpl WasiView for MyState {\n    fn ctx(&mut self) -> WasiCtxView<'_> {\n        WasiCtxView { ctx: &mut self.ctx, table: &mut self.table }\n    }\n}\n```"
          ]
        },
        {
          "title": "Panic when calling host function in async context #9515",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/9515",
          "excerpts": [
            "Oct 28, 2024 — Tokio does not permit this when already inside a runtime. Switch to using preview1::add_to_linker_async and it will not create this private ..."
          ]
        },
        {
          "title": "Memory and performance comparison: async vs threads in Rust",
          "url": "https://github.com/jimblandy/context-switch",
          "excerpts": [
            "Memory consumption per task (i.e. for a task that doesn't do much) starts at around a few hundred bytes for an async task, versus around 20KiB (9.5KiB user, 10KiB kernel) for a kernel thread ",
            "A context switch takes around 0.2µs between async tasks, versus 1.7µs\n  between kernel thread",
            "Creating a new task takes ~0.3µs for an async task, versus ~17µs for a new\n  kernel threa",
            "It's no problem to create 250,000 async tasks, but I was only able to get my\n  laptop to run 80,000 threads (4 core, two way HT, 32GiB",
            "The async advantage also\n  goes away in our microbenchmark if the program is pinned to a single cor",
            "per-async-task memory use in `async-brigade` is around 0.4KiB, a factor of ~20.",
            "In these runs, I'm seeing 18.19s / 26.91s ≅ 0.68 or a 30% speedup from going\nasync.",
            "\nHowever, if I pin the threaded version to a single core, the speed\nadvantage of async disappears"
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "Systematic Process to Reduce Linux OS Jitter",
          "url": "https://groups.google.com/g/mechanical-sympathy/c/DWlziVmyW-w",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache behavior, etc.). Same ..."
          ]
        },
        {
          "title": "sched_setscheduler(2) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man2/sched_setscheduler.2.html",
          "excerpts": [
            "The real-time policies that may be specified in policy are: SCHED_FIFO a first-in, first-out policy; and SCHED_RR a round-robin policy. For each of the above ..."
          ]
        },
        {
          "title": "sched(7) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man7/sched.7.html",
          "excerpts": [
            "SCHED_FIFO: First in-first out scheduling SCHED_FIFO can be used only with static priorities higher than 0, which means that when a SCHED_FIFO thread ...",
            "SCHED_RR: Round-robin scheduling SCHED_RR is a simple enhancement of SCHED_FIFO. Everything described above for SCHED_FIFO also applies to SCHED_RR, except ..."
          ]
        },
        {
          "title": "[Optional] cgroup v2",
          "url": "https://rootlesscontaine.rs/getting-started/common/cgroup2/",
          "excerpts": [
            "Enabling cgroup v2 is often needed for running Rootless Containers with limiting the consumption of the CPU, memory, I/O, and PIDs resources."
          ]
        },
        {
          "title": "Open Container Initiative Runtime Specification - GitHub Pages",
          "url": "https://wking.github.io/opencontainer-runtime-spec/runtime-spec.pdf",
          "excerpts": [
            "You can configure a container's cgroups via the resources field of the Linux configuration. ... CPU resources should be reallocated (CFS scheduler only)."
          ]
        },
        {
          "title": "Deploy with containerd's runwasi",
          "url": "https://wasmedge.org/docs/develop/deploy/cri-runtime/containerd/",
          "excerpts": [
            "The containerd-shim runwasi project supports WasmEdge. Prerequisites​. Install Rust because we need to compile the runwasi project. Download the runwasi project."
          ]
        },
        {
          "title": "Deploy with containerd's runwasi",
          "url": "https://wasmedge.org/docs/develop/deploy/cri-runtime/containerd",
          "excerpts": [
            "The containerd-shim runwasi project supports WasmEdge. Prerequisites​. Install Rust because we need to compile the runwasi project. Download the runwasi project."
          ]
        },
        {
          "title": "Running Wasm in a container - Atamel.Dev",
          "url": "https://atamel.dev/posts/2023/06-29_run_wasm_in_docker/",
          "excerpts": [
            "Jun 29, 2023 — With runwasi , Docker and Kubernetes can now use containerd to run Wasm modules alongside with regular containers: containerd and runwasi ..."
          ]
        },
        {
          "title": "Issues pulling Wasm image to run on the Wasmtime shim",
          "url": "https://github.com/containerd/runwasi/issues/722",
          "excerpts": [
            "Nov 7, 2024 — For the moment I am trying to make sure I can execute both native and WebAssembly containers, the latter using the wasmtime shim from runwasi."
          ]
        },
        {
          "title": "Pod Quality of Service Classes",
          "url": "https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/",
          "excerpts": [
            "Guaranteed\n\nPods that are `Guaranteed` have the strictest resource limits and are least likely\nto face eviction. They are guaranteed not to be killed until they exceed their limits\nor there are no lower-priority Pods that can be preempted from the Node. They may\nnot acquire resources beyond their specified limits. These Pods can also make\nuse of exclusive CPUs using the\n[`static`](/docs/tasks/administer-cluster/cpu-management-policies/) CPU management policy.",
            "QoS classes are used by Kubernetes to decide which Pods to evict from a Node experiencing Node Pressure.",
            "Memory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes. Memory requests and limits of containers in pod are used to set specific interfaces `memory.min`\nand `memory.high` provided by the memory controller."
          ]
        },
        {
          "title": "Configure Quality of Service for Pods",
          "url": "https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/",
          "excerpts": [
            "Aug 24, 2023 — Create a Pod that gets assigned a QoS class of Guaranteed · Every Container in the Pod must have a memory limit and a memory request. · For every ...",
            "This page shows how to configure Pods so that they will be assigned particular Quality of Service (QoS) classes."
          ]
        },
        {
          "title": "sched_setscheduler(2) - Linux man page",
          "url": "https://linux.die.net/man/2/sched_setscheduler",
          "excerpts": [
            "A call to sched_setscheduler() or sched_setparam(2) will put the SCHED_FIFO (or SCHED_RR) process identified by pid at the start of the list if it was runnable."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html",
          "excerpts": [
            "nohz_full=      [KNL,BOOT]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified list of CPUs whose tick will be stopped\n                        whenever possible. The boot CPU will be forced outside\n                        the range to maintain the timekeeping. Any CPUs\n                        in this list will have their RCU callbacks offloaded,\n                        just as if they had also been called out in the\n                        rcu_nocbs= boot parameter.",
            "   isolcpus=       [KNL,SMP] Isolate CPUs from the general scheduler. The argument is a cpu list, as described above. This option can be used to specify one or more CPUs\n                        to isolate from the general SMP balancing",
            "rcu_nocbs=      [KNL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y, set\n                        the specified list of CPUs to be no-callback CPUs. Invocation of these CPUs' RCU callbacks will\n                        be offloaded to \"rcuox/N\" kthreads created for\n                        that purpose, where \"x\" is \"b\" for RCU-bh, \"p\"\n                        for RCU-preempt, and \"s\" for RCU-sched, and \"N\"\n                        is the CPU number. This reduces OS jitter on the\n                        offloaded CPUs, which can be useful for HPC and\n                        real-time workloads. It can also improve energy\n                        efficiency for asy"
          ]
        },
        {
          "title": "Use Kubernetes CPU Manager Static Policy's Guaranteed QoS ...",
          "url": "https://docs.starlingx.io/usertasks/kubernetes/using-kubernetes-cpu-manager-static-policy.html",
          "excerpts": [
            "Missing: Topology HugePages cgroup",
            "You can launch a container pinned to a particular set of CPU cores using the Kubernetes CPU manager static policy's Guaranteed QoS class."
          ]
        },
        {
          "title": "Kubernetes v1.32 Adds A New CPU Manager Static Policy ...",
          "url": "https://kubernetes.io/blog/2024/12/16/cpumanager-strict-cpu-reservation/",
          "excerpts": [
            "Dec 16, 2024 — The CPU Manager static policy is used to reduce latency or improve performance. The reservedSystemCPUs defines an explicit CPU set for OS system daemons and ...",
            "The CPU Manager static policy is used to reduce latency or improve performance. The `reservedSystemCPUs` defines an explicit CPU set for OS system daemons and kubernetes system daemons. This option is designed for Telco/NFV type use cases where uncontrolled interrupts/timers may impact the workload performance.",
            "However, until the Kubernetes v1.32 release, this isolation was only implemented for guaranteed\npods that made requests for a whole number of CPUs. At pod admission time, the kubelet only\ncompares the CPU _requests_ against the allocatable CPUs. In Kubernetes, limits can be higher than\nthe requests; the previous implementation allowed burstable and best-effort pods to use up\nthe capacity of `reservedSystemCPUs` , which could then starve host OS services of CPU - and we\nknow that people saw this in real life deployments.",
            "When this new `strict-cpu-reservation` policy option is enabled, the CPU Manager static policy will not allow any workload to use the reserved system CPU cores.",
            "To enable this feature, you need to turn on both the `CPUManagerPolicyAlphaOptions` feature gate and the `strict-cpu-reservation` policy option. And you need to remove the `/var/lib/kubelet/cpu_manager_state` file if it exists and restart kubelet.",
            "If you want to protect your system daemons and interrupt processing, the obvious way is to use the `reservedSystemCPUs` option.",
            "```yaml",
            "kind : KubeletConfiguration",
            "apiVersion : kubelet.config.k8s.io/v1beta1",
            "featureGates :",
            "  ...",
            "  CPUManagerPolicyOptions : true",
            "  CPUManagerPolicyAlphaOptions : true",
            "cpuManagerPolicy : static",
            "CPUManagerPolicyOptions :",
            "  strict-cpu-reservation : \"true\"",
            "reservedSystemCPUs : \"0,32,1,33,16,48\"",
            "When `strict-cpu-reservation` is not set or set to false:",
            "{\"policyName\":\"static\",\"defaultCpuSet\":\"0-63\",\"checksum\":1058907510}",
            "When `strict-cpu-reservation` is set to true:",
            "```console",
            "```console",
            "# cat /var/lib/kubelet/cpu_manager_state",
            "# cat /var/lib/kubelet/cpu_manager_state",
            "{\"policyName\":\"static\",\"defaultCpuSet\":\"2-15,17-31,34-47,49-63\",\"checksum\":4141502832}",
            "```",
            "```",
            "```",
            "## Monitoring the feature",
            "You can monitor the feature impact by checking the following CPU Manager counters:",
            "* `cpu_manager_shared_pool_size_millicores` : report shared pool size, in millicores (e.g. 13500m)",
            "* `cpu_manager_exclusive_cpu_allocation_count` : report exclusively allocated cores, counting full cores (e.g. 16)",
            "Your best-effort workloads may starve if the `cpu_manager_shared_pool_size_millicores` count is zero for prolonged time. We believe any pod that is required for operational purpose like a log forwarder should not run as best-effort, but you can review and adjust the amount of CPU cores reserved as needed.",
            "## Conclusion",
            "Strict CPU reservation is critical for Telco/NFV use cases.",
            "It is also a prerequisite for enabling the all-in-one type of deployments where workloads are placed on nodes serving combined control+worker+storage roles."
          ]
        },
        {
          "title": "The WebAssembly Component Model - WIT Overview",
          "url": "https://component-model.bytecodealliance.org/design/wit.html",
          "excerpts": [
            "```"
          ]
        },
        {
          "title": "Kubernetes: How to use NUMA",
          "url": "https://medium.com/@martinko.komorny/kubernetes-appreciate-your-resources-dbbeaa73be74",
          "excerpts": [
            "Let's explore together the possibilities of utilizing NUMA in Kubernetes for efficient resource management on larger servers."
          ]
        },
        {
          "title": "Requests, Limits & QoS (with examples) : r/kubernetes",
          "url": "https://www.reddit.com/r/kubernetes/comments/1jyd114/kubernetes_resources_explained_requests_limits/",
          "excerpts": [
            "QoS Classes in Kubernetes: Guaranteed: Requests = Limits for all containers. Most protected. Burstable: Some requests, some limits, but not ..."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ...",
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_",
            "Turn HT (Hyper-threading) ON. (double the vcore run queues --> umpteen times lower likelihood of waiting for a cpu).",
            "They are all nice for advanced stuff, but I like to **clean the system up first, and only assign cores to things after tha"
          ]
        },
        {
          "title": "Control Topology Management Policies on a node",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/",
          "excerpts": [
            "Topology Manager_ is a kubelet component that aims to coordinate the set of components that are\nresponsible for these optimizations",
            "The Topology Manager receives topology information from the _Hint Providers_ as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication.",
            "The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result.",
            "To align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node.",
            "To align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory\nManager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to [Memory Manager](/docs/tasks/administer-cluster/memory-manager/) documentation.",
            " level. And the `policy` defines the actual policy used to\ncarry out the alignment, for example, `best-effort` , `restricted` , and `single-numa-node` . Details on the various `scopes` and `policies` available today can be found below. #### Note:\n\nTo align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node. See [Control CPU Management Policies on the Node](/docs/tasks/administer-cluster/cpu-management-policies/) . #### Note:\n\nTo align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory\nManager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to [Memory Manager](/docs/tasks/administer-cluster/memory-manager/) documentation. ## Topology manager scopes\n\nThe Topology Manager can deal with the alignment of resources in a couple of distinct scopes:\n\n* `container` (default)\n* `pod`\n\nEither option can be selected at a time of the kubelet startup, by setting the `topologyManagerScope` in the [kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) . ### `container` scope\n\nThe `container` scope is used by default.\n ... \nIn the case of the `Guaranteed` pod with integer CPU request, the `static` CPU Manager policy\nwould return topology hints relating to the exclusive CPU and the Device Manager would send back\nhints for the requested device. In the case of the `Guaranteed` pod with sharing CPU request, the `static` CPU Manager policy\nwould return default topology hint as there is no exclusive CPU request and the Device Manager\nwould send back hints for the requested device. In the above two cases of the `Guaranteed` pod, the `none` CPU Manager policy would return default\ntopology hint. In the case of the `BestEffort` pod, the `static` CPU Manager policy would send back the default\ntopology hint as there is no CPU request and the Device Manager would send back the hints for each\nof the requested devices. Using this information the Topology Manager calculates the optimal hint for the pod and stores\nthis information, which will be used by the Hint Providers when they are making their resource\nassignments. ## Known limitations\n\n1. The maximum number of NUMA nodes that Topology Manager allows is 8. With more than 8 NUMA nodes,\n   there will be a state explosion when trying to enumerate the possible NUMA affinities and\n   generating their hints. See [`max-allowable-numa-nodes`]() (beta) for more options. 2. The scheduler is not topology-aware, so it is possible to be scheduled on a node and then fail\n   on the node due to"
          ]
        },
        {
          "title": "Utilizing the NUMA-aware Memory Manager",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/memory-manager/",
          "excerpts": [
            "The Kubernetes Memory Manager enables the feature of guaranteed memory (and hugepages) allocation for pods in the Guaranteed QoS class.",
            "The Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod.",
            "The Memory Manager feeds the central manager (Topology Manager) with these affinity hints.",
            " Starting from v1.22, the Memory Manager is enabled by default through `MemoryManager`\n[feature gate]",
            "The Memory Manager is a Hint Provider, and it provides topology hints for\nthe Topology Manager which then aligns the requested resources according to these topology hints."
          ]
        },
        {
          "title": "4 Full-featured cpuset manipulation commands",
          "url": "https://documentation.suse.com/en-us/sle-rt/15-SP7/html/SLE-RT-all/cha-shielding-cpuset.html",
          "excerpts": [
            "The cset subcommand creates the cpusets based on a mounted cpuset file system. You do not need to know where that file system is mounted, although it is easy to ..."
          ]
        },
        {
          "title": "cset-shield - cpuset supercommand which implements cpu ...",
          "url": "https://manpages.ubuntu.com/manpages/trusty/man1/cset-shield.1.html",
          "excerpts": [
            "This is a supercommand that creates basic cpu shielding. The normal cset commands can of course be used to create this basic shield."
          ]
        },
        {
          "title": "2 The Basic Shielding Model",
          "url": "https://documentation.suse.com/en-us/sle-rt/12-SP5/html/SLE-RT-all/cha-shielding-model.html",
          "excerpts": [
            "The shield subcommand manages all of these cpuset s and lets you define the CPUs and memory nodes that are in the shielded and unshielded sets."
          ]
        },
        {
          "title": "numactl(8) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man8/numactl.8.html",
          "excerpts": [
            "Multiple nodes may be specified on --interleave, --membind and --cpunodebind. --weighted-interleave=nodes, -w nodes Set a weighted memory interleave policy."
          ]
        },
        {
          "title": "crictl inspect does not show any information about the ...",
          "url": "https://github.com/cri-o/cri-o/issues/3963",
          "excerpts": [
            "The crictl inspect shows CPU quota, but I did not find any place where do we update it for the container, except the update for the sandbox."
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "As A has \"cpu\" and \"memory\" enabled, A will control the distribution of CPU cycles and memory to its children, in this case, B.",
            "In all the above models, cycles distribution is defined only on a temporal\nbase and it does not account for the frequency at which tasks are executed.",
            "All weights are in the range [1, 10000] with the default at 100.",
            "\"io.max\" limits the maximum BPS and/or IOPS that a cgroup can consume",
            "This\ncontroller implements weight and absolute bandwidth limit models for\nnormal scheduling policy and absolute bandwidth allocation model for\nrealtime scheduling policy.",
            "cpu.rt.max\" hard-allocates realtime slices and is an example of this\ntype.",
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "SMP IRQ affinity - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/core-api/irq/irq-affinity.html",
          "excerpts": [
            "It's a bitmask (smp_affinity) or cpu list (smp_affinity_list) of allowed CPUs. It's not allowed to turn off all CPUs, and if an IRQ controller does not support ..."
          ]
        },
        {
          "title": "4.3. Interrupts and IRQ Tuning | Red Hat Enterprise Linux | 6",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/performance_tuning_guide/s-cpu-irq",
          "excerpts": [
            "IRQs have an associated \"affinity\" property, smp_affinity , which defines the CPU cores that are allowed to execute the ISR for that IRQ. This property can be ..."
          ]
        },
        {
          "title": "Linux SCHED_OTHER, SCHED_FIFO and SCHED_RR",
          "url": "https://stackoverflow.com/questions/9392415/linux-sched-other-sched-fifo-and-sched-rr-differences",
          "excerpts": [
            "Here is the differences between SCHED_OTHER, SCHED_FIFO and SCHED_RR based on Linux Manual (http://man7.org/linux/man-pages/man7/sched.7.html)."
          ]
        },
        {
          "title": "numactl --membind - Stack Overflow",
          "url": "https://stackoverflow.com/questions/14669900/numactl-membind",
          "excerpts": [
            "The manpage for numactl says : --membind=nodes, -m nodes Only allocate memory from nodes. Allocation will fail when there is not enough memory available on ..."
          ]
        },
        {
          "title": "mlockall",
          "url": "http://man.he.net/man2/mlockall",
          "excerpts": [
            "NAME mlock, mlock2, munlock, mlockall, munlockall - lock and unlock memory ... RLIMIT_MEMLOCK soft resource limit, but tried to lock more memory than ..."
          ]
        },
        {
          "title": "mlock(2) - Linux manual page",
          "url": "https://man7.org/linux/man-pages/man2/mlock.2.html",
          "excerpts": [
            "Such double accounting could incorrectly calculate a \"total locked memory\" value for the process that exceeded the RLIMIT_MEMLOCK limit, with the result that ..."
          ]
        },
        {
          "title": "getrlimit(2) - Linux manual page",
          "url": "https://man7.org/linux/man-pages/man2/getrlimit.2.html",
          "excerpts": [
            "RLIMIT_MEMLOCK This is the maximum number of bytes of memory that may be ... This limit affects mlock(2), mlockall(2), and the mmap(2) MAP_LOCKED operation."
          ]
        },
        {
          "title": "numactl(8) - Linux man page - die.net",
          "url": "https://linux.die.net/man/8/numactl",
          "excerpts": [
            "numactl runs processes with a specific NUMA scheduling or memory placement policy. The policy is set for command and inherited by all of its children."
          ]
        },
        {
          "title": "opencontainers/runc: CLI tool for spawning and running ...",
          "url": "https://github.com/opencontainers/runc",
          "excerpts": [
            "runc is a CLI tool for spawning and running containers on Linux according to the OCI specification. Releases. You can find official releases of runc on the ..."
          ]
        },
        {
          "title": "Topology Manager - Platform9 Docs",
          "url": "http://platform9.com/docs/kubernetes/topology-manager",
          "excerpts": [
            "The Topology Manager is a component of kubelet that coordinates the factors responsible for CPU and other hardware acceleration optimizations.",
            "Aug 7, 2023 — Single NUMA node: If kubelet employs single-numa-node topology management policy, kubelet calls each Hint Provider to locate the available ..."
          ]
        },
        {
          "title": "Usage, constrains and implications of isolcpus=, nohz_full= and rcu_nocbs= kernel parameters",
          "url": "https://access.redhat.com/articles/3720611",
          "excerpts": [
            "Usage, constrains and implications of isolcpus=, nohz_full= and rcu_nocbs= kernel parameters. Updated April 9 2024 at 6:34 AM -. English.",
            "Updated April 9 2024 at 6:34 AM - English"
          ]
        },
        {
          "title": "Linux Kernel Documentation: cgroup v2",
          "url": "https://www.kernel.org/doc/Documentation/cgroup-v2.txt",
          "excerpts": [
            "The \"io\" controller regulates the distribution of IO resources. This\ncontroller implements both weight based and absolute bandwidth or IOPS\nlimit distribution; however, weight based distribution is available\nonly if cfq-iosched is in use and neither scheme is available for\nblk-mq devices.",
            "WARNING: cgroup2 doesn't yet support control of realtime processes and the cpu controller can only be enabled when all RT processes are in the root cgroup.",
            "sec\ncpu.weight\nA read-write single value file which exists on non-root\ncgroups. The default is \"100\". The weight in the range [1, 10000].",
            "ight. cpu.max\nA read-write two value file which exists on non-root cgroups. The default is \"max 100000\". The maximum bandwidth limit. It's in the following format::\n$MAX $PERIOD\nwhich indicates that the group may consume upto $MAX in each\n$PERIOD duration. \"max\" for $MAX indicates no limit.",
            "The \"cpu\" controllers regulates distribution of CPU cycles. This\ncontroller implements weight and absolute bandwidth limit models for\nnormal scheduling policy and absolute bandwidth allocation model for\nrealtime scheduling policy.",
            "\"io.max\" limits the maximum BPS and/or IOPS that a cgroup can consume\non an IO device and is an example of this type.",
            "IO Interface Files\n~~~~~~~~~~~~~~~~~",
            " the following format::\n\n\t  $MAX $PERIOD\n\n\twhich indicates that the group may consume upto $MAX in each\n\t$PERIOD duration. \"max\" for $MAX indicates no limit",
            "All memory amounts are in bytes. If a value which is not aligned to\nPAGE_SIZE is written, the value may be rounded up to the closest\nPAGE_SIZE multiple when read back."
          ]
        },
        {
          "title": "Wasm containers and crun: OpenSource.com article on wasm containers",
          "url": "https://opensource.com/article/22/10/wasm-containers",
          "excerpts": [
            "One container runtime that can handle the task: crun. Crun is fast, has a low-memory footprint, and is a fully OCI-compliant container runtime that can be used as a drop-in replacement for your existing container runtime.",
            "crun-wasm-handler`, which delegates specially configured container images (a *Wasm compat image*) to the parts of existing Wasm runtimes in a native approach inside the crun sandbox",
            "Crun has native integration with [`wasmedge`](https://wasmedge.org/), [`wasmtime`](https://wasmtime.dev/), and [`wasmer`](https://wasmer.io/) to support this functionality out of the box.",
            "Podman runs the requested Wasm compat image `mywasm-image:latest` using crun's Wasm handler and returns output confirming that our workload was executed."
          ]
        },
        {
          "title": "How do I get tickless kernel to work? nohz_full, rcu_nocbs ...",
          "url": "https://stackoverflow.com/questions/20133523/how-do-i-get-tickless-kernel-to-work-nohz-full-rcu-nocbs-isolcpus-what-else",
          "excerpts": [
            "I thought nohz_full means when there's only one process running, it'll stop interrupting. The following is the output of /proc/sched_debug when ..."
          ]
        },
        {
          "title": "Performance difference between cpuset-cpu and ...",
          "url": "https://github.com/moby/moby/issues/38839",
          "excerpts": [
            "Use K8s to request and limit 1 CPU for container; Use docker command: docker run –cpuset-cpus=0 image command to limit 1 CPU for container; Use ..."
          ]
        },
        {
          "title": "Limiting Container Resources — SingularityCE User Guide ...",
          "url": "https://docs.sylabs.io/guides/4.0/user-guide/cgroups.html",
          "excerpts": [
            "So if you want to limit amount of CPU time to 20ms during period of 100ms: [cpu] period = 100000 quota = 20000. cpus/mems. You can also restrict access to ..."
          ]
        },
        {
          "title": "Low Latency Tuning Guide | Erik Rigtorp",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "This guide describes how to tune your AMD64/x86_64 hardware and Linux system for running real-time or low latency workloads."
          ]
        },
        {
          "title": "Limiting Container Resources with Cgroups",
          "url": "https://apptainer.org/docs/user/1.0/cgroups.html",
          "excerpts": [
            "Using cgroups you can limit memory and CPU usage. You can also rate limit block IO, network IO, and control access to device nodes."
          ]
        },
        {
          "title": "Error setting rlimits type 8 (operation not permitted",
          "url": "https://discuss.linuxcontainers.org/t/error-setting-rlimits-type-8-operation-not-permitted-unknown-in-lxd-container/9976",
          "excerpts": [
            "Jan 18, 2021 — This tries to increase RLIMIT_MEMLOCK in the container which works. The problem usually is that you're exceeding the limit set in the initial user namespace."
          ]
        },
        {
          "title": "Restricting cpu cores in a docker container",
          "url": "https://stackoverflow.com/questions/46621754/restricting-cpu-cores-in-a-docker-container",
          "excerpts": [
            "This means processes in container can be executed on cpu 0, cpu 1 and cpu 2. The --cpu-quota flag limits the container's CPU usage. The ..."
          ]
        },
        {
          "title": "cri: Support configuration of process RLIMIT_NOFILE limits",
          "url": "https://github.com/containerd/containerd/issues/6063",
          "excerpts": [
            "Sep 28, 2021 — There was a previous issue open for configurable support of rlimits in cri-containerd, but it was closed without the support being added."
          ]
        },
        {
          "title": "Docker container fails to start with CGroup v2 and partition ...",
          "url": "https://forums.docker.com/t/docker-container-fails-to-start-with-cgroup-v2-and-partition-other-than-member/141284",
          "excerpts": [
            "Docker is unable to start a container when I have a CGroup slice that is isolated or rooted, meaning not using the default set of CPUs from CGroup tree /."
          ]
        },
        {
          "title": "NUMA-aware memory manager and Topology ...",
          "url": "https://github.com/kubernetes/kubernetes/issues/128708",
          "excerpts": [
            "Set the memory manager policy to \"Static\" and topology manager policy to \"restricted\". On a two-NUMA-node worker node create a smallish Pod with ..."
          ]
        },
        {
          "title": "Chapter 7. Using CPU Manager and Topology Manager",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.12/html/scalability_and_performance/using-cpu-manager",
          "excerpts": [
            "If a single NUMA node affinity is not possible, the Topology Manager rejects the pod from the node. This results in a pod in a Terminated state with a pod ..."
          ]
        },
        {
          "title": "Kubernetes Documentation - About cgroup v2",
          "url": "https://kubernetes.io/docs/concepts/architecture/cgroups/",
          "excerpts": [
            "cgroup v2 is the next version of the Linux `cgroup` API. cgroup v2 provides a\nunified control system with enhanced resource management\ncapabilit",
            "cgroup v2 offers several improvements over cgroup v1, such as the following:",
            "Some Kubernetes features exclusively use cgroup v2 for enhanced resource\nmanagement and isolation. For example, the [MemoryQoS](/docs/concepts/workloads/pods/pod-qos/) feature improves memory QoS\nand relies on cgroup v2 primitives.",
            "Apr 20, 2024 — cgroup v2 is the next version of the Linux cgroup API. cgroup v2 provides a unified control system with enhanced resource management capabilities."
          ]
        },
        {
          "title": "Kubernetes resource management - Manage resources with cgroups, QoS, and kubelet policies",
          "url": "https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/",
          "excerpts": [
            "Missing: jitter QoS ## How Kubernetes applies resource requests and limits",
            "When the kubelet starts a container as part of a Pod, the kubelet passes that container's",
            "requests and limits for memory and CPU to the container runtime. On Linux, the container runtime typically configures",
            "kernel [cgroups](/docs/reference/glossary/?all=true \"A group of Linux processes with optional resource isolation, accounting and limits.\") that apply and enforce the",
            "limits you defined. * The CPU limit defines a hard ceiling on how much CPU time the container can use. During each scheduling interval (time slice), the Linux kernel checks to see if this",
            "  limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution. * The CPU request typically defines a weighting. If several different containers (cgroups)",
            "  want to run on a contended system, workloads with larger CPU requests are allocated more",
            "  CPU time than workloads with small requests. * The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses",
            "  cgroups v2, the container runtime might use the memory request as a hint to set `memory.min` and `memory.low` . * The memory limit defines a memory limit for that cgroup. If the container tries to",
            "  allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates",
            "  and, typically, intervenes by stopping one of the processes in the container that tried",
            "  to allocate memory.",
            "When you specify a Pod, you can optionally specify how much of each resource a container needs. The most common resources to specify are CPU and memory ...",
            "Manage HugePages · Schedule GPUs · Tutorials · Hello Minikube · Learn Kubernetes Basics · Create a Cluster · Using Minikube to Create a Cluster · Deploy an App.",
            "For example, on a system where the default page size is 4KiB, you could specify a limit, hugepages-2Mi: 80Mi . If the container tries allocating over 40 2MiB ..."
          ]
        },
        {
          "title": "An introduction to control groups (cgroups) v2",
          "url": "https://man7.org/conf/ndctechtown2021/cgroups-v2-part-1-intro-NDC-TechTown-2021-Kerrisk.pdf",
          "excerpts": [
            "Control groups (cgroups) v2 are a mechanism for hierarchically grouping processes, managed by controllers, and allow limiting resource usage and prioritizing ..."
          ]
        },
        {
          "title": "3.4. cpuset | Resource Management Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/resource_management_guide/sec-cpuset",
          "excerpts": [
            "The cpuset subsystem assigns individual CPUs and memory nodes to cgroups. Each cpuset can be specified according to the following parameters."
          ]
        },
        {
          "title": "cpuset(7) - Linux manual page",
          "url": "https://man7.org/linux/man-pages/man7/cpuset.7.html",
          "excerpts": [
            "A cpuset defines a list of CPUs and memory nodes. The CPUs of a system include all the logical processing units on which a process can execute, including, if ..."
          ]
        },
        {
          "title": "Chapter 27. Configuring CPU Affinity and NUMA policies ...",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/assembly_configuring-cpu-affinity-and-numa-policies-using-systemd_managing-monitoring-and-updating-the-kernel",
          "excerpts": [
            "To configure CPU affinity mask for a particular systemd service, systemd provides CPUAffinity= both as a unit file option and a manager configuration option."
          ]
        },
        {
          "title": "systemd.resource-control(5)",
          "url": "https://man.archlinux.org/man/systemd.resource-control.5.en",
          "excerpts": [
            "In this hierarchy, the cpu controller is enabled for all units shown except b1.service and b2.service. Because there is no explicit configuration for system."
          ]
        },
        {
          "title": "systemd.exec",
          "url": "https://www.freedesktop.org/software/systemd/man/systemd.exec.html",
          "excerpts": [
            "This man page lists the configuration options shared by these four unit types. See systemd.unit(5) for the common options of all unit configuration files, and ...",
            "Alternatively, takes a special \"numa\" value in which case systemd automatically derives allowed CPU range based on the value of NUMAMask= option. CPU ranges ..."
          ]
        },
        {
          "title": "systemd.directives(7) - Linux manual page",
          "url": "https://man7.org/linux/man-pages/man7/systemd.directives.7.html",
          "excerpts": [
            "... AllowedCPUs= systemd.resource-control(5) AllowedMemoryNodes= systemd ... $CACHE_DIRECTORY systemd.exec(5) $CONFIGURATION_DIRECTORY systemd.exec(5) ...",
            "... AllowedCPUs= systemd.resource-control(5) AllowedMemoryNodes= systemd ... systemd.resource-control(5), systemd.service(5). DNS RESOURCE RECORD TYPES ..."
          ]
        },
        {
          "title": "Chapter 25. Using systemd to manage resources ...",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/10/html/managing_monitoring_and_updating_the_kernel/using-systemd-to-manage-resources-used-by-applications",
          "excerpts": [
            "When you configure a strict NUMA policy, for example bind , make sure that you also appropriately set the CPUAffinity= unit file option. Procedure. Set the NUMA ..."
          ]
        },
        {
          "title": "Runtime Class",
          "url": "https://kubernetes.io/docs/concepts/containers/runtime-class/",
          "excerpts": [
            "Oct 29, 2022 — RuntimeClass is a feature for selecting the container runtime configuration. The container runtime configuration is used to run a Pod's containers."
          ]
        },
        {
          "title": "Cpu Manager - Intel® Smart Edge Open",
          "url": "https://intelsmartedge.github.io/docs/components/resource-management/cpu-manager/",
          "excerpts": [
            "Kubernetes CPU Management needs CPU manager policy to be set to static which is a default option in Intel® Smart Edge Open. This can be adjusted in the ESP ..."
          ]
        },
        {
          "title": "Chapter 13. What huge pages do and how they are ...",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.5/html/scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed",
          "excerpts": [
            "A huge page is a memory page that is larger than 4Ki. On x86_64 architectures, there are two common huge page sizes: 2Mi and 1Gi. Sizes vary on other ..."
          ]
        },
        {
          "title": "Memory Controller - cgroup2",
          "url": "https://facebookmicrosites.github.io/cgroup2/docs/memory-controller.html",
          "excerpts": [
            "Memory utilization is a key area where resource control can make big efficiency improvements. In this section we’ll look in detail at the cgroup2 memory controller, and how to get started configuring its interface files for controlling system memory resources.",
            "memory.high is the memory usage throttle limit. This is the main mechanism to control a cgroup's memory use."
          ]
        },
        {
          "title": "systemd.resource-control (OpenAI parsing of the provided excerpt)",
          "url": "https://www.freedesktop.org/software/systemd/man/systemd.resource-control.html",
          "excerpts": [
            " `AllowedCPUs=` , `StartupAllowedCPUs=` [¶](= \"Permalink to this term\")\n\n    This setting controls the `cpuset` controller in the unified hierarchy. Restrict processes to be executed on specific CPUs. Takes a list of CPU indices or ranges separated by either\n\n     whitespace or commas. CPU ranges are specified by the lower and upper CPU indices separated by a dash. ",
            "The effective configuration is\n\n     reported as `EffectiveCPUs=` . While `StartupAllowedCPUs=` applies to the startup and shutdown phases of the system, `AllowedCPUs=` applies to normal runtime of the system, and if the former is not set also to\n\n     the startup and shutdown phases. Using `StartupAllowedCPUs=` allows prioritizing specific services at\n\n     boot-up and shutdown differently than during normal runtime. This setting is supported only with the unified control group hierarchy. Add",
            "These settings control the `memory` controller in the unified hierarchy. Specify the absolute limit on memory usage of the executed processes in this unit. If memory usage\n\n     cannot be contained under the limit, out-of-memory killer is invoked inside the unit. It is recommended to\n\n     use `MemoryHigh=` as the main control mechanism and use `MemoryMax=` as the\n\n     last line of defense. Takes a memory size in bytes. If the value is suffixed with K, M, G or T, the specified memory size is\n\n     parsed as Kilobytes, Megabytes, Gigabytes, or Terabytes (with the b",
            "StartupMemoryMax= bytes",
            "These settings control the `cpuset` controller in the unified hierarchy. Restrict processes to be executed on specific memory NUMA nodes. Takes a list of memory NUMA nodes indices\n\n     or ranges separated by either whitespace or commas. Memory NUMA nodes ranges are specified by the lower and upper\n\n     NUMA nodes indices separated by a dash. Setting `AllowedMemoryNodes=` or `StartupAllowedMemoryNodes=` does not\n\n     guarantee that all of the memory NUMA nodes will be used by the processes as it may be limited by parent units. The effective configuration is reported as `EffectiveMemoryNodes=` . While `StartupAllowedMemoryNodes=` applies to the startup and shutdown phases of the system, `AllowedMemoryNodes=` applies to normal runtime of the system, and if the former is not set also to\n\n     the startup and shutdown phases. Using `StartupAllowedMemoryNodes=` allows prioritizing specific services at\n\n     boot-up and shutdown differently than during normal runtime. This setting is supported only with the unified control group hi",
            "These settings control the `cpuset` controller in the unified hierarchy. Restrict processes to be executed on specific memory NUMA nodes.",
            "MemoryMax= bytes"
          ]
        },
        {
          "title": "systemd.resource-control - systemd ",
          "url": "https://www.freedesktop.org/software/systemd/man/247/systemd.resource-control.html",
          "excerpts": [
            "Restrict processes to be executed on specific CPUs.",
            "\nTakes a list of CPU indices or ranges separated by either\n\n     whitespace or ",
            "CPUQuota=20%",
            "Assign the duration over which the CPU time quota specified by `CPUQuota=` is measured.",
            "Restrict processes to be executed on specific memory NUMA nodes.",
            "MemoryMax= bytes",
            "Specify the absolute limit on memory usage of the executed processes in this unit.",
            "If memory usage\n\n     cannot be contained under the limit, out-of-memory killer is invoked inside the"
          ]
        },
        {
          "title": "cgroup-v2.rst",
          "url": "https://www.kernel.org/doc/Documentation/admin-guide/cgroup-v2.rst",
          "excerpts": [
            "This is the authoritative documentation on the design, interface and conventions of cgroup v2. It describes all userland-visible aspects of cgroup including ..."
          ]
        },
        {
          "title": "Who's killing my pods? | (blog 'zezin)",
          "url": "https://gjhenrique.com/cgroups-k8s/",
          "excerpts": [
            "The cgroups API, like a lot of Linux abstractions, uses files to interface with userspace. You can limit the amount of memory of a process, CPU usage and the ..."
          ]
        },
        {
          "title": "Manage HugePages - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/",
          "excerpts": [
            "Kubernetes supports the allocation and consumption of pre-allocated huge pages by applications in a Pod. This page describes how users can consume huge pages."
          ]
        },
        {
          "title": "Container Runtimes - Kubernetes",
          "url": "https://kubernetes.io/docs/setup/production-environment/container-runtimes/",
          "excerpts": [
            "If you configure systemd as the cgroup driver for the kubelet, you must also configure systemd as the cgroup driver for the container runtime.",
            "Jun 30, 2025 — The systemd cgroup driver is recommended if you use cgroup v2. Note: If you installed containerd from a package (for example, RPM or .deb ) ..."
          ]
        },
        {
          "title": "Kubernetes CPU Manager Policies - the StarlingX Documentation",
          "url": "https://docs.starlingx.io/admintasks/kubernetes/kubernetes-cpu-manager-policies.html",
          "excerpts": [
            "When using the static policy, improved performance can be achieved if you also use the Isolated CPU behavior as described at Isolating CPU Cores to Enhance ..."
          ]
        },
        {
          "title": "Chapter 7. Using Topology Manager",
          "url": "https://docs.redhat.com/en/documentation/openshift_container_platform/4.3/html/scalability_and_performance/using-topology-manager",
          "excerpts": [
            "Using this information, the Topology Manager determines if a single NUMA Node affinity is possible. If it is, the pod will be admitted to the node. If this ..."
          ]
        },
        {
          "title": "How to correctly use runtimeClassName e.g. with wasmedge-v1",
          "url": "https://github.com/knative/serving/issues/15638",
          "excerpts": [
            "The RuntimeClass is used to determine which container runtime is used to run all containers in a pod. So I suspect QP needs to become compatible ..."
          ]
        },
        {
          "title": "Installation - Runwasi Developer Documentation",
          "url": "https://runwasi.dev/getting-started/installation.html",
          "excerpts": [
            "Download the appropriate shim for your preferred WebAssembly runtime: containerd-shim-wasmtime-v1 - for Wasmtime runtime; containerd-shim-wasmedge-v1 - for ..."
          ]
        },
        {
          "title": "Documentation/admin-guide/cgroup-v2.rst - kernel/common",
          "url": "https://android.googlesource.com/kernel/common/+/429fec28c835/Documentation/admin-guide/cgroup-v2.rst",
          "excerpts": [
            "cgroup is largely composed of two parts - the core and controllers. cgroup core is primarily responsible for hierarchically organizing. processes. A cgroup ..."
          ]
        },
        {
          "title": "Reserve Compute Resources for System Daemons",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/",
          "excerpts": [
            "Jun 4, 2024 — The kubelet exposes a feature named 'Node Allocatable' that helps to reserve compute resources for system daemons."
          ]
        },
        {
          "title": "Reserved CPU and memory in Kubernetes nodes",
          "url": "https://medium.com/@danielepolencic/reserved-cpu-and-memory-in-kubernetes-nodes-65aee1946afd",
          "excerpts": [
            "The following resources are reserved for the kubelet and operating system: 255MiB of memory. 60m of CPU. On top of that, 100MB is reserved ..."
          ]
        },
        {
          "title": "systemd.resource-control(5) - Linux manual page",
          "url": "https://man7.org/linux/man-pages/man5/systemd.resource-control.5.html",
          "excerpts": [
            "While StartupAllowedCPUs= applies to the startup and shutdown phases of the system, AllowedCPUs= applies to normal runtime of the system, and if the former is ..."
          ]
        },
        {
          "title": "systemd.directives - Freedesktop.org",
          "url": "https://www.freedesktop.org/software/systemd/man/systemd.directives.html",
          "excerpts": [
            "AllowedCPUs= ¶. systemd.resource-control(5). AllowedMemoryNodes= ¶. systemd ... systemd-sysctl.service(8), systemd.exec(5). /proc/sys/kernel/core_pattern ..."
          ]
        },
        {
          "title": "bytecodealliance/wasmtime: A lightweight WebAssembly ...",
          "url": "https://github.com/bytecodealliance/wasmtime",
          "excerpts": [
            "Wasmtime is built on the optimizing Cranelift code generator to quickly generate high-quality machine code either at runtime or ahead-of-time. Wasmtime is ...",
            "Configurable. Wasmtime uses sensible defaults, but can also be configured to provide more fine-grained control over things like CPU and memory consumption.",
            "Wasmtime is built on the optimizing Cranelift code generator to quickly generate high-quality machine code either at runtime or ahead-of-time. Wasmtime is ..."
          ]
        },
        {
          "title": "Kubernetes Topology Manager Moves to Beta - Align Up!",
          "url": "https://kubernetes.io/blog/2020/04/01/kubernetes-1-18-feature-topoloy-manager-beta/",
          "excerpts": [
            "There may be multiple sockets on a single NUMA node, or individual CPUs of a single socket may be connected to different NUMA nodes."
          ]
        },
        {
          "title": "Kubernetes Node Resource Managers",
          "url": "https://kubernetes.io/docs/concepts/policy/node-resource-managers/",
          "excerpts": [
            " deployments. If the `strict-cpu-reservation` policy option is enabled, the static policy will not allow\nany workload to use the CPU cores specified in `reservedSystemCPUs",
            "Static policy options",
            "Here are the available policy options for the static CPU management policy,",
            "\n\n`align-by-socket` (alpha, hidden by default)",
            "\n\n`distribute-cpus-across-cores` (alpha, hidden by default)",
            "\n\n`distribute-cpus-across-numa` (beta, visible by default)",
            "\n\n`full-pcpus-only` (GA, visible by default)",
            "\n\n`strict-cpu-reservation` (beta, visible by default)",
            "The `reservedSystemCPUs` parameter in [KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/) ,\nor the deprecated kubelet command line option `--reserved-cpus` , defines an explicit CPU set for OS system daemons\nand kubernetes system daemons.",
            "By default, this isolation is implemented only for guaranteed pods with integer CPU requests not for burstable and best-effort pods\n(and guaranteed pods with fractional CPU requests).",
            "Admission is only comparing the CPU requests against the allocatable CPUs.",
            "Since the CPU limit is higher than the request, the default behaviour allows burstable and best-effort pods to use up the capacity\nof `reservedSystemCPUs` and cause host OS services to starve in real life deployments.",
            "\n\n* `CPUManagerPolicyBetaOptions` (default enabled)."
          ]
        },
        {
          "title": "Control CPU Management Policies on the Node - Kubernetes",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/",
          "excerpts": [
            "Static policy options",
            "Static policy options"
          ]
        },
        {
          "title": "W-Kernel: An OS Kernel Architecture Designed With Isolation ...",
          "url": "https://dl.acm.org/doi/fullHtml/10.1145/3637792.3637796",
          "excerpts": [
            "by S Li · 2023 · Cited by 2 — W-kernel is designed with these two goals: isolation and customizability. W-kernel is developed from scratch in Rust to leverage the power of modern safe ..."
          ]
        },
        {
          "title": "When Wasm Meets eBPF: Writing, Distributing, Loading ... - Eunomia",
          "url": "https://eunomia.dev/en/blogs/ebpf-wasm/",
          "excerpts": [
            "The biggest difference between them is that eBPF runs in the Linux kernel, while WebAssembly runs in user space . We want to make an attempt to integrate them: using Wasm to write universal eBPF programs that can be distributed to different versions and architectures of Linux kernels without the need for recompiling."
          ]
        },
        {
          "title": "eBPF and Wasm: Unifying Userspace Extensions With ...",
          "url": "https://eunomia.dev/others/miscellaneous/bpftime-kubecon-draft/",
          "excerpts": [
            "In contrast to Wasm's runtime checks, eBPF uses a verifier to analyze programs before they run. ... bpftime can work with both AF_XDP and DPDK.",
            "Apr 21, 2025 — bpftime can work with both AF_XDP and DPDK. You can run your XDP eBPF programs as if they were in the kernel, just load them with bpftime, and ..."
          ]
        },
        {
          "title": "Wasmjit: Kernel Mode WebAssembly Runtime for Linux",
          "url": "https://news.ycombinator.com/item?id=18070540",
          "excerpts": [
            "The recent work on AF_XDP seems like it could became established middleground sweet spot almost as efficient as kernel bypass and userspace ..."
          ]
        },
        {
          "title": "4. Hello World Sample Application - Documentation - DPDK",
          "url": "https://doc.dpdk.org/guides-1.8/sample_app_ug/hello_world.html",
          "excerpts": [
            "The Hello World sample application is an example of the simplest DPDK application that can be written. The application simply prints an “helloworld” message on ..."
          ]
        },
        {
          "title": "1. Introduction to the DPDK Sample Applications",
          "url": "https://doc.dpdk.org/guides/sample_app_ug/intro.html",
          "excerpts": [
            "The DPDK Sample Applications are small standalone applications that demonstrate various features of DPDK. They can be considered as a cookbook of DPDK features."
          ]
        },
        {
          "title": "Security Evaluation: eBPF vs. WebAssembly - Medium",
          "url": "https://medium.com/all-things-ebpf/security-evaluation-ebpf-vs-webassembly-738df2566b18",
          "excerpts": [
            "Notably, eBPF prioritizes kernel integrity through a stringent verification process, while WebAssembly emphasizes user security by preventing ..."
          ]
        },
        {
          "title": "WasmEdge",
          "url": "https://wasmedge.org/",
          "excerpts": [
            "High-performance. Compared with Linux containers, WasmEdge could be 100x faster at start-up and 20% faster at runtime. · WASI-like Extensions. Network sockets ...",
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "Simple example - WasmEdge Runtime",
          "url": "https://wasmedge.org/book/en/kubernetes/demo/wasi.html",
          "excerpts": [
            "In this article, I will show you how to build a container image for a WebAssembly application. It can then be started and managed by Kubernetes ecosystem tools."
          ]
        },
        {
          "title": "Request: Support sharing mutable memory between host and guest",
          "url": "https://github.com/WebAssembly/WASI/issues/594",
          "excerpts": [
            "The i32 offsets into linear memory defined by the Canonical ABI can be used directly by the host to access linear memory for the duration of a host import call."
          ]
        },
        {
          "title": "How io_uring and eBPF Will Revolutionize Programming in Linux",
          "url": "https://news.ycombinator.com/item?id=25222243",
          "excerpts": [
            "To this end, io_uring lets you submit I/O requests without you having to make a single system call. This is done via a special submission ...",
            "io_uring lets you check if some i/o is done by just checking a ... wasmer · Tuna-Fish on Nov 26, 2020 | parent | prev | next [–]. Never ..."
          ]
        },
        {
          "title": "Wasmtime horizontal scaling results in poor performance #4637",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/4637",
          "excerpts": [
            "Call madvise with io_uring: It saves syscall but I don't know if it will work to avoid scaling bottleneck. Maintain 2 index groups, one is dirty ...",
            "Missing: 10x blocking"
          ]
        },
        {
          "title": "WASM on exotic architectures (a 2025 roundup)",
          "url": "https://anil.recoil.org/notes/wasm-on-exotic-targets",
          "excerpts": [
            "On the opposite end of the architecture spectrum, we have a [Linux in-kernel WASM runtime](https://github.com/wasmerio/kernel-wasm). This one allows running userspace code within the kernel space, as motivated by:\n\n> Since WASM is a virtual ISA protected by a virtual machine, we don't need to rely on external hardware and software checks to ensure safety. Running WASM in the kernel avoids most of the overhead introduced by those checks, e.g. system call (context switching) and `copy_{from,to}_user`, therefore improving performance. > Also, having low-level control means that we can implement a lot of features that were heavy or impossible in userspace, like virtual memory tricks and handling of intensive kernel events (like network packet filtering)."
          ]
        },
        {
          "title": "An Interface for Linux: WALI (Thin Kernel Interfaces for Wasm)",
          "url": "https://arxiv.org/html/2312.03858v3",
          "excerpts": [
            "This paper proposes _thin kernel interfaces_ for Wasm, which directly expose OS userspace syscalls without breaking intra-process sandboxing, enabling a new class of virtualization with Wasm as a universal binary format.",
            "By virtualizing the bottom layer of userspace, kernel interfaces enable effortless application ISA portability, compiler backend reusability, and armor programs with Wasm’s built-in control flow integrity and arbitrary code execution protection.",
            "WALI syscalls correspond nearly 1-to-1 with native Linux syscalls and need only translate data between the virtualized syscall interface and the native Linux syscall interface and vice versa.",
            "ce versa. By design, most WALI calls are ”passthrough”, with low-overhead zero-copy operations with appropriate translation between the Wasm and Linux memory spaces.",
            "Kernel interfaces hold a unique position in the Wasm ecosystem without diminishing the use-cases of existing capability-based security APIs. They enable a new ecosystem providing traditionally native binary software stacks (e.g. managed edge systems, OS packages, consumer mobile/desktop applications, and WASI implementations) with both a viable virtualization target for safety and portability, and an opportunity to run alternative security models.",
            " The custom interpreter mechanism allowed building the entire libuvwasi implementation unmodified (passing all tests) and many of the currently supported syscall tests in Linux Test Project (LTP)",
            " thin kernel interfaces do not define an entirely new API surface against which applications need to be refactored, but rather faithfully model the existing underlying operating system.",
            "\nThe performance of a WALI implementation is highly dependent on the underlying Wasm engine, which can vary drastically in performance based on how quickly it executes Wasm bytecode via interpretation or compilation",
            "Linux allows registering interpreters for custom binary formats, enabling WALI .wasm files to be directly executable.",
            "Such layering makes high-level Wasm API implementations more portable , safe , and reusable , allowing them to work on any Wasm engine that exposes the same kernel interfa",
            "WALI is exceptionally thin ( ≈ \\\\approx ≈ 2000 LoC) compared to other APIs like WASI ( ≈ \\\\approx ≈ 6000 LoC prior to preview2), minimizing the TCB’s attack surface and allowing engine developers to support numerous security policies over a",
            "Mar 18, 2025 — This paper proposes thin kernel interfaces for Wasm, which directly expose OS userspace syscalls without breaking intra-process sandboxing."
          ]
        },
        {
          "title": "Wasm-bpf: Bridging WebAssembly and eBPF for Kernel Programmability",
          "url": "https://eunomia.dev/en/blogs/introduce-to-wasm-bpf-bpf-community/",
          "excerpts": [
            "Wasm-bpf is a new open-source project[1] that defines a set of abstract eBPF-related system interfaces and provides a corresponding development toolchain.",
            "Wasm is a binary format that many different languages can compile to, and this binary format can run on a wide range of operating systems and architectures.",
            "Wasm was initially developed as a secure sandbox for browsers, and has since evolved into a high-performance, cross-platform, and multi-language software sandbox environment for cloud-native software components.",
            "Wasm applications are isolated in a sandbox and can only access the resources explicitly allowed by the user, such as files or environment variables.",
            "Wasm-bpf solution allows Wasm applications and containers to access various system resources in the kernel through the eBPF virtual machine interface.",
            "Thanks to the broad support for eBPF in the Linux kernel, as well as its portability between different kernel versions and architectures, and the reliability of the kernel BPF verifier engine, we can still ensure application portability and security boundaries to a certain extent.",
            "For observability applications that require updating data processing plugins, there is no need to go through the process of recompiling and deploying the entire application.",
            "For large eBPF applications, Wasm can serve as a plugin extension platform: extensions can be delivered and reloaded directly from the control plane at runtime.",
            "Lightweight`: WebAssembly microservices consume 1% of the resources compared to Linux container applications, and the cold start time is only 1% compared to Linux container applications",
            "We have also practiced developing user space data processing plugins for eBPF using multiple different languages simultaneously based on Wasm.",
            "With the latest Wasm-bpf framework, there are more exploratory possibilities to be pursued."
          ]
        },
        {
          "title": "WALI: A thin Linux kernel interface for WebAssembly",
          "url": "https://popl24.sigplan.org/details/waw-2024-papers/2/WALI-A-thin-Linux-kernel-interface-for-WebAssembly",
          "excerpts": [
            "This talk introduces WALI: The WebAssembly Linux Interface, a thin layer over Linux's userspace system calls, creating a new class of virtualization."
          ]
        },
        {
          "title": "Stop Hiding The Sharp Knives: The WebAssembly Linux ...",
          "url": "https://arxiv.org/html/2312.03858v1",
          "excerpts": [
            "This paper proposes WALI: The WebAssembly Linux Interface, a thin layer over Linux's userspace system calls, creating a new class of virtualization."
          ]
        },
        {
          "title": "WebAssembly Linux Interface (WALI)",
          "url": "https://www.emergentmind.com/articles/2312.03858",
          "excerpts": [
            "The paper introduces WALI, a Linux interface that lets WebAssembly modules interact with native system calls. It details a design using ..."
          ]
        },
        {
          "title": "AF_XDP",
          "url": "https://docs.kernel.org/networking/af_xdp.html",
          "excerpts": [
            "\nredirect ingress frames to other XDP enabled netdevs, using the `bpf_redirect_map()` function. AF\\_XDP sockets enable the possibility for\nXDP programs ",
            "An AF\\_XDP socket (XSK) is created with the normal socket()\nsyscall",
            "There are a four different kind of rings: FILL, COMPLETION, RX and\nTX.",
            "AF\\_XDP sockets enable the possibility for\nXDP programs to redirect frames to a memory buffer in a user-space\napplication",
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "eBPF Summit 2025 bpftime talk - eunomia",
          "url": "https://eunomia.dev/others/miscellaneous/talk-ebpf-summit/",
          "excerpts": [
            "We've seen kernel-bypass solutions like DPDK and AF_XDP. They can offer faster packet processing by bypassing the kernel. But with bpftime, you ...",
            "Sep 6, 2024 — bpftime can work with both AF_XDP and DPDK. You can run your XDP eBPF programs as if they were in the kernel, just load them with bpftime, and ..."
          ]
        },
        {
          "title": "michaelmelanson/wasm-kernel: Runs WebAssembly ...",
          "url": "https://github.com/michaelmelanson/wasm-kernel",
          "excerpts": [
            "A small Rust project that produces a bootable binary with an embedded WebAssembly interpreter. It can boot in QEMU using the included UEFI firmware."
          ]
        },
        {
          "title": "wasm32-wali-linux-musl - The rustc book",
          "url": "https://doc.rust-lang.org/rustc/platform-support/wasm32-wali-linux.html",
          "excerpts": [
            "WebAssembly targets that use the WebAssembly Linux Interface (WALI) with 32-bit memory. The latest status of the WALI specification and support are documented ...",
            "WALI offers seamless targetability of traditional Linux applications to Wasm by exposing Linux syscalls strategically into the sandbox. Numerous applications ..."
          ]
        },
        {
          "title": "Intel® DPDK Performance on the SAU5081I Server",
          "url": "https://www.accton.com/intel-dpdk-performance-on-the-sau5081i-server/",
          "excerpts": [
            "Apr 30, 2024 — For 64-Byte packets, the DPDK throughput performance fell to 4.8 Gbps ... As a separate measurement, the DPDK packet latency was recorded ..."
          ]
        },
        {
          "title": "NVIDIA NICs Performance Report with DPDK 24.07",
          "url": "https://fast.dpdk.org/doc/perf/DPDK_24_07_NVIDIA_NIC_performance_report.pdf",
          "excerpts": [
            "Nov 11, 2024 — (Mpps). Line Rate. [200G] (Mpps). Line Rate. [100G] (Mpps). Throughput. (Gbps). CPU Cycles per packet. NOTE: Lower is Better. 64. 79.20. 297.62."
          ]
        },
        {
          "title": "Recapitulating AF_XDP",
          "url": "https://medium.com/high-performance-network-programming/recapitulating-af-xdp-ef6c1ebead8",
          "excerpts": [
            "The official kernel documentation describes AF_XDP as “an address family that is optimised for high performance packet processing”."
          ]
        },
        {
          "title": "shunby/wasm-minikernel: A tiny kernel which runs ...",
          "url": "https://github.com/shunby/wasm-minikernel",
          "excerpts": [
            "A tiny kernel which runs WebAssembly runtime. Contribute to shunby/wasm-minikernel development by creating an account on GitHub."
          ]
        },
        {
          "title": "Understanding Delays in AF_XDP-based Applications - arXiv",
          "url": "https://arxiv.org/html/2402.10513v1",
          "excerpts": [
            "We conduct an experimental study to understand the XDP/AF_XDP ecosystem and detect microseconds delays to better architect future latency-sensitive ..."
          ]
        },
        {
          "title": "Securing and Simplifying eBPF Deployments with ...",
          "url": "https://eunomia.dev/others/miscellaneous/wasm-bpf-kubecon/",
          "excerpts": [
            "Feb 10, 2025 — Wasm currently operates in a 32-bit environment, whereas eBPF is designed for a 64-bit architecture. This difference in data layout can lead to ...",
            "Feb 10, 2025 — This blog post discusses how integrating eBPF programs with WebAssembly runtimes like WasmEdge and wasm-bpf can address many of the challenges of deploying ..."
          ]
        },
        {
          "title": "WebAssembly/wasi-filesystem: Filesystem API for WASI - GitHub",
          "url": "https://github.com/WebAssembly/wasi-filesystem",
          "excerpts": [
            "WASI filesystem is a WASI API primarily for accessing host filesystems. It has functions for opening, reading, and writing files, and for working with ...",
            "It has functions for opening, reading, and writing files, and for working with directories. Unlike many filesystem APIs, WASI filesystem is capability-oriented."
          ]
        },
        {
          "title": "Introducing io_uring_spawn - LWN.net",
          "url": "https://lwn.net/Articles/908268/",
          "excerpts": [
            "A new mechanism created by Josh Triplett adds process creation to the io_uring asynchronous I/O API and shows great promise."
          ]
        },
        {
          "title": "Access operating system resources from WebAssembly",
          "url": "https://www.secondstate.io/articles/wasi-access-system-resources/",
          "excerpts": [
            "The WebAssembly Systems Interface (WASI) is a standard extension for WebAssembly bytecode applications to make operating system calls.",
            "WASI allows your Rust functions to access the host computer's file system through the standard Rust std::fs API. A key idea in WASI is “ ...",
            "Jun 17, 2020 — A key idea in WASI is “capability-based security” meaning that access to system resources must be explicitly declared. ... It will start the ..."
          ]
        },
        {
          "title": "WebAssembly's Complete System Interface Evolution: From Two ...",
          "url": "https://thamizhelango.medium.com/webassemblys-complete-system-interface-evolution-from-two-phase-compilation-to-posix-like-491b36eeffc8",
          "excerpts": [
            "The WebAssembly System Interface (WASI) is a set of APIs for WASI being developed for eventual standardization by the WASI Subgroup. WASI ..."
          ]
        },
        {
          "title": "WALI",
          "url": "https://github.com/arjunr2/WALI",
          "excerpts": [
            "WALI is a complete(ish) abstraction over Linux for WebAssembly that aims to push lightweight virtualization down to prevalent, low-level Linux applications.",
            "We currently support 64-bit architectures (x86-64, aarch64, riscv64) with hopes to expand\nto more architectures.",
            "### 4\\. AoT Compiler\n\n[]()\n\nGenerates faster ahead-of time compiled executables. For our WAMR implementation, build with:\n\n```\nmake wamrc\n```",
            "More information about miscellaneous binary formats and troubleshooting can be found [here](https://docs.kernel.org/admin-guide/binfmt-misc.html)"
          ]
        },
        {
          "title": "Running WebAssembly on the Kernel",
          "url": "https://blog.wasmer.io/running-webassembly-on-the-kernel-8e04761f1d8e",
          "excerpts": [
            "Those two examples use WASI (for file abstraction and printing to console) and the asynchronous networking extension (via the `kernel-net` crate). Take a look at them to learn how to do high-performance networking in [kernel-wasm](https://github.com/wasmerio/kernel-wasm",
            "## How to run it",
            "Before running Wasmer on the kernel, ensure that:",
            "* Your system is running **Linux kernel 4.15 or higher** . * Your kernel has **preemption enabled** . Attempting to run WASM user code without kernel preemption will freeze your system.",
            "```\n\nLoad the modules into the kernel:\n\n```\nsudo insmod kernel-wasm.ko sudo insmod wasi/kwasm-wasi.ko sudo insmod networking/kwasm-networking.ko\n```",
            "When running Wasmer, select the `kernel` loader and `singlepass` backend:",
            "ate`\n\n```\nsudo wasmer run --backend singlepass --loader kernel the_file.wasm\n```",
            "### Hope you enjoyed reading this article! While running WebAssembly in the Kernel is certainly dangerous and not the recommended approach for most use cases, it helped us to experiment and learn from the experience first hand.",
            "If you love WebAssembly as much as we do, please contact [syrus@wasmer.io](mailto:syrus@wasmer.io) … your next job can be just an email away!",
            "two examples: echo-server and http-server (living in the examples directory of Wasmer main repo).",
            "Take a look at them to learn how to do high-performance networking in [kernel-wasm](https://github.com/wasmerio/kernel-wasm) .",
            "Load the modules into the kernel:",
            "> Make sure you are running on the latest version ( `0.4.2` ) by executing",
            "`wasmer self-update`",
            "Cowsay running on the Kernel! ### Hope you enjoyed reading this article!"
          ]
        },
        {
          "title": "Performance Evaluation of AF_XDP and DPDK",
          "url": "http://www.diva-portal.org/smash/get/diva2:1897043/FULLTEXT01.pdf",
          "excerpts": [
            "he Linux stack is over ten times slower than both DPDK and AF\\_XDP ,\n\nhighlighting the critical importance of optimized data paths and zero-copy\n\nfunctionality for performanc",
            "The same pattern is visible in Table 5\\.2\n\nshowing that the maximum latency for AF\\_XDP is often double that of DPDK . This shows that the worst case in AF\\_XDP is much slower than DPDK\n\nindicating that something is hindering the performance. These large differences in minimum and maximum latency also indicate\n\nthat there can be a lot of j",
            "_XDP\n\nworkloads have long distances between the fastest and slowest measurements,\n\nwhereas the DPDK measurements are much closer together.",
            "_XDP is\n\nfaster on some workloads. These results should be taken with a grain of salt,\n\nas the AF\\_XDP application was not as isolated as DPDK due to the inability\n\nto pin the kernel processes to a specific Linux cor"
          ]
        },
        {
          "title": "AF_XDP Sockets: High Performance Networking for Cloud-Native Networking - Intel Technology Guide",
          "url": "https://builders.intel.com/docs/networkbuilders/af-xdp-sockets-high-performance-networking-for-cloud-native-networking-technology-guide.pdf",
          "excerpts": [
            "Here the throughput is between 4.34 Mpps and 8.66 Mpps for 64-byte packets. This is around 20% more than for the stock OVS.",
            "Testing was performed on October 28, 2020.",
            "020. The system used was an Intel® Xeon® Gold 6230 processor 2.10 GHz with Intel® Turbo\n\nBoost Technology disabled.",
            ". The NIC was an Intel® Ethernet XXV710 DA2 Adapter with a total of two ports.",
            "AF\\_XDP is included with Linux 4.18, which was released in August 2018",
            "All measurements were performed according to RFC 2544 at 0% packet loss. See Appendix 1 for configuration details."
          ]
        },
        {
          "title": "Safe Kernel Extensibility and Instrumentation With Webassembly",
          "url": "http://ra.adm.cs.cmu.edu/anon/2025/CMU-CS-25-123.pdf",
          "excerpts": [
            "Aug 16, 2025 — This work presented a proof-of-concept framework for safe, language-agnostic kernel extensi- bility by embedding a WebAssembly runtime in a ..."
          ]
        },
        {
          "title": "arjunr2/wali-musl: Exploring Webassembly Linux Interface ...",
          "url": "https://github.com/arjunr2/wali-musl",
          "excerpts": [
            "This repo contains exploratory work towards creating a standard Linux syscall API for Webassembly. We build off of the base code of musl libc."
          ]
        },
        {
          "title": "AF_XDP",
          "url": "https://docs.ebpf.io/linux/concepts/af_xdp/",
          "excerpts": [
            "This page explains the concept of AF_XDP in depth, AF_XDP being a special socket type which in combination with an XDP program can perform full or partial ..."
          ]
        },
        {
          "title": "About",
          "url": "https://www.dpdk.org/about/",
          "excerpts": [
            "DPDK is a Linux Foundation project that consists of libraries to accelerate packet processing workloads running on a wide variety of CPU architectures."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.18/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "Using DPDK not only requires dedicating one core to it, but ...",
          "url": "https://news.ycombinator.com/item?id=34469546",
          "excerpts": [
            "Using DPDK not only requires dedicating one core to it, but also very often dedicating the NIC to your app. It's also a very heavyweight and complex ..."
          ]
        },
        {
          "title": "[PDF] Benchmarking Runtime Scripting Performance in Wasmer",
          "url": "https://ltb2022.eecs.yorku.ca/preprint/Devon-LTB2022-Final.pdf",
          "excerpts": [
            "Using this simulator, we demonstrate a 5-10x performance penalty for WASM compared to native execution. KEYWORDS. WebAssembly, benchmarking, ..."
          ]
        },
        {
          "title": "Introducing Wasmer 5.0 · Blog",
          "url": "https://wasmer.io/posts/introducing-wasmer-v5",
          "excerpts": [
            "Today we are presenting the latest stable version of Wasmer: v5.0 with tons of improvements and a better performance overall."
          ]
        },
        {
          "title": "Swivel: Hardening WebAssembly against Spectre",
          "url": "https://www.usenix.org/system/files/sec21-narayan.pdf",
          "excerpts": [
            "by S Narayan · 2021 · Cited by 98 — We examine the performance impact of these changes on Lucet's. Sightglass benchmark suite [9] and Wasm-compatible. SPEC 2006 [29] benchmarks."
          ]
        },
        {
          "title": "io_uring and networking in 2023",
          "url": "https://github.com/axboe/liburing/wiki/io_uring-and-networking-in-2023",
          "excerpts": [
            "io_uring is applicable to both storage and networking applications.",
            "One of the key benefits of io_uring is that multiple actions can be completed in a single system call.",
            "The io_uring system call for submitting new IO, io_uring_enter(2), also supports waiting for completions at the same time.",
            "This was an important design decision, as it allows IO that completes synchronously to be done efficiently, compared to async APIs that separate submission and wait-for-completion into two different operations.",
            "io_uring_submit_and_wait() helper, allowing an application to not only batch submissions, but also combine submissions and completions into a single system call."
          ]
        },
        {
          "title": "Reddit: Designing an Async runtime for Rust",
          "url": "https://www.reddit.com/r/rust/comments/1b3cl27/designing_an_async_runtime_for_wasi_02/",
          "excerpts": [
            "WASI 0.3 will likely switch to a completion-based system because Linux io_uring and Windows' ioringapi are completion-based and perform ... [r/rust icon](https://styles.redditmedia.com/t5_2s7lj/styles/communityIcon_pjg3ktzyju771.png?width=48&height=48&frame=1&auto=webp&crop=48%3A48%2Csmart&s=0a5e85c5ee6480526bf169287c697dc894c9acca)",
            "> WASI 0.3 will likely switch to a completion-based system because Linux io\\_uring and Windows' ioringapi are completion-based and perform really well, but we don't have that yet. Am I correct that the polling-based API will be removed in v0.3? What have prevented you from implementing an `io-uring`-like completion-based API from the start? track me",
            " WASI 0.3 will likely switch to a completion-based system because Linux io\\_uring and Windows' ioringapi are completion-based and perform really well, but we don't have that yet. Am I correct that the polling-based API will be removed in v0.3?",
            "n for WASI 0.3 at this point. WASI 0.2 was just stabilized last month, and I’m unsure what the completion based APIs for 0.3 would even look like. All we know is that that’s an active direction we want to explore, and over time we’ll have a better idea of what that will look like in practice."
          ]
        },
        {
          "title": "io_uring — Efficient IO (kernel.dk)",
          "url": "https://kernel.dk/io_uring.pdf",
          "excerpts": [
            "io\\_uring met the design goals that was set out for it. We have a very efficient delivery mechanism between\n\nthe kernel and the application, in the shape of two distinct ring",
            "If an application reuses IO buffers, then it's possible to do the mapping and unmapping once, instead\n\nof per IO operation. To register a fixed set of buffers for IO, io \\_ uring \\_ register ( 2 ) must be called with an opcode of\n\nIORING \\_ REGIS",
            "_uring handles this condition like it would for other resources that potentially could block the application. More\n\nimportantly, for operations that will not block, the data is served inline. That makes io\\_uring just as efficient for IO that\n\nis already in the page cache as the regular synchronous interfaces"
          ]
        },
        {
          "title": "io_uring - Linux manual page",
          "url": "https://man7.org/linux/man-pages/man7/io_uring.7.html",
          "excerpts": [
            "Linux-specific API for asynchronous I/O. It allows\n       the user to submit one or more I/O requests, which are processed\n       asynchronously without blocking the calling process.",
            "veral requests in one go, simply by queueing\n       up multiple SQEs, each describing an I/O operation you want and\n       make a single call to io_uring_enter(2) . This is possible due to\n       io_uring 's shared buffers based ",
            " This interface makes io_uring different from other UNIX\n       I/O APIs, wherein, rather than just communicate between kernel and\n       user space with system calls, ring buffers are used as the main\n       ",
            "io_uring\n       gets its name from ring buffers which are shared between user\n       space and"
          ]
        },
        {
          "title": "Empowering WebAssembly with Thin Kernel Interfaces",
          "url": "https://dl.acm.org/doi/10.1145/3689031.3717470",
          "excerpts": [
            "This paper proposes thin kernel interfaces for Wasm, which directly expose OS userspace syscalls without breaking intra-process sandboxing."
          ]
        },
        {
          "title": "PERFORMANCE ANALYSIS OF AF_XDP SOCKETS ON ...",
          "url": "https://openscholar.uga.edu/record/2663/files/ReillyJosephMS.pdf",
          "excerpts": [
            "This thesis analyzes the performance of AF_XDP sockets on commodity hardware such as Netronome and Mellanox. Smart NICs. Testing packet delivery speeds was ..."
          ]
        },
        {
          "title": "[PDF] Understanding Delays in AF_XDP-based Applications - HAL",
          "url": "https://hal.science/hal-04458274v1/file/main.pdf",
          "excerpts": [
            "In this paper, we presented a comprehensive study on the performance of AF XDP from the latency point of view, as we believe, it is a key point ..."
          ]
        },
        {
          "title": "280. DPDK PMD for AF_XDP Tests - Documentation",
          "url": "https://doc.dpdk.org/dts/test_plans/af_xdp_test_plan.html",
          "excerpts": [
            "The test plan contains 15 tests, 7 of which are focused on performance and the remaining 8 are focused on validating functionality."
          ]
        },
        {
          "title": "eunomia-bpf/bpftime: Userspace eBPF runtime for ...",
          "url": "https://github.com/eunomia-bpf/bpftime",
          "excerpts": [
            "bpftime is a High-Performance userspace eBPF runtime and General Extension Framework designed for userspace. It enables faster Uprobe, USDT, Syscall hooks, XDP ..."
          ]
        },
        {
          "title": "The WebAssembly Linux Interface - NASA/ADS",
          "url": "https://ui.adsabs.harvard.edu/abs/2023arXiv231203858R/abstract",
          "excerpts": [
            "by A Ramesh · 2023 · Cited by 2 — We present an implementation of WALI in a modern WebAssembly engine and evaluate its performance on a number of applications which we can now compile with ..."
          ]
        },
        {
          "title": "wasmerio/wasmer: 🚀 Fast, secure, lightweight containers ...",
          "url": "https://github.com/wasmerio/wasmer",
          "excerpts": [
            "Wasmer is a blazing fast and secure WebAssembly runtime that enables incredibly lightweight containers to run anywhere: from Desktop to the Cloud, Edge and ..."
          ]
        },
        {
          "title": "near/wasmer: 🚀 The leading WebAssembly Runtime ...",
          "url": "https://github.com/near/wasmer",
          "excerpts": [
            "Wasmer is a fast and secure WebAssembly runtime that enables super lightweight containers to run anywhere: from Desktop to the Cloud, Edge and IoT devices."
          ]
        },
        {
          "title": "Wasmer: Universal applications using WebAssembly",
          "url": "https://wasmer.io/",
          "excerpts": [
            "Run your apps. Secure. Fast. At Scale. Experience the new container technology powered by WebAssembly that runs your programs safely, anywhere."
          ]
        },
        {
          "title": "WasmEdge Runtime",
          "url": "https://github.com/wasmedge",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "WasmEdge is a lightweight, high-performance ...",
          "url": "https://github.com/WasmEdge/WasmEdge",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "Why WASI Preview 2 Makes WebAssembly Production ...",
          "url": "https://thenewstack.io/why-wasi-preview-2-makes-webassembly-production-ready/",
          "excerpts": [
            "WASI Preview 2 is the missing link that Wasm needed to become a viable option for production use cases.",
            "Why WASI Preview 2 Makes WebAssembly Production Ready",
            ". Until recently, Wasm’s reality didn’t live up to the hype. Preview 2 is the missing link that Wasm needed to become viable for production use cases.",
            ". Preview 2 represents a significant milestone in Wasm’s evolution because it provides a solid checkpoint from which developers can build with confidence knowing that the entire platform isn’t going to change out from under them.",
            "At the heart of WASI Preview 2 is the WebAssembly Component Model.",
            " The component model defines a Canonical ABI (application binary interface) that standardizes the way components talk to each other and prevents them from accessing other components’ memories.",
            "This eliminates the largest classes of bugs and security vulnerabilities.",
            "Another key aspect of WASI Preview 2 is the stabilization of APIs.",
            "This ensures backward compatibility in the future for Wasm applications, giving developers the confidence to build on top of Preview 2 without worrying about future disasters.",
            "Preview 2 changes all of that, paving the way for broader, even mainstream, adoption of Wasm.",
            "When building Wasm apps, you can now pick and choose libraries from any language ecosystem, compile them into components and compose them to make one app."
          ]
        },
        {
          "title": "Getting Started with WasmEdge",
          "url": "https://wasmedge.org/docs/start/overview/",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud-native, edge, and decentralized applications."
          ]
        },
        {
          "title": "Performance of WebAssembly runtimes in 2023",
          "url": "https://00f.net/2023/01/04/webassembly-benchmark-2023/",
          "excerpts": [
            "Jan 4, 2023 — Benchmarking abilities for wasm32-wasi were thus added to libsodium. This benchmark proved to be more representative of real-world performance ...",
            "Here, wasmedge , also based on LLVM manages to be faster than wasmer with the LLVM backend. Utilities benchmark. Utilities benchmark. Codecs, ...",
            "Calling external functions in wasmedge may have more overhead than with other runtimes. Signature benchmark. Signature benchmark. Arithmetic ...",
            "* [Wasm3](https://github.com/wasm3/wasm3) ’s development pace seems to have slowed down. However, it remains the only WebAssembly runtime that can easily be embedded into any project, with minimal footprint, and amazing performance for an interpreter.",
            "Jan 4, 2023 — node , wasmtime , wasmedge and wasmer are in the same ballpark. None of them has a stable API yet. This is especially true for Rust APIs. If ...",
            "In the `LLVM` category, I was expecting `wasmedge` to take the lead given how much performance-focused the project has been since the beginning. Its results were a little bit disappointin",
            "* [Wasmtime](https://wasmtime.dev) quickly went from version 0.40 to version 3.0.1, with [version 4 being round the corner](https://github.com/bytecodealliance/wasmtime/blob/main/RELEASES.md) . Every release is an opportunity to update Cranelift, the code generator it is based on.",
            "* [Wasmer](https://wasmer.io) kept releasing unique tools and features, such as the ability to generate standalone binaries. Their single-pass compiler also got updated.",
            "* SSVM became [WasmEdge](https://wasmedge.org) , the runtime from the Cloud Native Computing Foundation.",
            "* [Wamr](https://github.com/bytecodealliance/wasm-micro-runtime) saw a bunch of new releases. Pre-built binaries are also now available.",
            "### LLVM/Cranelift/V8-based runtimes",
            "Jan 4, 2023 — The tests mainly measure the overhead of WASI calls to get random bytes, and, very likely, the over of WASI calls in general. The difference ...",
            "Jan 4, 2023 — As a result, these AES-based tests were 80 times slower than native code when running WebAssembly. This is not representative of most ...",
            "For most users, there are no significant differences between these three runtimes. They share similar features (such as AOT compilation) and run code the same way, roughly at the same speed.",
            "In spite of some breaking changes, it seems more focused on API stability than alternatives, making it a reasonable choice for applications that are planned to be maintained for a long time.",
            "asmer` has the largest ecosystem, with libraries making it very easy to use from many programming languages, as well as applications such as PostgreSQL and Visual Studio Code.",
            "It also includes the ability to generate standalone binaries for all supported platforms, has a package manager and more.",
            "asmedge` is the runtime from the CNCF, and what Docker uses to run containers with WebAssembly applications.",
            "So, even if this is not your runtime of choice, testing that your WebAssembly code properly runs on it is highly recommended, as its popularity is likely to skyrocket."
          ]
        },
        {
          "title": "The State of Async Rust: Runtimes",
          "url": "https://corrode.dev/blog/async/",
          "excerpts": [
            "In the first article, we will focus on the current state of async Rust runtimes, their design choices, and their implications on the broader Rust async ..."
          ]
        },
        {
          "title": "Wasm-bpf - WebAssembly eBPF integration",
          "url": "https://github.com/eunomia-bpf/wasm-bpf",
          "excerpts": [
            "Wasm-bpf is a WebAssembly eBPF library, toolchain and runtime powered by CO-RE(Compile Once – Run Everywhere) libbpf.",
            "It can help you build almost every eBPF ... It also enables developers to write eBPF programs in familiar languages like `C/C++` , `Rust` , `Go` , and more than 30 other programming languages, and deploy them easily across different Linux distributio",
            "Additionally, cloud providers can leverage wasm-bpf to offer a `secure` and `high-performance` environment for their customers to develop and deploy eBPF applications in their cloud environments."
          ]
        },
        {
          "title": "Announcing Wasmer 6.0 - closer to Native speeds! · Blog",
          "url": "https://wasmer.io/posts/announcing-wasmer-6-closer-to-native-speeds",
          "excerpts": [
            "Apr 25, 2025 — Wasmer 6.0 packages all backends together, pushes the LLVM backend to within 5 % of native performance, introduces zero‐cost WebAssembly Exceptions and more!"
          ]
        },
        {
          "title": "Wasmtime Reaches 1.0: Fast, Safe and Production Ready!",
          "url": "https://bytecodealliance.org/articles/wasmtime-1-0-fast-safe-and-production-ready",
          "excerpts": [
            "Sep 20, 2022 — With Wasmtime, InfinyOn has been able to deliver a greater than 5x throughput ... Wasmtime performance, but here's a basic breakdown.",
            "Sep 20, 2022 — ... startup times under a millisecond. Embark — 2 years in production ... With virtual memory tricks, we don't need to create a new memory every time ...",
            "For example, it takes the fastest alternative—a JS isolate—about 5ms to start up. In contrast, it only takes a Wasm instance 5 microseconds to ..."
          ]
        },
        {
          "title": "WASI Cycles, an Open Source, 3D WebXR Game - Oracle Blogs",
          "url": "https://blogs.oracle.com/developers/post/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-open-source-3d-webxr-game",
          "excerpts": [
            "I imagine you know about these three extremely popular messaging and data solutions. I am showing how we can use WASI to (directly) invoke Kafka, [MongoDB](https://dzone.com/articles/mongodb-cluster), and Oracle Database and, in this way, achieve interoperability with and functionality of these popular and widely used systems.",
            "WASI Invoking Kafka, MongoDB, and Oracle Database",
            "WasmEdge",
            "* Built and deployed in a container image",
            "* Uses tokio and hyper for HTTPS calls to the Oracle Database for Kafka and SQL access",
            "Wasmtime",
            "* Built with wasm32-wasip2",
            "* Written in Rust",
            "* Written in Rust",
            "* Uses waki for HTTPS calls to the Oracle Database for Kafka and SQL access",
            "Wasmer",
            "* Built with wasm32-wasip1",
            "* Built with wasm32-wasip1",
            "* Wasmer is unique as it is deployed to Wasmer server (runs locally). Wasmer is free but requires an account, as it's tied to the cloud.",
            "python:",
            "Spring Boot with Kafka and MongoDB API via Oracle Database",
            "Oracle Database has built-in REST endpoints that provide most of the same calls as Confluent Kafka REST APIs and Java Kafka compatibility.",
            "In addition, Oracle Database provides transactional atomicity between messaging and database operations by allowing the underlying database to connect.",
            "Wasmtime: Developed by Bytecode Alliance. It is commonly considered the reference implementation of WASI. Strong standards compliance. Common ..."
          ]
        },
        {
          "title": "KafkaGoSaur: a WebAssembly powered Kafka client - Medium",
          "url": "https://medium.com/swlh/kafkagosaur-eac3c063388",
          "excerpts": [
            "KafkaGoSaur is a new Kafka client for Deno built with WebAssembly on top of kafka-go, the excellent Kafka client library written for Go."
          ]
        },
        {
          "title": "Kafka Broker filter",
          "url": "https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/network_filters/kafka_broker_filter",
          "excerpts": [
            "The Kafka Broker filter can mutate the contents of received responses to enable easier proxying of Kafka clusters. The below example shows a configuration for ..."
          ]
        },
        {
          "title": "WebAssembly/wasi-io: I/O Types proposal for WASI",
          "url": "https://github.com/WebAssembly/wasi-io",
          "excerpts": [
            "Wasi I/O is an API providing I/O stream abstractions. There are two types, input-stream, and output-stream, which support read and write, respectively.",
            "WASI I/O is currently in [Phase 3](https://github.com/WebAssembly/WASI/blob/main/Proposals.md) .",
            "WASI I/O must have host implementations which can pass the testsuite on at least Windows, macOS, and Linux.",
            " WASI I/O\n\n[]()\n\nA proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI)",
            "WASI I/O. A proposed WebAssembly System Interface API. Current Phase. WASI I/O is currently in Phase 3. Champions. Dan Gohman. Portability Criteria. WASI I/O ..."
          ]
        },
        {
          "title": "Networking API design · Issue #370 · WebAssembly/WASI - GitHub",
          "url": "https://github.com/WebAssembly/WASI/issues/370",
          "excerpts": [
            "WASI does have a poll_oneoff function for waiting for I/O on multiple handles. It won't scale to very-many handles, but it's likely better than ..."
          ]
        },
        {
          "title": "According to their roadmap, WASI preview 2 will have ...",
          "url": "https://news.ycombinator.com/item?id=38438239",
          "excerpts": [
            "Threads are actually removed from WASI Preview 2. WASI Preview 2 still doesn't support threads, fork, subprocesses or longjmp/setjmp (among others). Not even ..."
          ]
        },
        {
          "title": "Develop with WasmEdge, Wasmtime, and Wasmer Invoking ...",
          "url": "https://paul-parkinson.medium.com/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-ad2302fe961a",
          "excerpts": [
            "WASM (WebAssembly): A W3C standard for low-level, performance-optimized bytecode that is unmatched for portability, security, and speed."
          ]
        },
        {
          "title": "Wasmtime 1.0: A Look at Performance",
          "url": "https://bytecodealliance.org/articles/wasmtime-10-performance",
          "excerpts": [
            "Sep 6, 2022 — According to the benchmarks, this change improved the speed of stack walking by between 64% and 99.95%, depending on the situation, and it ...",
            "Sep 6, 2022 — wasm went from about 2 milliseconds (eager initialization of all heap and other data structures) to 5 microseconds, or 400 times faster. Not bad ...",
            "When introduced, it improved runtime performance of SpiderMonkey.wasm by about 5% and another CPU-intensive benchmark, bz2 , by 4% (for example) ...",
            "This first post will describe performance-related projects: making the compiler generate faster code, making the compiler itself run faster, making Wasmtime ...",
            "Over the past year, we have done substantial work to take module instantiation from milliseconds to microseconds. How is this possible? In brief ...",
            "Sep 6, 2022 — Now that we've started the Wasm program, we need to help it run as quickly as possible with fast runtime primitives! While most of the CPU time ..."
          ]
        },
        {
          "title": "Container Runtimes and WasmEdge benchmarking on Arm",
          "url": "https://community.arm.com/arm-community-blogs/b/servers-and-cloud-computing-blog/posts/container-runtimes-wasmedge-arm",
          "excerpts": [
            "In this blog, we give you a general introduction of runtimes, and shows you the benchmark of image size, boot time and memory footprint between runc and ...",
            "May 17, 2023 — In this blog, we give you a general introduction of runtimes, and shows you the benchmark of image size, boot time and memory footprint between runc and ..."
          ]
        },
        {
          "title": "Transform data between Kafka topics",
          "url": "https://docs.decodable.co/tutorials/kafka-transform.html",
          "excerpts": [
            "In this guide, we'll extract data from one Kafka topic, transform that data using SQL, and then load the transformed data to another Kafka topic."
          ]
        },
        {
          "title": "Kafka Patterns and Anti-Patterns - Adservio",
          "url": "https://www.adservio.fr/post/kafka-patterns-and-anti-patterns",
          "excerpts": [
            "Kafka patterns represent big-picture concepts that you need to align your implementation strategy with in order to make the most of Kafka."
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            " the libsodium benchmark has been widely used to measure to pick the best runtimes",
            "Libsodium is also worthwhile for testing some Wasm environments because it includes benchmarking tasks with a wide range of different requirement profiles, some probing for raw CPU or memory performance, while others check for more nuanced performance prof",
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said.",
            "b/main/README.md) is used to run WASM applications. It supports interpreter mode, ahead-of-time compilation (AOT) mode and just-in-time compilation (JIT) modes, LLVM JIT and Fast JIT, according to the project’s documentation."
          ]
        },
        {
          "title": "WarpDiff and Kafka WASM in server-side Wasm runtimes (arXiv:2309.12167)",
          "url": "https://arxiv.org/abs/2309.12167",
          "excerpts": [
            " WarpDiff to identify performance issues in server-side Wasm runtimes. It has recently been used increasingly on the server side, e.g., providing a safer, faster, and more portable alternative to Linux containers."
          ]
        },
        {
          "title": "The Significance of In-broker Data Transformations in Streaming Data",
          "url": "https://medium.com/event-driven-utopia/the-significance-of-in-broker-data-transformations-in-streaming-data-9292e440beaa",
          "excerpts": [
            ". Redpanda, the Kafka-API-compatible streaming data platform, recently released the [Redpanda Data Transforms Sandbox](https://redpa",
            "The sandbox contains a Go-based SDK seamlessly integrated within [rpk](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), Redpanda’s [CLI experience](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), and a single Redpanda broker (Docker container) that can deploy and run your transform functions on topics you create inside the fully-contained sandbox.",
            "What is WebAssemby (Wasm)?",
            "WebAssembly, or Wasm for short, is a new type of code that can be run in modern web browsers and provides new features and major gains in performance.",
            "That means developers can code web client applications in a programming language of their choice, compile them down to Wasm, and run them inside the browser at near-native speed.",
            "What if we use Wasm to ship code to server-side applications, especially to brokers, in the same way it ships code to the user’s computer?",
            "Returning to our original context, using Wasm to code the data transformation logic and shipping it to the broker provides several benefits. **Flexibility for developers:** Wasm allows developers to write their transformations in any supported languages (C, C++, Rust, JS, and Go as of today).",
            "Near-native performance:** Wasm has a small runtime footprint, less overhead, and runs in embedded mod",
            "Security:** Data transformations are executed in sandboxes, which are isolated execution environments, providing improved securit",
            "Portability:** Wasm [execution environments are standardized](https://webassembly.org/docs/portability/) to promote portability, enabling code to be shipped across many platform",
            "Redpanda Data Transforms — in-broker data transformations at scale",
            "Redpanda data transforms provide a framework to create, build, and deploy inline data transformations on data written to Redpanda topics.",
            "Redpanda data transforms provide a framework to create, build, and deploy inline data transformations on data written to Redpanda topics.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic.",
            "It is currently in [Technical Preview](https://docs.redpanda.com/current/labs/data-transform/?tab=tabs-2-transcoder-transform).",
            "It is currently in [Technical Preview](https://docs.redpanda.com/current/labs/data-transform/?tab=tabs-2-transcoder-transform)."
          ]
        },
        {
          "title": "Redpanda Data Transforms with WebAssembly",
          "url": "https://docs.redpanda.com/current/develop/data-transforms/how-transforms-work/",
          "excerpts": [
            "Data transforms use [WebAssembly](https://webassembly.org/) (Wasm) engines inside a Redpanda broker, allowing Redpanda to control the entire transform lifecycle. For example, Redpanda can stop and start transforms when partitions are moved or to free up system resources for other tasks.",
            "Data transforms take data from an input topic and map it to one or more output topics. For each topic partition, a leader is responsible for handling the data. Redpanda runs a Wasm virtual machine (VM) on the same CPU core (shard) as these partition leaders to execute the transform function.",
            "Transform functions are the specific implementations of code that carry out the transformations. They read data from input topics, apply the necessary processing logic, and write the transformed data to output topics.",
            "When you deploy a data transform to a Redpanda broker, it stores the Wasm bytecode and associated metadata, such as input and output topics and environment variables. The broker then replicates this data across the cluster using internal Kafka topics. When the data is distributed, each shard runs its own instance of the transform function. This process includes several resource management features:",
            "* Each shard can run only one instance of the transform function at a time to ensure efficient resource utilization and prevent overload.",
            "* Memory for each function is reserved within the broker with the `data_transforms_per_core_memory_reservation` and `data_transforms_per_function_memory_limit` properties. See [Configure memory for data transforms](../configure/).",
            "* CPU time is dynamically allocated to the Wasm runtime to ensure that the code does not run forever and cannot block the broker from handling traffic or doing other work, such as Tiered Storage uploads.",
            "Flow of data transforms",
            "When a shard becomes the leader of a given partition on the input topic of one or more active transforms, Redpanda does the following:",
            "1.\nSpins up a Wasm VM using the JIT-compiled Wasm module. 2. Pushes records from the input partition into the Wasm VM. 3. Writes the output. The output partition may exist on the same broker or on another broker in the cluster",
            "ransform function reads from a specified input topic and writes to a specified output topic. The transform function processes every record produced to an input topic and returns zero or more records that are then produced to the specified output topic. Data transforms are applied to all partitions on an input topic. A record is processed after it has been successfully written to disk on the input topic. Because the transform happens in the background after the write finishes, the transform doesn’t affect the original produced record, doesn’t block writes to the input topic, and doesn’t block produce and consume requests. A new transform function reads the input topic from the latest offset. That is, it only reads new data produced to the input topic: it does not read records produced to the input topic before the transform was dep"
          ]
        },
        {
          "title": "arXiv:2002.09344v1 [cs.DC] 21 Feb 2020",
          "url": "https://scispace.com/pdf/faasm-lightweight-isolation-for-efficient-stateful-4y5cck5ry0.pdf",
          "excerpts": [
            "machine learning inference, FAASM doubles the throughput and reduces tail latency by 90%. ... 5.2 Reducing Cold Start Latency. While Faaslets ... pact performance ..."
          ]
        },
        {
          "title": "Faasm: Lightweight Isolation for Efficient Stateful ...",
          "url": "https://www.usenix.org/system/files/atc20-shillaker.pdf",
          "excerpts": [
            "by S Shillaker · 2020 · Cited by 410 — Knative has a tail latency of over 2 s and more than 35% of calls have latencies of over 500 ms with 20% cold-starts. FAASM achieves a tail ...",
            "\nruntime then executes multiple Faaslets, each with a dedicated\nthread, within a single address space. For",
            "Faaslets have fast initialisation times. To reduce cold-\nstart time when a Faaslet executes for the first time, it\nis launched from a suspended state",
            "Proto-Faaslets are\nused to create fresh Faaslet instances quickly, e.g. avoid-\ning the time to initialise a language runtim",
            "Faaslets allocate shared process memory through\ncalls to mmap on the underlying host, passing MAP_SHARED and\nMAP_ANONYMOUS flags to create shared and private regions, re-\nspectivel",
            "Faaslets use a two-tier state architecture,\na local tier provides in-memory sharing, and a global tier\nsupports distributed access to state across hosts.",
            "The FAASM\nruntime provides a state management API to Faaslets that\ngives fine-grained control over state in both tiers.",
            "tion\nis executed by a dedicated thread of a shared runtime process. This thread is assigned to a cgroup with a share of CPU equal\nto that of all Faaslets. The Linux CFS [79] ensures that these\nthreads are scheduled with equal CPU time.",
            ". Faaslets achieve secure and fair network access using net-\nwork namespaces, virtual network interfaces and traffic shap",
            "Proto-Faaslets\n\nreduce initialisation times to hundreds of microseconds, and a\n\nsingle snapshot can be restored across hosts, quickly scaling\n\nhorizontally on cluste",
            "FAASM guarantees that private\ndata held in memory is cleared away after each function exe-\ncution, thereby allowing Faaslets to handle subsequent calls\nacross tenant",
            "Proto-Faaslet snapshot includes a function’s stack, heap,\n\nfunction table, stack pointer and data, as defined in the We-\n\nbAssembly specification [ 35"
          ]
        },
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "Upcoming WASI 0.3 releases​ ... At this time, WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025. Please see the ...",
            "WASI 0.3.0 will add **native async support** to the Component Model and refactor WASI 0.2 interfaces to take advantage of native as",
            "At this time, WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025.",
            "This includes explicit `stream<T>` and `future<T>` types for use anywhere in function parameters and results.",
            "At this time, WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025. Please see the presentation and notes from the ... Roadmap · WASI.dev",
            "## Upcoming WASI 0.3 releases",
            "When WASI 0.3 is launched, implementations may continue to support 0.2, either by implementing 0.3 alongside it, or by virtualizing (polyfilling) 0.2 in terms of 0.3.",
            "*Threads** : first cooperative, then preemptiv",
            "WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025."
          ]
        },
        {
          "title": "Launching WASI Preview 2 - The WebAssembly System ...",
          "url": "https://www.reddit.com/r/rust/comments/1ab5bj0/launching_wasi_preview_2_the_webassembly_system/",
          "excerpts": [
            "WASI is now officially based on the Wasm component model, which makes it cross-language and virtualizable. Preview 2 includes two worlds."
          ]
        },
        {
          "title": "Kafka: Event-Driven Architecture or Over-Engineered ...",
          "url": "https://medium.com/@techInFocus/kafka-event-driven-architecture-or-over-engineered-rabbit-hole-13293eca8b5b",
          "excerpts": [
            "May 11, 2025 — You'll wrestle with backpressure, lag monitoring, schema evolution, and message replay. It's not plug-and-play. You don't “just install Kafka” ..."
          ]
        },
        {
          "title": "Kafka sidecar of light-mesh to provide cross-cutting ...",
          "url": "https://github.com/networknt/kafka-sidecar",
          "excerpts": [
            "A fast and light-weight reverse proxy with embedded gateway to wrap third party APIs and bring them to the ecosystem of light-4j frameworks."
          ]
        },
        {
          "title": "Kafka Connectors | Confluent Documentation",
          "url": "https://docs.confluent.io/platform/current/connect/kafka_connectors.html",
          "excerpts": [
            "You can use self-managed Apache Kafka® connectors to move data in and out of Kafka. The self-managed connectors are for use with Confluent Platform."
          ]
        },
        {
          "title": "How to Use Kafka Connect - Get Started | Confluent Documentation",
          "url": "https://docs.confluent.io/platform/current/connect/userguide.html",
          "excerpts": [
            "Kafka Connect is the framework to integrate popular systems, such as databases and cloud services with Apache Kafka. This guide will help you get started."
          ]
        },
        {
          "title": "Cloud-native WebAssembly in Service Mesh - Wasm",
          "url": "https://medium.com/wasm/cloud-native-webassembly-in-service-mesh-b19e3a96ccf8",
          "excerpts": [
            "It manages and directs API requests to sidecars in a manner that keeps the system scalable. Developers need to script those proxies to route ..."
          ]
        },
        {
          "title": "Why Kafka Consumers Are Failing You: The Backpressure ...",
          "url": "https://medium.com/@himanshu675/why-kafka-consumers-are-failing-you-the-backpressure-crisis-killing-event-driven-architectures-6a16e7d0f460",
          "excerpts": [
            "Factors like unpredictable message volumes, complex processing logic, or resource constraints can disrupt this balance, leading to backpressure."
          ]
        },
        {
          "title": "IOPS vs Throughput vs Latency | Metrics Guide - Simplyblock",
          "url": "https://www.simplyblock.io/blog/iops-throughput-latency-explained/",
          "excerpts": [
            "IOPS and throughput are important metrics when measuring the performance of storage solutions. Learn all about and how to measure them."
          ]
        },
        {
          "title": "Introducing Fermyon Wasm Functions on Akamai",
          "url": "https://www.fermyon.com/blog/introducing-fermyon-wasm-functions-on-akamai",
          "excerpts": [
            "Missing: wasmCloud Faasm benchmark"
          ]
        },
        {
          "title": "Measuring the responsiveness of WebAssembly in edge ...",
          "url": "https://www.diva-portal.org/smash/get/diva2:1783251/FULLTEXT01.pdf",
          "excerpts": [
            "by R Scolati · 2023 — This project aims to measure the overall latency Wasm introduces in an example implementation of a CPS and to compare the latency against the ..."
          ]
        },
        {
          "title": "LLM Inference Benchmarking: Performance Tuning with TensorRT ...",
          "url": "https://developer.nvidia.com/blog/llm-inference-benchmarking-performance-tuning-with-tensorrt-llm/",
          "excerpts": [
            "This is the third post in the large language model latency-throughput benchmarking series, which aims to instruct developers on how to benchmark ..."
          ]
        },
        {
          "title": "Benchmarking AWS Lambda runtimes in 2021: cold start",
          "url": "https://filia-aleks.medium.com/benchmarking-all-aws-lambda-runtimes-in-2021-cold-start-part-1-e4146fe89385",
          "excerpts": [
            "We are going to test the cold start(the first invocation on AWS Lambda instance) delay. To measure this metric we will use X-Ray tracing."
          ]
        },
        {
          "title": "Handling Backpressure in Software Systems",
          "url": "https://dev.to/lovestaco/handling-backpressure-in-software-systems-23m1",
          "excerpts": [
            "Mar 26, 2025 — Backpressure occurs when a system receives more data than it can process in real-time. It's like a coffee shop with a single barista."
          ]
        },
        {
          "title": "Introducing the wasmCloud benchmark chart",
          "url": "https://wasmcloud.com/blog/2025-02-18-introducing-the-wasmcloud-benchmark-chart/",
          "excerpts": [
            "chart makes it easy to install (and uninstall) this stack at the push of a button.\nDeploying the benchmark",
            "To test the chart, you'll first need a Kubernetes cluster with wasmCloud deployed on it.",
            "To install the `benchmark` chart:",
            "helm upgrade --install my-benchmark --version 0.2.0 oci://ghcr.io/wasmcloud/charts/benchmark --wait --set test.url=http://hello-world:8000",
            "Feb 18, 2025 — In this post, we'll explore how the chart works and demonstrate how to use it. The need for speed (tests)​. Robust benchmarking is important for ..."
          ]
        },
        {
          "title": "Fermyon Serverless Guide: Speed and Execution Time",
          "url": "https://www.fermyon.com/serverless-guide/speed-and-execution-time",
          "excerpts": [
            ". Most Lambda functions take [more than 200 milliseconds to cold start](https://youtu.be/0_jfH6qijVY) – and sometimes as much as a few seconds. This is too long for typical HTTP endpoints. In contrast, [Spin](https://spinframework.dev) starts nearly instantly (typically 2ms or less, and often under 1msec).",
            "One distinctive feature of serverless apps is the focus on speed and execution time. Serverless functions are often limited to a specific length of time to run."
          ]
        },
        {
          "title": "Redpanda's WASM Architecture and Data Transforms",
          "url": "https://www.redpanda.com/blog/wasm-architecture",
          "excerpts": [
            "Redpanda Data Transforms is built on the Wasmtime engine, which powers many other modern serverless platforms.",
            "Data Transforms can be deployed in one command using\nrpk transform deploy --input-topic=foo --output-topic=bar",
            " To see the deployed transforms running in the cluster as well as their status and if there’s any lag, you can use\nrpk transform list .",
            "The data lifecycle\nCheck out our post on how Redpanda Data Transforms compares to Apache Flink to learn how they compare and when to use them.",
            "The architecture of Redpanda Data Transforms is composed of three important components:\n    * Client-side tooling and SDKs\n    * Data Transforms lifecycle management\n    * Data flow through the WebAssembly runtime",
            "Data Transforms lets you tell Redpanda what piece of code (.wasm) can run on what data (topics), on what location, at a specific time (on event triggers) in an auditable fashion.",
            "Embedding this virtual machine directly onto each shard in Redpanda's thread-per-core architecture provides an alternative to the classical data back-and-forth when trying to make sense of real-time streaming data.",
            "Embedding this virtual machine directly onto each shard in Redpanda's thread-per-core architecture provides an alternative to the classical data back-and-forth when trying to make sense of real-time streaming data.",
            "What JavaScript did for the web in the late 90s is what Wasm can do for server-side applications.",
            "\nWebAssembly will change server-side software permanently. Allowing injection of code inside Redpanda at strategic points fundamentally changes what the storage engine can do for you while giving you isolation, high function density, granularity of execution, native performance, and multi-tenant by design",
            "Currently, Redpanda Data Transforms is a beta feature for Redpanda Cloud's Dedicated and BYOC options.",
            "Lastly, follow us on LinkedIn and X to catch our upcoming product updates!"
          ]
        },
        {
          "title": "How we built our data transformation engine with the Wasm runtime (Redpanda Data Transform)",
          "url": "https://redpanda-data.medium.com/how-we-built-our-data-transformation-engine-with-the-wasm-runtime-2cb1b312b26b",
          "excerpts": [
            "Redpanda Data Transforms gives users the ability to push custom code to Redpanda.",
            "ns. There are three main goals of the Data Transforms capability:\n\n1. **Ease of use:** You should be able to code in your favorite Wasm-supported languages (only JS is supported in this release). No need to worry about which Apache Kafka® libraries you’ll need to leverage or any low-level external dependencies. Your code gets record batches sent directly into it, straight from the local Redpanda node.",
            "*Performance:** Your transform is pushed to all nodes in the cluster and calls to your code boil down to an RPC call over localhost. This avoids expensive raft chatter that you would otherwise incur if using an external producer and consumer.",
            "*Operational simplicity:** Creating a separate transformer comes with more configuration and set up. However, with Data Transforms, you just push your code to Redpanda and it is durably stored within a dedicated topic and completely managed by Redpanda.",
            "orms. The transformation’s response is then sent back to Redpanda where it will be written to a resultant topic we call a “materialized topic.",
            "Wasm engine performance will be addressed by a redesign where it will support concurrent execution of transforms, and full Wasm support will be addressed in the future, as well.",
            "We decided to go with Wasm as it has less overhead, a small runtime footprint, gives users the ability to write their transforms in any of the supported Wasm languages (C, C++, Rust and JS as of today), and can run in embedded mode.",
            "ect.\nIn the tech preview, the transforms will be deployed to a separate sidecar process called the “async engine.” It’s a NodeJS service deployed on each Redpanda node that responds to requests from the local Redpanda process over localhost.",
            "ect.\nIn the tech preview, the transforms will be deployed to a separate sidecar process called the “async engine.” It’s a NodeJS service deployed on each Redpanda node that responds to requests from the local Redpanda process over localhost.",
            "ary\n\nWhat we’ve done is create a system that allows users to asynchronously map one topic to another, solving a common problem in streaming. The idea to keep the Wasm engine itself simple and stateless provided a solid foundation."
          ]
        },
        {
          "title": "Integrations",
          "url": "https://wasmcloud.com/docs/integrations/",
          "excerpts": [
            "Add Kafka messaging functionality for wasmCloud applications with the first-party Kafka messaging capability provider. Kubernetes, Deploying wasmCloud on ..."
          ]
        },
        {
          "title": "Create",
          "url": "https://wasmcloud.com/docs/developer/providers/create/",
          "excerpts": [
            "This guide walks through the process of building a capability provider using a standard interface, wasmcloud:messaging . You can use this guide and this example ..."
          ]
        },
        {
          "title": "Capability Catalog",
          "url": "https://wasmcloud.com/docs/capabilities/",
          "excerpts": [
            "wasmCloud supports custom capabilities—use our provider SDKs for Rust and Go to create your own! Learn more in the documentation on creating a capability ..."
          ]
        },
        {
          "title": "Transform Your Data Pipeline: Real‐Time Event Streaming ...",
          "url": "https://medium.com/@FAANG/transform-your-data-pipeline-real-time-event-streaming-with-rust-and-apache-kafka-a88e0c64e7f1",
          "excerpts": [
            "In this article, we skip over basic Rust syntax and dive straight into architecting a resilient, high‐throughput streaming pipeline using Rust's ..."
          ]
        },
        {
          "title": "Smart data FaaS with WebAssembly & Rust - The Punch",
          "url": "https://punchplatform.com/2023/09/14/frugal-and-robust-data-processing-using-rust-and-webassembly/",
          "excerpts": [
            "Sep 14, 2023 — In a previous exploration, we demonstrated the potential of using Rust and WebAssembly for efficient data transformation within Punch data ...",
            "Smart data FaaS with WebAssembly & Rust. Explore REEF engine, Rust connectors, and WebAssembly-powered functions."
          ]
        },
        {
          "title": "Fluvio: A Kafka + Flink Built Using Rust + WASM",
          "url": "https://medium.com/@ftieben/fluvio-a-kafka-flink-built-using-rust-wasm-bdeef463fd64",
          "excerpts": [
            "One of the most notable things about Fluvio is its performance. Fluvio is able to achieve very high throughput and low latency, even when ..."
          ]
        },
        {
          "title": "Real-Time Event-Driven Architecture with Kafka, WebSockets and ...",
          "url": "https://medium.com/@akshat.available/real-time-event-driven-architecture-with-kafka-websockets-and-react-b4698361e68a",
          "excerpts": [
            "The architecture enables real-time server-to-client communication using Kafka and WebSockets for event-driven data streaming."
          ]
        },
        {
          "title": "WASI API proposal for managing sockets",
          "url": "https://github.com/WebAssembly/wasi-sockets",
          "excerpts": [
            "This proposal adds TCP & UDP sockets and domain name lookup to WASI. It adds the basic BSD socket interface with the intent to enable server and client ...",
            "Unlike BSD sockets, WASI sockets require capability handles to create sockets and perform domain name lookups. On top of capability handles, WASI Socket ..."
          ]
        },
        {
          "title": "wasi:sockets",
          "url": "https://wa.dev/wasi:sockets",
          "excerpts": [
            "If the TCP/UDP port is zero, the socket will be bound to a random free port. Bind can be attempted multiple times on the same socket, even with different ..."
          ]
        },
        {
          "title": "Hypercharge Through Components: Why WASI 0.3 and ...",
          "url": "https://medium.com/wasm-radar/hypercharge-through-components-why-wasi-0-3-and-composable-concurrency-are-a-game-changer-0852e673830a",
          "excerpts": [
            "WASI 0.3 Roadmap Drops a Timeline​​ The official WASI. The dev roadmap now confirms August 2025 for the WASI 0.3. 0 preview and November for the ...",
            "WASI 0.3 now enhances with native async support. That means every component can manage I/O or long-running tasks without blocking others.",
            "Wasmtime and Fermyon's Spin. Let's take Spin first. It's a lightweight framework for building serverless apps using WebAssembly, and it ...",
            "All of this lands as a preview around August 2025, with stable releases expected by November, according to the official WASI.dev roadmap. This ..."
          ]
        },
        {
          "title": "WASI 0.3 preview: native async added to the WebAssembly ...",
          "url": "https://progosling.com/en/programming-news/wasi-0-3-native-async-aug-2025",
          "excerpts": [
            "Aug 12, 2025 — The WASI roadmap now targets an August 2025 preview for WASI 0.3 and signals that the release will add native async to the WebAssembly ..."
          ]
        },
        {
          "title": "Apache Kafka vs. Fluvio Benchmarks : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/1invl4e/apache_kafka_vs_fluvio_benchmarks/",
          "excerpts": [
            "We figured we keep it simple and compare Apache Kafka and Fluvio. The results are as you'd expect. More details in the blog."
          ]
        },
        {
          "title": "WASM Runtimes vs. Containers: Cold Start Delays (Part 2)",
          "url": "https://levelup.gitconnected.com/wasm-runtimes-vs-containers-cold-start-delays-part-2-544f05ab4d78",
          "excerpts": [
            "Missing: wasmCloud throughput",
            "In this second part of the series, we will assess how do the cold start delays of WebAssembly (WASM) runtimes compare to those of containers ...",
            "Jan 10, 2024 — In this second part of the series, we will assess how do the cold start delays of WebAssembly (WASM) runtimes compare to those of containers ..."
          ]
        },
        {
          "title": "Providers",
          "url": "https://wasmcloud.com/docs/concepts/providers/",
          "excerpts": [
            "Providers are swappable host plugins—executables (usually dedicated to longer-lived processes) that deliver common functionalities called capabilities ...",
            "The wasmCloud Q3 2025 Roadmap sets out plans for an overhaul to capability providers in the next major release of wasmCloud.",
            "Because built-in providers are part of the wasmCloud host itself, they do not communicate over NATS and run in the same process as the host. Updating a built-in ..."
          ]
        },
        {
          "title": "Capabilities",
          "url": "https://wasmcloud.com/docs/concepts/capabilities/",
          "excerpts": [
            "Applications implement capabilities for common requirements like storing and retrieving data, accessing secrets, and more. Examples of capabilities include: Key ...",
            "Providers are fairly simple, only needing to handle NATS subscriptions for wRPC, generate types from WIT, and then execute as a standalone binary."
          ]
        },
        {
          "title": "Announcing Instaboot: Instant Cold Starts for Serverless Apps",
          "url": "https://wasmer.io/posts/announcing-instaboot-instant-cold-starts-for-serverless-apps",
          "excerpts": [
            "An unique technology based on WebAssembly that reduces cold starts of serverless apps up to 100-200x, greatly simplifying the maintainance and cost of running ..."
          ]
        },
        {
          "title": "A List of Common Event Types",
          "url": "https://www.fermyon.com/serverless-guide/event-types",
          "excerpts": [
            "Pubsub\n      + Redis\n      + Kafka",
            "Fermyon's Spin allows developers to write their own custom event triggers, though only a few event types are supported on Fermyon Cloud.",
            "* Pubsub",
            "+ Kafka"
          ]
        },
        {
          "title": "Fermyon Spin and Kafka/Kinesis integration (Spin blog)",
          "url": "https://www.fermyon.com/blog/how-i-wrote-a-kinesis-trigger-plugin-for-spin",
          "excerpts": [
            "* **Upleveling performance with polyglot capabilities -** Spin lets your Python/JavaScript services seamlessly invoke efficient Rust code.",
            "Spin is powered by the efficiency and security of Wasm. If you aren’t familiar with server-side Wasm, no worries - you don’t have to be an expert! Here are a few reasons why Spin stands out to me as a developer tool for writing event-driven functions powered by Wasm.",
            "Oct 8, 2024 — The Spin Kinesis trigger aims to replicate the ease of consuming Kinesis data inside AWS cloud via Fermyon Spin applications. [How I Wrote A Kinesis Trigger Plugin For Spin](https://www.fermyon.com/static/image/twc-kinesis-trigger-w-spin.jpg)",
            "This batch of records is processed in 23 milliseconds",
            "* **Observability with batteries included -** If you have a current set of observability tools built on the wonderful, open-source OTel ecosystem, Spin has been making rapid advances to fit into your existing developer toolkit.",
            "Fermyon Spin is powered by the efficiency and security of Wasm.",
            "Spin has an extensible plugin model that makes it easy to connect your Spin applications to external dependencies."
          ]
        },
        {
          "title": "Faasm Presentation: Faasm - Lightweight Isolation for Efficient Stateful Serverless Computing",
          "url": "http://faculty.washington.edu/wlloyd/courses/tcss562_f2020/presentations/T12_Presentation.pdf",
          "excerpts": [
            "Faasm also maintains tail latency of under 150ms for all cold start ratios whereas Knative has a tail latency of over 2 seconds for 35% of calls",
            "Faasm Runtime",
            "Serverless runtime that operates using Faaslets to \nprovide stateful application",
            "Proto-Faaslets are Faaslets that contain a snapshot with \nthe function’s stack, heap, function table, stack pointer \nand data",
            "Key Contributions",
            "Lightweight Isolation of serverless functions",
            "Compiled using WebAssembly",
            "Faasm Integration",
            "Proto-Faaslets",
            "Proto-Faaslets included to further reduce cold start \nlatenc",
            "Proto-Faaslets included to further reduce cold start \nlatenc",
            "\nKey Contributions From Author",
            "Faaslets share the same address space",
            "Fast initialization times",
            "Proto-Faaslets included to further reduce cold start\n\nlatency",
            "Proto-Faaslets are Faaslets that contain a snapshot with\n\nthe function’s stack, heap, function table, stack pointer\n\nand dat",
            "\nInitialization times have been addressed through Proto-Faaslet snapshots",
            "Two-tier state architecture",
            "Integration\n\n○\n\nReplicate Faasm runtime instances with Knative through the\n ... \nbenchmarks reach\n\n50\\-60% extra overhead vs nat",
            "Faaslets begin at ~5 ms initialization and maintain that until around 600 executions/sec",
            "Proto-Faaslets begin at ~0.5 ms initialization and manage that until about 4000 executions/sec"
          ]
        },
        {
          "title": "Spin Cron Trigger",
          "url": "https://www.fermyon.com/blog/spin-cron-trigger",
          "excerpts": [
            "May 22, 2024 — The experimental Cron trigger for Spin allows you to write and deploy Wasm-powered workloads that react to time-based events."
          ]
        },
        {
          "title": "Connect Pub/Sub to Apache Kafka - Google Cloud",
          "url": "https://cloud.google.com/pubsub/docs/connect_kafka",
          "excerpts": [
            "This document describes how to integrate Apache Kafka and Pub/Sub by using the Pub/Sub Group Kafka Connector."
          ]
        },
        {
          "title": "How Kafka Uses the Filesystem to Achieve Lightning Fast Messaging",
          "url": "https://medium.com/@sneharani2509/how-kafka-uses-the-filesystem-to-achieve-lightning-fast-messaging-32c7fccc7c6f",
          "excerpts": [
            "Mention that Kafka's performance comes from sequential I/O, not random writes; Highlight use of sendfile, page cache, and batching; Explain ..."
          ]
        },
        {
          "title": "Cap'n Proto - RPC at the speed of Rust - Part 1 of 2",
          "url": "https://dev.to/kushalj/capn-proto-rpc-at-the-speed-of-rust-part-1-4joo",
          "excerpts": [
            "Cap'n Proto claims to have no serialisation/deserialisation at all once a message is created, which means it should be very fast for transferring data around a ..."
          ]
        },
        {
          "title": "Protobuf vs Flatbuffers vs Cap'n proto which is faster?",
          "url": "https://stackoverflow.com/questions/61347404/protobuf-vs-flatbuffers-vs-capn-proto-which-is-faster",
          "excerpts": [
            "I decided to figure out which of Protobuf, Flatbuffers and Cap'n proto would be the best/fastest serialization for my application."
          ]
        },
        {
          "title": "Data serialization tools comparison: Avro vs Protobuf",
          "url": "https://softwaremill.com/data-serialization-tools-comparison-avro-vs-protobuf/",
          "excerpts": [
            "Jun 30, 2023 — In this post, we will compare the performance of Protobuf and Avro across a range of metrics, including encoding time, decoding time, serialized size, and ..."
          ]
        },
        {
          "title": "Rust vs Node — Kafka producer performance with AWS Lambda",
          "url": "https://mohdizzy.medium.com/rust-vs-node-kafka-producer-performance-with-aws-lambda-831d593a17f0",
          "excerpts": [
            "To explore how much of a difference we are talking I ran a very simple load test on two AWS Lambda functions; one on Node and the other on Rust."
          ]
        },
        {
          "title": "Benchmarking Kafka Write Throughput Performance [2019 UPDATE]",
          "url": "https://aiven-io.medium.com/benchmarking-kafka-write-throughput-performance-2019-update-b863a2cfd9f6",
          "excerpts": [
            "Missing: wasmCloud capability"
          ]
        },
        {
          "title": "Benchmarking Apache Kafka Performance Part 1: Write Throughput",
          "url": "https://aiven.io/blog/benchmarking-kafka-write-throughput",
          "excerpts": [
            "Missing: wasmCloud capability"
          ]
        },
        {
          "title": "I am evaluating Google Pub/Sub vs Kafka. What are the differences?",
          "url": "https://stackoverflow.com/questions/38572071/i-am-evaluating-google-pub-sub-vs-kafka-what-are-the-differences",
          "excerpts": [
            "Google Pub/Sub is a message queue (eg Rabbit MQ) where as Kafka is more of a streaming log. You can't \"re-read\" or \"replay\" messages with Pubsub."
          ]
        },
        {
          "title": "The Upper Limits of WebAssembly Net Performance - Ajani Bilby",
          "url": "https://www.ajanibilby.com/blog/the-upper-limit-of-wasm-performance/",
          "excerpts": [
            "js 1.0, however looking at the details of their benchmarks it shows that running Winter.js in wasm results in a 12x slow down in performance compared to native."
          ]
        },
        {
          "title": "A web application completely written in Rust",
          "url": "https://www.reddit.com/r/rust/comments/8wvw2w/a_web_application_completely_written_in_rust/",
          "excerpts": [
            "I guess my benefit was that I already knew capnproto. For me the protocol definition is one of the best out there, no comparison to proto2/3."
          ]
        },
        {
          "title": "rkyv: a zero-copy deserialization framework for Rust - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/jss6h4/rkyv_a_zerocopy_deserialization_framework_for_rust/",
          "excerpts": [
            "rkyv is similar to other zero-copy deserialization frameworks like Cap'n Proto and FlatBuffers, but it's 100% pure rust and uses macro magic ..."
          ]
        },
        {
          "title": "A faster, more compact, more reliable serialization ...",
          "url": "https://www.reddit.com/r/cpp/comments/1ihksh6/a_faster_more_compact_more_reliable_serialization/",
          "excerpts": [
            "You might want to check out flat buffers or captain proto. There are zero copy protocol generators out there. Unless this is an academic ..."
          ]
        },
        {
          "title": "0-copy your PyArrow array to rust | by Niklas Molin",
          "url": "https://medium.com/@niklas.molin/0-copy-you-pyarrow-array-to-rust-23b138cb5bf2",
          "excerpts": [
            "We've at least seen that its possible to move your PyArrow array to rust with zero data copy. We don't know exactly how to do it secure and nicely."
          ]
        },
        {
          "title": "Zero-copy Apache Arrow with WebAssembly",
          "url": "https://observablehq.com/@kylebarron/zero-copy-apache-arrow-with-webassembly",
          "excerpts": [
            "Zero-copy Apache Arrow with WebAssembly / Kyle Barron | Observable",
            "Aug 22, 2022 — This post will explain how Apache Arrow enables zero-copy data exchange, why it's useful in the browser, and give a live example of reading Arrow memory with ... Zero-copy Apache Arrow with WebAssembly / Kyle Barron | Observable\n",
            "Purpose-built for displays of data\n\nObservable is your go-to platform for exploring data and creating expressive data visualizations. Use reactive JavaScript notebooks for prototyping and a collaborative canvas for visual data exploration and dashboard creation."
          ]
        },
        {
          "title": "MIT 6.858 Final Project: Spark Wasm UDFs with Wasmer",
          "url": "https://css.csail.mit.edu/6.858/2022/projects/rhuffman.pdf",
          "excerpts": [
            "For this project, Wasmer was selected\n\nas it provides an officially supported package for interacting with Wasm modules",
            "Once the Spark session is initialized, the Wasmer JNI is used to load the freshly compiled. Wasm module, creating an object of type org.wasmer.Instance",
            "Unfortunately, a Wasmer Instance is not\n\nserializable, so it cannot be passed directly to Spark",
            "One option is to use Wasmer’s serialization feature, which serializes the module to a\n\nbyte [] , which can be passed around by Spark",
            "However, this byte array must be re-\n\ninstantiated within the UDF, meaning that a new org.wasmer.Instance is created for each\n\nr",
            "This worked with small datasets but quickly caused out-of-memory exceptions when\n\nused with larger datasets",
            "An alternative approach is to create a wrapper class that holds the org.wasmer.Instance\n\nas a static member. With this solution, the instance must be created once per JVM (once\n\nper cluster node) instead of once per row, a significant improvemen",
            "WebAssembly (Wasm) was first introduced by Mozilla in 2017, and enables programs written\n\nin any supported language to be compiled for and run in a web browser",
            "Last, the UDF is invoked on the Spark dataset, creating a new column when using a Map\n\nfunction, or filtering the rows when using a Filter function",
            "Results**\n\nThe current implementation of this system works as described for any numeric Map or Filter\n\nfunction. The compilation of the Wasm module and instantiation of the Wasmer runtime\n\ndo not add significant overhead to the processing of the Spark ",
            "However, using a Wasm\n\nUDF did introduce a significant increase in processing time when compared to a native\n\n4\n\nimplementation of the same opera",
            "On a 1 million row dataset, the Wasm UDF took\n\n2\\.5 times longer to execute, and on a 10 million row dataset, the Wasm UDF took 7 times\n\nlong",
            "These experiments were run with Spark in local mode on a 24-core machine with\n\n96GB of RAM",
            "Spark provides a simple interface for writing SQL-like queries, but also supports arbitrary user-defined functions (UDFs).",
            "Wasm’s memory model also restricts programs\n\nfrom escaping Wasmer’s allocation, preventing a malicious application from corrupting the\n\nmemory of the Spark cluste",
            "The Spark runner is implemented in Java and is responsible for setting up a Spark ses-\n\nsion, loading the Wasm module, and running the Wasm module as a U",
            "This instance must then be\n\npassed to the Spark executor to be used in a UDF",
            "With this solution, the instance must be created once per JVM (once\n\nper cluster node) instead of once per row, a significant improvement",
            "This project has successfully demonstrated that it is feasible to use a Wasm runtime to\n\nsafely execute untrusted code as a Spark User-defined function",
            "While the calls to Wasmer\n\ndo introduce significant overhead, this increase in execution time may be acceptable if it\n\nenables operations that would be otherwise impossible using native Spark, or if the increased\n\nexecution time is outweighed by the significantly decreased initialization ti"
          ]
        },
        {
          "title": "Announcing the first Java library to run WebAssembly",
          "url": "https://blog.wasmer.io/announcing-the-first-java-library-to-run-webassembly-wasmer-jni-89e319d2ac7c",
          "excerpts": [
            "Wasmer JNI is a library to execute WebAssembly directly in Java. It embeds the WebAssembly runtime Wasmer. The first releases provide the core ..."
          ]
        },
        {
          "title": "Wasmtime",
          "url": "https://wasmtime.dev/",
          "excerpts": [
            "Wasmtime is built on the optimizing Cranelift code generator to quickly generate high-quality machine code either at runtime or ahead-of-time. Wasmtime is ..."
          ]
        },
        {
          "title": "Wasmer Java (wasmer-java) documentation",
          "url": "https://github.com/wasmerio/wasmer-java",
          "excerpts": [
            "Easy to use: The wasmer API mimics the standard WebAssembly API,",
            "Fast: The Wasmer JNI executes the WebAssembly modules as fast as possible,",
            "close to **native spe",
            "Safe** : All calls to WebAssembly will be fast, but more\n  importantly, completely safe and sandboxe",
            "The JAR files are named as follows: `wasmer-jni-$(architecture)-$(os)-$(version).jar`",
            "s a toy program in `java/src/test/resources/simple.rs` , written\nin Rust (or any other language that compiles to WebAssembly):",
            "()\n\nQuoting [the WebAssembly site]",
            "There is a toy program in `java/src/test/resources/simple.rs` , written\nin Rust (or any other language that compiles to WebAssembly):",
            "The `Module` class",
            "The `Memory` class"
          ]
        },
        {
          "title": "spark-wasm-udf",
          "url": "https://github.com/slyons/spark-wasm-udf",
          "excerpts": [
            "The part that takes the most code (and the most CPU time) is the serialization/deserialization, which is currently implemented using Protocol Buffers. In a future version I'll replace this with Apache Arrow, much like Spark and DataFusion use already in many places.",
            "This repo is divided up into several sub-projects:\n    * csharp_wasm_udf ,\njs_wasm_udf ,\nrust_wasm_udf , etc are the sources for the various Plugins in their respective languages. Currently missing the implementations for Assemblyscript, C, F#, Go and Typescript. * wasm_spark is the Spark application that actually runs the plugins",
            "Runtime Engine\nThis repo is set up to use both the native Extism runtime and the (still under-development) Chicory runtime. Chicory is a JVM-native WASM runtime made by the same folks who are actively developing Extism. The advantage of having a Java-native runtime when running a Spark project are pretty obvious, but I set up this repo to use both so that they could be compared side-by-side. There is a drawback, however. As of the time of writing, WASI support isn't complete in the Chicory runtime, and as such some of the examples (C# and JavaScript) can't yet run in it.",
            "Performance\nThe current performance isn't up to par, and that's really because of the Serialization/Deserialization step. For better performance I'm definitely going to explore using Apache Arrow to cut down on this time and even support something like vectorization.",
            "Next steps\n    * Add plugin examples for the remaining languages\n    * Explore using a\nMapPartition node instead of a per-row node\n    * Migrate some of the examples to use Apache Arrow instead\n    * Have the UDFs register during some sort of startup step so that return type and arguments can be checked by the Query planner.",
            "Because the Spark rows are marshalled using Protobuf, it's possible to have as deeply nested data as you desire, of any of the types that Spark supports. Calling the plugins from Spark is as easy as using the custom Expression that I've put together:\nWASM_UDF (\npathOrUrl, \" add_two \" , IntegerType ,\ncol( \" id \" ).cast( IntegerType ),\nlit( 2 )\n)\nThis expression takes the path or URL to the WASM file, the function name to call and the expected return type. Everything after that is any number of argument columns that you want to pass to the UDF. This is a bit of an inelegant approach, and one that I hope to improve in the future."
          ]
        },
        {
          "title": "AWS Glue for Apache Spark - Optimize User-Defined Functions",
          "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/tuning-aws-glue-for-apache-spark/optimize-user-defined-functions.html",
          "excerpts": [
            "pache Arrow-based UDFs for vectorized\n  workloads**",
            "The Vectorized UDF is 5 times faster than the previous UDF example. Checking\n`Physical Plan`, you can see `ArrowEvalPython`, which\nshows this application is vectorized by Apache Arrow. To enable Vectorized UDF, you\nmust specify `spark.sql.execution.arrow.pyspark.enabled = true` in your\ncode.",
            "Vectorized UDF\n--------------\n\nExample time is 0.59 (sec). The Vectorized UDF is 5 times faster than the previous UDF example. Checking\n`Physical Plan`, you can see `ArrowEvalPython`, which\nshows this application is vectorized by Apache Arrow. To enable Vectorized UDF, you\nmust specify `spark.sql.execution.arrow.pyspark.enabled = true` in your\ncode.",
            "Spark SQL is much faster than Vectorized UDF, because the tasks are run on each\nexecutor's JVM without a Python runtime . If you can replace your UDF with a\nbuilt-in function, we recommend doing so."
          ]
        },
        {
          "title": "Apache Spark: UDFs, Pandas UDFs, and Arrow-Optimized UDFs (Medium) mixed with Wasm/JNI context",
          "url": "https://ange-kouame.medium.com/apache-spark-a-comparative-overview-of-udf-pandas-udf-and-arrow-optimized-udf-37c62c8d059f",
          "excerpts": [
            "Those functions can be written in Python, R, Java and Scala, enabling more personalized and complex data manipulation.",
            "By introducing Arrow-Optimized UDFs, Spark addressed a number of inherent issues that were not yet resolved with Pandas UDFs such as the support ... UDF stands for User Defined Function."
          ]
        },
        {
          "title": "Ballista Architecture - Apache DataFusion",
          "url": "https://datafusion.apache.org/ballista/contributors-guide/architecture.html",
          "excerpts": [
            "Ballista's primary purpose is to provide a distributed SQL query engine implemented in the Rust programming language and using the Apache Arrow memory model."
          ]
        },
        {
          "title": "First benchmark for DataFusion (Rust) vs Apache Spark for a trivial ...",
          "url": "https://www.reddit.com/r/rust/comments/84t3jj/first_benchmark_for_datafusion_rust_vs_apache/",
          "excerpts": [
            "The perf of DataFusion looks really impressive. You mentioned that Spark performs best on larger datasets than you were benchmarking?"
          ]
        },
        {
          "title": "Dataframe Showdown - Polars vs Spark vs Pandas vs DataFusion ...",
          "url": "https://www.confessionsofadataguy.com/dataframe-showdown-polars-vs-spark-vs-pandas-vs-datafusion-guess-who-wins/",
          "excerpts": [
            "DataFusion was the longest code to write, and super slow. I guess I can take away one thing from this all …"
          ]
        },
        {
          "title": "Home | Apache Gluten incubating - The Apache Software ...",
          "url": "https://gluten.apache.org/",
          "excerpts": [
            "Gluten + Velox backend Performance. The below test environment: single node with 3TB data on Intel® Xeon® Platinum 8592+; Spark-3.3.1 for both baseline and ..."
          ]
        },
        {
          "title": "Introduction to Gluten accelerated Spark engine",
          "url": "https://cloud.ibm.com/docs/watsonxdata?topic=watsonxdata-prov_cpp_gtn_intr",
          "excerpts": [
            "Gluten accelerated Spark engine is an optimized engine that uses Gluten to offload SQL execution to Velox, accelerating SparkSQL computation."
          ]
        },
        {
          "title": "Call WasmEdge functions from an Android APK app",
          "url": "https://wasmedge.org/docs/contribute/source/os/android/apk/",
          "excerpts": [
            "It uses JNI (Java Native Interface) to load a C shared library called wasmedge_lib . It then calls the nativeWasmFibonacci function in wasmedge_lib to execute ..."
          ]
        },
        {
          "title": "Java Native Access Performance - Dmitry Komanov - Medium",
          "url": "https://dkomanov.medium.com/java-native-access-performance-cf4ce0d68ddb",
          "excerpts": [
            "JNI empty just shows a baseline, which is about 22 nanoseconds on my laptop. Seemingly, not a huge overhead (for a simple function that takes ..."
          ]
        },
        {
          "title": "how to use jni in spark? - java native interface",
          "url": "https://stackoverflow.com/questions/21106548/how-to-use-jni-in-spark",
          "excerpts": [
            "First of all, the native library must be preinstalled on all worker nodes. Path to that library must be specified in spark-env.sh.",
            "I want to use jni to call my c++ lib in spark. When i sbt run my program, it shows that java.lang.UnsatisfiedLinkError: no hq_Image_Process in ..."
          ]
        },
        {
          "title": "JNI vs. JNA performance - java",
          "url": "https://stackoverflow.com/questions/22288909/jni-vs-jna-performance",
          "excerpts": [
            "JNA is much slower than JNI, but much easier. If performance is not an issue use JNA. Using direct buffers have the advantage that the most critical operations ..."
          ]
        },
        {
          "title": "How to compile Java to WASM (WebAssembly)?",
          "url": "https://stackoverflow.com/questions/68265863/how-to-compile-java-to-wasm-webassembly",
          "excerpts": [
            "Running Java bytecode inside of a Wasm runtime is possible a few ways. One is to use TeaVM. This page tracks the progress of various methods."
          ]
        },
        {
          "title": "Overhead of Calling Rust FFI from Java JNR/JNI",
          "url": "https://www.reddit.com/r/rust/comments/b0leg9/overhead_of_calling_rust_ffi_from_java_jnrjni/",
          "excerpts": [
            "When I benchmark the Rust code in my test suite, I get around 5ms. This left me wondering if the 5 -> 600ms is because of FFI calls, but I don't ..."
          ]
        },
        {
          "title": "Arrow-optimized Python UDFs in Apache Spark™ 3.5",
          "url": "https://www.databricks.com/blog/arrow-optimized-python-udfs-apache-sparktm-35",
          "excerpts": [
            "Nov 6, 2023 — As shown below, in the same cluster, an Arrow-optimized Python UDF can execute ~1.9 times faster than a pickled Python UDF on a 32 GBs dataset.",
            "Nov 6, 2023 — By harnessing Arrow, these UDFs bypass the traditional, slower methods of data (de)serialization, leading to swift data exchange between JVM and ..."
          ]
        },
        {
          "title": "wasmtime-java bindings and JNI overhead",
          "url": "https://github.com/kawamuray/wasmtime-java",
          "excerpts": [
            "Java (or any JVM) language binding for Wasmtime. Some basic examples are working, but many API implementations are work in progress.",
            "An artifact (JAR) of `wasmtime-java` ships along with prebuilt JNI libraries for some major platforms, so just adding the above dependency provides you a self-contained `wasmtime` runtime on supported platforms:",
            "example",
            "public class HelloWasm {\n    public static void main(String[] args) {\n        try (Store<Void> store = Store.withoutData();\n             Engine engine = store.engine();\n             Module module = Module.fromFile(engine, \"./hello.wat\");\n             Func helloFunc = WasmFunctions.wrap(store, () -> {\n                 System.err.println(\">>> Calling back...\");\n                 System.err.println(\">>> Hello World!\n\");\n             })) {\n            Collection<Extern> imports = Arrays.asList(Extern.fromFunc(helloFunc));\n            try (Instance instance = new Instance(store, module, imports)) {\n                try (Func f = instance.getFunc(\"run\").get()) {\n                    WasmFunctions.Consumer0 fn = WasmFunctions.consumer(f);\n                    fn.accept();\n                }\n            }\n        }\n    }\n}\n"
          ]
        },
        {
          "title": "Stack Overflow: What is the quantitative overhead of making a JNI call",
          "url": "https://stackoverflow.com/questions/13973035/what-is-the-quantitative-overhead-of-making-a-jni-call",
          "excerpts": [
            "``\n\n_Results (x86/i7500/Hotspot/Linux):_\n\n```\nScenario{benchmark=JniCall} 11.34 ns; σ=0.02 ns @ 3 trials\nScenario{benchmark=AddIntOperation} 0.47 ns; σ=0.02 ns @ 10 trials\nScenario{benchmark=AddLongOperation} 0.92 ns; σ=0.02 ns @ 10 trials\n\n       benchmark     ns linear runtime\n         JniCall 11.335 ==============================\n AddIntOperation  0.466 =\nAddLongOperation  0.921 ==\n```",
            "``\n\n_Results (amd64/phenom 960T/Hostspot/Linux):_\n\n```\nScenario{benchmark=JniCall} 6.66 ns; σ=0.22 ns @ 10 trials\nScenario{benchmark=AddIntOperation} 0.29 ns; σ=0.00 ns @ 3 trials\nScenario{benchmark=AddLongOperation} 0.26 ns; σ=0.00 ns @ 3 trials\n\n   benchmark    ns linear runtime\n         JniCall 6.657 ==============================\n AddIntOperation 0.291 =\nAddLongOperation 0.259 =\n```",
            "0.259 =\n```\n\n_Results (armv6/BCM2708/Zero/Linux):_\n\n```\nScenario{benchmark=JniCall} 678.59 ns; σ=1.44 ns @ 3 trials\nScenario{benchmark=AddIntOperation} 183.46 ns; σ=0.54 ns @ 3 trials\nScenario{benchmark=AddLongOperation} 199.36 ns; σ=0.65 ns @ 3 trials\n\n   benchmark  ns linear runtime\n         JniCall 679 ==============================\n AddIntOperation 183 ========\nAddLongOperation 199 ========\n```",
            "This question isn't asking for an absolute timing in nanoseconds, but rather a ball park \"work effort\" in units of \"lines of simple java code\"."
          ]
        },
        {
          "title": "A Java exploring JNI performance via decoding base64 (Medium/Wix Engineering)",
          "url": "https://medium.com/wix-engineering/a-java-exploring-jni-performance-via-decoding-base64-4388683102a2",
          "excerpts": [
            "It's actually pretty close: 12.4 μs (microseconds) with JNI vs 10.2 μs without JNI. Approximately 20% overhead of JNI. Let"
          ]
        },
        {
          "title": "bluejekyll/wasmtime-java: Wasmtime bindings for Java",
          "url": "https://github.com/bluejekyll/wasmtime-java",
          "excerpts": [
            "WASM, web-assembly, is a low-level stack execution engine. This library gives a JNI based binding to the Wasmtime WASM runtime."
          ]
        },
        {
          "title": "Memory Management — Apache Arrow v8.0.0",
          "url": "https://arrow.apache.org/docs/8.0/java/memory.html",
          "excerpts": [
            "Since Arrow always uses direct memory, JNI modules can directly wrap native memory addresses instead of copying data. We use this in modules like the C Data ...",
            "The JVM can optimize I/O operations when using direct memory/direct buffers; it will attempt to avoid copying buffer contents to/from an intermediate buffer."
          ]
        },
        {
          "title": "The Arrow C data interface — Apache Arrow v21.0.0",
          "url": "https://arrow.apache.org/docs/format/CDataInterface.html",
          "excerpts": [
            "The Arrow C data interface defines a very small, stable set of C definitions\nthat can be easily _copied_ in any project’s source code and used for columnar\ndata interchange in the Arrow forma",
            "Make it easy for third-party projects to implement support for (including partial\n  support where sufficient), with little initial investment. * \n  Allow zero-copy sharing of Arrow data between independent runtimes\n  and components running in the same proc",
            "Applications and libraries can therefore work with Arrow memory without\nnecessarily using Arrow libraries or reinventing the wheel.",
            "Expose an ABI-stable interface.",
            "zero-copy sharing of Arrow data between independent runtimes and components running in the same process.",
            "The following free-standing definitions are enough to support the Arrow\nC data interface in your project. Like the rest of the Arrow project, they\nare available under the Apache License 2.0. ```\n#ifndef ARROW_C_DATA_INTERFACE\n#define ARROW_C_DATA_INTERFACE\n\n#define ARROW_FLAG_DICTIONARY_ORDERED 1\n#define ARROW_FLAG_NULLABLE 2\n#define ARROW_FLAG_MAP_KEYS_SORTED 4\n\nstruct ArrowSchema {\n  // Array type description\n  const char * format ;\n  const char * name ;\n  const char * metadata ;\n  int64_t flags ;\n  int64_t n_children ;\n  struct ArrowSchema ** children ;\n  struct ArrowSchema * dictionary ;\n\n  // Release callback\n  void ( * release )( struct ArrowSchema * );\n  // Opaque producer-specific data\n  void * private_data ;\n};\n\nstruct ArrowArray {\n  // Array data description\n  int64_t length ;\n  int64_t null_count ;\n  int64_t offset ;\n  int64_t n_buffers ;\n  int64_t n_children ;\n  const void ** buffers ;\n  struct ArrowArray ** children ;\n  struct ArrowArray * dictionary ;\n\n  // Release callback\n  void ( * release )( struct ArrowArray * );\n  // Opaque producer-specific data\n  void * private_data ;\n};\n\n#endif  // ARROW_C_DATA_INTERFACE\n```"
          ]
        },
        {
          "title": "org.apache.arrow » arrow-c-data",
          "url": "https://mvnrepository.com/artifact/org.apache.arrow/arrow-c-data",
          "excerpts": [
            "Java implementation of C Data Interface. License, Apache 2.0. Tags, dataapachearrow. Ranking, #34630 in MvnRepository (See Top Artifacts)."
          ]
        },
        {
          "title": "Apache Arrow Java Memory Management",
          "url": "https://arrow.apache.org/docs/java/memory.html",
          "excerpts": [
            "Since Arrow always uses direct memory, JNI modules can directly wrap native memory addresses instead of copying data.",
            "The JVM can optimize I/O operations when using direct memory/direct buffers; it will attempt to avoid copying buffer contents to/from an intermediate buffer."
          ]
        },
        {
          "title": "Benchmarking WebAssembly Runtimes | by Brandon Fish",
          "url": "https://blog.wasmer.io/benchmarking-webassembly-runtimes-18497ce0d76e",
          "excerpts": [
            "We performed a small set of benchmarks designed to test runtime performance: fannkuch, fibonacci, n-body, and sha1."
          ]
        },
        {
          "title": "ArrowArray (Apache Arrow Java Root POM 19.0.0-SNAPSHOT API)",
          "url": "https://arrow.apache.org/java/main/reference/org.apache.arrow.c/org/apache/arrow/c/ArrowArray.html",
          "excerpts": [
            "C Data Interface ArrowArray. Represents a wrapper for the following C structure: struct ArrowArray { // Array data description int64_t length; ..."
          ]
        },
        {
          "title": "Java Implementation — Apache Arrow v21.0.0",
          "url": "https://arrow.apache.org/docs/java/index.html",
          "excerpts": [
            "The Java API of Apache Arrow includes creating ValueVectors, Fields, Schemas, and VectorSchemaRoots, and also covers memory management, and Vector life cycle."
          ]
        },
        {
          "title": "How to create Apache Arrow vectors in Java, pass them ...",
          "url": "https://stackoverflow.com/questions/75213449/how-to-create-apache-arrow-vectors-in-java-pass-them-to-c-code-through-jni-r",
          "excerpts": [
            "Use the C Data Interface module in Java to export your Java arrays and get the address of a struct ArrowArray, and then pass that address to your C++ code via ..."
          ]
        },
        {
          "title": "Introducing the Apache Arrow C Data Interface",
          "url": "https://arrow.apache.org/blog/2020/05/03/introducing-arrow-c-data-interface/",
          "excerpts": [
            "May 3, 2020 — Apache Arrow includes a cross-language, platform-independent in-memory columnar format allowing zero-copy data sharing and transfer between heterogenous ...",
            "The C Data Interface stays close to the natural way of expressing Arrow-like\ndata in C or C++. Only two aspects involve non-trivial marshalling:\n\n* the encoding of data types, using a very simple string-based language\n* the encoding of optional metadata, using a very simple length-prefixed for"
          ]
        },
        {
          "title": "Uses of Class org.apache.arrow.c.ArrowArray",
          "url": "https://arrow.apache.org/java/main/reference/org.apache.arrow.c/org/apache/arrow/c/class-use/ArrowArray.html",
          "excerpts": [
            "Export the current contents of a Java Table using the C data interface format. static void. Data. exportVector(BufferAllocator allocator, FieldVector vector, ..."
          ]
        },
        {
          "title": "Using Arrow Flight with Spark Java applications in IBM Cloud",
          "url": "https://developer.ibm.com/tutorials/awb-using-arrow-flight-spark-java-applications-ibm-cloud/",
          "excerpts": [
            "This tutorial demonstrates how the Arrow Flight service provided by IBM Cloud Pak for Data can be used to read and write data sets from within a Spark Java ..."
          ]
        },
        {
          "title": "DataFusion courtesy of Rust, vs Spark. Performance and other ...",
          "url": "https://www.confessionsofadataguy.com/datafusion-courtesy-of-rust-vs-spark-performance-and-other-thoughts/",
          "excerpts": [
            "The performance issue to me is a huge downside, it's over twice as slow as Spark on a small and simple workload. I can imagine that gap would ..."
          ]
        },
        {
          "title": "Accelerate Spark SQL on Arm64 with Gluten and Velox",
          "url": "https://community.arm.com/arm-community-blogs/b/servers-and-cloud-computing-blog/posts/spark-sql-arm64-gluten-velox",
          "excerpts": [
            "Aug 7, 2024 — The following chart, figure 4, shows the performance ratio comparing the Gluten + Velox backend with the Spark Java engine on Neoverse N2, using ..."
          ]
        },
        {
          "title": "Performance in Apache Spark: Benchmark 9 Different ...",
          "url": "https://medium.com/data-science/performance-in-apache-spark-benchmark-9-different-techniques-955d3cc93266",
          "excerpts": [
            "In this article, we will compare 9 different techniques that lead to the same result, but each of them has a very different performance."
          ]
        },
        {
          "title": "Spark UDFs, how to write them and some gotchas? - Medium",
          "url": "https://medium.com/@achilleus/spark-udfs-we-can-use-them-but-should-we-use-them-2c5a561fde6d",
          "excerpts": [
            "UDFs are one exciting aspect of spark which has evolved tremendously over the spark releases. We will try to cover different aspects of it."
          ]
        },
        {
          "title": "What are user-defined functions (UDFs)? | Databricks on AWS",
          "url": "https://docs.databricks.com/aws/en/udf/",
          "excerpts": [
            "Common use cases for UDFs include data encryption, decryption, hashing, JSON parsing, and validation. Use Apache Spark methods for operations on ..."
          ]
        },
        {
          "title": "webassembly",
          "url": "https://mvnrepository.com/tags/webassembly",
          "excerpts": [
            "This library contains the primitives that form the Compose UI Toolkit, such as drawing, measurement and layout."
          ]
        },
        {
          "title": "the Safer Alternative to Integrating Native Code in Java",
          "url": "https://www.reddit.com/r/java/comments/1es8779/webassembly_the_safer_alternative_to_integrating/",
          "excerpts": [
            "They are designed to reduce the chances of breaking fault isolation, whereas Wasm eliminates it by never leaving the JVM."
          ]
        },
        {
          "title": "Powered by | Apache Arrow",
          "url": "https://arrow.apache.org/powered_by/",
          "excerpts": [
            "It supports Arrow as input and output, uses Arrow internally to maximize performance, and can be used with existing Apache Spark™ APIs. Fletcher: Fletcher is a ..."
          ]
        },
        {
          "title": "Apache Arrow with Spark for Faster Data Processing & Analytics",
          "url": "https://medium.com/@biltumaji/apache-arrow-with-spark-for-faster-data-processing-analytics-3bc716c2a00a",
          "excerpts": [
            "In this Demonstration, I'll explain what PyArrow is and why its integration with Spark (Pyspark) and pandas may supercharge our data manipulation."
          ]
        },
        {
          "title": "Apache Arrow and Java: Lightning Speed Big Data Transfer",
          "url": "https://www.infoq.com/articles/apache-arrow-java/",
          "excerpts": [
            "Conceptually, Apache Arrow is designed as a backbone for Big Data systems, for example, Ballista or Dremio, or for Big Data system integrations."
          ]
        },
        {
          "title": "Apache Arrow Java Cookbook documentation",
          "url": "https://arrow.apache.org/cookbook/java/",
          "excerpts": [
            "The Apache Arrow Cookbook is a collection of recipes which demonstrate how to solve many common tasks that users might need to perform when working with Arrow ..."
          ]
        },
        {
          "title": "Apache Arrow: A Beginner's Guide with Practical Examples",
          "url": "https://www.datacamp.com/tutorial/apache-arrow",
          "excerpts": [
            "Arrow Array: An Array represents a single column of data. It stores values in a contiguous memory block, allowing fast access, efficient ..."
          ]
        },
        {
          "title": "Package org.apache.arrow.dataset.jni",
          "url": "https://javadoc.io/static/org.apache.arrow/arrow-dataset/18.3.0/org.apache.arrow.dataset/org/apache/arrow/dataset/jni/package-summary.html",
          "excerpts": [
            "Reserving Java direct memory bytes from java.nio.Bits. JniLoader. The JniLoader for Dataset API's native implementation. JniWrapper. JNI wrapper for Dataset ..."
          ]
        },
        {
          "title": "Benchmarking Runtime Scripting Performance in Wasmer",
          "url": "https://dl.acm.org/doi/pdf/10.1145/3491204.3527477",
          "excerpts": [
            "This is likely because the cost of calling into WASM is a fixed overhead, and this overhead gets amortized when the called function does some ..."
          ]
        },
        {
          "title": "A WebAssembly runtime for running Rust functions in any other ...",
          "url": "https://www.reddit.com/r/rust/comments/11r8bj2/announcing_scale_a_webassembly_runtime_for/",
          "excerpts": [
            "At the end of the day, there's a legitimate overhead to calling a scale function - you need to start the webassembly VM, serialize input, send ..."
          ]
        },
        {
          "title": "RisingWave user-defined functions: Overview",
          "url": "https://risingwave.com/blog/risingwave-user-defined-functions-overview/",
          "excerpts": [
            "Jun 21, 2024 — Example 1: Simplify queries with SQL UDFs · Example 2: Mathematical calculations with Rust UDFs · Example 3: Parse protobuf data with Rust UDFs.",
            "Jun 21, 2024 — Rust UDFs are suitable for heavy computation tasks, compiled into WebAssembly to run in RisingWave's built-in container.",
            "This code is compiled to WebAssembly in the RisingWave frontend and achieves near-native performance when run on computation nodes using JIT (Just-in-time) compilation.",
            "Rust UDFs are suitable for heavy computation tasks, compiled into WebAssembly to run in RisingWave's built-in container.",
            "They target different users and scenarios:",
            "Rust is a high-performance systems programming language and is also used to develop RisingWave. Rust UDFs are suitable for heavy computation tasks, compiled into WebAssembly to run in RisingWave's built-in container.",
            "The third dimension is the function’s execution method, which affects performance and capabilities.",
            "For embedded UDFs, including Rust (WebAssembly), JavaScript, and Python, we embed wasmtime, quickjs/deno, and CPython as the language runtime, respectively."
          ]
        },
        {
          "title": "WASM+UDF performance comparison in Scylladb, why ...",
          "url": "https://stackoverflow.com/questions/77028248/wasmudf-performance-comparison-in-scylladb-why-there-is-no-obvious-differance",
          "excerpts": [
            "In any case, the performance difference between Lua and WASM, if it's significant, will only be visible in long and complex functions that need ..."
          ]
        },
        {
          "title": "Performance questions #96 - bytecodealliance/wasmtime-py - GitHub",
          "url": "https://github.com/bytecodealliance/wasmtime-py/issues/96",
          "excerpts": [
            "I was trying to see what kind of overhead there is to call a wasm function from Python. I am using WSL2 on Windows, with a recent Fedora ..."
          ]
        },
        {
          "title": "Working with UDFs in Apache Spark | Blog",
          "url": "https://www.cloudera.com/blog/technical/working-with-udfs-in-apache-spark.html",
          "excerpts": [
            "Feb 3, 2017 — This blog post provided a walk-through of UDF and UDAF implementation and discussed integration steps to make use of existing Java Hive UDFs ..."
          ]
        },
        {
          "title": "Apache Arrow - C Data Interface",
          "url": "https://arrow.apache.org/docs/java/cdata.html",
          "excerpts": [
            "Arrow supports exchanging data without copying or serialization within the same process\nthrough the [The Arrow C data interface](../format/CDataInterface.html) , even between different language runtimes.",
            "This application uses JNI to call Java code, but transfers data (zero-copy) via the C Data Interface "
          ]
        },
        {
          "title": "A Substack/Mirrored Exploration of JNI/Wasmtime/Wasmer Overheads for Spark-WASM UDFs",
          "url": "https://connorahaskins.substack.com/p/jni-marshalling-performance-single",
          "excerpts": [
            "A rough number for the base cost of a JNI call is estimated anywhere between ~0 to ~104ns on modern devices.",
            "Sending an integer array can tack on a base cost upwards of 300ns on modern devices."
          ]
        },
        {
          "title": "Wasm UDFs for Postgres, implemented as a TLE on plv8 ...",
          "url": "https://www.github-zh.com/projects/680030284-pg_extism",
          "excerpts": [
            "Jan 26, 2025 — Wasm UDFs for Postgres, implemented as a TLE on plv8 using Extism.",
            "dylibso / pg_extism · 星标32 · 复刻1. 代码 问题. main. Ask AI ... 项目介绍及使用教程请前往GitHub 阅读README. 0 条讨论. 登录后发表评论 ..."
          ]
        },
        {
          "title": "Bringing WebAssembly to PostgreSQL using Extism",
          "url": "https://dylibso.com/blog/pg-extism/",
          "excerpts": [
            "Introducing `pg_extism`",
            "`pg_extism` uses [`pgrx`](https://github.com/pgcentralfoundation/pgrx) and the\nExtism Rust SDK to create a PostgreSQL extension that can run Extism plugins.",
            "The implementation is based on these 2 functions:\n\n`extism_call` takes a path of a Extism plugin (a `.wasm` file), and the name of\na function exported by the Extism plugin, and an input JSON object.",
            "ts/host-sdk) provide developer-friendly constructs on top of Wasm’s sandboxing and capability-based security features.",
            "Users can easily define their own custom plugins and use them in their SQL\nqueries.",
            "Notice how we’re using `extism_call` to invoke the Extism plugin when the SQL\nfunction is called.",
            "This will define an SQL function called `llm` with the following schema:",
            "CREATE OR REPLACE FUNCTION llm(prompt TEXT , payload TEXT ) RETURNS TEXT AS $$",
            "In PostgreSQL, users will only need to call `extism_define` explicitly:",
            "In PostgreSQL, users will only need to call `extism_define` explicitly:",
            "`extism_call` then instantiates the plugin using Extism, and calls it and\nreturns the result as a JSON object.",
            "This plugin is implemented in Go, using Extism’s [Go PDK](https://github.com/extism/go-pdk) .",
            "This plugin is implemented in Go, using Extism’s [Go PDK](https://github.com/extism/go-pdk) .",
            "### ChatGPT plugin",
            "This is an Extism plugin that sends a prompt to OpenAI’s ChatGPT API and returns\nthe result.",
            "Users can use the plugin in SQL like this:",
            "pg_extism = # select extism_define( '/path/to/chatgpt.wasm' , 'llm' );",
            "In PostgreSQL, users will only need to call `extism_define` explicitly:\n\n```\nselect extism_define( '/path/to/chatgpt.wasm' , 'llm' );\n```\n\nThis will define an SQL function called `llm` with the following schema:\n\n```\nCREATE OR REPLACE FUNCTION llm(prompt TEXT , payload TEXT ) RETURNS TEXT AS $$\nDECLARE\n    result_json json ;\n    input_param json ;\nBEGIN\n    -- Construct JSON object from parameters\n    input_param : = json_build_object( 'payload' , payload, 'prompt' , prompt);\n\n    -- Call the extism_define function using the provided parameters\n    SELECT extism_call( '/path/to/chatgpt.wasm' , 'chatgpt' , input_param) INTO result_json;\n\n    -- Return the desired field from the result JSON\n    RETURN (result_json -> 'response' ):: TEXT ;\nEXCEPTION\n    WHEN others THEN\n        -- Handle exceptions if necessary\n        RAISE NOTICE 'An error occurred: %' , SQLERRM;\n        RETURN NULL ;\nEND ;\n$$ LANGUAGE plpgsql;\n```\n\nNotice how we’re using `extism_call` to invoke the Extism plugin when the SQ"
          ]
        },
        {
          "title": "pg_extism - WebAssembly UDFs for Postgres (Extism integration via TLE on plv8)",
          "url": "https://github.com/dylibso/pg_extism",
          "excerpts": [
            "Wasm UDFs for Postgres, implemented as a TLE on plv8 using Extism",
            "Make sure [`plv8`](https://github.com/plv8/plv8) is installed on your server. Then enable it for your database:",
            "Now run the script in [dist/index.sql](/dylibso/pg_extism/blob/main/dist/index.sql) and you'll get two SQL functions: `extism_create_plugin` and `extism_call` .",
            "Assume you have a table called `plugins` :",
            "CREATE TABLE public .plugins (",
            "id int4 GENERATED ALWAYS AS IDENTITY( INCREMENT BY 1 MINVALUE 1 MAXVALUE 2147483647 START 1 CACHE 1 NO CYCLE) NOT NULL ,",
            "\" data \" bytea NULL ,",
            "\" name \" varchar NULL ,",
            "CONSTRAINT plugins_pk PRIMARY KEY (id)",
            ");",
            "The above example shows how you can use `extism_create_plugin` to supply your own host functions to the plugin.",
            "You can find th source code for the plugin [here](https://github.com/extism/plugins/tree/main/count_vowels_kvstore) ."
          ]
        },
        {
          "title": "Extism: make all software programmable",
          "url": "https://extism.org/blog/announcing-extism/",
          "excerpts": [
            "Dec 1, 2022 — Extism brings extensibility to software of all shapes and sizes in a way which we believe to have been previously unacheivable."
          ]
        },
        {
          "title": "Postgres WASM by Snaplet and Supabase",
          "url": "https://supabase.com/blog/postgres-wasm",
          "excerpts": [
            "Oct 3, 2022 — postgres-wasm is a PostgreSQL server that runs inside a browser. It provides a full suite of features, including persisting state to browser, restoring from pg ..."
          ]
        },
        {
          "title": "WebAssembly triggers in libSQL - Medium",
          "url": "https://medium.com/chiselstrike/webassembly-triggers-in-libsql-b5eb62cc1c6",
          "excerpts": [
            "User-defined WebAssembly functions in libSQL can be used to create powerful triggers, making it easy to automate your workflows.",
            "libSQL 0.1.0 allows users to create user-defined functions not only programmatically, but also with the CREATE FUNCTION statement, well known in ..."
          ]
        },
        {
          "title": "WebAssembly triggers in libSQL",
          "url": "https://turso.tech/blog/webassembly-triggers-in-libsql-b5eb62cc1c6",
          "excerpts": [
            "User-defined WebAssembly functions in libSQL can be used to create powerful triggers, making it easy to automate your workflows."
          ]
        },
        {
          "title": "Learning about sqlite3 WASM",
          "url": "https://deepakness.com/blog/sqlite3-wasm/",
          "excerpts": [
            "Apr 24, 2025 — sqlite3 WASM is a version of SQLite built for web browsers using WebAssembly, allowing a full SQLite database to run directly in the browser."
          ]
        },
        {
          "title": "PGlite",
          "url": "https://pglite.dev/",
          "excerpts": [
            "A complete WASM build of Postgres that's under 3MB Gzipped. Extendable. Dynamic extension loading mechanism, including support for pgvector. Reactive."
          ]
        },
        {
          "title": "snaplet/postgres-wasm: A PostgresQL server running in ...",
          "url": "https://github.com/snaplet/postgres-wasm",
          "excerpts": [
            "Sep 19, 2024 — A PostgresQL server running in your browser. Contribute to snaplet/postgres-wasm development by creating an account on GitHub."
          ]
        },
        {
          "title": "Why SQLite is so great for the edge - Turso",
          "url": "https://turso.tech/blog/why-sqlite-is-so-great-for-the-edge-ee00a3a9a55f",
          "excerpts": [
            "SQLite, as well as libSQL, can be compiled into a concise Wasm module — just 390 KiB compressed, less than 900 KiB uncompressed size. It can run ..."
          ]
        },
        {
          "title": "Blog | Extism - make all software programmable. Extend ...",
          "url": "https://extism.org/blog/",
          "excerpts": [
            "Sep 18, 2024 — Other projects with similar goals like sqlite or openssl , are low-level system components exposed to a multitude of language environments ..."
          ]
        },
        {
          "title": "Image Layer Details - danstaken/libsql:libsql-0.1.0-wasm-udf ...",
          "url": "https://hub.docker.com/layers/danstaken/libsql/libsql-0.1.0-wasm-udf/images/sha256-2a14b87b20380863f3a14b7f670692246c8712e5188fa5b0aa31efa0ffa1450f",
          "excerpts": [
            "danstaken/libsql:libsql-0.1.0-wasm-udf ; OS/ARCH. linux/amd64 ; Compressed size. 1.11 GB ; Last pushed. 9 months by danstaken ; Type. Image."
          ]
        },
        {
          "title": "RisingWave user-defined functions: Rust x WebAssembly",
          "url": "https://risingwave.com/blog/user-defined-functions-rust-x-webassembly/",
          "excerpts": [
            "Jul 4, 2024 — This article discusses the design and implementation of Rust UDFs in RisingWave. Rust UDFs achieve isolation by compiling to WebAssembly while maintaining high ...",
            "The input data is stored in a dynamically allocated memory within the module using its `alloc` function. Then, the function is called, and the output is obtained.",
            " Rust UDFs are compiled into WebAssembly, a virtual machine assembly language originally designed for Web browsers. They are then executed within a WebAssembly virtual machine embedded in RisingWave using Just-In-Time (JIT) compilation, offering high efficiency with minimal performance loss compared to native instructions",
            "iance/wasmtime) as the runtime. It first loads the decompressed WebAssembly module, then scans all functions and types according to the protocol.",
            "ave frontend generates a temporary Rust project locally. The embedded Rust code snippet is extracted and supplemented with necessary auxiliary code.",
            "The frontend adds use statements and SQL signatures, generating a `lib.rs` file like this:\n\n```\nuse arrow_udf::{function, types::StructType}; // import prelude\n\n#[function(\"gcd(int, int) -> int\")]\nfn gcd(mut a: i32, mut b: i32) -> i32 {\n    while b != 0 {\n        (a, b) = (b, a % b);\n    }\n    a\n}\n\n```",
            "The wasm file is compressed and stored as part of the function definition in the catalog metadata, to be read and decompressed during execution.",
            "Currently, WebAssembly modules only support pure computation, with almost no external access, including network and file system access. This is because wasmtime defaults to sandbox mode.",
            "In the future, we might support more languages that compile to WebAssembly. Internal implementation",
            "Performance\n\nCan Rust UDFs achieve high performance? If no WebAssembly layer is used for isolation, theoretically, by loading dynamic libraries and running native instructions directly, UDFs would perform identically to built-in functions. If users have complete control over RisingWave and pursue extreme performance, this may be the best solution. However, the downside is that UDFs might cause RisingWave to block or even crash. Users must bear the risk themselves. Using WebAssembly JIT execution, after warm-up, the execution time of pure functions is about 1.5-2x that of native execution.",
            "When receiving a `create function ... language rust ...` statement, the RisingWave frontend generates a temporary Rust project locally.",
            "UDFs in RisingWave. Rust UDFs achieve isolation by compiling to WebAssembly while maintaining high performance. We reused the `#[function]` procedural macro from the internal function framework to provide a concise interface for users.",
            "themselves. Using WebAssembly JIT execution, after warm-up, the execution time of pure functions is about 1.5-2x that of native execution. However, considering the additional data transmission overhead, the actual end-to-end execution time may be longer.",
            "Since each WebAssembly instance can only run single-threaded, we maintain an instance pool for multithreaded parallel execution."
          ]
        },
        {
          "title": "Support WASM plugins for UDF/TableFunctions/more",
          "url": "https://github.com/ClickHouse/ClickHouse/issues/36892",
          "excerpts": [
            "Executable table functions are available in clickhouse. I'm not sure how WASM will help here (apart from probably tighter security). **Describe the solution you'd like**",
            "Loadable WASM with known strictly typed interfaces that can be executed upon to not only be able to execute functions, but possibly even alter the AST of the query for optimization, or possibly an entire table function or table engine as a WASM blob. The advantage here would be passing rich types in and out of known interfaces that can be met by coding in any language that can compile to WASM, which is a fair number at this point. WASM runtimes for c[++] are around so this should be rather possible.",
            "executable UDFs are available in clickhouse but that is a strict stdin/stdout flow of data, and could not easily be used in a more constructive way like table functions or AST optimization."
          ]
        },
        {
          "title": "UDFs User Defined Functions | ClickHouse Docs",
          "url": "https://clickhouse.com/docs/sql-reference/functions/udf",
          "excerpts": [
            "ClickHouse can call any external executable program or script to process data. The configuration of executable user defined functions can be located in one ...",
            "Please contact ClickHouse Support at https://clickhouse.cloud/support to access. ClickHouse can call any external executable program or script to process data."
          ]
        },
        {
          "title": "Qdrant Summer of Code 2024 - WASM based Dimension Reduction",
          "url": "https://qdrant.tech/articles/dimension-reduction-qsoc/",
          "excerpts": [
            "My journey as a Qdrant Summer of Code 2024 participant working on enhancing vector visualization using WebAssembly (WASM) based dimension ..."
          ]
        },
        {
          "title": "Qdrant driver | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/zh/develop/rust/database/qdrant_driver",
          "excerpts": [
            "Qdrant driver. WasmEdge is emerging as a lightweight, portable, and embeddable runtime for large language models (LLMs). LLM inference applications, such as ..."
          ]
        },
        {
          "title": "Bringing WebAssembly to PostgreSQL Using Extism",
          "url": "https://news.ycombinator.com/item?id=37225441",
          "excerpts": [
            "Aug 22, 2023 — Here we have an example of embedding Extism[0] into Postgres and calling UDF/plugins to run arbitrary compute, call-outs to LLMs for enrichment, ..."
          ]
        },
        {
          "title": "Running PostgreSQL in the Browser with WebAssembly - InfoQ",
          "url": "https://www.infoq.com/news/2024/05/pglite-wasm-postgres-browser/",
          "excerpts": [
            "With PGlite, developers can run PostgreSQL queries in a JavaScript environment and persist data to storage in the file system (Node/Bun) or indexedDB (browser)."
          ]
        },
        {
          "title": "Wasmtime: Supporting UDFs in ScyllaDB with WebAssembly",
          "url": "https://www.p99conf.io/2022/09/15/wasmtime-supporting-udfs-in-scylladb-with-webassembly/",
          "excerpts": [
            "It turns out that WASM is also a perfect candidate for a user-defined functions (UDFs) back-end, thanks to its ease of integration, performance, ..."
          ]
        },
        {
          "title": "Bringing WebAssembly to PostgreSQL using Extism",
          "url": "https://www.reddit.com/r/PostgreSQL/comments/15ybmxe/bringing_webassembly_to_postgresql_using_extism/",
          "excerpts": [
            "I think this snippet is such a cool extension... running AI-assisted summarization / enrichment _inside_ the query! pg_extism=# select ..."
          ]
        },
        {
          "title": "Wind-Gone/awesome-olap-paper",
          "url": "https://github.com/Wind-Gone/awesome-olap-paper",
          "excerpts": [
            "A curated paper list of awesome Online Analytical Processing database systems, theory, frameworks, resources, tools and other awesomeness"
          ]
        },
        {
          "title": "Use UDFs in Rust",
          "url": "https://docs.risingwave.com/sql/udfs/use-udfs-in-rust",
          "excerpts": [
            "This article provides a step-by-step guide for defining Rust functions in RisingWave. Rust functions are compiled into WebAssembly modules. Use UDFs in Rust - RisingWave",
            "This article provides a step-by-step guide for defining Rust functions in RisingWave. Rust functions are compiled into WebAssembly modules and then run on the embedded WebAssembly virtual machine in RisingWave.",
            "Rust UDFs provide **higher performance** (near native) compared to Python and Java. The **RisingWave kernel** handles the management of Rust UDFs, eliminating the need for additional maintenance.",
            ". Declare your functions in RisingWave\n\nIn RisingWave, use the [CREATE FUNCTION](/sql/commands/sql-create-function) command to declare the functions you defined. There are two ways to load the WASM module:\n\n1.\nEmbed the WASM binary into SQL with base64 encoding. You can use the following command in psql:\n\nCopy\n\nAsk AI\n\n```\n\\ set wasm_binary \\`base64 -i path/to/udf.wasm\\`\nCREATE FUNCTION gcd ( int , int ) RETURNS int LANGUAGE wasm U",
            "The syntax is outlined below:\n\nCopy\n\nAsk AI\n\n```\nCREATE FUNCTION function_name ( arg_name arg_type [, ... ] )\n    [ RETURNS return_type | RETURNS TABLE ( column_name column_type [, ... ] ) ]\n    LANGUAGE rust\n    AS [ $$ function_body $$ | 'function_body' ];\n```\n",
            "Currently, in `CREATE FUNCTION` statement, Rust code can only use libraries from the standard library, `chrono` , `rust_decimal` , `serde_json` , and does not support other third-party libraries.",
            "Use UDFs in Rust - RisingWave"
          ]
        },
        {
          "title": "Wasmer Postgres - Wasmer GitHub",
          "url": "https://github.com/wasmerio/wasmer-postgres",
          "excerpts": [
            "The Postgres\nns_sum signature is\n(integer, integer) -> integer ,\nwhich maps the Rust\nsum signature\n(i32, i32) -> i32 . So far, only the WebAssembly types\ni32 ,\ni64 and\nv128 are\nsupported; they respectively map to\ninteger ,\nbigint and\ndecimal in Postgres.",
            "A complete and mature WebAssembly runtime for Postgres based on Wasmer .\nIt's an original way to extend your favorite database capabilities. Features:\n    * Easy to use : The\nwasmer API mimics the standard WebAssembly API",
            "smer Postgres\nA complete and mature WebAssembly runtime for Postgres based on Wasmer .\nIt's an original way to extend your favorite database capabilities. Features:\n    * Easy to use : The\nwasmer API mimics the standard WebAssembly API,\n    * Fast :\nwasmer executes the WebAssembly modules as fast as\npossible, close to native speed ,\n    * Safe : All calls to WebAssembly will be fast, but more\nimportantly, completely safe and sandboxed. Note: The project is still in heavy development. This is a\n0.1.0 version. Some API are missing and are under implementation. But\nit's fun to play with it.",
            "    * Fast :\nwasmer executes the WebAssembly modules as fast as\npossible, close to native speed ,",
            "Safe : All calls to WebAssembly will be fast, but more\nimportantly, completely safe and sandboxed. Note: The project is still in heavy development. This is a\n0.1.0 version. Some API are missing and are under implementation. But\nit's fun to play with it. Installation",
            "Supported Postgres versions\nSo far, the extension works on Postgres 10 only.",
            "Usage & documentation\nConsider the\nexamples/simple.rs program:\n# [ no_mangle ] pub extern fn sum ( x : i32 , y : i32 ) -> i32 { x + y }",
            "Benchmarks\nBenchmarks are useless most of the time, but it shows that WebAssembly\ncan be a credible alternative to procedural languages such as\nPL/pgSQL. Please, don't take those numbers for granted, it can change\nat any time, but it shows promising results:",
            "Benchmarks\nBenchmarks are useless most of the time, but it shows that WebAssembly\ncan be a credible alternative to procedural languages such as\nPL/pgSQL. Please, don't take those numbers for granted, it can change\nat any time, but it shows promising results:\n\n\n\n\n    * Benchmark: Fibonacci (n = 50)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.206\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 0.431\n    * Benchmark: 2×\n\n\n    * Benchmark: Fibonacci (n = 500)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.217\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 2.189\n    * Benchmark: 10×\n\n\n    * Benchmark: Fibonacci (n = 5000)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.257\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 18.643\n    * Benchmark: 73×"
          ]
        }
      ],
      "reasoning": "The finegrained field describes a Linux-based RTOS-like setup for WebAssembly workloads, emphasizing host OS features to reduce jitter and provide deterministic scheduling (PREEMPT_RT kernel, CPU isolation via isolcpus, nohz_full, rcu_nocbs, and CPU pinning via taskset/cset). The most directly relevant excerpts establish the kernel-level controls and boot parameters: isolcpus, nohz_full, and rcu_nocbs, as well as CPU affinity tooling. The following excerpts explicitly identify these mechanisms and how they contribute to reduced jitter and tighter scheduling boundaries on Linux: the kernel parameter listing including isolcpus, nohz_full, and rcu_nocbs; and explicit discussion of how isolating CPUs, stopping the scheduler tick on those CPUs, and offloading RCU callbacks reduces OS jitter. Additional supporting material shows high-level RTOS-like scheduling concepts (PREEMPT_RT) and guidance on CPU pinning and topology alignment to minimize latency, which complements the RTOS-like narrative for Wasm runtimes running on these hardened hosts. The combination of these kernel boot-time controls and OS scheduling strategies provides a practical path toward running Wasm modules with near real-time characteristics on Linux, aligning with the described finegrained field value. The evidence also notes that such host-driven real-time configurations are the responsibility of the OS and orchestration stack (e.g., Kubernetes CPU Manager static policy and topology manager) to ensure dedicated cores and deterministic behavior, reinforcing that the RTOS-like behavior is achieved at the host/cluster level rather than inside the Wasm module itself.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques",
      "citations": [
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified ",
            " cpusets instead to disable scheduler load\n                          balancing through the \"cpuset.sched_load_balance\" file. It offers a much more flexible interface where CPUs can\n                "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "realtime:documentation:start [Wiki]",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/start",
          "excerpts": [
            "Oct 2, 2023 — Each PREEMPT_RT stable release is tagged with the version number of the upstream Linux stable kernel as well as its own “-rt” counter that is ..."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ...",
            "Oct 3, 2023 — The main aim of the real-time preemption is to minimize the amount of kernel code that is non-preemptible."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "Control Group v2 - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/admin-guide/cgroup-v2.html",
          "excerpts": [
            "This is the authoritative documentation on the design, interface and conventions of cgroup v2. It describes all userland-visible aspects of cgroup including ..."
          ]
        },
        {
          "title": "Low Latency Tuning Guide | Erik Rigtorp",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "This guide describes how to tune your AMD64/x86_64 hardware and Linux system for running real-time or low latency workloads."
          ]
        },
        {
          "title": "Limiting Container Resources with Cgroups",
          "url": "https://apptainer.org/docs/user/1.0/cgroups.html",
          "excerpts": [
            "Using cgroups you can limit memory and CPU usage. You can also rate limit block IO, network IO, and control access to device nodes."
          ]
        },
        {
          "title": "Error setting rlimits type 8 (operation not permitted",
          "url": "https://discuss.linuxcontainers.org/t/error-setting-rlimits-type-8-operation-not-permitted-unknown-in-lxd-container/9976",
          "excerpts": [
            "Jan 18, 2021 — This tries to increase RLIMIT_MEMLOCK in the container which works. The problem usually is that you're exceeding the limit set in the initial user namespace."
          ]
        },
        {
          "title": "Use Kubernetes CPU Manager Static Policy's Guaranteed QoS ...",
          "url": "https://docs.starlingx.io/usertasks/kubernetes/using-kubernetes-cpu-manager-static-policy.html",
          "excerpts": [
            "Missing: Topology HugePages cgroup",
            "You can launch a container pinned to a particular set of CPU cores using the Kubernetes CPU manager static policy's Guaranteed QoS class."
          ]
        },
        {
          "title": "Kubernetes v1.32 Adds A New CPU Manager Static Policy ...",
          "url": "https://kubernetes.io/blog/2024/12/16/cpumanager-strict-cpu-reservation/",
          "excerpts": [
            "Dec 16, 2024 — The CPU Manager static policy is used to reduce latency or improve performance. The reservedSystemCPUs defines an explicit CPU set for OS system daemons and ...",
            "The CPU Manager static policy is used to reduce latency or improve performance. The `reservedSystemCPUs` defines an explicit CPU set for OS system daemons and kubernetes system daemons. This option is designed for Telco/NFV type use cases where uncontrolled interrupts/timers may impact the workload performance.",
            "# cat /var/lib/kubelet/cpu_manager_state",
            "# cat /var/lib/kubelet/cpu_manager_state",
            "{\"policyName\":\"static\",\"defaultCpuSet\":\"2-15,17-31,34-47,49-63\",\"checksum\":4141502832}",
            "You can monitor the feature impact by checking the following CPU Manager counters:",
            "* `cpu_manager_shared_pool_size_millicores` : report shared pool size, in millicores (e.g. 13500m)",
            "* `cpu_manager_exclusive_cpu_allocation_count` : report exclusively allocated cores, counting full cores (e.g. 16)",
            "Your best-effort workloads may starve if the `cpu_manager_shared_pool_size_millicores` count is zero for prolonged time. We believe any pod that is required for operational purpose like a log forwarder should not run as best-effort, but you can review and adjust the amount of CPU cores reserved as needed.",
            "## Conclusion",
            "Strict CPU reservation is critical for Telco/NFV use cases.",
            "It is also a prerequisite for enabling the all-in-one type of deployments where workloads are placed on nodes serving combined control+worker+storage roles.",
            "```",
            "```",
            "```"
          ]
        },
        {
          "title": "Kubernetes: How to use NUMA",
          "url": "https://medium.com/@martinko.komorny/kubernetes-appreciate-your-resources-dbbeaa73be74",
          "excerpts": [
            "Let's explore together the possibilities of utilizing NUMA in Kubernetes for efficient resource management on larger servers."
          ]
        },
        {
          "title": "Requests, Limits & QoS (with examples) : r/kubernetes",
          "url": "https://www.reddit.com/r/kubernetes/comments/1jyd114/kubernetes_resources_explained_requests_limits/",
          "excerpts": [
            "QoS Classes in Kubernetes: Guaranteed: Requests = Limits for all containers. Most protected. Burstable: Some requests, some limits, but not ..."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ...",
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_",
            "Turn HT (Hyper-threading) ON. (double the vcore run queues --> umpteen times lower likelihood of waiting for a cpu).",
            "They are all nice for advanced stuff, but I like to **clean the system up first, and only assign cores to things after tha"
          ]
        },
        {
          "title": "Control Topology Management Policies on a node",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/",
          "excerpts": [
            "Topology Manager_ is a kubelet component that aims to coordinate the set of components that are\nresponsible for these optimizations",
            "The Topology Manager receives topology information from the _Hint Providers_ as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication.",
            "The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result.",
            "To align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node.",
            "To align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory\nManager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to [Memory Manager](/docs/tasks/administer-cluster/memory-manager/) documentation.",
            " level. And the `policy` defines the actual policy used to\ncarry out the alignment, for example, `best-effort` , `restricted` , and `single-numa-node` . Details on the various `scopes` and `policies` available today can be found below. #### Note:\n\nTo align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node. See [Control CPU Management Policies on the Node](/docs/tasks/administer-cluster/cpu-management-policies/) . #### Note:\n\nTo align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory\nManager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to [Memory Manager](/docs/tasks/administer-cluster/memory-manager/) documentation. ## Topology manager scopes\n\nThe Topology Manager can deal with the alignment of resources in a couple of distinct scopes:\n\n* `container` (default)\n* `pod`\n\nEither option can be selected at a time of the kubelet startup, by setting the `topologyManagerScope` in the [kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) . ### `container` scope\n\nThe `container` scope is used by default.\n ... \nIn the case of the `Guaranteed` pod with integer CPU request, the `static` CPU Manager policy\nwould return topology hints relating to the exclusive CPU and the Device Manager would send back\nhints for the requested device. In the case of the `Guaranteed` pod with sharing CPU request, the `static` CPU Manager policy\nwould return default topology hint as there is no exclusive CPU request and the Device Manager\nwould send back hints for the requested device. In the above two cases of the `Guaranteed` pod, the `none` CPU Manager policy would return default\ntopology hint. In the case of the `BestEffort` pod, the `static` CPU Manager policy would send back the default\ntopology hint as there is no CPU request and the Device Manager would send back the hints for each\nof the requested devices. Using this information the Topology Manager calculates the optimal hint for the pod and stores\nthis information, which will be used by the Hint Providers when they are making their resource\nassignments. ## Known limitations\n\n1. The maximum number of NUMA nodes that Topology Manager allows is 8. With more than 8 NUMA nodes,\n   there will be a state explosion when trying to enumerate the possible NUMA affinities and\n   generating their hints. See [`max-allowable-numa-nodes`]() (beta) for more options. 2. The scheduler is not topology-aware, so it is possible to be scheduled on a node and then fail\n   on the node due to"
          ]
        },
        {
          "title": "Utilizing the NUMA-aware Memory Manager",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/memory-manager/",
          "excerpts": [
            "The Kubernetes Memory Manager enables the feature of guaranteed memory (and hugepages) allocation for pods in the Guaranteed QoS class.",
            "The Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod.",
            "The Memory Manager feeds the central manager (Topology Manager) with these affinity hints.",
            " Starting from v1.22, the Memory Manager is enabled by default through `MemoryManager`\n[feature gate]",
            "The Memory Manager is a Hint Provider, and it provides topology hints for\nthe Topology Manager which then aligns the requested resources according to these topology hints."
          ]
        },
        {
          "title": "4 Full-featured cpuset manipulation commands",
          "url": "https://documentation.suse.com/en-us/sle-rt/15-SP7/html/SLE-RT-all/cha-shielding-cpuset.html",
          "excerpts": [
            "The cset subcommand creates the cpusets based on a mounted cpuset file system. You do not need to know where that file system is mounted, although it is easy to ..."
          ]
        },
        {
          "title": "cset-shield - cpuset supercommand which implements cpu ...",
          "url": "https://manpages.ubuntu.com/manpages/trusty/man1/cset-shield.1.html",
          "excerpts": [
            "This is a supercommand that creates basic cpu shielding. The normal cset commands can of course be used to create this basic shield."
          ]
        },
        {
          "title": "2 The Basic Shielding Model",
          "url": "https://documentation.suse.com/en-us/sle-rt/12-SP5/html/SLE-RT-all/cha-shielding-model.html",
          "excerpts": [
            "The shield subcommand manages all of these cpuset s and lets you define the CPUs and memory nodes that are in the shielded and unshielded sets."
          ]
        },
        {
          "title": "numactl(8) - Linux manual page - man7.org",
          "url": "https://man7.org/linux/man-pages/man8/numactl.8.html",
          "excerpts": [
            "Multiple nodes may be specified on --interleave, --membind and --cpunodebind. --weighted-interleave=nodes, -w nodes Set a weighted memory interleave policy."
          ]
        },
        {
          "title": "crictl inspect does not show any information about the ...",
          "url": "https://github.com/cri-o/cri-o/issues/3963",
          "excerpts": [
            "The crictl inspect shows CPU quota, but I did not find any place where do we update it for the container, except the update for the sandbox."
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "As A has \"cpu\" and \"memory\" enabled, A will control the distribution of CPU cycles and memory to its children, in this case, B.",
            "In all the above models, cycles distribution is defined only on a temporal\nbase and it does not account for the frequency at which tasks are executed.",
            "All weights are in the range [1, 10000] with the default at 100.",
            "\"io.max\" limits the maximum BPS and/or IOPS that a cgroup can consume",
            "This\ncontroller implements weight and absolute bandwidth limit models for\nnormal scheduling policy and absolute bandwidth allocation model for\nrealtime scheduling policy.",
            "cpu.rt.max\" hard-allocates realtime slices and is an example of this\ntype.",
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "An introduction to control groups (cgroups) v2",
          "url": "https://man7.org/conf/ndctechtown2021/cgroups-v2-part-1-intro-NDC-TechTown-2021-Kerrisk.pdf",
          "excerpts": [
            "Control groups (cgroups) v2 are a mechanism for hierarchically grouping processes, managed by controllers, and allow limiting resource usage and prioritizing ..."
          ]
        },
        {
          "title": "Introducing io_uring_spawn - LWN.net",
          "url": "https://lwn.net/Articles/908268/",
          "excerpts": [
            "A new mechanism created by Josh Triplett adds process creation to the io_uring asynchronous I/O API and shows great promise."
          ]
        },
        {
          "title": "Performance Evaluation of AF_XDP and DPDK",
          "url": "http://www.diva-portal.org/smash/get/diva2:1897043/FULLTEXT01.pdf",
          "excerpts": [
            "The same pattern is visible in Table 5\\.2\n\nshowing that the maximum latency for AF\\_XDP is often double that of DPDK . This shows that the worst case in AF\\_XDP is much slower than DPDK\n\nindicating that something is hindering the performance. These large differences in minimum and maximum latency also indicate\n\nthat there can be a lot of j",
            "_XDP is\n\nfaster on some workloads. These results should be taken with a grain of salt,\n\nas the AF\\_XDP application was not as isolated as DPDK due to the inability\n\nto pin the kernel processes to a specific Linux cor"
          ]
        },
        {
          "title": "AF_XDP Sockets: High Performance Networking for Cloud-Native Networking - Intel Technology Guide",
          "url": "https://builders.intel.com/docs/networkbuilders/af-xdp-sockets-high-performance-networking-for-cloud-native-networking-technology-guide.pdf",
          "excerpts": [
            "Here the throughput is between 4.34 Mpps and 8.66 Mpps for 64-byte packets. This is around 20% more than for the stock OVS.",
            "Testing was performed on October 28, 2020.",
            "020. The system used was an Intel® Xeon® Gold 6230 processor 2.10 GHz with Intel® Turbo\n\nBoost Technology disabled.",
            ". The NIC was an Intel® Ethernet XXV710 DA2 Adapter with a total of two ports.",
            "AF\\_XDP is included with Linux 4.18, which was released in August 2018",
            "All measurements were performed according to RFC 2544 at 0% packet loss. See Appendix 1 for configuration details."
          ]
        },
        {
          "title": "Safe Kernel Extensibility and Instrumentation With Webassembly",
          "url": "http://ra.adm.cs.cmu.edu/anon/2025/CMU-CS-25-123.pdf",
          "excerpts": [
            "Aug 16, 2025 — This work presented a proof-of-concept framework for safe, language-agnostic kernel extensi- bility by embedding a WebAssembly runtime in a ..."
          ]
        },
        {
          "title": "arjunr2/wali-musl: Exploring Webassembly Linux Interface ...",
          "url": "https://github.com/arjunr2/wali-musl",
          "excerpts": [
            "This repo contains exploratory work towards creating a standard Linux syscall API for Webassembly. We build off of the base code of musl libc."
          ]
        },
        {
          "title": "AF_XDP",
          "url": "https://docs.ebpf.io/linux/concepts/af_xdp/",
          "excerpts": [
            "This page explains the concept of AF_XDP in depth, AF_XDP being a special socket type which in combination with an XDP program can perform full or partial ..."
          ]
        },
        {
          "title": "About",
          "url": "https://www.dpdk.org/about/",
          "excerpts": [
            "DPDK is a Linux Foundation project that consists of libraries to accelerate packet processing workloads running on a wide variety of CPU architectures."
          ]
        },
        {
          "title": "AF_XDP",
          "url": "https://docs.kernel.org/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "AF_XDP — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v4.18/networking/af_xdp.html",
          "excerpts": [
            "AF_XDP is an address family that is optimized for high performance packet processing. This document assumes that the reader is familiar with BPF and XDP."
          ]
        },
        {
          "title": "Using DPDK not only requires dedicating one core to it, but ...",
          "url": "https://news.ycombinator.com/item?id=34469546",
          "excerpts": [
            "Using DPDK not only requires dedicating one core to it, but also very often dedicating the NIC to your app. It's also a very heavyweight and complex ..."
          ]
        },
        {
          "title": "[PDF] Benchmarking Runtime Scripting Performance in Wasmer",
          "url": "https://ltb2022.eecs.yorku.ca/preprint/Devon-LTB2022-Final.pdf",
          "excerpts": [
            "Using this simulator, we demonstrate a 5-10x performance penalty for WASM compared to native execution. KEYWORDS. WebAssembly, benchmarking, ..."
          ]
        },
        {
          "title": "Introducing Wasmer 5.0 · Blog",
          "url": "https://wasmer.io/posts/introducing-wasmer-v5",
          "excerpts": [
            "Today we are presenting the latest stable version of Wasmer: v5.0 with tons of improvements and a better performance overall."
          ]
        },
        {
          "title": "Swivel: Hardening WebAssembly against Spectre",
          "url": "https://www.usenix.org/system/files/sec21-narayan.pdf",
          "excerpts": [
            "by S Narayan · 2021 · Cited by 98 — We examine the performance impact of these changes on Lucet's. Sightglass benchmark suite [9] and Wasm-compatible. SPEC 2006 [29] benchmarks."
          ]
        },
        {
          "title": "io_uring and networking in 2023",
          "url": "https://github.com/axboe/liburing/wiki/io_uring-and-networking-in-2023",
          "excerpts": [
            "io_uring is applicable to both storage and networking applications.",
            "One of the key benefits of io_uring is that multiple actions can be completed in a single system call.",
            "The io_uring system call for submitting new IO, io_uring_enter(2), also supports waiting for completions at the same time.",
            "This was an important design decision, as it allows IO that completes synchronously to be done efficiently, compared to async APIs that separate submission and wait-for-completion into two different operations.",
            "io_uring_submit_and_wait() helper, allowing an application to not only batch submissions, but also combine submissions and completions into a single system call."
          ]
        },
        {
          "title": "Reddit: Designing an Async runtime for Rust",
          "url": "https://www.reddit.com/r/rust/comments/1b3cl27/designing_an_async_runtime_for_wasi_02/",
          "excerpts": [
            "WASI 0.3 will likely switch to a completion-based system because Linux io_uring and Windows' ioringapi are completion-based and perform ... [r/rust icon](https://styles.redditmedia.com/t5_2s7lj/styles/communityIcon_pjg3ktzyju771.png?width=48&height=48&frame=1&auto=webp&crop=48%3A48%2Csmart&s=0a5e85c5ee6480526bf169287c697dc894c9acca)",
            "> WASI 0.3 will likely switch to a completion-based system because Linux io\\_uring and Windows' ioringapi are completion-based and perform really well, but we don't have that yet. Am I correct that the polling-based API will be removed in v0.3? What have prevented you from implementing an `io-uring`-like completion-based API from the start? track me",
            " WASI 0.3 will likely switch to a completion-based system because Linux io\\_uring and Windows' ioringapi are completion-based and perform really well, but we don't have that yet. Am I correct that the polling-based API will be removed in v0.3?",
            "n for WASI 0.3 at this point. WASI 0.2 was just stabilized last month, and I’m unsure what the completion based APIs for 0.3 would even look like. All we know is that that’s an active direction we want to explore, and over time we’ll have a better idea of what that will look like in practice."
          ]
        },
        {
          "title": "io_uring — Efficient IO (kernel.dk)",
          "url": "https://kernel.dk/io_uring.pdf",
          "excerpts": [
            "io\\_uring met the design goals that was set out for it. We have a very efficient delivery mechanism between\n\nthe kernel and the application, in the shape of two distinct ring",
            "If an application reuses IO buffers, then it's possible to do the mapping and unmapping once, instead\n\nof per IO operation. To register a fixed set of buffers for IO, io \\_ uring \\_ register ( 2 ) must be called with an opcode of\n\nIORING \\_ REGIS",
            "_uring handles this condition like it would for other resources that potentially could block the application. More\n\nimportantly, for operations that will not block, the data is served inline. That makes io\\_uring just as efficient for IO that\n\nis already in the page cache as the regular synchronous interfaces"
          ]
        },
        {
          "title": "io_uring - Linux manual page",
          "url": "https://man7.org/linux/man-pages/man7/io_uring.7.html",
          "excerpts": [
            "Linux-specific API for asynchronous I/O. It allows\n       the user to submit one or more I/O requests, which are processed\n       asynchronously without blocking the calling process.",
            "veral requests in one go, simply by queueing\n       up multiple SQEs, each describing an I/O operation you want and\n       make a single call to io_uring_enter(2) . This is possible due to\n       io_uring 's shared buffers based ",
            " This interface makes io_uring different from other UNIX\n       I/O APIs, wherein, rather than just communicate between kernel and\n       user space with system calls, ring buffers are used as the main\n       ",
            "io_uring\n       gets its name from ring buffers which are shared between user\n       space and"
          ]
        },
        {
          "title": "Empowering WebAssembly with Thin Kernel Interfaces",
          "url": "https://dl.acm.org/doi/10.1145/3689031.3717470",
          "excerpts": [
            "This paper proposes thin kernel interfaces for Wasm, which directly expose OS userspace syscalls without breaking intra-process sandboxing."
          ]
        },
        {
          "title": "PERFORMANCE ANALYSIS OF AF_XDP SOCKETS ON ...",
          "url": "https://openscholar.uga.edu/record/2663/files/ReillyJosephMS.pdf",
          "excerpts": [
            "This thesis analyzes the performance of AF_XDP sockets on commodity hardware such as Netronome and Mellanox. Smart NICs. Testing packet delivery speeds was ..."
          ]
        },
        {
          "title": "[PDF] Understanding Delays in AF_XDP-based Applications - HAL",
          "url": "https://hal.science/hal-04458274v1/file/main.pdf",
          "excerpts": [
            "In this paper, we presented a comprehensive study on the performance of AF XDP from the latency point of view, as we believe, it is a key point ..."
          ]
        },
        {
          "title": "280. DPDK PMD for AF_XDP Tests - Documentation",
          "url": "https://doc.dpdk.org/dts/test_plans/af_xdp_test_plan.html",
          "excerpts": [
            "The test plan contains 15 tests, 7 of which are focused on performance and the remaining 8 are focused on validating functionality."
          ]
        },
        {
          "title": "eunomia-bpf/bpftime: Userspace eBPF runtime for ...",
          "url": "https://github.com/eunomia-bpf/bpftime",
          "excerpts": [
            "bpftime is a High-Performance userspace eBPF runtime and General Extension Framework designed for userspace. It enables faster Uprobe, USDT, Syscall hooks, XDP ..."
          ]
        },
        {
          "title": "The WebAssembly Linux Interface - NASA/ADS",
          "url": "https://ui.adsabs.harvard.edu/abs/2023arXiv231203858R/abstract",
          "excerpts": [
            "by A Ramesh · 2023 · Cited by 2 — We present an implementation of WALI in a modern WebAssembly engine and evaluate its performance on a number of applications which we can now compile with ..."
          ]
        },
        {
          "title": "wasmerio/wasmer: 🚀 Fast, secure, lightweight containers ...",
          "url": "https://github.com/wasmerio/wasmer",
          "excerpts": [
            "Wasmer is a blazing fast and secure WebAssembly runtime that enables incredibly lightweight containers to run anywhere: from Desktop to the Cloud, Edge and ..."
          ]
        },
        {
          "title": "near/wasmer: 🚀 The leading WebAssembly Runtime ...",
          "url": "https://github.com/near/wasmer",
          "excerpts": [
            "Wasmer is a fast and secure WebAssembly runtime that enables super lightweight containers to run anywhere: from Desktop to the Cloud, Edge and IoT devices."
          ]
        },
        {
          "title": "Wasmer: Universal applications using WebAssembly",
          "url": "https://wasmer.io/",
          "excerpts": [
            "Run your apps. Secure. Fast. At Scale. Experience the new container technology powered by WebAssembly that runs your programs safely, anywhere."
          ]
        },
        {
          "title": "WasmEdge Runtime",
          "url": "https://github.com/wasmedge",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "WasmEdge",
          "url": "https://wasmedge.org/",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "WasmEdge is a lightweight, high-performance ...",
          "url": "https://github.com/WasmEdge/WasmEdge",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications."
          ]
        },
        {
          "title": "Why WASI Preview 2 Makes WebAssembly Production ...",
          "url": "https://thenewstack.io/why-wasi-preview-2-makes-webassembly-production-ready/",
          "excerpts": [
            "WASI Preview 2 is the missing link that Wasm needed to become a viable option for production use cases.",
            "Why WASI Preview 2 Makes WebAssembly Production Ready",
            ". Until recently, Wasm’s reality didn’t live up to the hype. Preview 2 is the missing link that Wasm needed to become viable for production use cases.",
            ". Preview 2 represents a significant milestone in Wasm’s evolution because it provides a solid checkpoint from which developers can build with confidence knowing that the entire platform isn’t going to change out from under them.",
            "At the heart of WASI Preview 2 is the WebAssembly Component Model.",
            " The component model defines a Canonical ABI (application binary interface) that standardizes the way components talk to each other and prevents them from accessing other components’ memories.",
            "This eliminates the largest classes of bugs and security vulnerabilities.",
            "Another key aspect of WASI Preview 2 is the stabilization of APIs.",
            "This ensures backward compatibility in the future for Wasm applications, giving developers the confidence to build on top of Preview 2 without worrying about future disasters.",
            "Preview 2 changes all of that, paving the way for broader, even mainstream, adoption of Wasm.",
            "When building Wasm apps, you can now pick and choose libraries from any language ecosystem, compile them into components and compose them to make one app."
          ]
        },
        {
          "title": "Getting Started with WasmEdge",
          "url": "https://wasmedge.org/docs/start/overview/",
          "excerpts": [
            "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud-native, edge, and decentralized applications."
          ]
        },
        {
          "title": "Performance of WebAssembly runtimes in 2023",
          "url": "https://00f.net/2023/01/04/webassembly-benchmark-2023/",
          "excerpts": [
            "Jan 4, 2023 — Benchmarking abilities for wasm32-wasi were thus added to libsodium. This benchmark proved to be more representative of real-world performance ...",
            "Here, wasmedge , also based on LLVM manages to be faster than wasmer with the LLVM backend. Utilities benchmark. Utilities benchmark. Codecs, ...",
            "Calling external functions in wasmedge may have more overhead than with other runtimes. Signature benchmark. Signature benchmark. Arithmetic ...",
            "* [Wasm3](https://github.com/wasm3/wasm3) ’s development pace seems to have slowed down. However, it remains the only WebAssembly runtime that can easily be embedded into any project, with minimal footprint, and amazing performance for an interpreter.",
            "Jan 4, 2023 — node , wasmtime , wasmedge and wasmer are in the same ballpark. None of them has a stable API yet. This is especially true for Rust APIs. If ...",
            "In the `LLVM` category, I was expecting `wasmedge` to take the lead given how much performance-focused the project has been since the beginning. Its results were a little bit disappointin",
            "* [Wasmtime](https://wasmtime.dev) quickly went from version 0.40 to version 3.0.1, with [version 4 being round the corner](https://github.com/bytecodealliance/wasmtime/blob/main/RELEASES.md) . Every release is an opportunity to update Cranelift, the code generator it is based on.",
            "* [Wasmer](https://wasmer.io) kept releasing unique tools and features, such as the ability to generate standalone binaries. Their single-pass compiler also got updated.",
            "* SSVM became [WasmEdge](https://wasmedge.org) , the runtime from the Cloud Native Computing Foundation.",
            "* [Wamr](https://github.com/bytecodealliance/wasm-micro-runtime) saw a bunch of new releases. Pre-built binaries are also now available.",
            "### LLVM/Cranelift/V8-based runtimes",
            "Jan 4, 2023 — The tests mainly measure the overhead of WASI calls to get random bytes, and, very likely, the over of WASI calls in general. The difference ...",
            "Jan 4, 2023 — As a result, these AES-based tests were 80 times slower than native code when running WebAssembly. This is not representative of most ...",
            "For most users, there are no significant differences between these three runtimes. They share similar features (such as AOT compilation) and run code the same way, roughly at the same speed.",
            "In spite of some breaking changes, it seems more focused on API stability than alternatives, making it a reasonable choice for applications that are planned to be maintained for a long time.",
            "asmer` has the largest ecosystem, with libraries making it very easy to use from many programming languages, as well as applications such as PostgreSQL and Visual Studio Code.",
            "It also includes the ability to generate standalone binaries for all supported platforms, has a package manager and more.",
            "asmedge` is the runtime from the CNCF, and what Docker uses to run containers with WebAssembly applications.",
            "So, even if this is not your runtime of choice, testing that your WebAssembly code properly runs on it is highly recommended, as its popularity is likely to skyrocket."
          ]
        },
        {
          "title": "The State of Async Rust: Runtimes",
          "url": "https://corrode.dev/blog/async/",
          "excerpts": [
            "In the first article, we will focus on the current state of async Rust runtimes, their design choices, and their implications on the broader Rust async ..."
          ]
        },
        {
          "title": "Wasm-bpf - WebAssembly eBPF integration",
          "url": "https://github.com/eunomia-bpf/wasm-bpf",
          "excerpts": [
            "Wasm-bpf is a WebAssembly eBPF library, toolchain and runtime powered by CO-RE(Compile Once – Run Everywhere) libbpf.",
            "It can help you build almost every eBPF ... It also enables developers to write eBPF programs in familiar languages like `C/C++` , `Rust` , `Go` , and more than 30 other programming languages, and deploy them easily across different Linux distributio",
            "Additionally, cloud providers can leverage wasm-bpf to offer a `secure` and `high-performance` environment for their customers to develop and deploy eBPF applications in their cloud environments."
          ]
        },
        {
          "title": "Announcing Wasmer 6.0 - closer to Native speeds! · Blog",
          "url": "https://wasmer.io/posts/announcing-wasmer-6-closer-to-native-speeds",
          "excerpts": [
            "Apr 25, 2025 — Wasmer 6.0 packages all backends together, pushes the LLVM backend to within 5 % of native performance, introduces zero‐cost WebAssembly Exceptions and more!"
          ]
        },
        {
          "title": "Wasmtime Reaches 1.0: Fast, Safe and Production Ready!",
          "url": "https://bytecodealliance.org/articles/wasmtime-1-0-fast-safe-and-production-ready",
          "excerpts": [
            "Sep 20, 2022 — With Wasmtime, InfinyOn has been able to deliver a greater than 5x throughput ... Wasmtime performance, but here's a basic breakdown.",
            "Sep 20, 2022 — ... startup times under a millisecond. Embark — 2 years in production ... With virtual memory tricks, we don't need to create a new memory every time ...",
            "For example, it takes the fastest alternative—a JS isolate—about 5ms to start up. In contrast, it only takes a Wasm instance 5 microseconds to ..."
          ]
        },
        {
          "title": "WASI Cycles, an Open Source, 3D WebXR Game - Oracle Blogs",
          "url": "https://blogs.oracle.com/developers/post/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-open-source-3d-webxr-game",
          "excerpts": [
            "I imagine you know about these three extremely popular messaging and data solutions. I am showing how we can use WASI to (directly) invoke Kafka, [MongoDB](https://dzone.com/articles/mongodb-cluster), and Oracle Database and, in this way, achieve interoperability with and functionality of these popular and widely used systems.",
            "WASI Invoking Kafka, MongoDB, and Oracle Database",
            "WasmEdge",
            "* Built and deployed in a container image",
            "* Uses tokio and hyper for HTTPS calls to the Oracle Database for Kafka and SQL access",
            "Wasmtime",
            "* Built with wasm32-wasip2",
            "* Written in Rust",
            "* Written in Rust",
            "* Uses waki for HTTPS calls to the Oracle Database for Kafka and SQL access",
            "Wasmer",
            "* Built with wasm32-wasip1",
            "* Built with wasm32-wasip1",
            "* Wasmer is unique as it is deployed to Wasmer server (runs locally). Wasmer is free but requires an account, as it's tied to the cloud.",
            "python:",
            "Spring Boot with Kafka and MongoDB API via Oracle Database",
            "Oracle Database has built-in REST endpoints that provide most of the same calls as Confluent Kafka REST APIs and Java Kafka compatibility.",
            "In addition, Oracle Database provides transactional atomicity between messaging and database operations by allowing the underlying database to connect.",
            "Wasmtime: Developed by Bytecode Alliance. It is commonly considered the reference implementation of WASI. Strong standards compliance. Common ..."
          ]
        },
        {
          "title": "KafkaGoSaur: a WebAssembly powered Kafka client - Medium",
          "url": "https://medium.com/swlh/kafkagosaur-eac3c063388",
          "excerpts": [
            "KafkaGoSaur is a new Kafka client for Deno built with WebAssembly on top of kafka-go, the excellent Kafka client library written for Go."
          ]
        },
        {
          "title": "Kafka Broker filter",
          "url": "https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/network_filters/kafka_broker_filter",
          "excerpts": [
            "The Kafka Broker filter can mutate the contents of received responses to enable easier proxying of Kafka clusters. The below example shows a configuration for ..."
          ]
        },
        {
          "title": "WebAssembly/wasi-io: I/O Types proposal for WASI",
          "url": "https://github.com/WebAssembly/wasi-io",
          "excerpts": [
            "Wasi I/O is an API providing I/O stream abstractions. There are two types, input-stream, and output-stream, which support read and write, respectively.",
            "WASI I/O is currently in [Phase 3](https://github.com/WebAssembly/WASI/blob/main/Proposals.md) .",
            "WASI I/O must have host implementations which can pass the testsuite on at least Windows, macOS, and Linux.",
            " WASI I/O\n\n[]()\n\nA proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI)",
            "WASI I/O. A proposed WebAssembly System Interface API. Current Phase. WASI I/O is currently in Phase 3. Champions. Dan Gohman. Portability Criteria. WASI I/O ..."
          ]
        },
        {
          "title": "Networking API design · Issue #370 · WebAssembly/WASI - GitHub",
          "url": "https://github.com/WebAssembly/WASI/issues/370",
          "excerpts": [
            "WASI does have a poll_oneoff function for waiting for I/O on multiple handles. It won't scale to very-many handles, but it's likely better than ..."
          ]
        },
        {
          "title": "According to their roadmap, WASI preview 2 will have ...",
          "url": "https://news.ycombinator.com/item?id=38438239",
          "excerpts": [
            "Threads are actually removed from WASI Preview 2. WASI Preview 2 still doesn't support threads, fork, subprocesses or longjmp/setjmp (among others). Not even ..."
          ]
        },
        {
          "title": "Develop with WasmEdge, Wasmtime, and Wasmer Invoking ...",
          "url": "https://paul-parkinson.medium.com/develop-with-wasmedge-wasmtime-and-wasmer-invoking-mongodb-kafka-and-oracle-wasi-cycles-an-ad2302fe961a",
          "excerpts": [
            "WASM (WebAssembly): A W3C standard for low-level, performance-optimized bytecode that is unmatched for portability, security, and speed."
          ]
        },
        {
          "title": "Wasmtime 1.0: A Look at Performance",
          "url": "https://bytecodealliance.org/articles/wasmtime-10-performance",
          "excerpts": [
            "Sep 6, 2022 — According to the benchmarks, this change improved the speed of stack walking by between 64% and 99.95%, depending on the situation, and it ...",
            "Sep 6, 2022 — wasm went from about 2 milliseconds (eager initialization of all heap and other data structures) to 5 microseconds, or 400 times faster. Not bad ...",
            "When introduced, it improved runtime performance of SpiderMonkey.wasm by about 5% and another CPU-intensive benchmark, bz2 , by 4% (for example) ...",
            "This first post will describe performance-related projects: making the compiler generate faster code, making the compiler itself run faster, making Wasmtime ...",
            "Over the past year, we have done substantial work to take module instantiation from milliseconds to microseconds. How is this possible? In brief ...",
            "Sep 6, 2022 — Now that we've started the Wasm program, we need to help it run as quickly as possible with fast runtime primitives! While most of the CPU time ..."
          ]
        },
        {
          "title": "Container Runtimes and WasmEdge benchmarking on Arm",
          "url": "https://community.arm.com/arm-community-blogs/b/servers-and-cloud-computing-blog/posts/container-runtimes-wasmedge-arm",
          "excerpts": [
            "In this blog, we give you a general introduction of runtimes, and shows you the benchmark of image size, boot time and memory footprint between runc and ...",
            "May 17, 2023 — In this blog, we give you a general introduction of runtimes, and shows you the benchmark of image size, boot time and memory footprint between runc and ..."
          ]
        },
        {
          "title": "Transform data between Kafka topics",
          "url": "https://docs.decodable.co/tutorials/kafka-transform.html",
          "excerpts": [
            "In this guide, we'll extract data from one Kafka topic, transform that data using SQL, and then load the transformed data to another Kafka topic."
          ]
        },
        {
          "title": "Kafka Patterns and Anti-Patterns - Adservio",
          "url": "https://www.adservio.fr/post/kafka-patterns-and-anti-patterns",
          "excerpts": [
            "Kafka patterns represent big-picture concepts that you need to align your implementation strategy with in order to make the most of Kafka."
          ]
        },
        {
          "title": "Performance Measured: How Good Is Your WebAssembly?",
          "url": "https://thenewstack.io/performance-measured-how-good-is-your-webassembly/",
          "excerpts": [
            " the libsodium benchmark has been widely used to measure to pick the best runtimes",
            "Libsodium is also worthwhile for testing some Wasm environments because it includes benchmarking tasks with a wide range of different requirement profiles, some probing for raw CPU or memory performance, while others check for more nuanced performance prof",
            " “The current results show the suite’s ability to reveal significant differences in performance between the various runtimes, both for compiled languages and for interpreted ones,” Volk said.",
            "b/main/README.md) is used to run WASM applications. It supports interpreter mode, ahead-of-time compilation (AOT) mode and just-in-time compilation (JIT) modes, LLVM JIT and Fast JIT, according to the project’s documentation."
          ]
        },
        {
          "title": "WarpDiff and Kafka WASM in server-side Wasm runtimes (arXiv:2309.12167)",
          "url": "https://arxiv.org/abs/2309.12167",
          "excerpts": [
            " WarpDiff to identify performance issues in server-side Wasm runtimes. It has recently been used increasingly on the server side, e.g., providing a safer, faster, and more portable alternative to Linux containers."
          ]
        },
        {
          "title": "The Significance of In-broker Data Transformations in Streaming Data",
          "url": "https://medium.com/event-driven-utopia/the-significance-of-in-broker-data-transformations-in-streaming-data-9292e440beaa",
          "excerpts": [
            ". Redpanda, the Kafka-API-compatible streaming data platform, recently released the [Redpanda Data Transforms Sandbox](https://redpa",
            "The sandbox contains a Go-based SDK seamlessly integrated within [rpk](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), Redpanda’s [CLI experience](https://docs.redpanda.com/current/reference/rpk/rpk-redpanda/rpk-redpanda-start/), and a single Redpanda broker (Docker container) that can deploy and run your transform functions on topics you create inside the fully-contained sandbox.",
            "What is WebAssemby (Wasm)?",
            "WebAssembly, or Wasm for short, is a new type of code that can be run in modern web browsers and provides new features and major gains in performance.",
            "That means developers can code web client applications in a programming language of their choice, compile them down to Wasm, and run them inside the browser at near-native speed.",
            "What if we use Wasm to ship code to server-side applications, especially to brokers, in the same way it ships code to the user’s computer?",
            "Returning to our original context, using Wasm to code the data transformation logic and shipping it to the broker provides several benefits. **Flexibility for developers:** Wasm allows developers to write their transformations in any supported languages (C, C++, Rust, JS, and Go as of today).",
            "Near-native performance:** Wasm has a small runtime footprint, less overhead, and runs in embedded mod",
            "Security:** Data transformations are executed in sandboxes, which are isolated execution environments, providing improved securit",
            "Portability:** Wasm [execution environments are standardized](https://webassembly.org/docs/portability/) to promote portability, enabling code to be shipped across many platform",
            "Redpanda Data Transforms — in-broker data transformations at scale",
            "Redpanda data transforms provide a framework to create, build, and deploy inline data transformations on data written to Redpanda topics.",
            "Redpanda data transforms provide a framework to create, build, and deploy inline data transformations on data written to Redpanda topics.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic.",
            "A transform function processes every message produced to an input topic and returns one or more messages that are then produced to an output topic.",
            "It is currently in [Technical Preview](https://docs.redpanda.com/current/labs/data-transform/?tab=tabs-2-transcoder-transform).",
            "It is currently in [Technical Preview](https://docs.redpanda.com/current/labs/data-transform/?tab=tabs-2-transcoder-transform)."
          ]
        },
        {
          "title": "Redpanda Data Transforms with WebAssembly",
          "url": "https://docs.redpanda.com/current/develop/data-transforms/how-transforms-work/",
          "excerpts": [
            "Data transforms use [WebAssembly](https://webassembly.org/) (Wasm) engines inside a Redpanda broker, allowing Redpanda to control the entire transform lifecycle. For example, Redpanda can stop and start transforms when partitions are moved or to free up system resources for other tasks.",
            "Data transforms take data from an input topic and map it to one or more output topics. For each topic partition, a leader is responsible for handling the data. Redpanda runs a Wasm virtual machine (VM) on the same CPU core (shard) as these partition leaders to execute the transform function.",
            "Transform functions are the specific implementations of code that carry out the transformations. They read data from input topics, apply the necessary processing logic, and write the transformed data to output topics.",
            "When you deploy a data transform to a Redpanda broker, it stores the Wasm bytecode and associated metadata, such as input and output topics and environment variables. The broker then replicates this data across the cluster using internal Kafka topics. When the data is distributed, each shard runs its own instance of the transform function. This process includes several resource management features:",
            "* Each shard can run only one instance of the transform function at a time to ensure efficient resource utilization and prevent overload.",
            "* Memory for each function is reserved within the broker with the `data_transforms_per_core_memory_reservation` and `data_transforms_per_function_memory_limit` properties. See [Configure memory for data transforms](../configure/).",
            "* CPU time is dynamically allocated to the Wasm runtime to ensure that the code does not run forever and cannot block the broker from handling traffic or doing other work, such as Tiered Storage uploads.",
            "Flow of data transforms",
            "When a shard becomes the leader of a given partition on the input topic of one or more active transforms, Redpanda does the following:",
            "1.\nSpins up a Wasm VM using the JIT-compiled Wasm module. 2. Pushes records from the input partition into the Wasm VM. 3. Writes the output. The output partition may exist on the same broker or on another broker in the cluster",
            "ransform function reads from a specified input topic and writes to a specified output topic. The transform function processes every record produced to an input topic and returns zero or more records that are then produced to the specified output topic. Data transforms are applied to all partitions on an input topic. A record is processed after it has been successfully written to disk on the input topic. Because the transform happens in the background after the write finishes, the transform doesn’t affect the original produced record, doesn’t block writes to the input topic, and doesn’t block produce and consume requests. A new transform function reads the input topic from the latest offset. That is, it only reads new data produced to the input topic: it does not read records produced to the input topic before the transform was dep"
          ]
        },
        {
          "title": "arXiv:2002.09344v1 [cs.DC] 21 Feb 2020",
          "url": "https://scispace.com/pdf/faasm-lightweight-isolation-for-efficient-stateful-4y5cck5ry0.pdf",
          "excerpts": [
            "machine learning inference, FAASM doubles the throughput and reduces tail latency by 90%. ... 5.2 Reducing Cold Start Latency. While Faaslets ... pact performance ..."
          ]
        },
        {
          "title": "Faasm: Lightweight Isolation for Efficient Stateful ...",
          "url": "https://www.usenix.org/system/files/atc20-shillaker.pdf",
          "excerpts": [
            "by S Shillaker · 2020 · Cited by 410 — Knative has a tail latency of over 2 s and more than 35% of calls have latencies of over 500 ms with 20% cold-starts. FAASM achieves a tail ...",
            "\nruntime then executes multiple Faaslets, each with a dedicated\nthread, within a single address space. For",
            "Faaslets have fast initialisation times. To reduce cold-\nstart time when a Faaslet executes for the first time, it\nis launched from a suspended state",
            "Proto-Faaslets are\nused to create fresh Faaslet instances quickly, e.g. avoid-\ning the time to initialise a language runtim",
            "Faaslets allocate shared process memory through\ncalls to mmap on the underlying host, passing MAP_SHARED and\nMAP_ANONYMOUS flags to create shared and private regions, re-\nspectivel",
            "Faaslets use a two-tier state architecture,\na local tier provides in-memory sharing, and a global tier\nsupports distributed access to state across hosts.",
            "The FAASM\nruntime provides a state management API to Faaslets that\ngives fine-grained control over state in both tiers.",
            "tion\nis executed by a dedicated thread of a shared runtime process. This thread is assigned to a cgroup with a share of CPU equal\nto that of all Faaslets. The Linux CFS [79] ensures that these\nthreads are scheduled with equal CPU time.",
            ". Faaslets achieve secure and fair network access using net-\nwork namespaces, virtual network interfaces and traffic shap",
            "Proto-Faaslets\n\nreduce initialisation times to hundreds of microseconds, and a\n\nsingle snapshot can be restored across hosts, quickly scaling\n\nhorizontally on cluste",
            "FAASM guarantees that private\ndata held in memory is cleared away after each function exe-\ncution, thereby allowing Faaslets to handle subsequent calls\nacross tenant",
            "Proto-Faaslet snapshot includes a function’s stack, heap,\n\nfunction table, stack pointer and data, as defined in the We-\n\nbAssembly specification [ 35"
          ]
        },
        {
          "title": "Roadmap · WASI.dev",
          "url": "https://wasi.dev/roadmap",
          "excerpts": [
            "Upcoming WASI 0.3 releases​ ... At this time, WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025. Please see the ...",
            "WASI 0.3.0 will add **native async support** to the Component Model and refactor WASI 0.2 interfaces to take advantage of native as",
            "At this time, WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025.",
            "This includes explicit `stream<T>` and `future<T>` types for use anywhere in function parameters and results.",
            "At this time, WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025. Please see the presentation and notes from the ... Roadmap · WASI.dev",
            "## Upcoming WASI 0.3 releases",
            "When WASI 0.3 is launched, implementations may continue to support 0.2, either by implementing 0.3 alongside it, or by virtualizing (polyfilling) 0.2 in terms of 0.3.",
            "*Threads** : first cooperative, then preemptiv",
            "WASI 0.3.0 previews are expected in August 2025 and completion is expected around November 2025."
          ]
        },
        {
          "title": "Launching WASI Preview 2 - The WebAssembly System ...",
          "url": "https://www.reddit.com/r/rust/comments/1ab5bj0/launching_wasi_preview_2_the_webassembly_system/",
          "excerpts": [
            "WASI is now officially based on the Wasm component model, which makes it cross-language and virtualizable. Preview 2 includes two worlds."
          ]
        },
        {
          "title": "Kafka: Event-Driven Architecture or Over-Engineered ...",
          "url": "https://medium.com/@techInFocus/kafka-event-driven-architecture-or-over-engineered-rabbit-hole-13293eca8b5b",
          "excerpts": [
            "May 11, 2025 — You'll wrestle with backpressure, lag monitoring, schema evolution, and message replay. It's not plug-and-play. You don't “just install Kafka” ..."
          ]
        },
        {
          "title": "Kafka sidecar of light-mesh to provide cross-cutting ...",
          "url": "https://github.com/networknt/kafka-sidecar",
          "excerpts": [
            "A fast and light-weight reverse proxy with embedded gateway to wrap third party APIs and bring them to the ecosystem of light-4j frameworks."
          ]
        },
        {
          "title": "Kafka Connectors | Confluent Documentation",
          "url": "https://docs.confluent.io/platform/current/connect/kafka_connectors.html",
          "excerpts": [
            "You can use self-managed Apache Kafka® connectors to move data in and out of Kafka. The self-managed connectors are for use with Confluent Platform."
          ]
        },
        {
          "title": "How to Use Kafka Connect - Get Started | Confluent Documentation",
          "url": "https://docs.confluent.io/platform/current/connect/userguide.html",
          "excerpts": [
            "Kafka Connect is the framework to integrate popular systems, such as databases and cloud services with Apache Kafka. This guide will help you get started."
          ]
        },
        {
          "title": "Cloud-native WebAssembly in Service Mesh - Wasm",
          "url": "https://medium.com/wasm/cloud-native-webassembly-in-service-mesh-b19e3a96ccf8",
          "excerpts": [
            "It manages and directs API requests to sidecars in a manner that keeps the system scalable. Developers need to script those proxies to route ..."
          ]
        },
        {
          "title": "Why Kafka Consumers Are Failing You: The Backpressure ...",
          "url": "https://medium.com/@himanshu675/why-kafka-consumers-are-failing-you-the-backpressure-crisis-killing-event-driven-architectures-6a16e7d0f460",
          "excerpts": [
            "Factors like unpredictable message volumes, complex processing logic, or resource constraints can disrupt this balance, leading to backpressure."
          ]
        },
        {
          "title": "IOPS vs Throughput vs Latency | Metrics Guide - Simplyblock",
          "url": "https://www.simplyblock.io/blog/iops-throughput-latency-explained/",
          "excerpts": [
            "IOPS and throughput are important metrics when measuring the performance of storage solutions. Learn all about and how to measure them."
          ]
        },
        {
          "title": "Introducing Fermyon Wasm Functions on Akamai",
          "url": "https://www.fermyon.com/blog/introducing-fermyon-wasm-functions-on-akamai",
          "excerpts": [
            "Missing: wasmCloud Faasm benchmark"
          ]
        },
        {
          "title": "Measuring the responsiveness of WebAssembly in edge ...",
          "url": "https://www.diva-portal.org/smash/get/diva2:1783251/FULLTEXT01.pdf",
          "excerpts": [
            "by R Scolati · 2023 — This project aims to measure the overall latency Wasm introduces in an example implementation of a CPS and to compare the latency against the ..."
          ]
        },
        {
          "title": "LLM Inference Benchmarking: Performance Tuning with TensorRT ...",
          "url": "https://developer.nvidia.com/blog/llm-inference-benchmarking-performance-tuning-with-tensorrt-llm/",
          "excerpts": [
            "This is the third post in the large language model latency-throughput benchmarking series, which aims to instruct developers on how to benchmark ..."
          ]
        },
        {
          "title": "Benchmarking AWS Lambda runtimes in 2021: cold start",
          "url": "https://filia-aleks.medium.com/benchmarking-all-aws-lambda-runtimes-in-2021-cold-start-part-1-e4146fe89385",
          "excerpts": [
            "We are going to test the cold start(the first invocation on AWS Lambda instance) delay. To measure this metric we will use X-Ray tracing."
          ]
        },
        {
          "title": "Handling Backpressure in Software Systems",
          "url": "https://dev.to/lovestaco/handling-backpressure-in-software-systems-23m1",
          "excerpts": [
            "Mar 26, 2025 — Backpressure occurs when a system receives more data than it can process in real-time. It's like a coffee shop with a single barista."
          ]
        },
        {
          "title": "Introducing the wasmCloud benchmark chart",
          "url": "https://wasmcloud.com/blog/2025-02-18-introducing-the-wasmcloud-benchmark-chart/",
          "excerpts": [
            "chart makes it easy to install (and uninstall) this stack at the push of a button.\nDeploying the benchmark",
            "To test the chart, you'll first need a Kubernetes cluster with wasmCloud deployed on it.",
            "To install the `benchmark` chart:",
            "helm upgrade --install my-benchmark --version 0.2.0 oci://ghcr.io/wasmcloud/charts/benchmark --wait --set test.url=http://hello-world:8000",
            "Feb 18, 2025 — In this post, we'll explore how the chart works and demonstrate how to use it. The need for speed (tests)​. Robust benchmarking is important for ..."
          ]
        },
        {
          "title": "Fermyon Serverless Guide: Speed and Execution Time",
          "url": "https://www.fermyon.com/serverless-guide/speed-and-execution-time",
          "excerpts": [
            ". Most Lambda functions take [more than 200 milliseconds to cold start](https://youtu.be/0_jfH6qijVY) – and sometimes as much as a few seconds. This is too long for typical HTTP endpoints. In contrast, [Spin](https://spinframework.dev) starts nearly instantly (typically 2ms or less, and often under 1msec).",
            "One distinctive feature of serverless apps is the focus on speed and execution time. Serverless functions are often limited to a specific length of time to run."
          ]
        },
        {
          "title": "Redpanda's WASM Architecture and Data Transforms",
          "url": "https://www.redpanda.com/blog/wasm-architecture",
          "excerpts": [
            "Redpanda Data Transforms is built on the Wasmtime engine, which powers many other modern serverless platforms.",
            "Data Transforms can be deployed in one command using\nrpk transform deploy --input-topic=foo --output-topic=bar",
            " To see the deployed transforms running in the cluster as well as their status and if there’s any lag, you can use\nrpk transform list .",
            "The data lifecycle\nCheck out our post on how Redpanda Data Transforms compares to Apache Flink to learn how they compare and when to use them.",
            "The architecture of Redpanda Data Transforms is composed of three important components:\n    * Client-side tooling and SDKs\n    * Data Transforms lifecycle management\n    * Data flow through the WebAssembly runtime",
            "Data Transforms lets you tell Redpanda what piece of code (.wasm) can run on what data (topics), on what location, at a specific time (on event triggers) in an auditable fashion.",
            "Embedding this virtual machine directly onto each shard in Redpanda's thread-per-core architecture provides an alternative to the classical data back-and-forth when trying to make sense of real-time streaming data.",
            "Embedding this virtual machine directly onto each shard in Redpanda's thread-per-core architecture provides an alternative to the classical data back-and-forth when trying to make sense of real-time streaming data.",
            "What JavaScript did for the web in the late 90s is what Wasm can do for server-side applications.",
            "\nWebAssembly will change server-side software permanently. Allowing injection of code inside Redpanda at strategic points fundamentally changes what the storage engine can do for you while giving you isolation, high function density, granularity of execution, native performance, and multi-tenant by design",
            "Currently, Redpanda Data Transforms is a beta feature for Redpanda Cloud's Dedicated and BYOC options.",
            "Lastly, follow us on LinkedIn and X to catch our upcoming product updates!"
          ]
        },
        {
          "title": "How we built our data transformation engine with the Wasm runtime (Redpanda Data Transform)",
          "url": "https://redpanda-data.medium.com/how-we-built-our-data-transformation-engine-with-the-wasm-runtime-2cb1b312b26b",
          "excerpts": [
            "Redpanda Data Transforms gives users the ability to push custom code to Redpanda.",
            "ns. There are three main goals of the Data Transforms capability:\n\n1. **Ease of use:** You should be able to code in your favorite Wasm-supported languages (only JS is supported in this release). No need to worry about which Apache Kafka® libraries you’ll need to leverage or any low-level external dependencies. Your code gets record batches sent directly into it, straight from the local Redpanda node.",
            "*Performance:** Your transform is pushed to all nodes in the cluster and calls to your code boil down to an RPC call over localhost. This avoids expensive raft chatter that you would otherwise incur if using an external producer and consumer.",
            "*Operational simplicity:** Creating a separate transformer comes with more configuration and set up. However, with Data Transforms, you just push your code to Redpanda and it is durably stored within a dedicated topic and completely managed by Redpanda.",
            "orms. The transformation’s response is then sent back to Redpanda where it will be written to a resultant topic we call a “materialized topic.",
            "Wasm engine performance will be addressed by a redesign where it will support concurrent execution of transforms, and full Wasm support will be addressed in the future, as well.",
            "We decided to go with Wasm as it has less overhead, a small runtime footprint, gives users the ability to write their transforms in any of the supported Wasm languages (C, C++, Rust and JS as of today), and can run in embedded mode.",
            "ect.\nIn the tech preview, the transforms will be deployed to a separate sidecar process called the “async engine.” It’s a NodeJS service deployed on each Redpanda node that responds to requests from the local Redpanda process over localhost.",
            "ect.\nIn the tech preview, the transforms will be deployed to a separate sidecar process called the “async engine.” It’s a NodeJS service deployed on each Redpanda node that responds to requests from the local Redpanda process over localhost.",
            "ary\n\nWhat we’ve done is create a system that allows users to asynchronously map one topic to another, solving a common problem in streaming. The idea to keep the Wasm engine itself simple and stateless provided a solid foundation."
          ]
        },
        {
          "title": "Integrations",
          "url": "https://wasmcloud.com/docs/integrations/",
          "excerpts": [
            "Add Kafka messaging functionality for wasmCloud applications with the first-party Kafka messaging capability provider. Kubernetes, Deploying wasmCloud on ..."
          ]
        },
        {
          "title": "Create",
          "url": "https://wasmcloud.com/docs/developer/providers/create/",
          "excerpts": [
            "This guide walks through the process of building a capability provider using a standard interface, wasmcloud:messaging . You can use this guide and this example ..."
          ]
        },
        {
          "title": "Capability Catalog",
          "url": "https://wasmcloud.com/docs/capabilities/",
          "excerpts": [
            "wasmCloud supports custom capabilities—use our provider SDKs for Rust and Go to create your own! Learn more in the documentation on creating a capability ..."
          ]
        },
        {
          "title": "Transform Your Data Pipeline: Real‐Time Event Streaming ...",
          "url": "https://medium.com/@FAANG/transform-your-data-pipeline-real-time-event-streaming-with-rust-and-apache-kafka-a88e0c64e7f1",
          "excerpts": [
            "In this article, we skip over basic Rust syntax and dive straight into architecting a resilient, high‐throughput streaming pipeline using Rust's ..."
          ]
        },
        {
          "title": "Smart data FaaS with WebAssembly & Rust - The Punch",
          "url": "https://punchplatform.com/2023/09/14/frugal-and-robust-data-processing-using-rust-and-webassembly/",
          "excerpts": [
            "Sep 14, 2023 — In a previous exploration, we demonstrated the potential of using Rust and WebAssembly for efficient data transformation within Punch data ...",
            "Smart data FaaS with WebAssembly & Rust. Explore REEF engine, Rust connectors, and WebAssembly-powered functions."
          ]
        },
        {
          "title": "Fluvio: A Kafka + Flink Built Using Rust + WASM",
          "url": "https://medium.com/@ftieben/fluvio-a-kafka-flink-built-using-rust-wasm-bdeef463fd64",
          "excerpts": [
            "One of the most notable things about Fluvio is its performance. Fluvio is able to achieve very high throughput and low latency, even when ..."
          ]
        },
        {
          "title": "Real-Time Event-Driven Architecture with Kafka, WebSockets and ...",
          "url": "https://medium.com/@akshat.available/real-time-event-driven-architecture-with-kafka-websockets-and-react-b4698361e68a",
          "excerpts": [
            "The architecture enables real-time server-to-client communication using Kafka and WebSockets for event-driven data streaming."
          ]
        },
        {
          "title": "WASI API proposal for managing sockets",
          "url": "https://github.com/WebAssembly/wasi-sockets",
          "excerpts": [
            "This proposal adds TCP & UDP sockets and domain name lookup to WASI. It adds the basic BSD socket interface with the intent to enable server and client ...",
            "Unlike BSD sockets, WASI sockets require capability handles to create sockets and perform domain name lookups. On top of capability handles, WASI Socket ..."
          ]
        },
        {
          "title": "wasi:sockets",
          "url": "https://wa.dev/wasi:sockets",
          "excerpts": [
            "If the TCP/UDP port is zero, the socket will be bound to a random free port. Bind can be attempted multiple times on the same socket, even with different ..."
          ]
        },
        {
          "title": "Hypercharge Through Components: Why WASI 0.3 and ...",
          "url": "https://medium.com/wasm-radar/hypercharge-through-components-why-wasi-0-3-and-composable-concurrency-are-a-game-changer-0852e673830a",
          "excerpts": [
            "WASI 0.3 Roadmap Drops a Timeline​​ The official WASI. The dev roadmap now confirms August 2025 for the WASI 0.3. 0 preview and November for the ...",
            "WASI 0.3 now enhances with native async support. That means every component can manage I/O or long-running tasks without blocking others.",
            "Wasmtime and Fermyon's Spin. Let's take Spin first. It's a lightweight framework for building serverless apps using WebAssembly, and it ...",
            "All of this lands as a preview around August 2025, with stable releases expected by November, according to the official WASI.dev roadmap. This ..."
          ]
        },
        {
          "title": "WASI 0.3 preview: native async added to the WebAssembly ...",
          "url": "https://progosling.com/en/programming-news/wasi-0-3-native-async-aug-2025",
          "excerpts": [
            "Aug 12, 2025 — The WASI roadmap now targets an August 2025 preview for WASI 0.3 and signals that the release will add native async to the WebAssembly ..."
          ]
        },
        {
          "title": "Apache Kafka vs. Fluvio Benchmarks : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/1invl4e/apache_kafka_vs_fluvio_benchmarks/",
          "excerpts": [
            "We figured we keep it simple and compare Apache Kafka and Fluvio. The results are as you'd expect. More details in the blog."
          ]
        },
        {
          "title": "WASM Runtimes vs. Containers: Cold Start Delays (Part 2)",
          "url": "https://levelup.gitconnected.com/wasm-runtimes-vs-containers-cold-start-delays-part-2-544f05ab4d78",
          "excerpts": [
            "Missing: wasmCloud throughput",
            "In this second part of the series, we will assess how do the cold start delays of WebAssembly (WASM) runtimes compare to those of containers ...",
            "Jan 10, 2024 — In this second part of the series, we will assess how do the cold start delays of WebAssembly (WASM) runtimes compare to those of containers ..."
          ]
        },
        {
          "title": "Providers",
          "url": "https://wasmcloud.com/docs/concepts/providers/",
          "excerpts": [
            "Providers are swappable host plugins—executables (usually dedicated to longer-lived processes) that deliver common functionalities called capabilities ...",
            "The wasmCloud Q3 2025 Roadmap sets out plans for an overhaul to capability providers in the next major release of wasmCloud.",
            "Because built-in providers are part of the wasmCloud host itself, they do not communicate over NATS and run in the same process as the host. Updating a built-in ..."
          ]
        },
        {
          "title": "Capabilities",
          "url": "https://wasmcloud.com/docs/concepts/capabilities/",
          "excerpts": [
            "Applications implement capabilities for common requirements like storing and retrieving data, accessing secrets, and more. Examples of capabilities include: Key ...",
            "Providers are fairly simple, only needing to handle NATS subscriptions for wRPC, generate types from WIT, and then execute as a standalone binary."
          ]
        },
        {
          "title": "Announcing Instaboot: Instant Cold Starts for Serverless Apps",
          "url": "https://wasmer.io/posts/announcing-instaboot-instant-cold-starts-for-serverless-apps",
          "excerpts": [
            "An unique technology based on WebAssembly that reduces cold starts of serverless apps up to 100-200x, greatly simplifying the maintainance and cost of running ..."
          ]
        },
        {
          "title": "A List of Common Event Types",
          "url": "https://www.fermyon.com/serverless-guide/event-types",
          "excerpts": [
            "Pubsub\n      + Redis\n      + Kafka",
            "Fermyon's Spin allows developers to write their own custom event triggers, though only a few event types are supported on Fermyon Cloud.",
            "* Pubsub",
            "+ Kafka"
          ]
        },
        {
          "title": "Fermyon Spin and Kafka/Kinesis integration (Spin blog)",
          "url": "https://www.fermyon.com/blog/how-i-wrote-a-kinesis-trigger-plugin-for-spin",
          "excerpts": [
            "* **Upleveling performance with polyglot capabilities -** Spin lets your Python/JavaScript services seamlessly invoke efficient Rust code.",
            "Spin is powered by the efficiency and security of Wasm. If you aren’t familiar with server-side Wasm, no worries - you don’t have to be an expert! Here are a few reasons why Spin stands out to me as a developer tool for writing event-driven functions powered by Wasm.",
            "Oct 8, 2024 — The Spin Kinesis trigger aims to replicate the ease of consuming Kinesis data inside AWS cloud via Fermyon Spin applications. [How I Wrote A Kinesis Trigger Plugin For Spin](https://www.fermyon.com/static/image/twc-kinesis-trigger-w-spin.jpg)",
            "This batch of records is processed in 23 milliseconds",
            "* **Observability with batteries included -** If you have a current set of observability tools built on the wonderful, open-source OTel ecosystem, Spin has been making rapid advances to fit into your existing developer toolkit.",
            "Fermyon Spin is powered by the efficiency and security of Wasm.",
            "Spin has an extensible plugin model that makes it easy to connect your Spin applications to external dependencies."
          ]
        },
        {
          "title": "Faasm Presentation: Faasm - Lightweight Isolation for Efficient Stateful Serverless Computing",
          "url": "http://faculty.washington.edu/wlloyd/courses/tcss562_f2020/presentations/T12_Presentation.pdf",
          "excerpts": [
            "Faasm also maintains tail latency of under 150ms for all cold start ratios whereas Knative has a tail latency of over 2 seconds for 35% of calls",
            "Faasm Runtime",
            "Serverless runtime that operates using Faaslets to \nprovide stateful application",
            "Proto-Faaslets are Faaslets that contain a snapshot with \nthe function’s stack, heap, function table, stack pointer \nand data",
            "Key Contributions",
            "Lightweight Isolation of serverless functions",
            "Compiled using WebAssembly",
            "Faasm Integration",
            "Proto-Faaslets",
            "Proto-Faaslets included to further reduce cold start \nlatenc",
            "Proto-Faaslets included to further reduce cold start \nlatenc",
            "\nKey Contributions From Author",
            "Faaslets share the same address space",
            "Fast initialization times",
            "Proto-Faaslets included to further reduce cold start\n\nlatency",
            "Proto-Faaslets are Faaslets that contain a snapshot with\n\nthe function’s stack, heap, function table, stack pointer\n\nand dat",
            "\nInitialization times have been addressed through Proto-Faaslet snapshots",
            "Two-tier state architecture",
            "Integration\n\n○\n\nReplicate Faasm runtime instances with Knative through the\n ... \nbenchmarks reach\n\n50\\-60% extra overhead vs nat",
            "Faaslets begin at ~5 ms initialization and maintain that until around 600 executions/sec",
            "Proto-Faaslets begin at ~0.5 ms initialization and manage that until about 4000 executions/sec"
          ]
        },
        {
          "title": "Spin Cron Trigger",
          "url": "https://www.fermyon.com/blog/spin-cron-trigger",
          "excerpts": [
            "May 22, 2024 — The experimental Cron trigger for Spin allows you to write and deploy Wasm-powered workloads that react to time-based events."
          ]
        },
        {
          "title": "Connect Pub/Sub to Apache Kafka - Google Cloud",
          "url": "https://cloud.google.com/pubsub/docs/connect_kafka",
          "excerpts": [
            "This document describes how to integrate Apache Kafka and Pub/Sub by using the Pub/Sub Group Kafka Connector."
          ]
        },
        {
          "title": "How Kafka Uses the Filesystem to Achieve Lightning Fast Messaging",
          "url": "https://medium.com/@sneharani2509/how-kafka-uses-the-filesystem-to-achieve-lightning-fast-messaging-32c7fccc7c6f",
          "excerpts": [
            "Mention that Kafka's performance comes from sequential I/O, not random writes; Highlight use of sendfile, page cache, and batching; Explain ..."
          ]
        },
        {
          "title": "Cap'n Proto - RPC at the speed of Rust - Part 1 of 2",
          "url": "https://dev.to/kushalj/capn-proto-rpc-at-the-speed-of-rust-part-1-4joo",
          "excerpts": [
            "Cap'n Proto claims to have no serialisation/deserialisation at all once a message is created, which means it should be very fast for transferring data around a ..."
          ]
        },
        {
          "title": "Protobuf vs Flatbuffers vs Cap'n proto which is faster?",
          "url": "https://stackoverflow.com/questions/61347404/protobuf-vs-flatbuffers-vs-capn-proto-which-is-faster",
          "excerpts": [
            "I decided to figure out which of Protobuf, Flatbuffers and Cap'n proto would be the best/fastest serialization for my application."
          ]
        },
        {
          "title": "Data serialization tools comparison: Avro vs Protobuf",
          "url": "https://softwaremill.com/data-serialization-tools-comparison-avro-vs-protobuf/",
          "excerpts": [
            "Jun 30, 2023 — In this post, we will compare the performance of Protobuf and Avro across a range of metrics, including encoding time, decoding time, serialized size, and ..."
          ]
        },
        {
          "title": "Rust vs Node — Kafka producer performance with AWS Lambda",
          "url": "https://mohdizzy.medium.com/rust-vs-node-kafka-producer-performance-with-aws-lambda-831d593a17f0",
          "excerpts": [
            "To explore how much of a difference we are talking I ran a very simple load test on two AWS Lambda functions; one on Node and the other on Rust."
          ]
        },
        {
          "title": "Benchmarking Kafka Write Throughput Performance [2019 UPDATE]",
          "url": "https://aiven-io.medium.com/benchmarking-kafka-write-throughput-performance-2019-update-b863a2cfd9f6",
          "excerpts": [
            "Missing: wasmCloud capability"
          ]
        },
        {
          "title": "Benchmarking Apache Kafka Performance Part 1: Write Throughput",
          "url": "https://aiven.io/blog/benchmarking-kafka-write-throughput",
          "excerpts": [
            "Missing: wasmCloud capability"
          ]
        },
        {
          "title": "I am evaluating Google Pub/Sub vs Kafka. What are the differences?",
          "url": "https://stackoverflow.com/questions/38572071/i-am-evaluating-google-pub-sub-vs-kafka-what-are-the-differences",
          "excerpts": [
            "Google Pub/Sub is a message queue (eg Rabbit MQ) where as Kafka is more of a streaming log. You can't \"re-read\" or \"replay\" messages with Pubsub."
          ]
        },
        {
          "title": "The Upper Limits of WebAssembly Net Performance - Ajani Bilby",
          "url": "https://www.ajanibilby.com/blog/the-upper-limit-of-wasm-performance/",
          "excerpts": [
            "js 1.0, however looking at the details of their benchmarks it shows that running Winter.js in wasm results in a 12x slow down in performance compared to native."
          ]
        },
        {
          "title": "A web application completely written in Rust",
          "url": "https://www.reddit.com/r/rust/comments/8wvw2w/a_web_application_completely_written_in_rust/",
          "excerpts": [
            "I guess my benefit was that I already knew capnproto. For me the protocol definition is one of the best out there, no comparison to proto2/3."
          ]
        },
        {
          "title": "rkyv: a zero-copy deserialization framework for Rust - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/jss6h4/rkyv_a_zerocopy_deserialization_framework_for_rust/",
          "excerpts": [
            "rkyv is similar to other zero-copy deserialization frameworks like Cap'n Proto and FlatBuffers, but it's 100% pure rust and uses macro magic ..."
          ]
        },
        {
          "title": "A faster, more compact, more reliable serialization ...",
          "url": "https://www.reddit.com/r/cpp/comments/1ihksh6/a_faster_more_compact_more_reliable_serialization/",
          "excerpts": [
            "You might want to check out flat buffers or captain proto. There are zero copy protocol generators out there. Unless this is an academic ..."
          ]
        },
        {
          "title": "0-copy your PyArrow array to rust | by Niklas Molin",
          "url": "https://medium.com/@niklas.molin/0-copy-you-pyarrow-array-to-rust-23b138cb5bf2",
          "excerpts": [
            "We've at least seen that its possible to move your PyArrow array to rust with zero data copy. We don't know exactly how to do it secure and nicely."
          ]
        },
        {
          "title": "Zero-copy Apache Arrow with WebAssembly",
          "url": "https://observablehq.com/@kylebarron/zero-copy-apache-arrow-with-webassembly",
          "excerpts": [
            "Zero-copy Apache Arrow with WebAssembly / Kyle Barron | Observable",
            "Aug 22, 2022 — This post will explain how Apache Arrow enables zero-copy data exchange, why it's useful in the browser, and give a live example of reading Arrow memory with ... Zero-copy Apache Arrow with WebAssembly / Kyle Barron | Observable\n",
            "Purpose-built for displays of data\n\nObservable is your go-to platform for exploring data and creating expressive data visualizations. Use reactive JavaScript notebooks for prototyping and a collaborative canvas for visual data exploration and dashboard creation."
          ]
        },
        {
          "title": "MIT 6.858 Final Project: Spark Wasm UDFs with Wasmer",
          "url": "https://css.csail.mit.edu/6.858/2022/projects/rhuffman.pdf",
          "excerpts": [
            "For this project, Wasmer was selected\n\nas it provides an officially supported package for interacting with Wasm modules",
            "Once the Spark session is initialized, the Wasmer JNI is used to load the freshly compiled. Wasm module, creating an object of type org.wasmer.Instance",
            "Unfortunately, a Wasmer Instance is not\n\nserializable, so it cannot be passed directly to Spark",
            "One option is to use Wasmer’s serialization feature, which serializes the module to a\n\nbyte [] , which can be passed around by Spark",
            "However, this byte array must be re-\n\ninstantiated within the UDF, meaning that a new org.wasmer.Instance is created for each\n\nr",
            "This worked with small datasets but quickly caused out-of-memory exceptions when\n\nused with larger datasets",
            "An alternative approach is to create a wrapper class that holds the org.wasmer.Instance\n\nas a static member. With this solution, the instance must be created once per JVM (once\n\nper cluster node) instead of once per row, a significant improvemen",
            "WebAssembly (Wasm) was first introduced by Mozilla in 2017, and enables programs written\n\nin any supported language to be compiled for and run in a web browser",
            "Last, the UDF is invoked on the Spark dataset, creating a new column when using a Map\n\nfunction, or filtering the rows when using a Filter function",
            "Results**\n\nThe current implementation of this system works as described for any numeric Map or Filter\n\nfunction. The compilation of the Wasm module and instantiation of the Wasmer runtime\n\ndo not add significant overhead to the processing of the Spark ",
            "However, using a Wasm\n\nUDF did introduce a significant increase in processing time when compared to a native\n\n4\n\nimplementation of the same opera",
            "On a 1 million row dataset, the Wasm UDF took\n\n2\\.5 times longer to execute, and on a 10 million row dataset, the Wasm UDF took 7 times\n\nlong",
            "These experiments were run with Spark in local mode on a 24-core machine with\n\n96GB of RAM",
            "Spark provides a simple interface for writing SQL-like queries, but also supports arbitrary user-defined functions (UDFs).",
            "Wasm’s memory model also restricts programs\n\nfrom escaping Wasmer’s allocation, preventing a malicious application from corrupting the\n\nmemory of the Spark cluste",
            "The Spark runner is implemented in Java and is responsible for setting up a Spark ses-\n\nsion, loading the Wasm module, and running the Wasm module as a U",
            "This instance must then be\n\npassed to the Spark executor to be used in a UDF",
            "With this solution, the instance must be created once per JVM (once\n\nper cluster node) instead of once per row, a significant improvement",
            "This project has successfully demonstrated that it is feasible to use a Wasm runtime to\n\nsafely execute untrusted code as a Spark User-defined function",
            "While the calls to Wasmer\n\ndo introduce significant overhead, this increase in execution time may be acceptable if it\n\nenables operations that would be otherwise impossible using native Spark, or if the increased\n\nexecution time is outweighed by the significantly decreased initialization ti"
          ]
        },
        {
          "title": "Announcing the first Java library to run WebAssembly",
          "url": "https://blog.wasmer.io/announcing-the-first-java-library-to-run-webassembly-wasmer-jni-89e319d2ac7c",
          "excerpts": [
            "Wasmer JNI is a library to execute WebAssembly directly in Java. It embeds the WebAssembly runtime Wasmer. The first releases provide the core ..."
          ]
        },
        {
          "title": "bytecodealliance/wasmtime: A lightweight WebAssembly ...",
          "url": "https://github.com/bytecodealliance/wasmtime",
          "excerpts": [
            "Wasmtime is built on the optimizing Cranelift code generator to quickly generate high-quality machine code either at runtime or ahead-of-time. Wasmtime is ..."
          ]
        },
        {
          "title": "Wasmtime",
          "url": "https://wasmtime.dev/",
          "excerpts": [
            "Wasmtime is built on the optimizing Cranelift code generator to quickly generate high-quality machine code either at runtime or ahead-of-time. Wasmtime is ..."
          ]
        },
        {
          "title": "Wasmer Java (wasmer-java) documentation",
          "url": "https://github.com/wasmerio/wasmer-java",
          "excerpts": [
            "Easy to use: The wasmer API mimics the standard WebAssembly API,",
            "Fast: The Wasmer JNI executes the WebAssembly modules as fast as possible,",
            "close to **native spe",
            "Safe** : All calls to WebAssembly will be fast, but more\n  importantly, completely safe and sandboxe",
            "The JAR files are named as follows: `wasmer-jni-$(architecture)-$(os)-$(version).jar`",
            "s a toy program in `java/src/test/resources/simple.rs` , written\nin Rust (or any other language that compiles to WebAssembly):",
            "()\n\nQuoting [the WebAssembly site]",
            "There is a toy program in `java/src/test/resources/simple.rs` , written\nin Rust (or any other language that compiles to WebAssembly):",
            "The `Module` class",
            "The `Memory` class"
          ]
        },
        {
          "title": "spark-wasm-udf",
          "url": "https://github.com/slyons/spark-wasm-udf",
          "excerpts": [
            "The part that takes the most code (and the most CPU time) is the serialization/deserialization, which is currently implemented using Protocol Buffers. In a future version I'll replace this with Apache Arrow, much like Spark and DataFusion use already in many places.",
            "This repo is divided up into several sub-projects:\n    * csharp_wasm_udf ,\njs_wasm_udf ,\nrust_wasm_udf , etc are the sources for the various Plugins in their respective languages. Currently missing the implementations for Assemblyscript, C, F#, Go and Typescript. * wasm_spark is the Spark application that actually runs the plugins",
            "Runtime Engine\nThis repo is set up to use both the native Extism runtime and the (still under-development) Chicory runtime. Chicory is a JVM-native WASM runtime made by the same folks who are actively developing Extism. The advantage of having a Java-native runtime when running a Spark project are pretty obvious, but I set up this repo to use both so that they could be compared side-by-side. There is a drawback, however. As of the time of writing, WASI support isn't complete in the Chicory runtime, and as such some of the examples (C# and JavaScript) can't yet run in it.",
            "Performance\nThe current performance isn't up to par, and that's really because of the Serialization/Deserialization step. For better performance I'm definitely going to explore using Apache Arrow to cut down on this time and even support something like vectorization.",
            "Next steps\n    * Add plugin examples for the remaining languages\n    * Explore using a\nMapPartition node instead of a per-row node\n    * Migrate some of the examples to use Apache Arrow instead\n    * Have the UDFs register during some sort of startup step so that return type and arguments can be checked by the Query planner.",
            "Because the Spark rows are marshalled using Protobuf, it's possible to have as deeply nested data as you desire, of any of the types that Spark supports. Calling the plugins from Spark is as easy as using the custom Expression that I've put together:\nWASM_UDF (\npathOrUrl, \" add_two \" , IntegerType ,\ncol( \" id \" ).cast( IntegerType ),\nlit( 2 )\n)\nThis expression takes the path or URL to the WASM file, the function name to call and the expected return type. Everything after that is any number of argument columns that you want to pass to the UDF. This is a bit of an inelegant approach, and one that I hope to improve in the future."
          ]
        },
        {
          "title": "AWS Glue for Apache Spark - Optimize User-Defined Functions",
          "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/tuning-aws-glue-for-apache-spark/optimize-user-defined-functions.html",
          "excerpts": [
            "pache Arrow-based UDFs for vectorized\n  workloads**",
            "The Vectorized UDF is 5 times faster than the previous UDF example. Checking\n`Physical Plan`, you can see `ArrowEvalPython`, which\nshows this application is vectorized by Apache Arrow. To enable Vectorized UDF, you\nmust specify `spark.sql.execution.arrow.pyspark.enabled = true` in your\ncode.",
            "Vectorized UDF\n--------------\n\nExample time is 0.59 (sec). The Vectorized UDF is 5 times faster than the previous UDF example. Checking\n`Physical Plan`, you can see `ArrowEvalPython`, which\nshows this application is vectorized by Apache Arrow. To enable Vectorized UDF, you\nmust specify `spark.sql.execution.arrow.pyspark.enabled = true` in your\ncode.",
            "Spark SQL is much faster than Vectorized UDF, because the tasks are run on each\nexecutor's JVM without a Python runtime . If you can replace your UDF with a\nbuilt-in function, we recommend doing so."
          ]
        },
        {
          "title": "Apache Spark: UDFs, Pandas UDFs, and Arrow-Optimized UDFs (Medium) mixed with Wasm/JNI context",
          "url": "https://ange-kouame.medium.com/apache-spark-a-comparative-overview-of-udf-pandas-udf-and-arrow-optimized-udf-37c62c8d059f",
          "excerpts": [
            "Those functions can be written in Python, R, Java and Scala, enabling more personalized and complex data manipulation.",
            "By introducing Arrow-Optimized UDFs, Spark addressed a number of inherent issues that were not yet resolved with Pandas UDFs such as the support ... UDF stands for User Defined Function."
          ]
        },
        {
          "title": "Ballista Architecture - Apache DataFusion",
          "url": "https://datafusion.apache.org/ballista/contributors-guide/architecture.html",
          "excerpts": [
            "Ballista's primary purpose is to provide a distributed SQL query engine implemented in the Rust programming language and using the Apache Arrow memory model."
          ]
        },
        {
          "title": "First benchmark for DataFusion (Rust) vs Apache Spark for a trivial ...",
          "url": "https://www.reddit.com/r/rust/comments/84t3jj/first_benchmark_for_datafusion_rust_vs_apache/",
          "excerpts": [
            "The perf of DataFusion looks really impressive. You mentioned that Spark performs best on larger datasets than you were benchmarking?"
          ]
        },
        {
          "title": "Dataframe Showdown - Polars vs Spark vs Pandas vs DataFusion ...",
          "url": "https://www.confessionsofadataguy.com/dataframe-showdown-polars-vs-spark-vs-pandas-vs-datafusion-guess-who-wins/",
          "excerpts": [
            "DataFusion was the longest code to write, and super slow. I guess I can take away one thing from this all …"
          ]
        },
        {
          "title": "Home | Apache Gluten incubating - The Apache Software ...",
          "url": "https://gluten.apache.org/",
          "excerpts": [
            "Gluten + Velox backend Performance. The below test environment: single node with 3TB data on Intel® Xeon® Platinum 8592+; Spark-3.3.1 for both baseline and ..."
          ]
        },
        {
          "title": "Introduction to Gluten accelerated Spark engine",
          "url": "https://cloud.ibm.com/docs/watsonxdata?topic=watsonxdata-prov_cpp_gtn_intr",
          "excerpts": [
            "Gluten accelerated Spark engine is an optimized engine that uses Gluten to offload SQL execution to Velox, accelerating SparkSQL computation."
          ]
        },
        {
          "title": "Call WasmEdge functions from an Android APK app",
          "url": "https://wasmedge.org/docs/contribute/source/os/android/apk/",
          "excerpts": [
            "It uses JNI (Java Native Interface) to load a C shared library called wasmedge_lib . It then calls the nativeWasmFibonacci function in wasmedge_lib to execute ..."
          ]
        },
        {
          "title": "Java Native Access Performance - Dmitry Komanov - Medium",
          "url": "https://dkomanov.medium.com/java-native-access-performance-cf4ce0d68ddb",
          "excerpts": [
            "JNI empty just shows a baseline, which is about 22 nanoseconds on my laptop. Seemingly, not a huge overhead (for a simple function that takes ..."
          ]
        },
        {
          "title": "how to use jni in spark? - java native interface",
          "url": "https://stackoverflow.com/questions/21106548/how-to-use-jni-in-spark",
          "excerpts": [
            "First of all, the native library must be preinstalled on all worker nodes. Path to that library must be specified in spark-env.sh.",
            "I want to use jni to call my c++ lib in spark. When i sbt run my program, it shows that java.lang.UnsatisfiedLinkError: no hq_Image_Process in ..."
          ]
        },
        {
          "title": "JNI vs. JNA performance - java",
          "url": "https://stackoverflow.com/questions/22288909/jni-vs-jna-performance",
          "excerpts": [
            "JNA is much slower than JNI, but much easier. If performance is not an issue use JNA. Using direct buffers have the advantage that the most critical operations ..."
          ]
        },
        {
          "title": "How to compile Java to WASM (WebAssembly)?",
          "url": "https://stackoverflow.com/questions/68265863/how-to-compile-java-to-wasm-webassembly",
          "excerpts": [
            "Running Java bytecode inside of a Wasm runtime is possible a few ways. One is to use TeaVM. This page tracks the progress of various methods."
          ]
        },
        {
          "title": "Overhead of Calling Rust FFI from Java JNR/JNI",
          "url": "https://www.reddit.com/r/rust/comments/b0leg9/overhead_of_calling_rust_ffi_from_java_jnrjni/",
          "excerpts": [
            "When I benchmark the Rust code in my test suite, I get around 5ms. This left me wondering if the 5 -> 600ms is because of FFI calls, but I don't ..."
          ]
        },
        {
          "title": "Arrow-optimized Python UDFs in Apache Spark™ 3.5",
          "url": "https://www.databricks.com/blog/arrow-optimized-python-udfs-apache-sparktm-35",
          "excerpts": [
            "Nov 6, 2023 — As shown below, in the same cluster, an Arrow-optimized Python UDF can execute ~1.9 times faster than a pickled Python UDF on a 32 GBs dataset.",
            "Nov 6, 2023 — By harnessing Arrow, these UDFs bypass the traditional, slower methods of data (de)serialization, leading to swift data exchange between JVM and ..."
          ]
        },
        {
          "title": "wasmtime-java bindings and JNI overhead",
          "url": "https://github.com/kawamuray/wasmtime-java",
          "excerpts": [
            "Java (or any JVM) language binding for Wasmtime. Some basic examples are working, but many API implementations are work in progress.",
            "An artifact (JAR) of `wasmtime-java` ships along with prebuilt JNI libraries for some major platforms, so just adding the above dependency provides you a self-contained `wasmtime` runtime on supported platforms:",
            "example",
            "public class HelloWasm {\n    public static void main(String[] args) {\n        try (Store<Void> store = Store.withoutData();\n             Engine engine = store.engine();\n             Module module = Module.fromFile(engine, \"./hello.wat\");\n             Func helloFunc = WasmFunctions.wrap(store, () -> {\n                 System.err.println(\">>> Calling back...\");\n                 System.err.println(\">>> Hello World!\n\");\n             })) {\n            Collection<Extern> imports = Arrays.asList(Extern.fromFunc(helloFunc));\n            try (Instance instance = new Instance(store, module, imports)) {\n                try (Func f = instance.getFunc(\"run\").get()) {\n                    WasmFunctions.Consumer0 fn = WasmFunctions.consumer(f);\n                    fn.accept();\n                }\n            }\n        }\n    }\n}\n"
          ]
        },
        {
          "title": "Stack Overflow: What is the quantitative overhead of making a JNI call",
          "url": "https://stackoverflow.com/questions/13973035/what-is-the-quantitative-overhead-of-making-a-jni-call",
          "excerpts": [
            "``\n\n_Results (x86/i7500/Hotspot/Linux):_\n\n```\nScenario{benchmark=JniCall} 11.34 ns; σ=0.02 ns @ 3 trials\nScenario{benchmark=AddIntOperation} 0.47 ns; σ=0.02 ns @ 10 trials\nScenario{benchmark=AddLongOperation} 0.92 ns; σ=0.02 ns @ 10 trials\n\n       benchmark     ns linear runtime\n         JniCall 11.335 ==============================\n AddIntOperation  0.466 =\nAddLongOperation  0.921 ==\n```",
            "``\n\n_Results (amd64/phenom 960T/Hostspot/Linux):_\n\n```\nScenario{benchmark=JniCall} 6.66 ns; σ=0.22 ns @ 10 trials\nScenario{benchmark=AddIntOperation} 0.29 ns; σ=0.00 ns @ 3 trials\nScenario{benchmark=AddLongOperation} 0.26 ns; σ=0.00 ns @ 3 trials\n\n   benchmark    ns linear runtime\n         JniCall 6.657 ==============================\n AddIntOperation 0.291 =\nAddLongOperation 0.259 =\n```",
            "0.259 =\n```\n\n_Results (armv6/BCM2708/Zero/Linux):_\n\n```\nScenario{benchmark=JniCall} 678.59 ns; σ=1.44 ns @ 3 trials\nScenario{benchmark=AddIntOperation} 183.46 ns; σ=0.54 ns @ 3 trials\nScenario{benchmark=AddLongOperation} 199.36 ns; σ=0.65 ns @ 3 trials\n\n   benchmark  ns linear runtime\n         JniCall 679 ==============================\n AddIntOperation 183 ========\nAddLongOperation 199 ========\n```",
            "This question isn't asking for an absolute timing in nanoseconds, but rather a ball park \"work effort\" in units of \"lines of simple java code\"."
          ]
        },
        {
          "title": "A Java exploring JNI performance via decoding base64 (Medium/Wix Engineering)",
          "url": "https://medium.com/wix-engineering/a-java-exploring-jni-performance-via-decoding-base64-4388683102a2",
          "excerpts": [
            "It's actually pretty close: 12.4 μs (microseconds) with JNI vs 10.2 μs without JNI. Approximately 20% overhead of JNI. Let"
          ]
        },
        {
          "title": "bluejekyll/wasmtime-java: Wasmtime bindings for Java",
          "url": "https://github.com/bluejekyll/wasmtime-java",
          "excerpts": [
            "WASM, web-assembly, is a low-level stack execution engine. This library gives a JNI based binding to the Wasmtime WASM runtime."
          ]
        },
        {
          "title": "Memory Management — Apache Arrow v8.0.0",
          "url": "https://arrow.apache.org/docs/8.0/java/memory.html",
          "excerpts": [
            "Since Arrow always uses direct memory, JNI modules can directly wrap native memory addresses instead of copying data. We use this in modules like the C Data ...",
            "The JVM can optimize I/O operations when using direct memory/direct buffers; it will attempt to avoid copying buffer contents to/from an intermediate buffer."
          ]
        },
        {
          "title": "The Arrow C data interface — Apache Arrow v21.0.0",
          "url": "https://arrow.apache.org/docs/format/CDataInterface.html",
          "excerpts": [
            "The Arrow C data interface defines a very small, stable set of C definitions\nthat can be easily _copied_ in any project’s source code and used for columnar\ndata interchange in the Arrow forma",
            "Make it easy for third-party projects to implement support for (including partial\n  support where sufficient), with little initial investment. * \n  Allow zero-copy sharing of Arrow data between independent runtimes\n  and components running in the same proc",
            "Applications and libraries can therefore work with Arrow memory without\nnecessarily using Arrow libraries or reinventing the wheel.",
            "Expose an ABI-stable interface.",
            "zero-copy sharing of Arrow data between independent runtimes and components running in the same process.",
            "The following free-standing definitions are enough to support the Arrow\nC data interface in your project. Like the rest of the Arrow project, they\nare available under the Apache License 2.0. ```\n#ifndef ARROW_C_DATA_INTERFACE\n#define ARROW_C_DATA_INTERFACE\n\n#define ARROW_FLAG_DICTIONARY_ORDERED 1\n#define ARROW_FLAG_NULLABLE 2\n#define ARROW_FLAG_MAP_KEYS_SORTED 4\n\nstruct ArrowSchema {\n  // Array type description\n  const char * format ;\n  const char * name ;\n  const char * metadata ;\n  int64_t flags ;\n  int64_t n_children ;\n  struct ArrowSchema ** children ;\n  struct ArrowSchema * dictionary ;\n\n  // Release callback\n  void ( * release )( struct ArrowSchema * );\n  // Opaque producer-specific data\n  void * private_data ;\n};\n\nstruct ArrowArray {\n  // Array data description\n  int64_t length ;\n  int64_t null_count ;\n  int64_t offset ;\n  int64_t n_buffers ;\n  int64_t n_children ;\n  const void ** buffers ;\n  struct ArrowArray ** children ;\n  struct ArrowArray * dictionary ;\n\n  // Release callback\n  void ( * release )( struct ArrowArray * );\n  // Opaque producer-specific data\n  void * private_data ;\n};\n\n#endif  // ARROW_C_DATA_INTERFACE\n```"
          ]
        },
        {
          "title": "org.apache.arrow » arrow-c-data",
          "url": "https://mvnrepository.com/artifact/org.apache.arrow/arrow-c-data",
          "excerpts": [
            "Java implementation of C Data Interface. License, Apache 2.0. Tags, dataapachearrow. Ranking, #34630 in MvnRepository (See Top Artifacts)."
          ]
        },
        {
          "title": "Apache Arrow Java Memory Management",
          "url": "https://arrow.apache.org/docs/java/memory.html",
          "excerpts": [
            "Since Arrow always uses direct memory, JNI modules can directly wrap native memory addresses instead of copying data.",
            "The JVM can optimize I/O operations when using direct memory/direct buffers; it will attempt to avoid copying buffer contents to/from an intermediate buffer."
          ]
        },
        {
          "title": "Benchmarking WebAssembly Runtimes | by Brandon Fish",
          "url": "https://blog.wasmer.io/benchmarking-webassembly-runtimes-18497ce0d76e",
          "excerpts": [
            "We performed a small set of benchmarks designed to test runtime performance: fannkuch, fibonacci, n-body, and sha1."
          ]
        },
        {
          "title": "ArrowArray (Apache Arrow Java Root POM 19.0.0-SNAPSHOT API)",
          "url": "https://arrow.apache.org/java/main/reference/org.apache.arrow.c/org/apache/arrow/c/ArrowArray.html",
          "excerpts": [
            "C Data Interface ArrowArray. Represents a wrapper for the following C structure: struct ArrowArray { // Array data description int64_t length; ..."
          ]
        },
        {
          "title": "Java Implementation — Apache Arrow v21.0.0",
          "url": "https://arrow.apache.org/docs/java/index.html",
          "excerpts": [
            "The Java API of Apache Arrow includes creating ValueVectors, Fields, Schemas, and VectorSchemaRoots, and also covers memory management, and Vector life cycle."
          ]
        },
        {
          "title": "How to create Apache Arrow vectors in Java, pass them ...",
          "url": "https://stackoverflow.com/questions/75213449/how-to-create-apache-arrow-vectors-in-java-pass-them-to-c-code-through-jni-r",
          "excerpts": [
            "Use the C Data Interface module in Java to export your Java arrays and get the address of a struct ArrowArray, and then pass that address to your C++ code via ..."
          ]
        },
        {
          "title": "Introducing the Apache Arrow C Data Interface",
          "url": "https://arrow.apache.org/blog/2020/05/03/introducing-arrow-c-data-interface/",
          "excerpts": [
            "May 3, 2020 — Apache Arrow includes a cross-language, platform-independent in-memory columnar format allowing zero-copy data sharing and transfer between heterogenous ...",
            "The C Data Interface stays close to the natural way of expressing Arrow-like\ndata in C or C++. Only two aspects involve non-trivial marshalling:\n\n* the encoding of data types, using a very simple string-based language\n* the encoding of optional metadata, using a very simple length-prefixed for"
          ]
        },
        {
          "title": "Uses of Class org.apache.arrow.c.ArrowArray",
          "url": "https://arrow.apache.org/java/main/reference/org.apache.arrow.c/org/apache/arrow/c/class-use/ArrowArray.html",
          "excerpts": [
            "Export the current contents of a Java Table using the C data interface format. static void. Data. exportVector(BufferAllocator allocator, FieldVector vector, ..."
          ]
        },
        {
          "title": "Using Arrow Flight with Spark Java applications in IBM Cloud",
          "url": "https://developer.ibm.com/tutorials/awb-using-arrow-flight-spark-java-applications-ibm-cloud/",
          "excerpts": [
            "This tutorial demonstrates how the Arrow Flight service provided by IBM Cloud Pak for Data can be used to read and write data sets from within a Spark Java ..."
          ]
        },
        {
          "title": "DataFusion courtesy of Rust, vs Spark. Performance and other ...",
          "url": "https://www.confessionsofadataguy.com/datafusion-courtesy-of-rust-vs-spark-performance-and-other-thoughts/",
          "excerpts": [
            "The performance issue to me is a huge downside, it's over twice as slow as Spark on a small and simple workload. I can imagine that gap would ..."
          ]
        },
        {
          "title": "Accelerate Spark SQL on Arm64 with Gluten and Velox",
          "url": "https://community.arm.com/arm-community-blogs/b/servers-and-cloud-computing-blog/posts/spark-sql-arm64-gluten-velox",
          "excerpts": [
            "Aug 7, 2024 — The following chart, figure 4, shows the performance ratio comparing the Gluten + Velox backend with the Spark Java engine on Neoverse N2, using ..."
          ]
        },
        {
          "title": "Performance in Apache Spark: Benchmark 9 Different ...",
          "url": "https://medium.com/data-science/performance-in-apache-spark-benchmark-9-different-techniques-955d3cc93266",
          "excerpts": [
            "In this article, we will compare 9 different techniques that lead to the same result, but each of them has a very different performance."
          ]
        },
        {
          "title": "Spark UDFs, how to write them and some gotchas? - Medium",
          "url": "https://medium.com/@achilleus/spark-udfs-we-can-use-them-but-should-we-use-them-2c5a561fde6d",
          "excerpts": [
            "UDFs are one exciting aspect of spark which has evolved tremendously over the spark releases. We will try to cover different aspects of it."
          ]
        },
        {
          "title": "What are user-defined functions (UDFs)? | Databricks on AWS",
          "url": "https://docs.databricks.com/aws/en/udf/",
          "excerpts": [
            "Common use cases for UDFs include data encryption, decryption, hashing, JSON parsing, and validation. Use Apache Spark methods for operations on ..."
          ]
        },
        {
          "title": "webassembly",
          "url": "https://mvnrepository.com/tags/webassembly",
          "excerpts": [
            "This library contains the primitives that form the Compose UI Toolkit, such as drawing, measurement and layout."
          ]
        },
        {
          "title": "the Safer Alternative to Integrating Native Code in Java",
          "url": "https://www.reddit.com/r/java/comments/1es8779/webassembly_the_safer_alternative_to_integrating/",
          "excerpts": [
            "They are designed to reduce the chances of breaking fault isolation, whereas Wasm eliminates it by never leaving the JVM."
          ]
        },
        {
          "title": "Powered by | Apache Arrow",
          "url": "https://arrow.apache.org/powered_by/",
          "excerpts": [
            "It supports Arrow as input and output, uses Arrow internally to maximize performance, and can be used with existing Apache Spark™ APIs. Fletcher: Fletcher is a ..."
          ]
        },
        {
          "title": "Apache Arrow with Spark for Faster Data Processing & Analytics",
          "url": "https://medium.com/@biltumaji/apache-arrow-with-spark-for-faster-data-processing-analytics-3bc716c2a00a",
          "excerpts": [
            "In this Demonstration, I'll explain what PyArrow is and why its integration with Spark (Pyspark) and pandas may supercharge our data manipulation."
          ]
        },
        {
          "title": "Apache Arrow and Java: Lightning Speed Big Data Transfer",
          "url": "https://www.infoq.com/articles/apache-arrow-java/",
          "excerpts": [
            "Conceptually, Apache Arrow is designed as a backbone for Big Data systems, for example, Ballista or Dremio, or for Big Data system integrations."
          ]
        },
        {
          "title": "Apache Arrow Java Cookbook documentation",
          "url": "https://arrow.apache.org/cookbook/java/",
          "excerpts": [
            "The Apache Arrow Cookbook is a collection of recipes which demonstrate how to solve many common tasks that users might need to perform when working with Arrow ..."
          ]
        },
        {
          "title": "Apache Arrow: A Beginner's Guide with Practical Examples",
          "url": "https://www.datacamp.com/tutorial/apache-arrow",
          "excerpts": [
            "Arrow Array: An Array represents a single column of data. It stores values in a contiguous memory block, allowing fast access, efficient ..."
          ]
        },
        {
          "title": "Package org.apache.arrow.dataset.jni",
          "url": "https://javadoc.io/static/org.apache.arrow/arrow-dataset/18.3.0/org.apache.arrow.dataset/org/apache/arrow/dataset/jni/package-summary.html",
          "excerpts": [
            "Reserving Java direct memory bytes from java.nio.Bits. JniLoader. The JniLoader for Dataset API's native implementation. JniWrapper. JNI wrapper for Dataset ..."
          ]
        },
        {
          "title": "Benchmarking Runtime Scripting Performance in Wasmer",
          "url": "https://dl.acm.org/doi/pdf/10.1145/3491204.3527477",
          "excerpts": [
            "This is likely because the cost of calling into WASM is a fixed overhead, and this overhead gets amortized when the called function does some ..."
          ]
        },
        {
          "title": "A WebAssembly runtime for running Rust functions in any other ...",
          "url": "https://www.reddit.com/r/rust/comments/11r8bj2/announcing_scale_a_webassembly_runtime_for/",
          "excerpts": [
            "At the end of the day, there's a legitimate overhead to calling a scale function - you need to start the webassembly VM, serialize input, send ..."
          ]
        },
        {
          "title": "RisingWave user-defined functions: Overview",
          "url": "https://risingwave.com/blog/risingwave-user-defined-functions-overview/",
          "excerpts": [
            "Jun 21, 2024 — Example 1: Simplify queries with SQL UDFs · Example 2: Mathematical calculations with Rust UDFs · Example 3: Parse protobuf data with Rust UDFs.",
            "Jun 21, 2024 — Rust UDFs are suitable for heavy computation tasks, compiled into WebAssembly to run in RisingWave's built-in container.",
            "This code is compiled to WebAssembly in the RisingWave frontend and achieves near-native performance when run on computation nodes using JIT (Just-in-time) compilation.",
            "Rust UDFs are suitable for heavy computation tasks, compiled into WebAssembly to run in RisingWave's built-in container.",
            "They target different users and scenarios:",
            "Rust is a high-performance systems programming language and is also used to develop RisingWave. Rust UDFs are suitable for heavy computation tasks, compiled into WebAssembly to run in RisingWave's built-in container.",
            "The third dimension is the function’s execution method, which affects performance and capabilities.",
            "For embedded UDFs, including Rust (WebAssembly), JavaScript, and Python, we embed wasmtime, quickjs/deno, and CPython as the language runtime, respectively."
          ]
        },
        {
          "title": "WASM+UDF performance comparison in Scylladb, why ...",
          "url": "https://stackoverflow.com/questions/77028248/wasmudf-performance-comparison-in-scylladb-why-there-is-no-obvious-differance",
          "excerpts": [
            "In any case, the performance difference between Lua and WASM, if it's significant, will only be visible in long and complex functions that need ..."
          ]
        },
        {
          "title": "Performance questions #96 - bytecodealliance/wasmtime-py - GitHub",
          "url": "https://github.com/bytecodealliance/wasmtime-py/issues/96",
          "excerpts": [
            "I was trying to see what kind of overhead there is to call a wasm function from Python. I am using WSL2 on Windows, with a recent Fedora ..."
          ]
        },
        {
          "title": "Working with UDFs in Apache Spark | Blog",
          "url": "https://www.cloudera.com/blog/technical/working-with-udfs-in-apache-spark.html",
          "excerpts": [
            "Feb 3, 2017 — This blog post provided a walk-through of UDF and UDAF implementation and discussed integration steps to make use of existing Java Hive UDFs ..."
          ]
        },
        {
          "title": "Apache Arrow - C Data Interface",
          "url": "https://arrow.apache.org/docs/java/cdata.html",
          "excerpts": [
            "Arrow supports exchanging data without copying or serialization within the same process\nthrough the [The Arrow C data interface](../format/CDataInterface.html) , even between different language runtimes.",
            "This application uses JNI to call Java code, but transfers data (zero-copy) via the C Data Interface "
          ]
        },
        {
          "title": "A Substack/Mirrored Exploration of JNI/Wasmtime/Wasmer Overheads for Spark-WASM UDFs",
          "url": "https://connorahaskins.substack.com/p/jni-marshalling-performance-single",
          "excerpts": [
            "A rough number for the base cost of a JNI call is estimated anywhere between ~0 to ~104ns on modern devices.",
            "Sending an integer array can tack on a base cost upwards of 300ns on modern devices."
          ]
        },
        {
          "title": "Wasm UDFs for Postgres, implemented as a TLE on plv8 ...",
          "url": "https://www.github-zh.com/projects/680030284-pg_extism",
          "excerpts": [
            "Jan 26, 2025 — Wasm UDFs for Postgres, implemented as a TLE on plv8 using Extism.",
            "dylibso / pg_extism · 星标32 · 复刻1. 代码 问题. main. Ask AI ... 项目介绍及使用教程请前往GitHub 阅读README. 0 条讨论. 登录后发表评论 ..."
          ]
        },
        {
          "title": "Bringing WebAssembly to PostgreSQL using Extism",
          "url": "https://dylibso.com/blog/pg-extism/",
          "excerpts": [
            "Introducing `pg_extism`",
            "`pg_extism` uses [`pgrx`](https://github.com/pgcentralfoundation/pgrx) and the\nExtism Rust SDK to create a PostgreSQL extension that can run Extism plugins.",
            "The implementation is based on these 2 functions:\n\n`extism_call` takes a path of a Extism plugin (a `.wasm` file), and the name of\na function exported by the Extism plugin, and an input JSON object.",
            "ts/host-sdk) provide developer-friendly constructs on top of Wasm’s sandboxing and capability-based security features.",
            "Users can easily define their own custom plugins and use them in their SQL\nqueries.",
            "Notice how we’re using `extism_call` to invoke the Extism plugin when the SQL\nfunction is called.",
            "This will define an SQL function called `llm` with the following schema:",
            "CREATE OR REPLACE FUNCTION llm(prompt TEXT , payload TEXT ) RETURNS TEXT AS $$",
            "In PostgreSQL, users will only need to call `extism_define` explicitly:",
            "In PostgreSQL, users will only need to call `extism_define` explicitly:",
            "`extism_call` then instantiates the plugin using Extism, and calls it and\nreturns the result as a JSON object.",
            "This plugin is implemented in Go, using Extism’s [Go PDK](https://github.com/extism/go-pdk) .",
            "This plugin is implemented in Go, using Extism’s [Go PDK](https://github.com/extism/go-pdk) .",
            "### ChatGPT plugin",
            "This is an Extism plugin that sends a prompt to OpenAI’s ChatGPT API and returns\nthe result.",
            "Users can use the plugin in SQL like this:",
            "pg_extism = # select extism_define( '/path/to/chatgpt.wasm' , 'llm' );",
            "In PostgreSQL, users will only need to call `extism_define` explicitly:\n\n```\nselect extism_define( '/path/to/chatgpt.wasm' , 'llm' );\n```\n\nThis will define an SQL function called `llm` with the following schema:\n\n```\nCREATE OR REPLACE FUNCTION llm(prompt TEXT , payload TEXT ) RETURNS TEXT AS $$\nDECLARE\n    result_json json ;\n    input_param json ;\nBEGIN\n    -- Construct JSON object from parameters\n    input_param : = json_build_object( 'payload' , payload, 'prompt' , prompt);\n\n    -- Call the extism_define function using the provided parameters\n    SELECT extism_call( '/path/to/chatgpt.wasm' , 'chatgpt' , input_param) INTO result_json;\n\n    -- Return the desired field from the result JSON\n    RETURN (result_json -> 'response' ):: TEXT ;\nEXCEPTION\n    WHEN others THEN\n        -- Handle exceptions if necessary\n        RAISE NOTICE 'An error occurred: %' , SQLERRM;\n        RETURN NULL ;\nEND ;\n$$ LANGUAGE plpgsql;\n```\n\nNotice how we’re using `extism_call` to invoke the Extism plugin when the SQ"
          ]
        },
        {
          "title": "pg_extism - WebAssembly UDFs for Postgres (Extism integration via TLE on plv8)",
          "url": "https://github.com/dylibso/pg_extism",
          "excerpts": [
            "Wasm UDFs for Postgres, implemented as a TLE on plv8 using Extism",
            "Make sure [`plv8`](https://github.com/plv8/plv8) is installed on your server. Then enable it for your database:",
            "Now run the script in [dist/index.sql](/dylibso/pg_extism/blob/main/dist/index.sql) and you'll get two SQL functions: `extism_create_plugin` and `extism_call` .",
            "Assume you have a table called `plugins` :",
            "CREATE TABLE public .plugins (",
            "id int4 GENERATED ALWAYS AS IDENTITY( INCREMENT BY 1 MINVALUE 1 MAXVALUE 2147483647 START 1 CACHE 1 NO CYCLE) NOT NULL ,",
            "\" data \" bytea NULL ,",
            "\" name \" varchar NULL ,",
            "CONSTRAINT plugins_pk PRIMARY KEY (id)",
            ");",
            "The above example shows how you can use `extism_create_plugin` to supply your own host functions to the plugin.",
            "You can find th source code for the plugin [here](https://github.com/extism/plugins/tree/main/count_vowels_kvstore) ."
          ]
        },
        {
          "title": "Extism: make all software programmable",
          "url": "https://extism.org/blog/announcing-extism/",
          "excerpts": [
            "Dec 1, 2022 — Extism brings extensibility to software of all shapes and sizes in a way which we believe to have been previously unacheivable."
          ]
        },
        {
          "title": "Postgres WASM by Snaplet and Supabase",
          "url": "https://supabase.com/blog/postgres-wasm",
          "excerpts": [
            "Oct 3, 2022 — postgres-wasm is a PostgreSQL server that runs inside a browser. It provides a full suite of features, including persisting state to browser, restoring from pg ..."
          ]
        },
        {
          "title": "WebAssembly triggers in libSQL - Medium",
          "url": "https://medium.com/chiselstrike/webassembly-triggers-in-libsql-b5eb62cc1c6",
          "excerpts": [
            "libSQL 0.1.0 allows users to create user-defined functions not only programmatically, but also with the CREATE FUNCTION statement, well known in ...",
            "User-defined WebAssembly functions in libSQL can be used to create powerful triggers, making it easy to automate your workflows."
          ]
        },
        {
          "title": "Learning about sqlite3 WASM",
          "url": "https://deepakness.com/blog/sqlite3-wasm/",
          "excerpts": [
            "Apr 24, 2025 — sqlite3 WASM is a version of SQLite built for web browsers using WebAssembly, allowing a full SQLite database to run directly in the browser."
          ]
        },
        {
          "title": "PGlite",
          "url": "https://pglite.dev/",
          "excerpts": [
            "A complete WASM build of Postgres that's under 3MB Gzipped. Extendable. Dynamic extension loading mechanism, including support for pgvector. Reactive."
          ]
        },
        {
          "title": "snaplet/postgres-wasm: A PostgresQL server running in ...",
          "url": "https://github.com/snaplet/postgres-wasm",
          "excerpts": [
            "Sep 19, 2024 — A PostgresQL server running in your browser. Contribute to snaplet/postgres-wasm development by creating an account on GitHub."
          ]
        },
        {
          "title": "Why SQLite is so great for the edge - Turso",
          "url": "https://turso.tech/blog/why-sqlite-is-so-great-for-the-edge-ee00a3a9a55f",
          "excerpts": [
            "SQLite, as well as libSQL, can be compiled into a concise Wasm module — just 390 KiB compressed, less than 900 KiB uncompressed size. It can run ..."
          ]
        },
        {
          "title": "Blog | Extism - make all software programmable. Extend ...",
          "url": "https://extism.org/blog/",
          "excerpts": [
            "Sep 18, 2024 — Other projects with similar goals like sqlite or openssl , are low-level system components exposed to a multitude of language environments ..."
          ]
        },
        {
          "title": "Image Layer Details - danstaken/libsql:libsql-0.1.0-wasm-udf ...",
          "url": "https://hub.docker.com/layers/danstaken/libsql/libsql-0.1.0-wasm-udf/images/sha256-2a14b87b20380863f3a14b7f670692246c8712e5188fa5b0aa31efa0ffa1450f",
          "excerpts": [
            "danstaken/libsql:libsql-0.1.0-wasm-udf ; OS/ARCH. linux/amd64 ; Compressed size. 1.11 GB ; Last pushed. 9 months by danstaken ; Type. Image."
          ]
        },
        {
          "title": "RisingWave user-defined functions: Rust x WebAssembly",
          "url": "https://risingwave.com/blog/user-defined-functions-rust-x-webassembly/",
          "excerpts": [
            "Jul 4, 2024 — This article discusses the design and implementation of Rust UDFs in RisingWave. Rust UDFs achieve isolation by compiling to WebAssembly while maintaining high ...",
            "The input data is stored in a dynamically allocated memory within the module using its `alloc` function. Then, the function is called, and the output is obtained.",
            " Rust UDFs are compiled into WebAssembly, a virtual machine assembly language originally designed for Web browsers. They are then executed within a WebAssembly virtual machine embedded in RisingWave using Just-In-Time (JIT) compilation, offering high efficiency with minimal performance loss compared to native instructions",
            "iance/wasmtime) as the runtime. It first loads the decompressed WebAssembly module, then scans all functions and types according to the protocol.",
            "ave frontend generates a temporary Rust project locally. The embedded Rust code snippet is extracted and supplemented with necessary auxiliary code.",
            "The frontend adds use statements and SQL signatures, generating a `lib.rs` file like this:\n\n```\nuse arrow_udf::{function, types::StructType}; // import prelude\n\n#[function(\"gcd(int, int) -> int\")]\nfn gcd(mut a: i32, mut b: i32) -> i32 {\n    while b != 0 {\n        (a, b) = (b, a % b);\n    }\n    a\n}\n\n```",
            "The wasm file is compressed and stored as part of the function definition in the catalog metadata, to be read and decompressed during execution.",
            "Currently, WebAssembly modules only support pure computation, with almost no external access, including network and file system access. This is because wasmtime defaults to sandbox mode.",
            "In the future, we might support more languages that compile to WebAssembly. Internal implementation",
            "Performance\n\nCan Rust UDFs achieve high performance? If no WebAssembly layer is used for isolation, theoretically, by loading dynamic libraries and running native instructions directly, UDFs would perform identically to built-in functions. If users have complete control over RisingWave and pursue extreme performance, this may be the best solution. However, the downside is that UDFs might cause RisingWave to block or even crash. Users must bear the risk themselves. Using WebAssembly JIT execution, after warm-up, the execution time of pure functions is about 1.5-2x that of native execution.",
            "When receiving a `create function ... language rust ...` statement, the RisingWave frontend generates a temporary Rust project locally.",
            "UDFs in RisingWave. Rust UDFs achieve isolation by compiling to WebAssembly while maintaining high performance. We reused the `#[function]` procedural macro from the internal function framework to provide a concise interface for users.",
            "themselves. Using WebAssembly JIT execution, after warm-up, the execution time of pure functions is about 1.5-2x that of native execution. However, considering the additional data transmission overhead, the actual end-to-end execution time may be longer.",
            "Since each WebAssembly instance can only run single-threaded, we maintain an instance pool for multithreaded parallel execution."
          ]
        },
        {
          "title": "Support WASM plugins for UDF/TableFunctions/more",
          "url": "https://github.com/ClickHouse/ClickHouse/issues/36892",
          "excerpts": [
            "Executable table functions are available in clickhouse. I'm not sure how WASM will help here (apart from probably tighter security). **Describe the solution you'd like**",
            "Loadable WASM with known strictly typed interfaces that can be executed upon to not only be able to execute functions, but possibly even alter the AST of the query for optimization, or possibly an entire table function or table engine as a WASM blob. The advantage here would be passing rich types in and out of known interfaces that can be met by coding in any language that can compile to WASM, which is a fair number at this point. WASM runtimes for c[++] are around so this should be rather possible.",
            "executable UDFs are available in clickhouse but that is a strict stdin/stdout flow of data, and could not easily be used in a more constructive way like table functions or AST optimization."
          ]
        },
        {
          "title": "UDFs User Defined Functions | ClickHouse Docs",
          "url": "https://clickhouse.com/docs/sql-reference/functions/udf",
          "excerpts": [
            "ClickHouse can call any external executable program or script to process data. The configuration of executable user defined functions can be located in one ...",
            "Please contact ClickHouse Support at https://clickhouse.cloud/support to access. ClickHouse can call any external executable program or script to process data."
          ]
        },
        {
          "title": "Qdrant Summer of Code 2024 - WASM based Dimension Reduction",
          "url": "https://qdrant.tech/articles/dimension-reduction-qsoc/",
          "excerpts": [
            "My journey as a Qdrant Summer of Code 2024 participant working on enhancing vector visualization using WebAssembly (WASM) based dimension ..."
          ]
        },
        {
          "title": "Qdrant driver | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/zh/develop/rust/database/qdrant_driver",
          "excerpts": [
            "Qdrant driver. WasmEdge is emerging as a lightweight, portable, and embeddable runtime for large language models (LLMs). LLM inference applications, such as ..."
          ]
        },
        {
          "title": "Bringing WebAssembly to PostgreSQL Using Extism",
          "url": "https://news.ycombinator.com/item?id=37225441",
          "excerpts": [
            "Aug 22, 2023 — Here we have an example of embedding Extism[0] into Postgres and calling UDF/plugins to run arbitrary compute, call-outs to LLMs for enrichment, ..."
          ]
        },
        {
          "title": "Running PostgreSQL in the Browser with WebAssembly - InfoQ",
          "url": "https://www.infoq.com/news/2024/05/pglite-wasm-postgres-browser/",
          "excerpts": [
            "With PGlite, developers can run PostgreSQL queries in a JavaScript environment and persist data to storage in the file system (Node/Bun) or indexedDB (browser)."
          ]
        },
        {
          "title": "Wasmtime: Supporting UDFs in ScyllaDB with WebAssembly",
          "url": "https://www.p99conf.io/2022/09/15/wasmtime-supporting-udfs-in-scylladb-with-webassembly/",
          "excerpts": [
            "It turns out that WASM is also a perfect candidate for a user-defined functions (UDFs) back-end, thanks to its ease of integration, performance, ..."
          ]
        },
        {
          "title": "Bringing WebAssembly to PostgreSQL using Extism",
          "url": "https://www.reddit.com/r/PostgreSQL/comments/15ybmxe/bringing_webassembly_to_postgresql_using_extism/",
          "excerpts": [
            "I think this snippet is such a cool extension... running AI-assisted summarization / enrichment _inside_ the query! pg_extism=# select ..."
          ]
        },
        {
          "title": "Wind-Gone/awesome-olap-paper",
          "url": "https://github.com/Wind-Gone/awesome-olap-paper",
          "excerpts": [
            "A curated paper list of awesome Online Analytical Processing database systems, theory, frameworks, resources, tools and other awesomeness"
          ]
        },
        {
          "title": "Use UDFs in Rust",
          "url": "https://docs.risingwave.com/sql/udfs/use-udfs-in-rust",
          "excerpts": [
            "This article provides a step-by-step guide for defining Rust functions in RisingWave. Rust functions are compiled into WebAssembly modules. Use UDFs in Rust - RisingWave",
            "This article provides a step-by-step guide for defining Rust functions in RisingWave. Rust functions are compiled into WebAssembly modules and then run on the embedded WebAssembly virtual machine in RisingWave.",
            "Rust UDFs provide **higher performance** (near native) compared to Python and Java. The **RisingWave kernel** handles the management of Rust UDFs, eliminating the need for additional maintenance.",
            ". Declare your functions in RisingWave\n\nIn RisingWave, use the [CREATE FUNCTION](/sql/commands/sql-create-function) command to declare the functions you defined. There are two ways to load the WASM module:\n\n1.\nEmbed the WASM binary into SQL with base64 encoding. You can use the following command in psql:\n\nCopy\n\nAsk AI\n\n```\n\\ set wasm_binary \\`base64 -i path/to/udf.wasm\\`\nCREATE FUNCTION gcd ( int , int ) RETURNS int LANGUAGE wasm U",
            "The syntax is outlined below:\n\nCopy\n\nAsk AI\n\n```\nCREATE FUNCTION function_name ( arg_name arg_type [, ... ] )\n    [ RETURNS return_type | RETURNS TABLE ( column_name column_type [, ... ] ) ]\n    LANGUAGE rust\n    AS [ $$ function_body $$ | 'function_body' ];\n```\n",
            "Currently, in `CREATE FUNCTION` statement, Rust code can only use libraries from the standard library, `chrono` , `rust_decimal` , `serde_json` , and does not support other third-party libraries.",
            "Use UDFs in Rust - RisingWave"
          ]
        },
        {
          "title": "Wasmer Postgres - Wasmer GitHub",
          "url": "https://github.com/wasmerio/wasmer-postgres",
          "excerpts": [
            "The Postgres\nns_sum signature is\n(integer, integer) -> integer ,\nwhich maps the Rust\nsum signature\n(i32, i32) -> i32 . So far, only the WebAssembly types\ni32 ,\ni64 and\nv128 are\nsupported; they respectively map to\ninteger ,\nbigint and\ndecimal in Postgres.",
            "A complete and mature WebAssembly runtime for Postgres based on Wasmer .\nIt's an original way to extend your favorite database capabilities. Features:\n    * Easy to use : The\nwasmer API mimics the standard WebAssembly API",
            "smer Postgres\nA complete and mature WebAssembly runtime for Postgres based on Wasmer .\nIt's an original way to extend your favorite database capabilities. Features:\n    * Easy to use : The\nwasmer API mimics the standard WebAssembly API,\n    * Fast :\nwasmer executes the WebAssembly modules as fast as\npossible, close to native speed ,\n    * Safe : All calls to WebAssembly will be fast, but more\nimportantly, completely safe and sandboxed. Note: The project is still in heavy development. This is a\n0.1.0 version. Some API are missing and are under implementation. But\nit's fun to play with it.",
            "    * Fast :\nwasmer executes the WebAssembly modules as fast as\npossible, close to native speed ,",
            "Safe : All calls to WebAssembly will be fast, but more\nimportantly, completely safe and sandboxed. Note: The project is still in heavy development. This is a\n0.1.0 version. Some API are missing and are under implementation. But\nit's fun to play with it. Installation",
            "Supported Postgres versions\nSo far, the extension works on Postgres 10 only.",
            "Usage & documentation\nConsider the\nexamples/simple.rs program:\n# [ no_mangle ] pub extern fn sum ( x : i32 , y : i32 ) -> i32 { x + y }",
            "Benchmarks\nBenchmarks are useless most of the time, but it shows that WebAssembly\ncan be a credible alternative to procedural languages such as\nPL/pgSQL. Please, don't take those numbers for granted, it can change\nat any time, but it shows promising results:",
            "Benchmarks\nBenchmarks are useless most of the time, but it shows that WebAssembly\ncan be a credible alternative to procedural languages such as\nPL/pgSQL. Please, don't take those numbers for granted, it can change\nat any time, but it shows promising results:\n\n\n\n\n    * Benchmark: Fibonacci (n = 50)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.206\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 0.431\n    * Benchmark: 2×\n\n\n    * Benchmark: Fibonacci (n = 500)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.217\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 2.189\n    * Benchmark: 10×\n\n\n    * Benchmark: Fibonacci (n = 5000)\n    * Benchmark: postgres-ext-wasm\n    * Benchmark: 0.257\n    * Benchmark: 1×\n\n\n    * Benchmark: PL/pgSQL\n    * Benchmark: 18.643\n    * Benchmark: 73×",
            "Isn't it awesome? Calling Rust from Postgres through WebAssembly!"
          ]
        },
        {
          "title": "WebAssembly functions for your SQLite-compatible database",
          "url": "https://turso.tech/blog/webassembly-functions-for-your-sqlite-compatible-database-7e1ad95a2aa7",
          "excerpts": [
            "\n\nSQLite supports user-defined functions. UDFs in SQLite works by allowing users to provide a piece of C code that will be executed directly into the data, performing some in-statement computation.",
            "— With the first official release of libSQL, it is possible to execute user defined functions (UDF) written in WebAssembly. WebAssembly functions for your SQLite-compatible database",
            "Our goal was to create a community of database enthusiasts that want to explore other directions that could be taken for an OLTP-oriented embedded database if SQLite would be more open, while standing on the shoulders of giants, as we all should aspire to do.",
            "t WebAssembly isolation rules will prevent",
            "In addition to WebAssembly isolation rules, we require that you limit the function parameters and return type to something that both WebAssembly and libSQL can handle, specifically, for our Rust example:",
            "Any primitive integer type for libSQL’s `INTEGER` type",
            "* `f32` or `f64` for libSQL’s `REAL` type",
            "* `String` or `&str` for libSQL’s `TEXT` type",
            "* `Vec<u8>` for libSQL’s `BLOB` type",
            "The language is publicly specified. So, true to the original SQLite’s approach, it would not be impossible to write an entire new WebAssembly runtime into SQLite.",
            "To make this process easier, we provide a crate, libsql\\_bindgen. It allows you to add the `#[libsql_bindgen]` macro to a Rust function, making sure all types in the function signature are transformed the the types libSQL understands",
            "To make this process easier, we provide a crate, libsql\\_bindgen. It allows you to add the `#[libsql_bindgen]` macro to a Rust function, making sure all types in the function signature are transformed the the types libSQL understands"
          ]
        },
        {
          "title": "WasmEdge libSQL UDFs",
          "url": "https://wasmedge.org/docs/embed/use-case/libsql/",
          "excerpts": [
            ". libSQL is a fork of the popular SQLite database. One of the key enhancements libSQL brings is UDF (User Defined Functions) support.",
            "Through Wasm-based UDFs, users can add their own processing logic to the database, and then process the data in and out of the database on the fly.",
            "libSQL uses WasmEdge to run these UDFs."
          ]
        },
        {
          "title": "SQLite WebAssembly & JavaScript Documentation",
          "url": "https://sqlite.org/wasm",
          "excerpts": [
            "This documentation covers sqlite3's WebAssembly and JavaScript APIs, including a HOWTO, API index, and a cookbook for client-level code.",
            "i-index.md) describes the various API\n  variants and how to load and access them.",
            "*postgres-wasm**](https://supabase.com/blog/postgres-wasm) runs a\n  [Postgres database server](https://www.postgresql.org) in a browser."
          ]
        },
        {
          "title": "Extism Plug-in Development Kit (PDK) for Rust - GitHub",
          "url": "https://github.com/extism/rust-pdk",
          "excerpts": [
            "The goal of writing an Extism plug-in is to compile your Rust code to a Wasm module with exported functions that the host application can invoke."
          ]
        },
        {
          "title": "User-Defined Functions (UDFs) in Databend",
          "url": "https://docs.databend.com/guides/query/udf",
          "excerpts": [
            "WASM UDFs allow you to use rust to define the functions and build it into wasm module, then load it into Databend. Example: Fibonacci Calculation. Create a new ..."
          ]
        },
        {
          "title": "UDFs in DuckDB: Unlocking Custom Functionality with ...",
          "url": "https://www.getorchestra.io/guides/udfs-in-duckdb-unlocking-custom-functionality-with-web-assembly",
          "excerpts": [
            "Jan 31, 2023 — This article focuses on the integration of WASM with DuckDB, particularly through the use of User Defined Functions (UDFs), to significantly optimize data ..."
          ]
        },
        {
          "title": "DUCKDB - WASM Scalar UDF with Utf-8 return type not ...",
          "url": "https://stackoverflow.com/questions/79067369/duckdb-wasm-scalar-udf-with-utf-8-return-type-not-working-as-expected",
          "excerpts": [
            "In my project, I am utilizing duckdb wasm. I encountered a need for a custom user-defined function (UDF), so I created one with a return type of UTF-8."
          ]
        },
        {
          "title": "Semantic Versioning UDF in ClickHouse",
          "url": "https://clickhouse.com/blog/semantic-versioning-udf",
          "excerpts": [
            "In this blog post, Juan S. Carrillo shows us how to create a user-defined function to work with semantic versions."
          ]
        },
        {
          "title": "Welcome to Turso - Turso",
          "url": "https://docs.turso.tech/",
          "excerpts": [
            "Get started with Go, Java, JavaScript, Python, Rust, and Wasm using one of the available bindings in progress. ​. Turso Cloud. Create unlimited SQLite databases ..."
          ]
        },
        {
          "title": "Havelsan's Revenue in 2024",
          "url": "https://blog.wasmer.io/announcing-the-first-postgres-extension-to-run-webassembly-561af2cfcb1",
          "excerpts": [
            "The goal is to compare the execution time between WebAssembly and PL/pgSQL, and see how both approaches scale.",
            "Wasmer is a WebAssembly runtime.",
            "We run the benchmark with Postgres 10, on a MacBook Pro 15\" from 2016, 2.9Ghz Core i7 with 16Gb of memory.",
            "This section shows how to inspect a WebAssembly instance. At the same time, it quickly explains how the extension works under the hood. The extension provides two foreign data wrappers, gathered together in the `wasm` foreign schema:",
            "Calling a WebAssembly function from Postgres",
            "wasm.instances` is a table with the `id` and `wasm_file` columns, respectively for the unique instance ID, and the path of the WebAssembly module",
            "wasm.exported_functions` is a table with the `instance_id`,`name`, `inputs`, and `outputs` columns, respectively for the instance ID of the exported function, its name, its input types (already formatted for Postgres), and its output types (already formatted for Postgres)"
          ]
        },
        {
          "title": "WebAssembly triggers in libSQL",
          "url": "https://turso.tech/blog/webassembly-triggers-in-libsql-b5eb62cc1c6",
          "excerpts": [
            "WebAssembly triggers in libSQL",
            "User-defined WebAssembly functions in libSQL can be used to create powerful triggers, making it easy to automate your workflows.",
            "WebAssembly was chosen due to its robustness, portability and security, as well as performance.",
            "libSQL 0.1.0 allows users to create user-defined functions not only programmatically, but also with the `CREATE FUNCTION` statement, well known in the world of SQL.",
            "With the help of both features, more business logic can be pushed to the database layer, with computations performed closer to the data. This improves performance, security, and level of control over your data.",
            "They are useful not only for queries, but also as building blocks for database triggers.",
            "\nWebAssembly integration brought portable and secure user-defined functions to libSQL in its [first official release](https://github.com/libsql/libsql/releases/tag/libsql-0.1.0)",
            "the following Rust snippet can be compiled to Wasm and registered as a user-defined function called `encrypt` :",
            "Next, we need an encryption function. Here's where WebAssembly user-defined functions become useful."
          ]
        },
        {
          "title": "Run a Plug-in - make all software programmable. Extend from within.",
          "url": "https://extism.org/docs/quickstart/host-quickstart",
          "excerpts": [
            "A minimal guide to running an Extism plug-in in your language and platform of choice. This document should get you from 0 to \"Hello, World!\" as quickly as ..."
          ]
        },
        {
          "title": "Server-side plugins / extensions #3562 - LemmyNet/lemmy",
          "url": "https://github.com/LemmyNet/lemmy/issues/3562",
          "excerpts": [
            "Jul 9, 2023 — Not for manipulating data on the disk? Not sure if it applies to your use case, but there's this: https://dylibso.com/blog/pg-extism/ ..."
          ]
        },
        {
          "title": "Introducing MySQL & Postgres support for PHP in Wasmer Edge",
          "url": "https://wasmer.io/posts/introducing-mysql-postgres-support-for-php-in-wasmer-edge",
          "excerpts": [
            "Following good practices from Serverless providers, we have created a database benchmark measuring different DBs performance with Wasmer Edge."
          ]
        },
        {
          "title": "User-Defined Function - Databend",
          "url": "https://docs.databend.com/sql/sql-commands/ddl/udf/",
          "excerpts": [
            "This page provides a comprehensive overview of User-Defined Function (UDF) operations in Databend, organized by functionality for easy reference."
          ]
        },
        {
          "title": "Deep Dive into Databend UDF, implementing your data solutions ...",
          "url": "https://medium.com/@databend/deep-dive-into-databend-udf-implementing-your-data-solutions-with-python-wasm-and-beyond-ec1cbb7957c1",
          "excerpts": [
            "WASM UDFs: Near-native performance for compute-intensive operations; External UDF Servers: Scalable microservices for enterprise workloads. In ..."
          ]
        },
        {
          "title": "🤖 How to Use Wasm to Add UDFs to the Database",
          "url": "https://www.secondstate.io/articles/udf-saas-extension/",
          "excerpts": [
            "Mar 24, 2022 — For example, YoMo embeds WasmEdge into the real-time data stream, and then uses WasmEdge for AI inference of data, and also uses Wasm as an ..."
          ]
        },
        {
          "title": "Introducing pgzx: create PostgreSQL extensions using Zig",
          "url": "https://news.ycombinator.com/item?id=39779024",
          "excerpts": [
            "Mar 21, 2024 — Being a trusted language would mean it could be used to create Trusted Language Extensions (TLEs) and could be installed on cloud hosted databases without ..."
          ]
        },
        {
          "title": "Idea: WASM for Library Sandboxing - Joseph Lewis",
          "url": "https://josephlewis.net/2024/09/idea-wasm-for-library-sandboxing/",
          "excerpts": [
            "If you wanted to achieve capability based security today, you might try sandboxing at the language, process, or service level. Traditionally ..."
          ]
        },
        {
          "title": "WebAssembly is eating the database? - Hacker News",
          "url": "https://news.ycombinator.com/item?id=36044112",
          "excerpts": [
            "What WebAssembly brings to the table is the ability for unprivileged users to upload their own arbitrary UDF to the DB for execution."
          ]
        },
        {
          "title": "Introduction - The bindgen User Guide",
          "url": "https://rust-lang.github.io/rust-bindgen/",
          "excerpts": [
            "The bindgen User Guide. Introduction. bindgen automatically generates Rust FFI bindings to C and C++ libraries. For example, given the C header cool.h ..."
          ]
        },
        {
          "title": "How WebAssembly is Eating the Database, One UDF At a Time",
          "url": "https://dylibso.com/blog/wasm-udf/",
          "excerpts": [
            "Missing: sqlite O"
          ]
        },
        {
          "title": "libSQL Remote Protocol Reference - Turso Docs",
          "url": "https://docs.turso.tech/sdk/http/reference",
          "excerpts": [
            "The API enable developers to perform SQL operations using the libSQL remote protocol, retrieve server version information, and monitor the health status."
          ]
        },
        {
          "title": "singlestore-wasm-toolkit vs pg_extism",
          "url": "https://www.libhunt.com/compare-singlestore-wasm-toolkit-vs-pg_extism",
          "excerpts": [
            "Here we have an example of embedding Extism[0] into Postgres and calling UDF/plugins to run arbitrary compute, call-outs to LLMs for enrichment, and more. Code ..."
          ]
        },
        {
          "title": "Releases · bytecodealliance/wasm-micro-runtime",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime/releases",
          "excerpts": [
            "WAMR-2.0.0. Breaking Changes. The AOT ABI was changed after GC and memory64 features were introduced: Implement GC feature for interpreter, AOT and LLVM-JIT (# ..."
          ]
        },
        {
          "title": "[RFC] Basic support for Multi-Memory proposal #3381",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime/issues/3381",
          "excerpts": [
            "When the multi-memory macro is turned on, the wasm/aot runtime will be able to support both non-multi-memory and multi-memory wasm/aot files."
          ]
        },
        {
          "title": "wasmer - Rust - GitHub Pages",
          "url": "https://wasmerio.github.io/wasmer/crates/doc/wasmer/",
          "excerpts": [
            "Wasmer is the most popular WebAssembly runtime for Rust. It supports JIT (Just In Time) and AOT (Ahead Of Time) compilation as well as pluggable compilers ..."
          ]
        },
        {
          "title": "Wasmer vs. Wasmtime vs. Wasmedge — Unveiling the ...",
          "url": "https://medium.com/@siashish/comparing-webassembly-runtimes-wasmer-vs-wasmtime-vs-wasmedge-unveiling-the-power-of-wasm-ff1ecf2e64cd",
          "excerpts": [
            "In this interactive blog, we explored three powerful WebAssembly runtimes: Wasmer, Wasmtime, and Wasmedge. Each runtime offers unique features."
          ]
        },
        {
          "title": "[RFC] Basic support for Memory64 proposal · Issue #3091",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime/issues/3091",
          "excerpts": [
            "For the wamrc AOT compiler, no extra work is required during compile time. To compile a memory64 aot file, use wamrc --enable-memory64 -o test."
          ]
        },
        {
          "title": "Releases · WasmEdge/WasmEdge - GitHub",
          "url": "https://github.com/WasmEdge/WasmEdge/releases",
          "excerpts": [
            "[Test] Switch to the new spectest repo. test(spec): turn on the AOT/JIT tests for GC proposal; feat(test): the trap cases should fit their wasm phase."
          ]
        },
        {
          "title": "Pushing Serverless to the Edge with WebAssembly Runtimes",
          "url": "http://faculty.washington.edu/wlloyd/courses/tcss562_f2023/papers/PushingServerlesstotheEdgewithWebAssemblyRuntimes.pdf",
          "excerpts": [
            "by P Gackstatter · Cited by 73 — The runtime has a JIT and a native engine. Support for high-quality AoT compilation with. LLVM and fast startup times make wasmer a promising option."
          ]
        },
        {
          "title": "Leaps and bounds: Analyzing WebAssembly's performance ...",
          "url": "https://www.research.ed.ac.uk/files/296056934/Leaps_and_bounds_SZEWCZYK_DOA02092022_AFV.pdf",
          "excerpts": [
            "by R Szewczyk · 2022 · Cited by 15 — The WebAssembly runtimes are able to provide isolation, so the WebAssembly benchmark runner spawns one instance of the runtime for each benchmark instance,."
          ]
        },
        {
          "title": "The AoT Compiler | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/start/build-and-run/aot/",
          "excerpts": [
            "Missing: atomics JIT"
          ]
        },
        {
          "title": "Wasmtime Release Notes (RELEASES.md)",
          "url": "https://docs.rs/crate/wasmtime-cli/12.0.2/source/RELEASES.md",
          "excerpts": [
            "## 1.0.0",
            "Released 2022-09-20",
            "This release marks the official 1.0 release of Wasmtime and represents the\nculmination of the work amongst over 300 contributors. Wasmtime has been\nbattle-tested in production through multiple embeddings for quite some time now\nand we're confident in releasing a 1.0 version to signify the stability and\nquality of the Wasmtime engine."
          ]
        },
        {
          "title": "Rust wasm32-wasi threads - README (loganek/rust-wasi-threads repository)",
          "url": "https://github.com/loganek/rust-wasi-threads",
          "excerpts": [
            "Rust wasm32-wasi threads",
            "This repository demonstrates how to prepare a Rust toolchain for wasi-threads. Quick start. ",
            "This will generate a WASM binary in `tests/simple/target/wasm32-wasi-preview1-threads/debug/simple.wasm` that can be run in WASM runtime that implements `wasi-threads` (e.g.",
            "\n\n# running on Wasmtime\nwasmtime run --wasm-features=threads --wasi-modules=experimental-wasi-threads \\\n    tests/simple/target/wasm32-wasi-preview1-threads/debug/simple.wasm"
          ]
        },
        {
          "title": "Multithreading Rust and Wasm",
          "url": "https://rustwasm.github.io/2018/10/24/multithreading-rust-and-wasm.html",
          "excerpts": [
            "Features like threads for wasm can have a huge impact on Rust and how it's used on the web, and we want to be sure that Rust is ready and well suited for wasm ...",
            "Oct 24, 2018 — Many languages (including Rust) also rely on shared memory as a primitive to build all sorts of concurrency paradigms like message passing or ..."
          ]
        },
        {
          "title": "Native implementation vs WASM for Go, Python and Rust ...",
          "url": "https://karnwong.me/posts/2024/12/native-implementation-vs-wasm-for-go-python-and-rust-benchmark/",
          "excerpts": [
            "Dec 4, 2024 — Benchmark Results. Note: wasm binary execution has a cold start. This means on the first run, it would be significantly slower than subsequent ..."
          ]
        },
        {
          "title": "wasm-bindgen-rayon 1.2.0 release: you can now use ...",
          "url": "https://www.reddit.com/r/WebAssembly/comments/1995a8b/wasmbindgenrayon_120_release_you_can_now_use/",
          "excerpts": [
            "wasm-bindgen-rayon 1.2.0 release: you can now use parallel Rust iterators from the main Wasm thread."
          ]
        },
        {
          "title": "Tiers of support",
          "url": "https://docs.wasmtime.dev/stability-tiers.html",
          "excerpts": [
            "Wasmtime's support for platforms and features can be distinguished with three different tiers of support. The description of these tiers are intended to be ...",
            "Tier 1 WebAssembly feature are required to be supported by at least one compiler on all Tier 1 targets, and similarly for Tier 2 features and so on. Note that ..."
          ]
        },
        {
          "title": "Wasmtime and Cranelift in 2023",
          "url": "https://bytecodealliance.org/articles/wasmtime-and-cranelift-in-2023",
          "excerpts": [
            "Nov 21, 2023 — The pooling allocator — Wasmtime's fastest WebAssembly instance allocation strategy that reserves a pool of memory for Wasm instances ahead ..."
          ]
        },
        {
          "title": "wasm64 support · Issue #572 · bytecodealliance/wasmtime",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/572",
          "excerpts": [
            "Nov 14, 2019 — We should consider supporting wasm64 modules, not just wasm32; people will want to run with large linear address spaces, both to process ..."
          ]
        },
        {
          "title": "Wasm runtime overrides high CPU usage · Issue #9858",
          "url": "https://github.com/paritytech/substrate/issues/9858",
          "excerpts": [
            "Sep 24, 2021 — We noticed that when using the wasm overrides the CPU cores go though the roof - to a point the node stops syncing."
          ]
        },
        {
          "title": "How does CPU affinity interact with cgroups in Linux?",
          "url": "https://serverfault.com/questions/775906/how-does-cpu-affinity-interact-with-cgroups-in-linux",
          "excerpts": [
            "May 10, 2016 — From the cpusets documentation: Calls to sched_setaffinity are filtered to just those CPUs allowed in that task's cpuset."
          ]
        },
        {
          "title": "How to allocate \"huge\" pages for C++ application on Linux",
          "url": "https://stackoverflow.com/questions/32652833/how-to-allocate-huge-pages-for-c-application-on-linux",
          "excerpts": [
            "Your best bet is to allocate blocks on a 2MB boundary with posix_memalign and then do a madvise(MADV_HUGEPAGE) on the allocated region before touching it for ..."
          ]
        },
        {
          "title": "Wasmtime Pooling Allocation and Memory Configuration (api/struct.PoolingAllocationConfig.html)",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.PoolingAllocationConfig.html",
          "excerpts": [
            "The main benefit of the pooling allocator is to make WebAssembly instantiation both faster and more scalable in terms of parallelism.",
            "sm. Allocation is faster ... The pooling allocator operates over slots to allocate from, and each\nslot is considered “cold” if it’s never been used before or “warm” if\nit’s been used by some module in the past.",
            "Slots in the pooling\nallocator additionally track an “affinity” flag to a particular core\nwasm module.",
            "When a module is instantiated into a slot then the slot is\nconsidered affine to that module, even after the instance has been\ndeallocated.",
            " When a new instance is created then a slot must be chosen, and the\ncurrent algorithm for selecting a slot is:\n\n* \n  If there are slots that are affine to the module being instantiated,\n  then the most recently used slot is selected to be allocated from. This is done to improve reuse of resources such as memory mappings and\n  additionally try to benefit from temporal locality for things like\n  caches. * \n  Otherwise if there are more than N affine slots to other modules, then\n  one of those affine slots is chosen to be allocated. The slot chosen\n  is picked on a least-recently-used basis. * \n  Finally, if there are less than N affine slots to other modules, then\n  the non-affine slots are allocated from. This setting, `max_unused_warm_slots` , is the value for N in the above\nalgorithm. The purpose of this setting is to have a knob over the RSS\nimpact of “unused slots” for a long-running wasm server. If this setting is set to 0, for example, then affine slots are\naggressively reused on a least-recently-used basis.",
            "The virtual memory reservation size of each linear memory is controlled\nby the [`Config::memory_reservation`](struct.Config.html.memory_reservation \"method wasmtime::Config::memory\\_reservation\") setting and this method’s\nconfiguration cannot exceed [`Config::memory_reservation`](struct.Config.html.memory_reservation \"method wasmtime::Config::memory\\_reservation\") . [Source](../src/wasmtime/config.rs.htm",
            "The maximum byte size that any WebAssembly linear memory may grow to. This option defaults to 4 GiB meaning that for 32-bit linear memories\nthere is no restrictions. 64-bit linear memories will not be allowed to\ngrow beyond 4 GiB by default. If a memory’s minimum size is greater than this value, the module will\nfail to instantiate. If a memory’s maximum size is unbounded or greater than this value, the\nmaximum will be `max_memory_size` for the purpose of any `memory.grow` instruction. This value is used to control the maximum accessible space for each\nlinear memory of a core instance. This can be thought of as a simple\nmechanism like [`Store::limiter`](struct.Store.html.limiter \"method wasmtime::Store::limiter\") to limit memory\nat runtime. This value can also affect striping/coloring behavior when\nused in conjunction with [`memory_protection_keys`](struct.PoolingAllocationConfig.html.memory_protection_keys \"method wasmtime::PoolingAllocationConfig::memory\\_protection\\_keys\"",
            "When using the pooling allocator (see [`Config::allocation_strategy`](struct.Config.html.allocation_strategy \"method wasmtime::Config::allocation\\_strategy\") , [`InstanceAllocationStrategy::Pooling`](enum.InstanceAllocationStrategy.html.Pooling \"variant wasmtime::InstanceAllocationStrategy::Pooling\") ), memory protection keys can\nreduce the total amount of allocated virtual memory by eliminating guard\nregions between WebAssembly memories in the pool. It does so by\n“coloring” memory regions with different memory keys and setting which\nregions are accessible each time executions switches from host to guest\n(or vice ver",
            " WebAssembly 32-bit linear memories in\nWasmtime are, by default 4G address space reservations with a small guard\nregion both before and after the linear memory. Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard.",
            "For example a pooling allocator can be configured with:",
            "Configuration options used with `InstanceAllocationStrategy::Pooling` to change the behavior of the pooling instance allocator. By avoiding [`mmap`](https://man7.org/linux/man-pages/man2/mmap.2.html) this\navoids whole-process virtual memory locks which can improve scalability and\nperformance through avoiding this. Additionally with pooled allocation it’s possible to create “affine slots”\nto a particular WebAssembly module or component over time. For example if\nthe same module is multiple times over time the pooling allocator will, by\ndefault, attempt to reuse the same slot. This mean that the slot has been\npre-configured and can retain virtual memory mappings for a copy-on-write\nimage, for example (see [`Config::memory_init_cow`](struct.Config.html.memory_init_cow \"method wasmtime::Config::memory\\_init\\_cow\") for more information. This means that in a steady state instance deallocation is a single [`madvise`](https://man7.org/linux/man-pages/man2/madvise.2.html) to reset linear memory to its original contents followed by a\nsingle (optional) [`mprotect`](https://man7.org/linux/man-pages/man2/mprotect.2.html) during the next instantiation to shrink\nmemory back to its original size. Compared to non-pooled allocation this\navoids the need to [`mmap`](https://man7.org/linux/man-pages/man2/mmap.2.html) a new region of memory, [`munmap`](https://man7.org/linux/man-pages/man2/munmap.2.html) it, and [`mprotect`](https://man7.org/linux/m",
            "\nWith all these options in place no virtual memory tricks are used at all and\neverything is manually managed by Wasmtime (for example resetting memory is\na `memset(0)` ). This is not as fast in a single-threaded scenario but can\nprovide benefits in high-parallelism situations as no virtual memory locks\nor IPIs need happen.",
            "Despite the above advantages to instantiation performance the pooling\nallocator is not enabled by default in Wasmtime.",
            "One reason is that the\nperformance advantages are not necessarily portable, for example while the\npooling allocator works on Windows it has not been tuned for performance on\nWindows in the same way it has on Linux. Additionally the main cost of the pooling allocator is that it requires a\nvery large reservation of virtual memory (on the order of most of the\naddressable virtual address space). WebAssembly 32-bit linear memories in\nWasmtime are, by default 4G address space reservations with a small guard\nregion both before and after the linear memory.",
            "Memories in the pooling\nallocator are contiguous which means that we only need a guard after linear\nmemory because the previous linear memory’s slot post-guard is our own\npre-guard. This means that, by default, the pooling allocator uses roughly\n4G of virtual memory per WebAssembly linear memory slot. 4G of virtual\nmemory is 32 bits of a 64-bit address",
            "The main benefit of the pooling allocator is to make WebAssembly instantiation both faster and more scalable in terms of parallelism. Allocation is faster ..."
          ]
        },
        {
          "title": "lib.rs - source",
          "url": "https://docs.wasmtime.dev/api/src/wasmtime_wasi_threads/lib.rs.html",
          "excerpts": [
            "1//! Implement [`wasi-threads`]. 2//! 3//! [`wasi-threads`]: https://github.com/WebAssembly/wasi-threads 4 5use anyhow::{Result, anyhow}; 6use ..."
          ]
        },
        {
          "title": "Introducing Wasmer 5.0 : r/rust - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/1geznqm/introducing_wasmer_50/",
          "excerpts": [
            "One feature I'm really interested in, which wasmtime has, is the ability to implement host calls asynchronously."
          ]
        },
        {
          "title": "WasmEdge",
          "url": "https://news.ycombinator.com/item?id=33792322",
          "excerpts": [
            "The wasmedge CLI tool will execute the WebAssembly in ahead-of-time(AOT) mode if available in the input WASM file."
          ]
        },
        {
          "title": "Performance Benefits of Using Huge Pages for Code.",
          "url": "https://easyperf.net/blog/2022/09/01/Utilizing-Huge-Pages-For-Code",
          "excerpts": [
            "Sep 1, 2022 — In this article, I show how to speed up source code compilation for the clang compiler by 5% if you allocate its code section on Huge Pages."
          ]
        },
        {
          "title": "How to specify AOT mode or JIT mode in Rust api? #3014 - GitHub",
          "url": "https://github.com/wasmerio/wasmer/issues/3014",
          "excerpts": [
            "Wasmer use AOT all the time. There is no JIT. You can use pre-compiled binary with the .wasmu or use the bytecode wenassembly .wasm, that ..."
          ]
        },
        {
          "title": "rayon-rs/rayon - A data parallelism library for Rust - GitHub",
          "url": "https://github.com/rayon-rs/rayon",
          "excerpts": [
            "Rayon is a data-parallelism library for Rust. It is extremely lightweight and makes it easy to convert a sequential computation into a parallel one."
          ]
        },
        {
          "title": "CPU pinning? What's the purpose and what are ...",
          "url": "https://www.reddit.com/r/unRAID/comments/ucf0uf/cpu_pinning_whats_the_purpose_and_what_are_the/",
          "excerpts": [
            "CPU pinning allows you to have dockers or VMs only running in the the assigned cores. My use case is for SABnzbd and Plex. I have SAB set to ..."
          ]
        },
        {
          "title": "WASM will replace containers",
          "url": "https://news.ycombinator.com/item?id=43020684",
          "excerpts": [
            "The sandboxing abilities of WASM are near unmatched, along with it's startup time and execution speed compared to native."
          ]
        },
        {
          "title": "WebAssembly in 2025: The Full Story (Frontend, Web3 & ...",
          "url": "https://medium.com/@p.reaboi.frontend/webassembly-in-2025-the-full-story-frontend-web3-limitations-7ee7cf0f9292",
          "excerpts": [
            "In 2025, Wasm is widely used for high-performance UIs — think Figma, cloud-based IDEs, browser‑based video processing, or crypto‑heavy logic in ..."
          ]
        },
        {
          "title": "A Survey of Popular Wasm Runtimes",
          "url": "https://arxiv.org/html/2404.12621v2",
          "excerpts": [
            "The standalone Wasm runtimes, such as WAMR  (WAM, 2022 ) , wasmtime  (was, 2022c ) , and WasmEdge  (Was, 2022 ) , could be called in the programs written in high-level languages as a library to allow developers execute Wasm binaries in any possible cases with various l",
            " Wasmtime is a standalone Wasm runtime proposed by the Bytecode Alliance, designed for both WebAssembly and the WebAssembly System Interface (WASI)",
            " WAMR is a lightweight standalone Wasm runtime with high performance and highly configurable features",
            "Wasmer supports different compiler frameworks(LLVM  (llv, 2022 ) , cranelift  (cra, 2024 ) , singlepass  (sin, 2022 ) ) and can generate native code through these Wasm c",
            "The Wasm bytecode can be embedded into programs developed in other languages through these high-level language APIs."
          ]
        },
        {
          "title": "Wasm Micro Runtime benchmarks and discussion (Issue #3752)",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime/issues/3752",
          "excerpts": [
            "Total operations: 2599323 (866387.19 per second)",
            "Throughput:\n    events/s (eps):                      866387.1913",
            "Total operations: 532424 (177466.05 per second)",
            "Throughput:\n    events/s (eps):                      177466.0549",
            "CPU speed:\n    events per second: 773738.73",
            "Throughput:\n    events/s (eps):                      315.7304",
            "In gcc compilation of sysbench i am getting below results.",
            "sysbench memory --memory-block-size=1K --memory-total-size=3G --time=3 run",
            "sysbench memory --memory-block-size=1K --memory-total-size=3G --time=3 run",
            "admin1@admin1-VivoBook-ASUSLaptop-X515EA-P1511CEA:~/sysbench_main$ sysbench cpu --cpu-max-prime=20000 --time=3 run",
            "CPU speed:\n    events per second:  1340.5"
          ]
        },
        {
          "title": "From Two-Phase Compilation to POSIX-like Environments ...",
          "url": "https://javascript.plainenglish.io/webassemblys-complete-system-interface-evolution-from-two-phase-compilation-to-posix-like-491b36eeffc8",
          "excerpts": [
            "Jun 4, 2025 — WASI will cover much of the same ground as POSIX, including things such as files, network connections, clocks, and random numbers. And it will ...",
            "Jun 4, 2025 — The two-phase compilation model provides the foundation for WASI's system interface capabilities: System Call Translation. In the two-phase ..."
          ]
        },
        {
          "title": "High-level bindings for wasi-nn system calls",
          "url": "https://github.com/bytecodealliance/wasi-nn",
          "excerpts": [
            "This project provides high-level wasi-nn bindings for Rust and AssemblyScript. The basic idea: write your machine learning application in a high-level language ..."
          ]
        },
        {
          "title": "Capabilities-Based Security with WASI - WebAssembly",
          "url": "https://medium.com/webassembly/capabilities-based-security-with-wasi-c523a34c1944",
          "excerpts": [
            "WASI implements capabilities-based security by granting WebAssembly modules only explicitly assigned accesses. Instead of global user rights ..."
          ]
        },
        {
          "title": "What's The State of WASI?",
          "url": "https://www.fermyon.com/blog/whats-the-state-of-wasi",
          "excerpts": [
            "May 14, 2025 — Designed with security at the fore, WASI describes mechanisms for Wasm applications to interact with system resources such as clocks, sockets, ..."
          ]
        },
        {
          "title": "WebAssembly: An Updated Roadmap for Developers",
          "url": "https://bytecodealliance.org/articles/webassembly-the-updated-roadmap-for-developers",
          "excerpts": [
            "This roadmap reflects changes occurring in standards within the WebAssembly Community Group (CG) and the WASI Subgroup within the W3C.",
            "WebAssembly: An Updated Roadmap for Developers",
            "Jul 24, 2023 Bailey Hayes",
            "The WebAssembly (Wasm) ecosystem is transforming. Developers can look forward to a modular, virtualizable, and robust environment for building applications, libraries, and services."
          ]
        },
        {
          "title": "WebAssembly: The future that wasn't",
          "url": "https://tyk.io/blog/webassembly-the-future-that-wasnt/",
          "excerpts": [
            "Apr 11, 2025 — Making WASI a reality, and fully-fledged, would make it possible to easily spin up web servers, TCP Clients, and file-servers using out-of-the- ..."
          ]
        },
        {
          "title": "Compiling Rust to WASI - benwis",
          "url": "https://benw.is/posts/compiling-rust-to-wasi",
          "excerpts": [
            "Mar 18, 2024 — Recently I've been taking a bit of a break from building and interacting with Rust compiled to the Webassembly target wasm32-unknown-unknown ..."
          ]
        },
        {
          "title": "Measurements of system call performance and overhead",
          "url": "http://arkanis.de/weblog/2017-01-05-measurements-of-system-call-performance-and-overhead/",
          "excerpts": [
            "The benchmark is run 10 times. The average of those runs is then divided by 10 million to get the time per function or system call. The ...",
            "Jan 5, 2017 — The benchmark is run 10 times. The average of those runs is then divided by 10 million to get the time per function or system call. The function ..."
          ]
        },
        {
          "title": "Understanding the Performance of WebAssembly Applications",
          "url": "https://weihang-wang.github.io/papers/imc21.pdf",
          "excerpts": [
            "by Y Yan · 2021 · Cited by 98 — However, the context switch between JavaScript and WebAssembly causes additional runtime overhead. WebAssembly requires JavaScript to access Web APIs. (e.g. ..."
          ]
        },
        {
          "title": "io - I/O Performance Benchmarking Linux",
          "url": "https://serverfault.com/questions/219739/i-o-performance-benchmarking-linux",
          "excerpts": [
            "I'd recommend using bonnie++ for disk performance testing. It's made specifically for doing that sort of thing."
          ]
        },
        {
          "title": "How to use Rust's async/await syntax in WASI - Stack Overflow",
          "url": "https://stackoverflow.com/questions/66574538/how-to-use-rusts-async-await-syntax-in-wasi",
          "excerpts": [
            "There is some mention on github that reqwests can be used with wasm, but it's not yet fully supported, and I couldn't find much on how to make ..."
          ]
        },
        {
          "title": "WASI Interfaces and Proposals",
          "url": "https://wasi.dev/interfaces",
          "excerpts": [
            "Documentation for WASI 0.1 and WITX can be found in the legacy directory of the WASI GitHub repository, along with a complete list of 0.1 types and modules.",
            " WASI 0.2 was released in February 2024",
            "WASI 0.3[​]( \"Direct link to WASI 0.3\")",
            "For more information on the forthcoming WASI 0.3 release, see the [Roadmap](/roadmap). WASI 0.2[​]( \"Direct link to WASI 0.2\")",
            "APIs designed for WASI 0.2 and the Component Model are defined with the WebAssembly Interface Type (WIT) Interface Description Language (IDL).",
            "WIT API definitions are made in `.wit` files which are composed into Wasm component binaries.",
            "The following interfaces are included in WASI P2:",
            "You can explore the types and definitions for a given WASI 0.2 API in its WIT files.",
            "WASI 0.2 includes several patch releases:",
            "Phase 5 - The Feature is Standardized (WG)",
            "Phase 4 - Standardize the Feature (WG)",
            "Phase 3 - Implementation Phase (CG + WG)"
          ]
        },
        {
          "title": "WebAssembly System Interface (WASI) Documentation",
          "url": "https://github.com/WebAssembly/WASI",
          "excerpts": [
            "WASI Preview 2 is now stable, and is a modular collection of\nAPIs defined with the Wit IDL , and it incorporates many of the lessons\nlearned from Preview 1, including adding support for a wider range of\nsource languages, modularity, a more expressive type system,\nvirtualizability, and more.",
            "Preview 1 , an API using\nthe witx IDL, and it is now widely used. Its major influences are POSIX and\nCloudABI. WASI",
            "WASI High Level Goals",
            "Define a set of portable, modular, runtime-independent, and\nWebAssembly-native APIs which can be used by WebAssembly code to interact\nwith the outside world.",
            "WASI is designed with capability-based security principles, using the\nfacilities provided by the Wasm component model . All access to external\nresources is provided by capabilities"
          ]
        },
        {
          "title": "WASI 0.2.0 and Why It Matters",
          "url": "https://wasmcloud.com/blog/wasi-preview-2-officially-launches/",
          "excerpts": [
            "Jan 25, 2024 — With the release of WASI Preview 2, we take a step back and look at why it matters for wasmCloud and the WebAssembly ecosystem."
          ]
        },
        {
          "title": "WASI 0.2 Preview: A New Dawn for WebAssembly",
          "url": "https://thenewstack.io/wasi-preview-2-a-new-dawn-for-webassembly/",
          "excerpts": [
            "Jan 26, 2024 — The launch of WebAssembly System Interface is poised to redefine the boundaries of web applications and server-side processing. Jan 26th, 2024 4 ..."
          ]
        },
        {
          "title": "Enables zero copy shared memory, zero allocation return ...",
          "url": "https://github.com/WebAssembly/component-model/issues/398",
          "excerpts": [
            "Sep 21, 2024 — It uses WASI 0.3 streams to broadcast host side shared memory buffers (a resource) from one publisher to two subscribers. The components pre- ..."
          ]
        },
        {
          "title": "wasmtime - Rust",
          "url": "https://docs.rs/wasmtime",
          "excerpts": [
            "async - Enabled by default, this feature enables APIs and runtime support for defining asynchronous host functions and calling WebAssembly asynchronously.",
            "Wasmtime is a WebAssembly engine for JIT-compiled or ahead-of-time compiled WebAssembly modules and components.",
            "A WebAssembly global descriptor. GuestProfiler profiling and runtime: Collects basic profiling data for a single WebAssembly guest. I31 gc and runtime: A 31 ...",
            "Wasmtime is a WebAssembly engine for JIT-compiled or ahead-of-time compiled WebAssembly modules and components. More information about the Wasmtime project as a ..."
          ]
        },
        {
          "title": "Posix-compatibility.md - WebAssembly/wasi-sockets",
          "url": "https://github.com/WebAssembly/wasi-sockets/blob/main/Posix-compatibility.md",
          "excerpts": [
            "This document provides an overview of the POSIX interface along with common non-standard extensions and their mapping to functionalities provided by this ..."
          ]
        },
        {
          "title": "May WASI turn out to harm Wasm use cases on the Web? · Issue #401",
          "url": "https://github.com/WebAssembly/WASI/issues/401",
          "excerpts": [
            "As of today, WASI is the most standard-ish API we have to perform common tasks like obtaining the current time or printing to console."
          ]
        },
        {
          "title": "Help, wasmtime rust asynchronous how to achieve #8273",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/8273",
          "excerpts": [
            "Mar 31, 2024 — You can invoke multiple host functions concurrently but Wasmtime cannot invoke multiple guest functions concurrently at this time. React ..."
          ]
        },
        {
          "title": "Standardizing WASI: A system interface to run ...",
          "url": "https://news.ycombinator.com/item?id=19501614",
          "excerpts": [
            "Mar 27, 2019 — WASI would give me near-native performance for this one without any additional work, and also fix issues like how file watchers don't work ..."
          ]
        },
        {
          "title": "Func in wasmtime - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Func.html",
          "excerpts": [
            "You might have an async function in Rust, however, which you'd like to make available from WebAssembly. Wasmtime supports asynchronously calling WebAssembly ..."
          ]
        },
        {
          "title": "WASI: secure capability based networking",
          "url": "https://jdriven.com/blog/2022/08/WASI-capability-based-networking",
          "excerpts": [
            "Aug 24, 2022 — Wasm and WASI are secure by design, running sandboxed and using a capability based security model. That means a Wasm module cannot access the filesystem or ..."
          ]
        },
        {
          "title": "WASI's Capability-based Security Model - Yuki Nakata",
          "url": "https://www.chikuwa.it/blog/2023/capability/",
          "excerpts": [
            "Jan 29, 2023 — WASI uses the concept of Capability-based Security to prevent escape from the sandbox by granting minimum privileges to applications."
          ]
        },
        {
          "title": "wasi_common - Rust",
          "url": "https://docs.wasmtime.dev/api/wasi_common/index.html",
          "excerpts": [
            "Wasi-common internally treats files and directories as two distinct resource types in the table - Box<dyn WasiFile> and Box<dyn WasiDir>.",
            "In order for wasi-common to access the local filesystem at all, you need to provide WasiFile and WasiDir impls through either the new wasi-cap-std-sync crate ..."
          ]
        },
        {
          "title": "wasi-common - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/wasi-common",
          "excerpts": [
            "A library providing a common implementation of WASI hostcalls for re-use in any WASI-enabled runtime. The wasi-common crate will ultimately serve as a library ..."
          ]
        },
        {
          "title": "Releases · WebAssembly/WASI",
          "url": "https://github.com/WebAssembly/WASI/releases",
          "excerpts": [
            "Jun 12, 2025 — The WIT packages for this release are pushed to OCI based on the Wasm OCI Artifact layout: wasi:io@0.2.2 · wasi:random@0.2.2 · wasi:clocks@0.2.2 ..."
          ]
        },
        {
          "title": "wasi-filesystem/wit/types.wit at main - GitHub",
          "url": "https://github.com/WebAssembly/wasi-filesystem/blob/main/wit/types.wit",
          "excerpts": [
            "WASI filesystem is a filesystem API primarily intended to let users run WASI programs that access their files on their existing filesystems, without ..."
          ]
        },
        {
          "title": "Running `*.wasm` files directly doesn't sandbox ...",
          "url": "https://github.com/wasmerio/wasmer/issues/4267",
          "excerpts": [
            "Oct 21, 2023 — In wasmtime or other Wasm runtimes with WASI support, you have to explicit preopen a directory to have access to the host filesystem."
          ]
        },
        {
          "title": "WASI 0.2.0 and Why It Matters",
          "url": "https://cosmonic.com/blog/engineering/wasi-preview-2-officially-launches",
          "excerpts": [
            "They are fundamentally lower levels of abstraction meant for library developers and implementers. WASI 0.2.0 is important not because it gives ..."
          ]
        },
        {
          "title": "Socket client | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/develop/rust/socket_networking/client/",
          "excerpts": [
            "It is possible for WasmEdge applications to open TCP/IP or UDP network sockets in the host system to communicate directly with external applications."
          ]
        },
        {
          "title": "Faqs",
          "url": "https://wasix.org/docs/explanation/faqs",
          "excerpts": [
            "WASIX aims to avert refactoring fatigue that is impending on WASI, locking in the existing ABI for long term support and extension."
          ]
        },
        {
          "title": "wasi::sockets - Rust",
          "url": "https://docs.rs/wasi/latest/wasi/sockets/index.html",
          "excerpts": [
            "API documentation for the Rust `sockets` mod in crate `wasi`."
          ]
        },
        {
          "title": "Tokio Feels Slow? You're Just Doing Rust Async Wrong - Medium",
          "url": "https://medium.com/@premchandak_11/tokio-feels-slow-youre-just-doing-rust-async-wrong-f66fda77ecf3",
          "excerpts": [
            "Tokio isn't slow — your code might be blocking the event loop. Replace blocking I/O with async I/O. Avoid excessive task spawning — batch or ..."
          ]
        },
        {
          "title": "Since Wasmtime 20, preopens of non-existent directories fail ...",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/8552",
          "excerpts": [
            "Maybe either wasmtime-wasi or cap-std could provide a Windows-specific convenience function which returns a collection of all available drives."
          ]
        },
        {
          "title": "Does Tokio on Linux use blocking IO or not? : r/rust - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/1kaxyq3/does_tokio_on_linux_use_blocking_io_or_not/",
          "excerpts": [
            "Missing: wasmtime 10x"
          ]
        },
        {
          "title": "View of A Comparative Study of WebAssembly Runtimes",
          "url": "https://ojs.bonviewpress.com/index.php/AAES/article/view/4965/1367",
          "excerpts": [
            "by M Ţălu · 2025 · Cited by 3 — A Comparative Study of WebAssembly Runtimes: Performance Metrics, Integration Challenges, Application ..."
          ]
        },
        {
          "title": "WASI Support - wasmCloud",
          "url": "https://wasmcloud.com/docs/reference/wasi/support/",
          "excerpts": [
            "Thanks to the embedded wit in each component, you can see what APIs a component uses by running wash inspect --wit <component_reference> with either a file or ..."
          ]
        },
        {
          "title": "WASI 0.2.0 and Why It Matters",
          "url": "https://news.ycombinator.com/item?id=39143054",
          "excerpts": [
            "The WebAssembly standard breaks into two parts, \"core WASM\" which is the spec the browsers use and everyone else. Then in the non-browser space ..."
          ]
        },
        {
          "title": "Container Runtimes and WasmEdge benchmarking on Arm",
          "url": "https://news.ycombinator.com/item?id=36110253",
          "excerpts": [
            "The whole advantage of wasm should be very lightweight low overhead wasm runtime instances atop a common wasm process. Having a process or ..."
          ]
        },
        {
          "title": "Evolution of Random Number Generators | Hacker News",
          "url": "https://news.ycombinator.com/item?id=26991278",
          "excerpts": [
            "Missing: WASI getrandom"
          ]
        },
        {
          "title": "How to decide which is the most secure way to generate random ...",
          "url": "https://stackoverflow.com/questions/37470792/how-to-decide-which-is-the-most-secure-way-to-generate-random-numbers-in-java-us",
          "excerpts": [
            "I am wondering which is the best way to generate highly secured random number in range 0-255 and moreover with fast speed performance."
          ]
        },
        {
          "title": "WebAssembly/memory64: Memory with 64-bit indexes",
          "url": "https://github.com/WebAssembly/memory64",
          "excerpts": [
            "Jan 15, 2025 — It is meant for discussion, prototype specification and implementation of a proposal to add support for 64-bit memory indexes to WebAssembly.",
            "This proposal has reached phase 4 and has been merged upstream.",
            "Memory with 64-bit indexes"
          ]
        },
        {
          "title": "Wasmtime just implemented memory64 - Developers",
          "url": "https://forum.dfinity.org/t/wasmtime-just-implemented-memory64/6434",
          "excerpts": [
            "Aug 12, 2021 — reserved pages to grow into to help amortize the cost of memory.grow. Seems to suggest that the current implementation may not be quite ...",
            "Aug 12, 2021 — Seems like one of the biggest blockers for increasing the canister memory limit has just been removed. Please give us canisters with terabytes ..."
          ]
        },
        {
          "title": "WebAssembly.Memory - MDN",
          "url": "https://developer.mozilla.org/en-US/docs/WebAssembly/Reference/JavaScript_interface/Memory",
          "excerpts": [
            "Apr 22, 2025 — The WebAssembly.Memory object is a resizable ArrayBuffer or SharedArrayBuffer that holds raw bytes of memory accessed by a WebAssembly."
          ]
        },
        {
          "title": "Wasm unknown vs emscripten - Rust Users Forum",
          "url": "https://users.rust-lang.org/t/wasm-unknown-vs-emscripten/22997",
          "excerpts": [
            "Just to clarify about the allocator, the default allocator for wasm32-unknown-unknown is dlmalloc. You can use wee_alloc instead, but you ..."
          ]
        },
        {
          "title": "Memory in wasmtime - Rust",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/struct.Memory.html",
          "excerpts": [
            "WebAssembly memories represent a contiguous array of bytes that have a size that is always a multiple of the WebAssembly page size, currently 64 kilobytes."
          ]
        },
        {
          "title": "rustwasm/wee_alloc: The Wasm-Enabled, Elfin Allocator",
          "url": "https://github.com/rustwasm/wee_alloc",
          "excerpts": [
            "wee_alloc is focused on targeting WebAssembly, producing a small .wasm code size, and having a simple, correct implementation."
          ]
        },
        {
          "title": "wasm and wasm-unknown: allocator always allocates 2 pages (2x64k)",
          "url": "https://github.com/tinygo-org/tinygo/issues/4223",
          "excerpts": [
            "This is for growing the heap when it runs out of memory, it's not directly related to the initial memory size. This function is called when even ..."
          ]
        },
        {
          "title": "How does wasm-bindgen manage memory allocation? : r/rust - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/qvq5c9/how_does_wasmbindgen_manage_memory_allocation/",
          "excerpts": [
            "I originally thought std::alloc only worked on wasm32-wasi but TIL but also works on wasm32-unknown-unknown now that I see the mod.rs file that ..."
          ]
        },
        {
          "title": "Avoiding allocations in Rust to shrink Wasm modules",
          "url": "https://nickb.dev/blog/avoiding-allocations-in-rust-to-shrink-wasm-modules/",
          "excerpts": [
            "Oct 7, 2022 — In theory, wee_alloc is great. It's a quick and painless solution that is small and fast enough when not too many allocations are needed."
          ]
        },
        {
          "title": "Calling a WASM method that accepts a byte array with embedded ...",
          "url": "https://users.rust-lang.org/t/calling-a-wasm-method-that-accepts-a-byte-array-with-embedded-wasmtime/57241",
          "excerpts": [
            "Your code needs to expose malloc() and free() functions for allocating memory inside the WebAssembly memory space using the default allocator ( ..."
          ]
        },
        {
          "title": "Allocating/Deallocating Memory in Rust compiled to ...",
          "url": "https://stackoverflow.com/questions/78000437/allocating-deallocating-memory-in-rust-compiled-to-webassembly",
          "excerpts": [
            "I have been trying to make an IO game with parts of the client in Rust, compiled with wasm-pack to Webassembly, which I access through Javascript."
          ]
        },
        {
          "title": "Properly preallocating memory for a collection of items",
          "url": "https://users.rust-lang.org/t/properly-preallocating-memory-for-a-collection-of-items/75893",
          "excerpts": [
            "May 21, 2022 — I preallocated memory with the with_capacity function, but inserting didn't work. Then I took the opportunity to learn more about unsafe Rust ..."
          ]
        },
        {
          "title": "Building a Memory Allocator for WebAssembly - Charly Castes",
          "url": "https://charlycst.github.io/posts/wasm-memory-allocator/",
          "excerpts": [
            "the [Go allocator](https://golang.org/src/runtime/malloc.go) has a huge focus on concurrency with thread local memory pools, while OCaml integers are only 63 bits longs because the lowest bit is used by the GC to differentiate integers from pointers (that is why there is an [int64 module](https://caml.inria.fr/pub/docs/manual-ocaml/libref/Int64.html)).",
            "In the context of programming languages, the memory allocator is both heavily influenced by the language design and influences the language itself:",
            "As you will see, linked list allocators are slow, the `malloc` operation takes *O(n)* steps, but they are also very flexible and are often used as a basis upon which more sophisticated allocation methods can be built.",
            "A list of blocks",
            "Linked list allocators are conceptually simple:",
            "they consist in a (linked) list of blocks, when an allocation of size *s* is requested the allocator iterates through the list, selects a block of size at least *s* and removes it from the list before returning a pointer to the first byte of that block.",
            "Such an allocator that return the first big enough block is said to use a *first fit* strategy",
            "another common strategy is the *best fit*, where the smallest block that is big enough is returned.",
            "The other subtlety is *alignment*. In Web assembly alignment has no effect on the semantic, meaning that it does not change the program behavior, but it may affect performances.",
            "A 32 bits load is aligned if its address is a multiple of 32 bits (or 4 bytes), and as you expect a 64 bits load is aligned if its address is a multiple of 64 bits (or 8 bytes).",
            "The design space is huge, choices had to be made and for this post I propose the following:",
            "In WebAssembly the largest load can fetch 64 bits (or 8 bytes) and thus an alignment of 8 bytes is safe for any purpose.",
            "Ok, but why a doubly linked list? That’s because we will need the two way linking to be able to free blocks in O(1) while coalescing at the same time.",
            "In fact you will see that we will also need footers, otherwise there is no way to check if the preceding block is free.",
            "Let’s recap what happens here:",
            "First we read the memory at index 0, remember that its where we decided to store the pointer to the first block.",
            "We can easily find the header of the next block because we know the size of our block, if it happens to be free (the ‘allocated’ flag is set to 0) we can merge both blocks into a single one.",
            "When freeing the right block, we can use the left block's footer to check if it is free and get its size.",
            "the choice of the *real* size of the block is crucial for preserving the alignment:",
            "The second body does not have an alignment of 8. As you can see, the next block’s body is only aligned to 4, we are breaking our part of the contract, which may result in unaligned reads and a drop in performance.",
            "A frequent solution to this problem is to use *segregated lists*, where instead of having a single free list with blocks of various sizes, we can maintain a bunch of lists with fixed block sizes, let’s say 8, 16, 32, 64 and so on…",
            "When the user requests a block, we round the size up to the next power of 2 and take the first block of the corresponding free list, that’s O(1) for the allocation, and we put it back in the list on free, that’s O(1) too.",
            "The allocator design",
            "First fit strategy.",
            "First fit strategy.",
            "Memory blocks are aligned to 8 bytes.",
            "Memory blocks are aligned to 8 bytes.",
            "Doubly linked list.",
            "Doubly linked list.",
            "Coalesce on free.",
            "Coalesce on free."
          ]
        },
        {
          "title": "MemoryType in wasmtime - Rust",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/struct.MemoryType.html",
          "excerpts": [
            "The minimum and maximum values here are specified in units of WebAssembly pages, which are 64KiB by default. Use MemoryTypeBuilder if you want a non-default ..."
          ]
        },
        {
          "title": "Allocator won't use the provided memory when using",
          "url": "https://github.com/rustwasm/wasm-bindgen/issues/1389",
          "excerpts": [
            "Mar 24, 2019 — I have a program that uses a big amount of heap memory but it grows sequentially, so say, I need to use 1GB of heap memory by allocating chunks of ~20MB every ..."
          ]
        },
        {
          "title": "Making WebAssembly and Wasmtime More Portable",
          "url": "https://bytecodealliance.org/articles/wasmtime-portability",
          "excerpts": [
            "First, when loaded into memory, the compiled Wasm's machine code must be executable, and non-portable assumptions around the presence of mmap ..."
          ]
        },
        {
          "title": "WebAssembly Threads and Atomics - Can I Use",
          "url": "https://caniuse.com/wasm-threads",
          "excerpts": [
            "WebAssembly Threads and Atomics",
            "An extension to WebAssembly adding shared memory and atomic memory operations",
            "#### Chrome",
            "1. ❌ 4 \\- 73 : Not supported",
            "2. ✅ 74 \\- 129 : Supported",
            "#### Edge",
            "1. ❌ 12 \\- 18 : Not supported",
            "2. ✅ 79 \\- 129 : Supported",
            "3. ✅ 130 : Supported",
            "3. ✅ 130 : Supported",
            "#### Safari",
            "1. ❌ 3\\.1 - 14 : Not supported",
            "2. ✅ 14\\.1 - 17.6 : Supported",
            "3. ✅ 18\\.0 : Supported",
            "4. ✅ 18\\.1 - TP : Supported",
            "#### Firefox",
            "1. ❌ 2 \\- 78 : Not supported",
            "2. ✅ 79 \\- 131 : Supported",
            "3. ✅ 132 : Supported",
            "4. ✅ 133 \\- 135 : Supported"
          ]
        },
        {
          "title": "WebAssembly Features and Memory Architecture",
          "url": "https://webassembly.org/features/",
          "excerpts": [
            "Memory64](https://github.com/WebAssembly/memory64/blob/master/proposals/memory64/Overview.md) | ✓ Supported | 133",
            "Supported in desktop Safari since 14.1 and iOS Safari since 14.5; Supported in Safari 18.2, only supports WebAssembly.Module.imports and WebAssembly.Module.",
            "✓ Supported since version 134 | ✗ Not supported | 24\\.0",
            "Multiple Memories](https://github.com/WebAssembly/multi-memory/blob/master/proposals/multi-memory/Overview.md) | ✓ Supported",
            "✓ Supported since version 2.2 | <sup>[[w]]()</sup>",
            "Requires flag\n`--wasm.Memory64=true` | N/A",
            "gine |\n| [Threads](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md) | ✓ Supported | 74",
            "✓ Supported since version 74 | 79",
            "✓ Supported since version 79 | 14\\.1 <sup>[[j]]()</sup>",
            "Supported in desktop Safari since 14.1 and iOS Safari since 14.5 | 16\\.4",
            "✓ Supported since version 16.4 | 1\\.9",
            "✓ Supported since version 1.9 | <sup>[[y]]()</sup>",
            "Requires flag `--wasm.Threads=true` | N/A",
            "Memory64"
          ]
        },
        {
          "title": "The State of WebAssembly – 2024 and 2025",
          "url": "https://platform.uno/blog/state-of-webassembly-2024-2025/",
          "excerpts": [
            "Memory64"
          ]
        },
        {
          "title": "GitHub WebAssembly/design issue 1397 (Memory architecture discussion for Wasm memory and memory64)",
          "url": "https://github.com/WebAssembly/design/issues/1397",
          "excerpts": [
            "So basically Wasm memory story is \"you can only grab more memory, with no guarantee if the memory you got is a reserve or a commit\". These are ... The",
            ". The way that Wasm VM implementations seem to tackle these issues is to try to be smart/automatic under the hood about reserve vs commit behavior, and esp. around shared vs non-shared memory",
            "#c7 . On desktop, the Wasm spec memory issues have so far fallen in the \"awkward\" category at most, because i) all OSes and browsers have completed migration to 64-bit already, ii) desktops can afford large 16GB+ RAM sizes (and RAM sizes are expandable on many desktops), and iii) desktops have large disk sizes for the OS to swap pages out to, so even large numbers of committed pages may not be the end of the world (just \"awkward\") esp. if they go unused for most parts. On mobile, none of that is true.",
            ". Note that wasm memory64 proposal does not relate or solve to this problem. That proposal is about letting applications to use more than 4GB of memory, but this issue is about Wasm applications not being able to safely manage much smaller amounts of memory on mobile devices.",
            ")\nCurrently allocating more than ~300MB of memory is not reliable on Chrome on Android without resorting to Chrome-specific workarounds , nor in Safari on iOS . As per the suggestions in the Chromium thread, applications should either know up front at compile time how much memory they will need, or gratuitously reserve everything that they can.",
            "e\nThe Wasm spec says that one can conveniently set initial memory size to what they need to launch, and then grow more when the situation demands it. Setting maximum is optional, to allow for unbounded growth."
          ]
        },
        {
          "title": "Timeline update? · Issue #36 · WebAssembly/memory64",
          "url": "https://github.com/WebAssembly/memory64/issues/36",
          "excerpts": [
            "Memory64 is currently \"staged\" in V8, meaning it is enabled if you enable \"Experimental WebAssembly\" in the chrome flags."
          ]
        },
        {
          "title": "Comments & Activity - Chrome Platform Status",
          "url": "https://chromestatus.com/feature/5070065734516736/activity",
          "excerpts": [
            "summary: The memory64 proposal adds support for linear WebAssembly memories with size larger than 2^32 bits. It provides no new instructions, but instead ..."
          ]
        },
        {
          "title": "How to set max memory limit in wasm - Rust Users Forum",
          "url": "https://users.rust-lang.org/t/how-to-set-max-memory-limit-in-wasm/64725",
          "excerpts": [
            "Wasmer has an in-depth example of limiting memory and wasmtime lets you customise linear memory's behaviour with Config::with_host_memory() ."
          ]
        },
        {
          "title": "Config in wasmtime - Rust - Docs.rs",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html",
          "excerpts": [
            "\nMemory in the `initial` range is accessible to the instance and can be\nread/written by wasm code. Memory in the `guard` regions is never\naccessible to wasm code and memory in `capacity` is initially\ninaccessible but may become accessible through `memory.grow` instructions",
            "Configures whether execution of WebAssembly will “consume fuel” to either halt or yield execution as desired. This can be used to deterministically prevent ..."
          ]
        },
        {
          "title": "InstanceAllocationStrategy in wasmtime - Rust",
          "url": "https://docs.rs/wasmtime/latest/wasmtime/enum.InstanceAllocationStrategy.html",
          "excerpts": [
            "Available on crate feature pooling-allocator only. The pooling instance allocation strategy. A pool of resources is created in advance and module instantiation ..."
          ]
        },
        {
          "title": "wasi-libc.git/history - dlmalloc - git.proxmox.com Git",
          "url": "https://git.proxmox.com/?p=wasi-libc.git;a=history;f=dlmalloc;hb=1792d02ac94c6c18f28ef38704c8d957f37fb879",
          "excerpts": [
            "WASI libc prototype implementation. tree | commitdiff. 2019-01-15, Dan Gohman · Merge pull request #9 from WebAssembly/dlmalloc · tree | commitdiff. 2019-01-10 ..."
          ]
        },
        {
          "title": "Don't use wee_alloc in production code targeting ...",
          "url": "https://www.reddit.com/r/rust/comments/x1cle0/dont_use_wee_alloc_in_production_code_targeting/",
          "excerpts": [
            "The standard library contains an allocator for wasm now. So you can either pull in the whole standard library, or use the alloc crate in wasm."
          ]
        },
        {
          "title": "Jemalloc was just removed from the standard library",
          "url": "https://internals.rust-lang.org/t/jemalloc-was-just-removed-from-the-standard-library/8759",
          "excerpts": [
            "Nov 3, 2018 — This might seem like a strange question, but does the jemallocator crate work on the wasm32-unknown-unknown target? steveklabnik November 6, ..."
          ]
        },
        {
          "title": "Consider linking with mimalloc in release executables?",
          "url": "https://github.com/WebAssembly/binaryen/issues/5561",
          "excerpts": [
            "Just like for native ones. That's correct, although the mimalloc codebase currently really just supports single-threaded wasm32-wasi.",
            "Mar 10, 2023 — I've seen huge performance improvement when wasm-opt is linked with mimalloc and optimizes a big wasm module on a many-cores machine!"
          ]
        },
        {
          "title": "wasmtime/runtime/vm/instance/allocator/ pooling.rs",
          "url": "https://docs.wasmtime.dev/api/src/wasmtime/runtime/vm/instance/allocator/pooling.rs.html",
          "excerpts": [
            "1//! Implements the pooling instance allocator. 2//! 3//! The pooling instance allocator maps memory in advance and allocates 4//! instances, memories, ..."
          ]
        },
        {
          "title": "Config in wasmtime - Rust",
          "url": "https://paritytech.github.io/try-runtime-cli/wasmtime/struct.Config.html",
          "excerpts": [
            "Additionally this means that all memories are synthetically limited by the Config::static_memory_maximum_size option, irregardless of what the actual maximum ..."
          ]
        },
        {
          "title": "enable-threads does not enable shared memory support #1658",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/1658",
          "excerpts": [
            "I'm using Wasmtime 0.16.0. I've noticed that, even with, --enable-threads, Wasmtime fails to parse modules defining shared memories, ..."
          ]
        },
        {
          "title": "Executing Wasm (Rust compiled to wasm) from multiple threads with ...",
          "url": "https://www.reddit.com/r/rust/comments/1jm5tyo/executing_wasm_rust_compiled_to_wasm_from/",
          "excerpts": [
            "I would like to initialize multiple instances that share the same memory so that I can call WASM code from multiple threads for parallel algorithm execution."
          ]
        },
        {
          "title": "How to work with wasm runtime memory? - Rust Users Forum",
          "url": "https://users.rust-lang.org/t/how-to-work-with-wasm-runtime-memory/88297",
          "excerpts": [
            "In wasm binary which was compiled with wasm32-unknown-unknown I have 16 blocks of initial memory by default (btw can't get rid of it)."
          ]
        },
        {
          "title": "Wasmtime Memory Management Discussion (Memory and Allocator)",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/2273",
          "excerpts": [
            "You can use static_memory_maximum_size, static_memory_guard_size, and dynamic_memory_guard_size to tweak how wasmtime allocates memory.",
            "The MemoryCreator is configured on the `Engine` which contains many VMs.",
            "I found the answer: Code in a Wasm VM cannot make new Memory objects.",
            "Every Wasm binary can define exactly one Memory object with a specified initial size and an optional max size."
          ]
        },
        {
          "title": "Chrome 133 | Release notes - Chrome for Developers",
          "url": "https://developer.chrome.com/release-notes/133",
          "excerpts": [
            "The memory64 proposal adds support for linear WebAssembly memories with size larger than 2^32 bits. It provides no new instructions, but instead ..."
          ]
        },
        {
          "title": "How does wasmtime implement memory64 proposal? #8010",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/8010",
          "excerpts": [
            "Feb 27, 2024 — For wasm this means that WebAssembly can have a memory larger than 4G, and pointers are generally 64-bits instead of 32-bits."
          ]
        },
        {
          "title": "MemoryType in wasmtime - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.MemoryType.html",
          "excerpts": [
            "Returns whether this is a 64-bit memory or not. Note that 64-bit memories are part of the memory64 proposal for WebAssembly which is not standardized yet."
          ]
        },
        {
          "title": "Config in wasmtime",
          "url": "https://rustdocs.bsx.fi/wasmtime/struct.Config.html",
          "excerpts": [
            "Custom memory creators are used when creating host Memory objects or when creating instance linear memories for the on-demand instance allocation strategy."
          ]
        },
        {
          "title": "Why does WebAssembly.Memory take `initial` and ...",
          "url": "https://stackoverflow.com/questions/74229102/why-does-webassembly-memory-take-initial-and-maximum-in-units-of-number-of-p",
          "excerpts": [
            "Wasm memory size can only be chosen in steps of pages (64 KiB), because that generally makes memory bounds checks using hardware virtual memory ..."
          ]
        },
        {
          "title": "Emscripten Compiler Settings",
          "url": "https://emscripten.org/docs/tools_reference/settings_reference.html",
          "excerpts": [
            "This allows emscripten to produce binaries are more WASI compliant and also allows it to process and execute WASI binaries built with other SDKs (e.g. wasi-sdk) ...",
            "Wasm Workers enable applications to create threads using a lightweight web-specific API that builds on top of Wasm SharedArrayBuffer + Atomics API. [compile ..."
          ]
        },
        {
          "title": "Wasm32-unknown-unknown vs wasm32-wasi",
          "url": "https://users.rust-lang.org/t/wasm32-unknown-unknown-vs-wasm32-wasi/78325",
          "excerpts": [
            "Jul 14, 2022 — WASI is an extended WASM environment specification, which tries to emulate a full OS, including system calls and functionality related to file system, ..."
          ]
        },
        {
          "title": "`mimalloc` memory allocator is broken",
          "url": "https://forum.dfinity.org/t/mimalloc-memory-allocator-is-broken/50541",
          "excerpts": [
            "Jun 13, 2025 — I notice the use of ic-wasi-polyfill here, and I believe mimalloc cannot be compiled for headless WASM. Could this be at fault? The CDK does ..."
          ]
        },
        {
          "title": "Is there a good, small-footprint memory allocator for ...",
          "url": "https://www.reddit.com/r/rust/comments/7mmqs8/is_there_a_good_smallfootprint_memory_allocator/",
          "excerpts": [
            "Are you sure wasm32-unknown-unknown uses jemalloc? No, I'm not sure. I assumed jemalloc was the default. Thank you! As far as I've been able ...",
            "According to this blog post here , it looks like the jemalloc allocator adds about 230K to a Rust executable."
          ]
        },
        {
          "title": "Node + MEMORY64 + MAXIMUM_MEMORY · Issue #19455 - GitHub",
          "url": "https://github.com/emscripten-core/emscripten/issues/19455",
          "excerpts": [
            "I'm trying to build a WASM file for node where I can allocate more than 4GB. I am compiling with the following settings"
          ]
        },
        {
          "title": "A practical guide to WebAssembly memory | radu's blog",
          "url": "https://radu-matei.com/blog/practical-guide-to-wasm-memory/",
          "excerpts": [
            "Jan 11, 2021 — In this article we explore using memory in WebAssembly in various scenarios - passing JavaScript arrays to Rust and AssemblyScript modules, checking for some ..."
          ]
        },
        {
          "title": "malloc() in WASI (dlmalloc) throwing \"out of bounds ...",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime/issues/539",
          "excerpts": [
            "Feb 22, 2021 — I've run into an odd issue with WASI where its implementation of malloc appears to throw an out of bounds memory access."
          ]
        },
        {
          "title": "Memory64 - Hacker News",
          "url": "https://news.ycombinator.com/item?id=42075403",
          "excerpts": [
            "The trick takes advantage of 32-bit registers automatically being zero-extended to 64. It actually uses 8GB of allocated address space."
          ]
        },
        {
          "title": "c++ - Differences between dynamic memory and \"ordinary ...",
          "url": "https://stackoverflow.com/questions/1021138/differences-between-dynamic-memory-and-ordinary-memory",
          "excerpts": [
            "However, since dynamic memory management serves a different purpose than static MM, a comparison does not make sense. And hey, it's only 4 bytes ..."
          ]
        },
        {
          "title": "Redesign Wasmtime's CLI flags · Issue #6741 - GitHub",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/6741",
          "excerpts": [
            "--dynamic-memory-guard-size <SIZE>, -O dynamic-memory-guard-size ... I.e., can I do wasmtime help run -O (or wasmtime help -O ) and get ..."
          ]
        },
        {
          "title": "Memory Management in C. Static, Automatic and Dynamic ...",
          "url": "https://medium.com/@connect.ajkumar/memory-allocation-in-c-d48f2974e0c",
          "excerpts": [
            "Memory is allocated dynamically to the entities of the program. Programmer has freedom to allocate and free the memory for the program entities."
          ]
        },
        {
          "title": "WASI-libc: Make dlmalloc optional (issue #244)",
          "url": "https://github.com/WebAssembly/wasi-libc/issues/244",
          "excerpts": [
            "Make dlmalloc optional #244",
            "Apr 15, 2021 — Basically, what I'm asking for is for wasi-libc to build with make BUILD_DLMALLOC=no , so that linking would fail with missing overrides, and ... ](/WebAssembly/wasi-libc/security)\n*",
            "This is perhaps a more minimal version of issue [](https://github.com/WebAssembly/wasi-libc/issues/94) . I would like to use a custom allocator because the environment I'm using wasi-libc in (TinyGo) already implements a heap. I think the easiest way to get that working is to not compile dlmalloc as part of wasi-libc. Any idea how this might be implemented?",
            "Missing: jemalloc jemallocator"
          ]
        },
        {
          "title": "Is Memory64 actually worth using? Is Memory64 actually worth using? • SpiderMonkey Blog (Jan 15, 2025)",
          "url": "https://spidermonkey.dev/blog/2025/01/15/is-memory64-actually-worth-using.html",
          "excerpts": [
            "Memory64 may be of limited use today, but there are some exciting possibilities for the future:",
            "Memory64 may be of limited use today, but there are some exciting possibilities for the future:",
            "64-bit pointers in WebAssembly simply allow you to address more memory, at the cost of slower loads and stores.",
            "the WebAssembly JS API constrains memories to a maximum size of 16GB.",
            "If you need more memory, and can tolerate worse performance, then Memory64 might be the right choice for you.",
            "Memory64 may not matter for most developers today, but we think it is an important stepping stone to an exciting future for memory in WebAssembly.",
            "Memory64 may not matter for most developers today, but we think it is an important stepping stone to an exciting future for memory in WebAssembly.",
            "s. Memory64 may not matter for most developers today, but we think it is an important stepping stone to an exciting future for memory in WebAssembly. * * *\n\n1. The rest of the proposal fleshes out the "
          ]
        },
        {
          "title": "Rust wasm64-unknown-unknown target (Rustc book)",
          "url": "https://doc.rust-lang.org/beta/rustc/platform-support/wasm64-unknown-unknown.html",
          "excerpts": [
            "This target comes by default with an allocator,\ncurrently [dlmalloc](http://gee.cs.oswego.edu/dl/html/malloc.html) which is [ported to rust](https://github.com/alexcrichton/dlmalloc-rs).",
            "The difference of this target with `wasm32-unknown-unknown` is that it's\ncompiled for 64-bit memories instead of 32-bit memories. This means that `usize`\nis 8-bytes large as well as pointers.",
            "the maximum\nmemory size is now the full 64-bit address space instead of the 4GB as limited\nby the 32-bit address space for `wasm32-unknown-unknown`.",
            "memory64) WebAssembly proposal is still\nin-progress and not standardized. Th",
            "This target is cross-compiled. The target supports `std` in the same manner as\nthe `wasm32-unknown-unknown` target which is to say that it comes with the\nstandard library but many I/O functions such as `std::fs` and `std::net` will\nsimply return error.",
            "This means that there are not many engines\nwhich implement the `memory64` feature and if they do they're likely behind a\nflag, for example:\n\n* Nodejs - `--experimental-wasm-memory64`\n* Wasmtime - `--wasm memory64`",
            "The `wasm64-unknown-unknown` target intends to match the default Clang targets\nfor its `\"C\"` ABI, which is likely to be the same as Clang's\n`wasm32-unknown-unknown` largely."
          ]
        },
        {
          "title": "WebAssembly binary size (wasm32-unknown-unknown)",
          "url": "https://www.reddit.com/r/rust/comments/7w2wsu/webassembly_binary_size_wasm32unknownunknown/",
          "excerpts": [
            "The biggest code size offenders, I've found, are: dlmalloc, the default allocator used with wasm32-unknown-unknown . It is somewhere around 8 or ..."
          ]
        },
        {
          "title": "Efficient memory passing between WASM and host #314 - GitHub",
          "url": "https://github.com/WebAssembly/component-model/issues/314",
          "excerpts": [
            "I've explored a few different options to stay up-to-date with the tooling (mainly, wasi-libc) and still remain compatible with the old runtimes ...",
            "Feb 28, 2024 — There is the additional overhead of the realloc call, but perhaps it could be inlined. loganek commented ..."
          ]
        },
        {
          "title": "I was understanding WASM all wrong! 🤯 | by Yuji Isobe",
          "url": "https://medium.com/@yujiisobe/i-was-understanding-wasm-all-wrong-e4bcab8d077c",
          "excerpts": [
            "WASM is designed to run faster and more efficiently. WASM's speed comes from its low-level binary format, simple memory model, and ahead-of- ...",
            "This minimizes overhead, allowing performance close to native code. I used Rust and WASM to optimize ULID generation in wa-ulid. The result was ...",
            "First Steps with WASM and Rust. I began experimenting with WASM using wasm-bindgen, a tool that helps WASM modules and JavaScript work together."
          ]
        },
        {
          "title": "The WebAssembly Component Model: Introduction",
          "url": "https://component-model.bytecodealliance.org/",
          "excerpts": [
            "The WebAssembly Component Model is a broad-reaching architecture for building interoperable WebAssembly libraries, applications, and environments.",
            "Compiler and Wasm runtime developers can take a look at the Component Model specification to see how to add support for the component model to their project.",
            "Running Components",
            "Compiler and Wasm runtime developers_ can take a look at the [Component Model specification](https://github.com/WebAssembly/component-model) to\nsee how to add support for the component model to their project.",
            "Creating Components",
            "WIT By Example",
            "WIT Reference",
            "Using WebAssembly Components"
          ]
        },
        {
          "title": "The Wasm Interface Type text format",
          "url": "https://docs.wa2.dev/wasm/concepts-and-features/5-wit/",
          "excerpts": [
            "The Wasm Interface Type text format (WIT) is an Interface Description Language (IDL)—a language that facilitates communication between programs ..."
          ]
        },
        {
          "title": "The WebAssembly Component Model - Fermyon",
          "url": "https://www.fermyon.com/blog/webassembly-component-model",
          "excerpts": [
            "Jul 1, 2022 — The WebAssembly Component Model is a proposal to build upon the core WebAssembly standard by defining how modules may be composed within an application or ...",
            "Jul 1, 2022 — The core WebAssembly Specification defines an architecture-, platform-, and language-agnostic format for representing executable code. A ...",
            "A canonical ABI which specifies how high-level types are represented in terms of the low-level types of core WebAssembly; Module and ..."
          ]
        },
        {
          "title": "Component Model design and specification",
          "url": "https://github.com/WebAssembly/component-model",
          "excerpts": [
            ")\n\nThis repository is where the component model is being standardized. For a more\nuser-focused explanation, take a look at the **[Component Model Documentation](https://component-model.bytecodealliance.org/)",
            "For a more\nuser-focused explanation, take a look at the **[Component Model Documentation](https://component-model.bytecodealliance.org/)** .",
            "\nThe Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2)",
            "The Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2) . The subsequent [WASI Preview 3](https://github.com/WebAssembly/WASI/tree/main/wasip2) milestone will be\nprimarily concerned with the addition of [async support](/WebAssembly/component-model/blob/main/design/mvp/Async.md) ."
          ]
        },
        {
          "title": "bytecodealliance/wit-bindgen: A language binding ...",
          "url": "https://github.com/bytecodealliance/wit-bindgen",
          "excerpts": [
            "The end-goal of wit-bindgen is to facilitate creation of a component. Once a component is created it can then be handed off to any one of a number of host ...",
            "This project is a suite of bindings generators for languages that are compiled to WebAssembly and use the component model."
          ]
        },
        {
          "title": "component - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/component/struct.Component.html",
          "excerpts": [
            "Also note that this method is not required to be called, it's available as a performance optimization if required but is otherwise handled automatically."
          ]
        },
        {
          "title": "Component loading performance issue #6353",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/6353",
          "excerpts": [
            "It takes 20 seconds to load a component in wasmtime 7.0.0. It is definitely much slower than loading a wasm module of comparable size."
          ]
        },
        {
          "title": "About the benchmark · Issue #15 · Pauan/rust-dominator - GitHub",
          "url": "https://github.com/Pauan/rust-dominator/issues/15",
          "excerpts": [
            "wasm-bindgen slower in some benchmarks. There are some benchmarks ... That adds a small amount of unavoidable overhead. All reactions."
          ]
        },
        {
          "title": "Component Model | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/start/wasmedge/component_model/",
          "excerpts": [
            "The component model for WASM would dramatically improve WASM module's reusability and composability. It will allow better access from one WASM module to other ..."
          ]
        },
        {
          "title": "wit-bindgen - Rust Package Registry",
          "url": "https://crates.io/crates/wit-bindgen",
          "excerpts": [
            "45.0. Rust bindings generator and runtime support for WIT and the component model. Used when compiling Rust programs to the component model."
          ]
        },
        {
          "title": "JS -> wasm vs JS -> native function call overhead? - Mozilla Discourse",
          "url": "https://discourse.mozilla.org/t/js-wasm-vs-js-native-function-call-overhead/77120",
          "excerpts": [
            "My question is: how the overhead of calling a wasm function from javascript compares to the overhead of calling a native function (linked to the JS runtime)"
          ]
        },
        {
          "title": "WebAssembly Interface Types: Interoperate with All the Things!",
          "url": "https://www.reddit.com/r/programming/comments/ctk53a/webassembly_interface_types_interoperate_with_all/",
          "excerpts": [
            "64-bit integers and floats were added to WASM because most of host CPUs have native support for them, so emulating them using 32 bits would've ..."
          ]
        },
        {
          "title": "Document Object Model (DOM) - MDN - Mozilla",
          "url": "https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model",
          "excerpts": [
            "The DOM represents a document with a logical tree. Each branch of the tree ends in a node, and each node contains objects."
          ]
        },
        {
          "title": "Web Components - MDN - Mozilla",
          "url": "https://developer.mozilla.org/en-US/docs/Web/API/Web_components",
          "excerpts": [
            "Web Components is a suite of different technologies allowing you to create reusable custom elements — with their functionality encapsulated away ..."
          ]
        },
        {
          "title": "The WebAssembly Component Model - WIT Overview",
          "url": "https://component-model.bytecodealliance.org/design/wit.html",
          "excerpts": [
            "The WIT (Wasm Interface Type) language is used to define Component Model [interfaces]() and [worlds]() . WIT isn't a general-purpose programming language and doesn't define behaviour;\nit defines only _contracts_ between components. To define a new component, you will need to define worlds and interfaces\nby writing code in the Wasm Interface Type (WIT) language. WIT also serves as documentation for existing components\nthat you may wish to use. This topic provides an overview of key elements of the WIT language",
            "A WIT file contains one or more **interfaces** or **worlds** . An interface or world can define **types** and/or **functions** . > Types and functions can't be defined outside of interfaces or worlds. A file may optionally start with a **package** declaration.",
            "An interface is a named set of types and functions,\nenclosed in braces and introduced with the `interface` keyword:",
            "world describes the contract of a component. A world describes a set of imports and exports,\nenclosed in braces and introduced with the `world` keyword. Imports and exports may be interfaces or specific functions. Exports describe the interfaces or functions provided by a component. Imports describe the interfaces or functions that a component depends on. ",
            "To import and export interfaces defined in other packages,\nyou can use `package/name` syntax:",
            "A package is a set of interfaces and worlds,\npotentially defined across multiple files.",
            "This package defines request and response types in `types.wit` ,\nan incoming handler interface in `incoming wit` ,\nan outgoing handler interface in `outgoing.wit` ,\nand declares the package and defines a world that uses these interfaces\nin `http.wit` .",
            "package documentation:example@1.0.1;",
            "```",
            "All files must have the `.wit` extension and must be in the same directory. If a package spans multiple files,",
            "only one file needs to contain a package declaration,",
            "but if multiple files contain package declarations,",
            "the package IDs must all match each other. For example, the following `documentation:http` package is spread across four files:",
            "```wit",
            "```wit",
            "// types.wit",
            "interface types {",
            "    record request { /* ... */ }",
            "    record response { /* ... */ }",
            "// incoming.wit",
            "interface incoming-handler {",
            "// outgoing.wit",
            "interface outgoing-handler {",
            "    use types. {request, response};",
            "    // ...",
            "    // ...",
            "// http.wit",
            "package documentation:http@1.0.0;",
            "world proxy {",
            "    export incoming-handler;",
            "    import outgoing-handler;",
            "}",
            "}",
            "}",
            "}"
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/packages.html",
          "excerpts": [
            "WIT is an [interface definition language](https://en.wikipedia.org/wiki/Interface_description_language) (IDL) for the component model. Packages provide a way for worlds and interfaces to refer to each other,\nand thus for an ecosystem of components to share common definitions. A WIT package groups related interfaces and worlds together\nfor ease of discovery and reference.",
            "WIT is an interface definition language (IDL) for the component model. Packages provide a way for worlds and interfaces to refer to each other, and thus for an ...",
            ". A WIT package groups related interfaces and worlds together for ease of discovery and reference.",
            ". A world is a bundle of imported and exported types and interfaces.",
            "- The WebAssembly Component Model"
          ]
        },
        {
          "title": "The WebAssembly Component Model, Part 1",
          "url": "https://blog.nginx.org/blog/wasm-component-model-part-1",
          "excerpts": [
            "The WebAssembly (Wasm) Component Model outlines how different Wasm modules, or components, can communicate with each other and the runtime envir",
            "It establishes specific contracts that must be met to ensure that code compiled into a Wasm component can be hosted on a compatible runtime and seamlessly exchange data with other Wasm components during runtime.",
            "The two essential parts of the Wasm Component Model are the WebAssembly System Interface (WASI) and WebAssembly Interface Types (WIT).",
            "WASI is short for “WebAssembly System Interface” and was introduced by the Wasmtime project and was designed from the ground up for Wasm.",
            "WIT files used by the Wasm Component Model and the **wasi:http/proxy** world in specific, are created and maintained by the Bytecode Allia"
          ]
        },
        {
          "title": "NGINX Unit wasm component model",
          "url": "https://unit.nginx.org/news/2024/wasm-component-model-part-1/",
          "excerpts": [
            "The WebAssembly (Wasm) Component Model outlines how different Wasm modules, or components, can communicate with each other and the runtime environment.",
            " WASI 0.2 APIs",
            "WIT files used by the Wasm Component Model and the **wasi:http/proxy** world in specific, are created and maintained by the Bytecode Allia",
            "At the time of writing this blog post, the best way to make use of them are the Wasmtime project’s GitHub repository and a manual pull.",
            "the host implementing the Wasm runtime as well as the component we are about to build are creating bindings for the contracts defined by the WIT files, it is important to target the same version of those contracts or choose a runtime that supports multiple versions of those WIT files.",
            "For the use cases of NGINX Unit, it was pretty clear we will be targeting the **wasi:http/proxy** world.",
            "You can think of the **wasi:http/proxy** world as the set of interfaces describing how a HTTP request and response will look like, including all its data (HTTP Method, Headers, Body, and more).",
            "If you are an old-school web developer, this might remind you of CGI.",
            "NGINX Unit, Wasmtime and Rust - A runtime implementation",
            "After a brief investigation, we discovered that the WASI/WIT pair we’d already heard of plays a vital role in supporting the Component Model.",
            "As the host, Unit must implement the WASI HTTP proxy interfaces defined by WIT files to fulfill the contract.",
            "This isn’t new information; we were already aware of this fact.",
            "Since we’re using Wasmtime as the Wasm runtime, we could delegate this task to the runtime, right? Indeed, we could!",
            "However, there was a small but significant detail: our implementation at the time was written entirely in C using the Wasmtime C-APIs.",
            "Unfortunately, these APIs lack the necessary functions to support the Component Model.",
            "As mentioned at the beginning of this article, any challenge, regardless of its complexity, can be solved when the right people and community share the same mindset.",
            "Fermyon has been, and continues to be, a valuable and significant partner for us.",
            "After a late-night Slack and Zoom Session, we found it too complex to add native support for the Component Model to the Wasmtime C-API.",
            "Additionally, implementing the interfaces manually with WIT files without help from automation tools like **bindgen** would result in a significant amount of ongoing upkeep work.",
            "While explaining how the internals of NGINX Unit and the current C-based Language Module work to Fermyon, they shared a prototype of a Rust-based Unit Language Module targeting the Rust API of Wasmtime.",
            "Not the C-API anymore.",
            "Now we are equipped with the necessary knowledge to write some code.",
            "What’s next?",
            "In the next part we will covering the process of creating a Wasm Component using Rust and the WASI 0.2 APIs."
          ]
        },
        {
          "title": "Weekly WASM Watch: The Top 5 You Shouldn't Miss This Week",
          "url": "https://medium.com/wasm-radar/%EF%B8%8F-weekly-wasm-watch-the-top-5-you-shouldnt-miss-this-week-2519f4dd5359",
          "excerpts": [
            "The Bytecode Alliance has released a preview of **Wasmtime 26**, featuring significant improvements in performance, memory efficiency, and support for the Component Model.",
            "This release shows that the Component Model is not just a future standard but also a practical tool for creating real-world applications.",
            "Wasmtime continues to lead the way among WebAssembly runtimes. By focusing on performance and the Component Model, it offers a way to build modular applications that run securely across different environments.",
            "Component Model improvements**: better tooling and API support for composing Wasm modules using the official Wasm Component Model, part of the move toward Preview 3 of WAS",
            "ols. Preview 3 marks a move from early experimental APIs to a more stable base that developers can depend on for creating portable applications."
          ]
        },
        {
          "title": "Canonical ABI",
          "url": "https://component-model.bytecodealliance.org/advanced/canonical-abi.html",
          "excerpts": [
            "The component model defines a canonical ABI - an ABI to which all components adhere. This guarantees that components can talk to each other without confusion.",
            "An ABI is an agreement on how to pass data. The canonical ABI is an ABI that all components adhere to, ensuring they can communicate without confusion. Canonical ABI - The WebAssembly Component Model"
          ]
        },
        {
          "title": "What is the WebAssembly Component Model?",
          "url": "https://www.f5.com/company/blog/what-is-the-webassembly-component-model",
          "excerpts": [
            "Feb 12, 2025 — The Component Model is a system that facilitates interactions between individual units of WebAssembly code and interactions between WebAssembly code and a host ..."
          ]
        },
        {
          "title": "Why the Component Model? - The WebAssembly ...",
          "url": "https://component-model.bytecodealliance.org/design/why-component-model.html",
          "excerpts": [
            "At a high level, the component model builds upon WebAssembly core modules to enhance interoperability between languages and libraries, both by enriching the ..."
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/components.html",
          "excerpts": [
            "Each component is described by a world, which potentially collects together multiple interfaces to describe all the imports and exports of the component. The ..."
          ]
        },
        {
          "title": "bytecodealliance/cargo-component",
          "url": "https://github.com/bytecodealliance/cargo-component",
          "excerpts": [
            "cargo component is considered to be experimental and is not currently stable in terms of the code it supports building. Until the component model stabilizes ...",
            "Overview. cargo component is a cargo subcommand for creating WebAssembly components using Rust as the component's implementation language."
          ]
        },
        {
          "title": "WIT IDL - Visual Studio Marketplace",
          "url": "https://marketplace.visualstudio.com/items?itemName=BytecodeAlliance.wit-idl",
          "excerpts": [
            "This package is a Visual Studio Code extension to recognize and highlight the WebAssembly Interface Type (WIT) Interface Definition Language (IDL)."
          ]
        },
        {
          "title": "Modular WASM Is No Longer a Theory | by Enrico Piovesan",
          "url": "https://medium.com/wasm-radar/modular-wasm-is-no-longer-a-theory-fefae1585980",
          "excerpts": [
            "Recently, I've discussed the potential of modular WebAssembly extensively, from the Component Model's design aims to its role as a portable"
          ]
        },
        {
          "title": "Wasm-bindgen performance",
          "url": "https://users.rust-lang.org/t/wasm-bindgen-performance/55726",
          "excerpts": [
            "The Rust function takes about 10 times as long to run. Is there a more efficient way to make the Float64Array available to the Rust code?"
          ]
        },
        {
          "title": "Benchmark of WebAssembly runtimes - 2021 Q1",
          "url": "https://00f.net/2021/02/22/webassembly-runtimes-benchmarks/",
          "excerpts": [
            "The benchmark covers different types of cryptographic primitives. Some are purely computational tasks, some are memory-hard, some require efficient register ...",
            "On this platform, with benchmarks not having platform-specific optimizations, wasmtime runs the test suite at ~54% the speed of native code."
          ]
        },
        {
          "title": "microbenchmark: Accurate Timing Functions",
          "url": "https://cran.r-project.org/web/packages/microbenchmark/microbenchmark.pdf",
          "excerpts": [
            "The function measures the overhead of timing a C function call rounds times and returns all non-zero timings observed. This can be used to judge ..."
          ]
        },
        {
          "title": "microbenchmark function",
          "url": "https://www.rdocumentation.org/packages/microbenchmark/versions/1.5.0/topics/microbenchmark",
          "excerpts": [
            "This function is only meant for micro-benchmarking small pieces of source code and to compare their relative performance characteristics."
          ]
        },
        {
          "title": "WebAssembly Beyond the Browser: Building Cloud-Native and ...",
          "url": "https://em360tech.com/tech-articles/webassembly-beyond-browser-building-cloud-native-and-edge-native-apps-2025",
          "excerpts": [
            "WebAssembly (Wasm) is no longer confined to browsers. In 2025, it's powering ultra-fast, portable, and secure applications across ..."
          ]
        },
        {
          "title": "The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/design/worlds.html",
          "excerpts": [
            "A world can be used to describe a component,\nand a hosting environment for other components,\ndepending on which imports and exports are specified. Worlds can represent either a component or host environment\nbecause components can be composed:",
            "a world describes how a component relates to other components:\nit describes the functionality the component exposes\nand declares the functionality it depends on in order to be able to run. Functionality is exposed by defining interfaces to export,\nand dependencies are declared by importing interfaces.",
            "A world only defines the surface of a component, not its internal behaviour. On the other hand, a world defines a hosting environment for components:\nthat is, an environment in which a component can be instantiated\nand its functionality can be invoked.",
            "A world is defined in a WIT file; a single WIT files can contain multiple worlds.",
            "Worlds and interfaces",
            "A world is a collection of interfaces, where each interface is _directional_ . Each interface is explicitly labeled as either an export or an import. Exported interfaces are available for outside code to call,\nwhereas imported interfaces must be fulfilled by outside code",
            "These interfaces define a strict boundary for a component. The only ways a component can interact with anything outside itself\nare by having its exports called,\nor by calling its imports.",
            " Connecting up some or all of a component's imports to other components' matching exports is called _composition_",
            "WASI (the WebAssembly System Interface) defines a \"command line\" world\nthat imports interfaces that command-line programs typically expect to have available to them:\nfor example, file input/output, random number generation, and clocks. This world has a single export for running the command-line tool.",
            "For example, WASI (the WebAssembly System Interface) defines a \"command line\" world\nthat imports interfaces that command-line programs typically expect to have available to them:\nfor example, file input/output, random number generation, and clocks.",
            "A hosting environment that supports this world\nmust provide implementations for all of the imports\nand may invoke the single export.",
            "\nA world is a collection of interfaces, where each interface is _directional_",
            "Exported interfaces are available for outside code to call,\nwhereas imported interfaces must be fulfilled by outside code.",
            "Targeting a world is analogous to relying on a particular version of a standard library,\nexcept that components give you the ability to precisely specify\nexactly what functions your code depends on.",
            "WIT By Example",
            "WIT Reference",
            "Using WebAssembly Components"
          ]
        },
        {
          "title": "Wasmtime",
          "url": "https://component-model.bytecodealliance.org/runtimes/wasmtime.html",
          "excerpts": [
            "Wasmtime is the reference implementation of the Component Model. It supports running components that implement the wasi:cli/command world and serving components ..."
          ]
        },
        {
          "title": "Get WITtier with this handy guide to Wasm Interface Types | Cosmonic",
          "url": "https://cosmonic.com/blog/engineering/wit-cheat-sheet",
          "excerpts": [
            "WIT, or Wasm Interface Types, allows WebAssembly modules to communicate with each other using complex data types. WIT is a language agnostic ..."
          ]
        },
        {
          "title": "Rust - The WebAssembly Component Model",
          "url": "https://component-model.bytecodealliance.org/language-support/rust.html",
          "excerpts": [
            "We will be using the cargo component subcommand to create WebAssembly components using Rust as the component's implementation language."
          ]
        },
        {
          "title": "Reasoning about ABIs — 2023-11-18",
          "url": "https://blog.yoshuawuyts.com/reasoning-about-abis/",
          "excerpts": [
            "Nov 18, 2023 — Taking this back to Wasm Components: while there is something called the \"Canonical ABI\" which defines the encoding of Wasm Components. If ..."
          ]
        },
        {
          "title": "The WebAssembly Component Model - Part 2 - NGINX Unit",
          "url": "https://unit.nginx.org/news/2024/wasm-component-model-part-2/",
          "excerpts": [
            "This is Part 2 of our blog series about the Wasm Component Model, it's ecosytem and how to use Wasm Components with NGINX Unit."
          ]
        },
        {
          "title": "Wasmtime Component Model Embedding API",
          "url": "https://docs.wasmtime.dev/api/wasmtime/component/index.html",
          "excerpts": [
            "This module contains the embedding API for the Component Model in Wasmtime. This module requires the component-model feature to be enabled, which is enabled by ... Embedd",
            "The embedding API here is mirrored after the\ncore wasm embedding API at the crate root and is intended to have the same\nlook-and-feel while handling concepts of the component model.",
            "The component model is a broad topic which can’t be explained here fully, so\nit’s recommended to read over individual items’ documentation to see more\nabout the capabilities of the embedding API.",
            "erhaps the most interesting items in this module are:\n\n* [`Component`](struct.Component.html \"struct wasmtime::component::Component\") - a compiled component ready to be instantiated. Similar to\n  a [`Module`](../struct.Module.html \"struct wasmtime::Module\") for core wasm. * [`Linker`](struct.Linker.html \"struct wasmtime::component::Linker\") - a component-style location for defining host functions",
            "This\n  is not the same as [`wasmtime::Linker`](../struct.Linker.html \"struct wasmtime::Linker\") for core wasm\n  modu",
            " [`bindgen!`](macro.bindgen.html \"macro wasmtime::component::bindgen\") - a macro to generate Rust bindings for a [WIT](https://component-model.bytecodealliance.org/design/wit.html) [world](https://component-model.bytecodealliance.org/design/worlds.html). This\n  maps all WIT types into Rust automatically and generates traits for\n  embedders to implement",
            "The embedder then implements these traits, adds\nfunctionality via the generated `add_to_linker` method (see [`bindgen!`](macro.bindgen.html \"macro wasmtime::component::bindgen\") for\nmore info), and then instantiates/executes a component.",
            "It’s recommended to read over the [documentation for the Component\nModel](https://component-model.bytecodealliance.org) to get an overview about how to build components\nfrom various languages.",
            "Imagine you have the following WIT package definition in a file called world.wit\nalong with a component (my\\_component.wasm) that targets `my-world`:\n\n```\npackage component:my-package;\n\nworld my-world {\n    import name: func() -> string;\n    export greet: func() -> string;\n}\n```\n\nYou can instantiate and call the component like so:\n\n```\nfn main() -> wasmtime::Result<()> {\n    // Instantiate the engine and store\n    let engine = wasmtime::Engine::default();\n    let mut store = wasmtime::Store::new(&engine, ());\n\n    // Load the component from disk\n    let bytes = std::fs::read(\"my_component.wasm\")? ;\n    let component = wasmtime::component::Component::new(&engine, bytes)? ;\n\n    // Configure the linker\n    let mut linker = wasmtime::component::Linker::new(&engine);\n    // The component expects one import `name` that\n    // takes no params and returns a string\n    linker\n        .root()\n        .func_wrap(\"name\", |_store, _params: ()| {\n            Ok((String::from(\"Alice\"),))\n        })? ;\n\n    // Instantiate the component\n    let instance = linker.instantiate(&mut store, &component)? ;\n\n    // Call the `greet` function\n    let func = instance.get_func(&mut store, \"greet\").expect(\"greet export not found\");\n    let mut result = [wasmtime::component::Val::String(\"\".into())];\n    func.call(&mut store, &[], &mut result)? ;\n\n    // This should print out `Greeting: [String(\"Hello, Alice! \")]`\n    println! (\"Greeting: {:?\n}\", result);\n\n    Ok(())\n}\n```\n\nManually configuring the linker and calling untyped component exports is\na bit tedious and error prone. The [`bindgen!`](macro.bindgen.html \"macro wasmtime::component::bindgen\") macro can be used to\ngenerate bindings eliminating much of this boilerplat",
            "e it. Modules[§]()\n--------------------\n\n[bindgen\\_examples](bindgen_examples/index.html \"mod wasmtime::component::bindgen_examples\")`docsrs`"
          ]
        },
        {
          "title": "Revolutionizing Distributed Software with WebAssembly Component ...",
          "url": "https://medium.com/wasi-articles/revolutionizing-distributed-software-with-webassembly-component-model-412574d2881a",
          "excerpts": [
            "The WebAssembly component model fulfills the above wishes by providing a Canonical ABI which is an agreed upon binary format for data ..."
          ]
        },
        {
          "title": "no-std wit-bindgen - WebAssembly",
          "url": "https://lib.rs/crates/wit-bindgen",
          "excerpts": [
            "Rust bindings generator and runtime support for WIT and the component model. Used when compiling Rust programs to the component model | Rust/Cargo package."
          ]
        },
        {
          "title": "Engine in wasmtime - Rust",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.Engine.html",
          "excerpts": [
            "This setup can take on the order of a few hundred microseconds, whereas the overhead of calling WebAssembly is otherwise on the order of a few nanoseconds. This ..."
          ]
        },
        {
          "title": "Understanding Function Call Overhead: Optimization with ...",
          "url": "https://medium.com/@mayank1807.singh/understanding-function-call-overhead-optimization-with-inline-functions-44e3bf014e53",
          "excerpts": [
            "Function call overhead refers to the extra time and memory resources used by a program when invoking a function. This overhead arises from ..."
          ]
        },
        {
          "title": "The WebAssembly value proposition is write once, not ...",
          "url": "https://nickb.dev/blog/the-webassembly-value-proposition-is-write-once-not-performance/",
          "excerpts": [
            "Jan 3, 2024 — Too many lead with performance when talking about the benefits of Wasm. This is misguided as Wasm may not even beat JS in speed, ...",
            "Jan 3, 2024 — Wasm frameworks have a ton of potential for performance, but not in how quickly they can update the DOM because that binding is done through ..."
          ]
        },
        {
          "title": "What advantages have WebAssembly over the 'traditional' ...",
          "url": "https://www.reddit.com/r/rust/comments/16m0y7o/what_advantages_have_webassembly_over_the/",
          "excerpts": [
            "But this performance question is basically “is WASM fast enough to compete?” and the answer is “Yes.” The only meaningful/real advantages are."
          ]
        },
        {
          "title": "How much overhead is there in calling a function in C++?",
          "url": "https://stackoverflow.com/questions/144993/how-much-overhead-is-there-in-calling-a-function-in-c",
          "excerpts": [
            "The overhead of function calls on this computer is about 3 nanoseconds. The fastest I measured something at it was about 0.3ns so that would suggest a function ..."
          ]
        },
        {
          "title": "Rust Release Notes",
          "url": "https://doc.rust-lang.org/beta/releases.html",
          "excerpts": [
            "§Cargo. Added cargo remove to remove dependencies from Cargo.toml · cargo publish now waits for the new version to be downloadable before ..."
          ]
        },
        {
          "title": "Releases · bytecodealliance/wit-bindgen",
          "url": "https://github.com/bytecodealliance/wit-bindgen/releases",
          "excerpts": [
            "Releases: bytecodealliance/wit-bindgen ; v0.42.1. May 6 · v0.42.1 · d1b5fb1 · on May 6, 2025, 03:00 PM ; v0.42.0. May 6 · v0.42.0 · 52556d6 · on May 6, 2025, 01:00 PM."
          ]
        },
        {
          "title": "WebAssembly + JavaScript in 2025 — Superpowers for the ...",
          "url": "https://medium.com/@theNewGenCoder/webassembly-javascript-in-2025-superpowers-for-the-web-are-here-eacd5a6e94d0",
          "excerpts": [
            "WebAssembly will soon support modular, reusable components — just like React! This means we'll soon write entire Wasm components and import them ..."
          ]
        },
        {
          "title": "Schema Versioning and Generations - iTwin.js",
          "url": "https://www.itwinjs.org/bis/guide/schema-evolution/schema-versioning-and-generations/",
          "excerpts": [
            "Missing: WIT Component"
          ]
        },
        {
          "title": "WebAssembly Component Model Discussion (GitHub issue 193)",
          "url": "https://github.com/WebAssembly/component-model/issues/193",
          "excerpts": [
            " \n`use` statement will still be somewhat similar grammatically where it will look  \nlike:",
            "The current intention is that the versions here are glossed over during parsing, perhaps with light validation but not an actually semver resolver/querier/etc.",
            "I like the idea of a top-level declaration of package like `package wasi:http@0.1` but I think we should de-couple wit syntax from versions and put the semver of a wit-package outside of the `.wit` files.",
            "he conclusion was that to handle this `wit-parser` will require at least one `*.wit` file in a package to have a `package ...` declaration, but it won't require it on all of them",
            "If we have packages with multiple `.wit` files, is the expectation that every single file would have a `package` signifier at the top?",
            "The `@version` syntax is allowed in `use` statements to disambiguate which  \nversion is being imported if multiple exis"
          ]
        },
        {
          "title": "Wasmtime: Introduction",
          "url": "https://docs.wasmtime.dev/",
          "excerpts": [
            "Wasmtime is a standalone runtime for WebAssembly, WASI, and the Component Model by the Bytecode Alliance. WebAssembly (abbreviated Wasm) is a binary instruction ..."
          ]
        },
        {
          "title": "profiling.rs - source",
          "url": "https://docs.wasmtime.dev/api/src/wasmtime/runtime/profiling.rs.html",
          "excerpts": [
            "Source of the Rust file `crates/wasmtime/src/runtime/profiling.rs`."
          ]
        },
        {
          "title": "wasmtime_internal_jit_debug/ perf_jitdump.rs",
          "url": "https://docs.wasmtime.dev/api/src/wasmtime_internal_jit_debug/perf_jitdump.rs.html",
          "excerpts": [
            "1//! Support for jitdump files which can be used by perf for profiling jitted code. 2//! Spec definitions for the output format is as described here: 3//! ..."
          ]
        },
        {
          "title": "wasmtime package - github.com/bytecodealliance ...",
          "url": "https://pkg.go.dev/github.com/bytecodealliance/wasmtime-go",
          "excerpts": [
            "Sep 21, 2022 — Package wasmtime is a WebAssembly runtime for Go powered by Wasmtime. This package provides everything necessary to compile and execute WebAssembly modules as ..."
          ]
        },
        {
          "title": "Profiling WebAssembly - Wasmtime Documentation (examples-profiling.html)",
          "url": "https://docs.wasmtime.dev/examples-profiling.html",
          "excerpts": [
            "Profiling WebAssembly",
            "One of WebAssembly's major goals is to be quite close to native code in terms of\nperformance, so typically when executing Wasm you'll be quite interested in how\nwell your Wasm module is performing! From time to time you might want to dive a\nbit deeper into the performance of your Wasm, and this is where profiling comes\ninto the picture",
            "For best results, ideally you'd use hardware performance counters for your\ntiming measurements.",
            "However, that requires special support from your CPU and\noperating system.",
            "Because Wasmtime is a JIT, that also requires hooks from\nWasmtime to your platform's native profiling tools.",
            "As a result, Wasmtime support for native profiling is limited to certain\nplatforms.",
            "On Linux, we support [perf",
            "For Intel's x86 CPUs on Linux or Windows, we support [VTune",
            "For Linux and macOS, we support [samply",
            "For everything else, see the [cross-platform\n  profil",
            "On Linux, we support [perf](./examples-profiling-perf.html) . * \n  For Intel's x86 CPUs on Linux or Windows, we support [VTune](./examples-profiling-vtune.html) . * \n  For Linux and macOS, we support [samply](./examples-profiling-samply.html) . * \n  For everything else, see the [cross-platform\n  profiler](./examples-profiling-guest.html) . The native profilers can measure time spent in WebAssembly guest code as well as\ntime spent in the Wasmtime host and potentially even time spent in the",
            "The cross-platform-profiler can only measure time spent in WebAssembly guest\ncode, and its timing measurements are not as precise as the native profilers."
          ]
        },
        {
          "title": "Wasmtime profiling API (profiling.h)",
          "url": "https://docs.wasmtime.dev/c-api/profiling_8h.html",
          "excerpts": [
            "profiling.h File Reference",
            "API for Wasmtime guest profiler.",
            "Collects basic profiling data for a single WebAssembly guest.",
            "The most straightforward way to do that is to call it from a callback registered with [wasmtime\\_store\\_epoch\\_deadline\\_callback](store_8h.html \"Configures epoch deadline callback to C function.\") . For more information see the Rust documentation at: <https://docs.wasmtime.dev/api/wasmtime/struct.GuestProfiler.h",
            "Zero can be passed as `delta_nanos` if recording CPU usage information is not needed."
          ]
        },
        {
          "title": "Profiling with Perf",
          "url": "https://docs.wasmtime.dev/examples-profiling-perf.html",
          "excerpts": [
            "Wasmtime uses perf for profiling with `perfmap` (simple) and `jitdump` (detailed) agents. `perfmap` maps code to symbols, while `jitdump` provides more ...",
            "Profiling with jitdump. Profiling support with perf uses the \"jitdump\" support in the perf CLI. This requires runtime support from Wasmtime itself, so you ...",
            "Command Line - you'll want to pass the --profile=jitdump flag on the command line. Once jitdump support is enabled, you'll use perf record like usual to record ...",
            "One profiler supported by Wasmtime is the perf profiler for Linux. This is an extremely powerful profiler with lots of documentation on the web."
          ]
        },
        {
          "title": "wasmtime/profiling_agent/ jitdump.rs",
          "url": "https://docs.wasmtime.dev/api/src/wasmtime/profiling_agent/jitdump.rs.html",
          "excerpts": [
            "wasmtime/profiling_agent/. jitdump.rs. Settings · Help. Summary. 1//! Support for jitdump files which can be used by perf for profiling jitted code."
          ]
        },
        {
          "title": "Support DWARF in wasm-bindgen · Issue #2389",
          "url": "https://github.com/rustwasm/wasm-bindgen/issues/2389",
          "excerpts": [
            "It seems the DWARF info is kept when using wasm-bindgen with --keep-debug, but chrome doesn't fully support the dwarf debugging features for rust code."
          ]
        },
        {
          "title": "Steps run by cargo-wasi - The cargo-wasi Subcommand",
          "url": "https://bytecodealliance.github.io/cargo-wasi/steps.html",
          "excerpts": [
            "Whenever cargo wasi is used it will automatically pass --target wasm32-wasi to all Cargo subcommands that are invoked. ... wasm32-wasi target that contains DWARF ..."
          ]
        },
        {
          "title": "Attach debug info from source language · Issue #8658 - GitHub",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/8658",
          "excerpts": [
            "If you're going to add support for generating debug info to wasm-tools , use DWARF instead of source maps. You don't have to emit all the info ..."
          ]
        },
        {
          "title": "wasm-objdump - Man Page - ManKier",
          "url": "https://www.mankier.com/1/wasm-objdump",
          "excerpts": [
            "--debug. Print extra debug information. -x, --details. Show section details. -r, --reloc. Show relocations inline with disassembly. --section-offsets."
          ]
        },
        {
          "title": "iovisor/bcc: BCC - Tools for BPF-based Linux IO analysis ...",
          "url": "https://github.com/iovisor/bcc",
          "excerpts": [
            "BCC is a toolkit for creating efficient kernel tracing and manipulation programs, and includes several useful tools and examples."
          ]
        },
        {
          "title": "bcc/libbpf-tools/offcputime.c at master · iovisor/bcc - GitHub",
          "url": "https://github.com/iovisor/bcc/blob/master/libbpf-tools/offcputime.c",
          "excerpts": [
            "BCC - Tools for BPF-based Linux IO analysis, networking, monitoring, and more - bcc/libbpf-tools/offcputime.c at master · iovisor/bcc."
          ]
        },
        {
          "title": "eBPF Tutorial by Example 9: Capturing Scheduling ...",
          "url": "https://eunomia.dev/en/tutorials/9-runqlat/",
          "excerpts": [
            "Aug 10, 2023 — Here, we will introduce a program called runqlat, which records process scheduling latency as a histogram."
          ]
        },
        {
          "title": "First steps in system-wide Linux tracing - Martin Pitt",
          "url": "https://piware.de/post/2020-02-28-bpftrace/",
          "excerpts": [
            "Feb 28, 2020 — I picked bpftrace. It takes a reasonably high-level program as input (feels similar to awk), compiles it into an EBPF program, and runs it in ..."
          ]
        },
        {
          "title": "Debug Information | Sentry for WebAssembly",
          "url": "https://docs.sentry.io/platforms/native/guides/wasm/debug-information/",
          "excerpts": [
            "Learn about allowing Sentry to fully process native crashes and provide you with symbolicated stack traces, by uploading Debug Information Files."
          ]
        },
        {
          "title": "Observability in WebAssembly | Dylibso Developer Resources",
          "url": "https://dev.dylibso.com/docs/observe/overview",
          "excerpts": [
            "Dylibso provides observability SDKs for WebAssembly (Wasm), enabling continuous\nmonitoring of WebAssembly code as it executes within a runtime. It provides\ndevelopers with the tools necessary to capture and emit telemetry data from Wasm\ncode, including function execution and memory allocation traces, logs, and\nmetrics.",
            " Emit WebAssembly telemetry to many supported platforms\nWhether you use a managed platform such as Datadog, or a self-hosted solution\nlike Jaeger, Observe SDK Adapters make it easy to send Wasm telemetry wherever\nit needs to go. See the officially supported platforms by choosing\nwhich language library you would use. Open Source\nThis GitHub repository contains the\nRuntime SDKs and the Adapters necessary to have live profiling & tracing, logs,\nand metrics.",
            "In order for any telemetry data to be extracted, the code you want to monitor\nneeds to send some signals about what it's doing. This is where instrumentation comes in to play."
          ]
        },
        {
          "title": "Jit profiling support broken when instances created with ...",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/1017",
          "excerpts": [
            "I've traced the issue to a patch submitted right before merging the perf jitdump patch that apparently my last manual end-to-end tests weren't ..."
          ]
        },
        {
          "title": "State of debugging in Rust -> Wasm",
          "url": "https://www.reddit.com/r/rust/comments/sl56m0/state_of_debugging_in_rust_wasm/",
          "excerpts": [
            "You can debug WASM, but you can't use the dwarf source maps to put breakpoint in your code instead of the wat."
          ]
        },
        {
          "title": "Investigating Causes of Jitter in Container Networking",
          "url": "https://www.diva-portal.org/smash/get/diva2:1610208/FULLTEXT01.pdf",
          "excerpts": [
            "by F Maurer · 2021 — This thesis investigates jitter in container networking by examining packet processing in the Linux kernel using different container networking solutions."
          ]
        },
        {
          "title": "runqlat-bpfcc(8)",
          "url": "https://manpages.debian.org/unstable/bpfcc-tools/runqlat-bpfcc.8.en.html",
          "excerpts": [
            "This measures the time a task spends waiting on a run queue (or equivalent scheduler data structure) for a turn on-CPU, and shows this time as a histogram."
          ]
        },
        {
          "title": "Linux eBPF Off-CPU Flame Graph",
          "url": "https://www.brendangregg.com/blog/2016-01-20/ebpf-offcpu-flame-graph.html",
          "excerpts": [
            "Seeing both on-CPU and off-CPU flame graphs shows you the full picture: what's consuming CPUs, what's blocked, and by how much."
          ]
        },
        {
          "title": "tools/runqlat.py - platform/external/bcc - Git at Google",
          "url": "https://android.googlesource.com/platform/external/bcc/+/HEAD/tools/runqlat.py",
          "excerpts": [
            "#!/usr/bin/env python. # @lint-avoid-python-3-compatibility-imports. #. # runqlat Run queue (scheduler) latency as a histogram. # For Linux, uses BCC, eBPF."
          ]
        },
        {
          "title": "Wasmtime Guest Profiler and Rust+Wasm profiling",
          "url": "https://docs.wasmtime.dev/api/wasmtime/struct.GuestProfiler.html",
          "excerpts": [
            "The data collection granularity is limited by the mechanism you use to\ninterrupt guest execution and collect a profiling sample.",
            "If you use epoch interruption, then samples will only be collected at\nfunction entry points and loop headers.",
            "The only current requirement is that the guest\nyou wish to profile must be on the same stack where you call `sample` ,\nand executing within the same thread.",
            "However, the `GuestProfiler::sample` method is not currently async-signal-safe, so doing this correctly is not\neasy.",
            "Collects basic profiling data for a single WebAssembly guest. This profiler can’t provide measurements that are as accurate or detailed\nas a platform-specific profiler, such as `perf` on Linux.",
            "On the other\nhand, this profiler works on every platform that Wasmtime supports."
          ]
        },
        {
          "title": "Wasmtime profiling and guest profiling documentation",
          "url": "https://docs.wasmtime.dev/examples-profiling-guest.html",
          "excerpts": [
            "Cross-platform Profiling - Wasmtime",
            "The guest profiling strategy enables in-process sampling and will write the captured profile to a file which can be viewed at https://profiler.firefox.com/.",
            "To use this profiler with the Wasmtime CLI, pass the `--profile=guest[,path[,interval]]` flag.",
            "* `path` is where to write the profile, `wasmtime-guest-profile.json` by default",
            "* `interval` is the duration between samples, 10ms by default",
            "When used with `-W timeout=N` , the timeout will be rounded up to the nearest\nmultiple of the profiling interval."
          ]
        },
        {
          "title": "Traces | OpenTelemetry",
          "url": "https://opentelemetry.io/docs/concepts/signals/traces/",
          "excerpts": [
            "A Tracer creates spans containing more information about what is happening for a given operation, such as a request in a service."
          ]
        },
        {
          "title": "Metrics",
          "url": "https://opentelemetry.io/docs/concepts/signals/metrics/",
          "excerpts": [
            "A metric is a measurement of a service captured at runtime. The moment of capturing a measurements is known as a metric event."
          ]
        },
        {
          "title": "Observability primer",
          "url": "https://opentelemetry.io/docs/concepts/observability-primer/",
          "excerpts": [
            "Observability lets you understand a system from the outside by letting you ask questions about that system without knowing its inner workings."
          ]
        },
        {
          "title": "unlocking-observability-in-webassembly-with- ...",
          "url": "https://2025.wasm.io/slides/unlocking-observability-in-webassembly-with-opentelemetry-wasmio25.pdf",
          "excerpts": [
            "Implement wasi-otel support in more Wasm runtimes than Spin",
            "A quick introduction to observability and OpenTelemetry. ○ Three places we can collect telemetry in WebAssembly. ○ WebAssembly runtime auto instrumentation ...",
            "Implement opentelemetry-wasi SDKs in more languages than Rust.",
            "**Design metrics and logs support for wasi-otel",
            "wasi-otel vs wasi-observe",
            "Design and progress of the wasi-otel proposal"
          ]
        },
        {
          "title": "tracing_opentelemetry - Rust - Docs.rs",
          "url": "https://docs.rs/tracing-opentelemetry",
          "excerpts": [
            "OpenTelemetry tracing is a framework for instrumenting Rust programs to collect structured, event-based diagnostic information."
          ]
        },
        {
          "title": "Rust",
          "url": "https://opentelemetry.io/docs/languages/rust/",
          "excerpts": [
            "Feb 10, 2025 — OpenTelemetry in Rust is a language-specific implementation of an observability framework for generating and collecting application telemetry ..."
          ]
        },
        {
          "title": "tracing-opentelemetry - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/tracing-opentelemetry",
          "excerpts": [
            "This crate provides a subscriber that connects spans from multiple systems into a trace and emits them to OpenTelemetry-compatible distributed tracing systems."
          ]
        },
        {
          "title": "opentelemetry 0.30.0",
          "url": "https://docs.rs/crate/opentelemetry/latest/source/Cargo.toml",
          "excerpts": [
            "0. OpenTelemetry API for Rust. Crate · Source · Builds · Feature flags · Documentation ... wasm32\", not(target_os = \"wasi\")))'.dependencies.js-sys] version ..."
          ]
        },
        {
          "title": "WASM Performance Analysis - Linux-perf",
          "url": "https://www.alibabacloud.com/blog/wasm-performance-analysis---linux-perf_602062",
          "excerpts": [
            "\nperf is a powerful performance analysis tool provided by the Linux kernel. It analyzes programs based on a predefined set of performance events known as perf\\_events.",
            " ## WASM and perf\n\nBased on the information provided above, it appears that perf does not support WASM. `perf record` outputs function addresses or complete contents of the stack/register. For ELF files, this raw data can be directly pinpointed or parsed through the .debug section to obtain the function name. However, for WASM, the VM executes the machine code, and there is no direct mapping between the WASM code and the machine code. As a result, subsequent analysis tools provided by perf are unable to interpret the results.",
            "\n\nThe solution is to modify the runtime to record function names and their address mappings in the format specified by the map file. Generally, WASM execution goes through a load module phase and an instantiation phase. JIT compilation to machine code occurs during instantiation, so our task is to write the map file during this phase.",
            "\n\nlbr sampling is quite similar to frame pointer sampling, except it uses Intel's newer set of registers to sample branch addresses. I tested this approach and found that the solution by using /tmp/perf-pid.map for frame pointer sampling also works with lbr sampling.",
            "However, for WASM, the VM executes the machine code, and there is no direct mapping between the WASM code and the machine code.",
            "\nThe current WASM performance analysis is still not fully mature:\n\n1. Frame pointer sampling requires disabling frame pointer optimization, which may introduce a performance overhead of around 1%. 2.\nDwarf sampling lacks complete support for unwinding. 3. lbr sampling is limited by Intel's hardware constraints and sampling depth.",
            "\n\n### Dwarf Sampling\n\n#### Principles and Problems\n\nCompared with frame pointer sampling, dwarf sampling does not require additional compilation parameters.",
            ".\n### Frame Pointer Sampling\n\n#### Sample Content: Unknown\n\nLet's run a random WASM program by using WAVM with frame pointer sampling and examine the output in perf.unfold:",
            "For ELF files, this raw data can be directly pinpointed or parsed through the .debug section to obtain the function name."
          ]
        },
        {
          "title": "WAMR: WebAssembly Micro Runtime",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime",
          "excerpts": [
            "WAMR is a lightweight standalone WebAssembly (Wasm) runtime with small footprint, high performance and highly configurable features for applications cross from ... Understand WAMR heaps](https://bytecodealliance.github.io/wamr.dev/blog/understand-the-wamr-heaps/) and [stacks](https://bytecodealliance.github.io/wamr.dev/blog/understand-the-wamr-stacks/)",
            "* [Memory usage tuning](/bytecodealliance/wasm-micro-runtime/blob/main/doc/memory_tune.md) : the memory model and how to tune the memory usage",
            "Memory usage profiling](/bytecodealliance/wasm-micro-runtime/blob/main/doc/build_wamr.md) : how to profile the memory usage",
            "Performance and footprint data](https://github.com/bytecodealliance/wasm-micro-runtime/wiki/Performance) : the performance and footprint data",
            "\n\n# Project Technical Steering Committee",
            "The [WAMR PTSC Charter](/bytecodealliance/wasm-micro-runtime/blob/main/TSC_Charter.md) governs the",
            "Performance tuning](/bytecodealliance/wasm-micro-runtime/blob/main/doc/perf_tune.md) : how to tune the performance",
            "Performance tuning](/bytecodealliance/wasm-micro-runtime/blob/main/doc/perf_tune.md) : how to tune the performance",
            "Benchmarks](/bytecodealliance/wasm-micro-runtime/blob/main/tests/benchmarks) : checkout these links for how to run the benchmarks",
            "Benchmarks](/bytecodealliance/wasm-micro-runtime/blob/main/tests/benchmarks) : checkout these links for how to run the benchmarks"
          ]
        },
        {
          "title": "WAMR 2024 Summary",
          "url": "https://bytecodealliance.org/articles/wamr-2024-summary",
          "excerpts": [
            "Feb 19, 2025 — Linux perf for Wasm Functions: This tool allows developers to profile Wasm functions directly, providing insights into performance bottlenecks.",
            "New Features in WAMR 2024\n-------------------------\n\nSeveral exciting new features have been added to WAMR in 2024, aimed at enhancing the development experience and expanding the capabilities of WAMR. Here are some of the key features:\n\n### Development Tools: Simplifying Wasm Development\n\nOne of the most exciting additions to WAMR in 2024 is the introduction of new development tools aimed at simplifying Wasm development. These tools include:\n\n* Linux perf for Wasm Functions: This tool allows developers to profile Wasm functions directly, providing insights into performance bottlenecks. * AOT Debugging: Ahead-of-time (AOT) debugging support has been added, making it easier to debug Wasm applications. * Call Stack Dumps: Enhanced call stack dumps provide detailed information about the execution flow, aiding in troubleshooting and optimization. Before these tools, developing a Wasm application or plugin using a host language was a complex task. Mapping Wasm functions back to the source code written in the host language required deep knowledge and was often cumbersome."
          ]
        },
        {
          "title": "Unlocking Otel in WebAssembly - Wasm I/O 2025",
          "url": "https://www.fermyon.com/blog/unlocking-otel-in-wasm",
          "excerpts": [
            "ches:\n\n#### Approach 1: HTTP Exporter with WASI-HTTP\n\nUsing familiar OpenTelemetry SDKs with an HTTP exporter backed by WASI-HTTP. This approach feels familiar to developers but has limitations:\n\n* Difficult to compile OpenTelemetry libraries to WebAssembly\n* Can’t access parent trace context in non-HTTP components\n* Unable to properly associate host operations with guest span",
            "The proposal is wrapped in language-specific SDKs (starting with Rust) that make it transparent for developers to use standard OpenTelemetry APIs while benefiting from WebAssembly-specific optimizations.",
            "Trace data includes:\n\n* HTTP request handling\n* Component execution spans\n* Host resource interactions (like key-value store operations)\n\nThis auto-instrumentation is cross-language and requires zero code changes - you simply get observability “for free” by running your components in an instrumented runtime.",
            "pans\n\n#### Approach 2: WASI-OTel Processor\n\nA more sophisticated approach using a special processor backed by the new WASI-OTel proposal.\nThis enables:\n\n* Proper parent-child relationships between host and guest spans\n* Full trace context propagation across the host-guest boundary\n* Seamless integration of host operations into guest-initiated traces",
            "Caleb outlines three key places where telemetry can be collected in WebAssembly systems:\n\n1. Host-Guest Telemetry - Observing interactions between the WebAssembly runtime and components\n2. Inter-Guest Telemetry - Tracking communication between different WebAssembly components\n3. Intra-Guest Telemetry - Emitting traces from within WebAssembly components themselves",
            "\nOne of WebAssembly’s superpowers is the ability to provide automatic instrumentation without much developer effort. Caleb shows this using [Spin](https://spinframework.dev/) , the serverless WebAssembly framework for developing apps. He shows how a simple Rust component automatically generates rich tracing data.",
            "While auto-instrumentation provides valuable baseline visibility, production apps need custom tracing that reflects business logic and application-specific workflows. This is where OpenTelemetry’s standard APIs _shine_ .",
            "\nThe [WASI-OTel proposal](https://github.com/calebschoepp/wasi-otel) represents a significant step forward for WebAssembly observability. Currently in Phase 0 of the WASI process, it provides:\n\n* **Standardized interfaces** for telemetry data exchange between host and guest\n* **Processor callbacks** (onStart, onEnd) that notify the host of guest span lifecycle events\n* **Context propagation** methods that enable proper trace parenting\n\nThe proposal is wrapped in language-specific SDKs (starting with Rust) that make it transparent for developers to use standard OpenTelemetry APIs while benefiting from WebAssembly-specific optimizations",
            "Next steps include:\n\n* Advancing WASI-OTel through the WASI standardization process\n* Expanding language support beyond Rust\n* Adding metrics and logs support (currently focused on tracing)\n* Gaining broader runtime support beyond Spin"
          ]
        },
        {
          "title": "wasmCloud 0.82: WASI 0.2, OTEL logging, and more",
          "url": "https://wasmcloud.com/blog/wasmcloud-0.82-wasi-p2-is-here/",
          "excerpts": [
            "The Docker Compose files are [available on GitHu",
            "An OTEL collector for ingesting OTLP signals from a wasmCloud host · Grafana for dashboards · Loki for logging · Tempo for traces · Prometheus for ...",
            "Quality-of-life improvements for builds and deployment",
            "Support for OpenTelemetry logging",
            "OpenTelemetry (OTEL) logging joins our existing tracing support to provide more options for observability. (Metrics will complete the trifecta in wasmCloud 1.0.) By default, plaintext logs are written to `stderr` as before. You can enable structured logging and emit JSON as an OTEL signal by launching a wasmCloud host with the OTEL exporter enabled via environment variable:",
            "OTEL_TRACES_EXPORTER=otlp wash up",
            "Grafana** for dashboar",
            "Prometheus** for metrics (in preparation for wasmCloud 1."
          ]
        },
        {
          "title": "wasmerio/wasm-debug: A runtime-independent crate ...",
          "url": "https://github.com/wasmerio/wasm-debug",
          "excerpts": [
            "A general purpose crate for dealing with Wasm DWARF for and transforming it into DWARF that can be included in a compiled object file or given to a debugger."
          ]
        },
        {
          "title": "DWARF debugging information",
          "url": "https://docs.wa2.dev/debugging/dwarf/",
          "excerpts": [
            "Sep 14, 2023 — In WebAssembly, DWARF debugging information can be automatically added to WebAssembly files by certain tool chains or tools that support adding ..."
          ]
        },
        {
          "title": "VS Code WebAssembly DWARF Debugging Extension",
          "url": "https://marketplace.visualstudio.com/items?itemName=ms-vscode.wasm-dwarf-debugging",
          "excerpts": [
            "This extension enables a capability in the VS Code JavaScript debugger, allowing it to read DWARF debugging information included in WebAssembly files."
          ]
        },
        {
          "title": "Performance in the spotlight: WebAssembly profiling for ...",
          "url": "https://dispatch.run/blog/performance-in-the-spotlight-webassembly-profiling-for-everyone",
          "excerpts": [
            "May 11, 2023 — We developed wzprof, a cross-language CPU and memory profiler for WebAssembly, laying the grounds for what could become a universal profiler."
          ]
        },
        {
          "title": "bpftrace/bpftrace: High-level tracing language for Linux - GitHub",
          "url": "https://github.com/bpftrace/bpftrace",
          "excerpts": [
            "bpftrace is a high-level tracing language for Linux. bpftrace uses LLVM as a backend to compile scripts to eBPF-bytecode and makes use of libbpf and bcc."
          ]
        },
        {
          "title": "One-Liner Tutorial - bpftrace",
          "url": "https://bpftrace.org/tutorial-one-liners",
          "excerpts": [
            "Lesson 10. Scheduler Tracing​ · sched: The sched category has tracepoints for different kernel CPU scheduler events: sched_switch, sched_wakeup, ..."
          ]
        },
        {
          "title": "Beyond Benchmarks: What Kernel-Level Observability ...",
          "url": "https://medium.com/@chuka-uzo-j/beyond-benchmarks-what-kernel-level-observability-taught-us-about-running-llms-on-the-edge-153ed8ac1f24",
          "excerpts": [
            "WasmEdge maintained lower cycle counts while yielding better structured output by leveraging deterministic memory prefetching and reduced ..."
          ]
        },
        {
          "title": "offcputime - Summarize off-CPU time by kernel stack trace. ...",
          "url": "https://manpages.ubuntu.com/manpages/focal/man8/offcputime-bpfcc.8.html",
          "excerpts": [
            "This program shows stack traces and task names that were blocked and \"off-CPU\", and the total duration they were not running: their \"off-CPU time\"."
          ]
        },
        {
          "title": "Kubernetes Networking: eBPF in Action | by Quân Huỳnh - Medium",
          "url": "https://medium.com/@hmquan08011996/kubernetes-networking-ebpf-in-action-f0df2592dade",
          "excerpts": [
            "Missing: runqlat cgroup v2"
          ]
        },
        {
          "title": "A thorough introduction to bpftrace",
          "url": "https://www.brendangregg.com/blog/2019-08-19/bpftrace.html",
          "excerpts": [
            "Aug 19, 2019 — In this post I'll summarize the bpftrace language, variable types, probes, and tools. bpftrace uses BPF (Berkeley Packet Filter), an in-kernel ..."
          ]
        },
        {
          "title": "Calling kfuncs from bpftrace #3167",
          "url": "https://github.com/bpftrace/bpftrace/discussions/3167",
          "excerpts": [
            "I'm working on a runqlat program and I need to filter out some tasks based on the info the the task struct. The sched_new and sched_switch hooks only provide ..."
          ]
        },
        {
          "title": "Wasmer Profiling and Observability",
          "url": "https://docs.wasmer.io/runtime/cli",
          "excerpts": [
            "Profiling WebAssembly Code []()\n\nYou can enable profiling data generation for your WebAssembly code using the `--profiler` option:\n\n```\nwasmer run my_program.wasm --profiler=perfmap\n```\n\nThis generates profiling information in the [perfmap format (opens in a new tab)](https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/jit-interface.txt) , which can be used with Linux perf tools to analyze the performance of your WebAssembly code. The profiler data is written to `/tmp/perf-{PID}.map` . This option is available for the Cranelift, LLVM, and Singlepass compiler backends. ## Help"
          ]
        },
        {
          "title": "Coroot blog: runqlat and runqslower - eBPF command line tools",
          "url": "https://coroot.com/blog/engineering/runqlat-and-runqslower-ebpf-command-line-tools/",
          "excerpts": [
            "In this blog post we will look at runqlat and runqslower commands. They are available in both BCC and bpftrace tool collections.",
            " runqlat and runqslower - eBPF command line tools",
            "Tracing run queue latency… Hit Ctrl-C to end.",
            "Note we have significantly more cases when the process had to wait for 50ms or longer to be scheduled. In this case we’re looking at the run queue latency / scheduling latency for system overall, where it is common to be interested only latency impact for specific process or cgroup – the tools also allows it (see Command Line Options Section)",
            "Interested to easily see what process are impacted by scheduling latency  ? You can see *runqslower* tool for th",
            "Command Line Options",
            "Usage: runqlat [OPTION…]",
            "\n-c, –cgroup=/sys/fs/cgroup/unified",
            "Trace process in cgroup path",
            "           Print a histogram per thread ID",
            "Related Features in Coroot",
            "eBPF Linux Command Line tools",
            "Try Coroot",
            "Stop guessing, start *seeing* with eBPF-powered instant observabilit"
          ]
        },
        {
          "title": "C API 0.11.2 Documentation | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/embed/c/reference/0.11.x/",
          "excerpts": [
            "The instruction counter can help developers to profile the performance of WASM running. Developers can retrieve the Statistics context from the VM context ..."
          ]
        },
        {
          "title": "WasmEdge Features | WasmEdge Developer Guides",
          "url": "https://wasmedge.org/docs/start/wasmedge/features/",
          "excerpts": [
            "WasmEdge (a sandbox project under CNCF) is a safe, fast, lightweight, portable, and extensible WebAssembly runtime."
          ]
        },
        {
          "title": "otlphttp exporter - GitHub",
          "url": "https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter/otlphttpexporter",
          "excerpts": [
            "No information is available for this page. · Learn why"
          ]
        },
        {
          "title": "Unlocking Observability in WebAssembly with OpenTelemetry by ...",
          "url": "https://www.youtube.com/watch?v=iKh8YlJh618",
          "excerpts": [
            "... open-source standards like WASI OTel. Additionally, we'll highlight the unique advantages Wasm offers to observability, such as automatic ..."
          ]
        },
        {
          "title": "OTLP Exporter Configuration | OpenTelemetry",
          "url": "https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/",
          "excerpts": [
            "The following environment variables let you configure an OTLP/gRPC or OTLP/HTTP endpoint for your traces, metrics, and logs."
          ]
        },
        {
          "title": "Higress's New WASM Runtime Greatly Improves ...",
          "url": "https://www.alibabacloud.com/blog/higresss-new-wasm-runtime-greatly-improves-performance_601025",
          "excerpts": [
            "Apr 11, 2024 — By switching to WAMR and turning on the AOT mode, the Wasm plug-in performance has been greatly improved by 50% on average, and the performance ..."
          ]
        },
        {
          "title": "Build WAMR vmcore - bytecodealliance/wasm-micro-runtime",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime/blob/main/doc/build_wamr.md",
          "excerpts": [
            "WAMR vmcore is a set of runtime libraries for loading and running Wasm modules. This document introduces how to build the WAMR vmcore."
          ]
        },
        {
          "title": "wasmtime_jit_debug::perf_jitdump - Rust - Parity",
          "url": "https://paritytech.github.io/try-runtime-cli/wasmtime_jit_debug/perf_jitdump/index.html",
          "excerpts": [
            "Describes debug information for a jitted function. An array of debug entries are appended to this record during writting. Note, this record must preceed the ..."
          ]
        },
        {
          "title": "A Fast WebAssembly Interpreter design in WASM-Micro- ...",
          "url": "https://software.intel.com/content/www/us/en/developer/articles/technical/webassembly-interpreter-design-wasm-micro-runtime.html",
          "excerpts": [
            "Oct 7, 2021 — To improve the WASM interpreter's performance, WASM-Micro-Runtime (WAMR) pre-compiles the WASM bytecode array into new bytecode array with many ..."
          ]
        },
        {
          "title": "Time-series data ingestion from Rust WebAssembly application ...",
          "url": "https://greptime.com/blogs/2023-12-22-wasmedge",
          "excerpts": [
            "First, create a new Rust project using cargo new . Our compilation target will be wasm32-wasi . You can create a .cargo/config.toml file in the ..."
          ]
        },
        {
          "title": "iWASM has a special performance planned you won't ...",
          "url": "https://www.facebook.com/IntlWomensAirSpaceMuseum/posts/iwasm-has-a-special-performance-planned-you-wont-want-to-miss/1543323217118807/",
          "excerpts": [
            "iWASM has a special performance planned you won't want to miss! · Learn more about Katharine Wright with a special performance by Vicky Amsdell ..."
          ]
        },
        {
          "title": "WasmEdge/WasmEdge Issue 1061",
          "url": "https://github.com/WasmEdge/WasmEdge/issues/1061",
          "excerpts": [
            "WasmEdge is designed for the general purpose wasm runtime. However, when running rustpython, we found the performance is worse than other runtimes such as wasmtime, even after using the ahead-of-time compilation.",
            "Profile the WasmEdge runtime with rustpython workload and generate profiling reports.",
            "> Time:  \n> WasmEdge 0.9.0: ~40s  \n> Wasmtime: ~1s",
            "To apply this LFX mentorship, there are some minimum qualifications:\n\n1. Experience with profiling tools such as perf, gprof, oprofile, or valgrind. 2. Experience with C++ programming language"
          ]
        },
        {
          "title": "WasmEdge Issue 3205 - Coredump and Post-Mortem Debugging",
          "url": "https://github.com/WasmEdge/WasmEdge/issues/3205",
          "excerpts": [
            "This data is saved in a coredump file, which, along with DWARF information, can be analyzed post-mortem to debug the error.",
            "The project involves creating or enhancing tools to generate and analyze Wasm coredumps, addressing the unique challenges of WebAssembly's memory model and execution semantics.",
            "The coredump file must be compatible with the current toolchain, including [wasmgdb](https://github.com/xtuc/wasm-coredump/tree/main/bin/wasmgdb) and [wasm-edit](https://github.com/xtuc/wasm-coredump/tree/main/bin/rewriter) ."
          ]
        },
        {
          "title": "Performance",
          "url": "https://github.com/bytecodealliance/wasm-micro-runtime/wiki/Performance",
          "excerpts": [
            "WAMR provides two interpreters - the fast version for advanced performance and and the classic version for smaller footprint respectively.",
            "performance comparison:",
            "| workload | wamr-aot / wasm3 | wasm3 / wamr-interpreter | wamr-aot / GCC(O3) | wamr interpreters (fast/classic) |",
            "| --- | --- | --- | --- | --- |",
            "| CoreMark | 8\\.79 | 1\\.32 | 0\\.79 | 2\\.",
            "Memory consumption",
            "| WASM interpreter | Peak memory usage |",
            "| --- | --- |",
            "| WAMR classic | 365KB |",
            "| WAMR fast | 485KB |",
            "| wasm3 | 514KB |"
          ]
        },
        {
          "title": "Proposal: wasi-otel #646",
          "url": "https://github.com/WebAssembly/WASI/issues/646",
          "excerpts": [
            "WASI Otel exposes an OpenTelemetry interface to Wasm components to allow them to collect trace, metric, and log signals conveniently using ... Proposal: wasi-otel",
            "Goals\n\n* Enable Wasm components to use standard [OTel SDKs](https://opentelemetry.io/docs/languages/)",
            "Significant work has been done to prototype the tracing component of this interface."
          ]
        },
        {
          "title": "[PDF] Firecracker: Lightweight Virtualization for Serverless Applications",
          "url": "https://www.usenix.org/system/files/nsdi20-paper-agache.pdf",
          "excerpts": [
            "For a 1024MB function, this means 102MB of memory overhead. Performance is somewhat complex, as it is measured against the function's ...",
            "While the 125ms boot times offered by Firecracker are fast, they are not fast enough for the scale-up path of Lambda, which is sometimes ...",
            "Does Firecracker Achieve Its Goals? **\n\nUsing the six criteria from Section 2 , we found that while\n\nthere is scope for improving Firecracker (as there is scope"
          ]
        },
        {
          "title": "firecracker-boot-time.md",
          "url": "https://gist.github.com/sameo/0647d6aaa36e73e6b536b51c29db1ead",
          "excerpts": [
            "Here we're going to measure the time it takes for a Firecracker guest VM to reach userspace. To do so we're going to build 3 components."
          ]
        },
        {
          "title": "Understanding Firecracker MicroVMs: The Next Evolution ...",
          "url": "https://medium.com/@meziounir/understanding-firecracker-microvms-the-next-evolution-in-virtualization-cb9eb8bbeede",
          "excerpts": [
            "Isolation: Firecracker: Provides hardware-level isolation using KVM, which is more secure and robust against attacks that exploit the shared ...",
            "Lightweight: Firecracker VMs have a memory overhead of less than 5 MiB per microVM. · Fast Boot Time: MicroVMs can boot in less than 125 ..."
          ]
        },
        {
          "title": "r/rust - WasmEdge, a high-performance WebAssembly ...",
          "url": "https://www.reddit.com/r/rust/comments/z8r7rq/wasmedge_a_highperformance_webassembly_runtime/",
          "excerpts": [
            "Compared with the Java, Python, or JavaScript runtimes, it can be 10x to 100x faster. It is also much faster than Docker, especially in the cold ..."
          ]
        },
        {
          "title": "Firecracker memory usage slowly going up - mysql - Fly.io Community",
          "url": "https://community.fly.io/t/firecracker-memory-usage-slowly-going-up/6767",
          "excerpts": [
            "The memory used will change over time as it will be used by various things: logs, buffers, query cache and the number of connections, as each connection will ..."
          ]
        },
        {
          "title": "Resource constraints",
          "url": "https://docs.docker.com/engine/containers/resource_constraints/",
          "excerpts": [
            "The maximum amount of memory the container can use. If you set this option, the minimum allowed value is 6m (6 megabytes). That is, you must set the value to at ..."
          ]
        },
        {
          "title": "Firecracker",
          "url": "https://firecracker-microvm.github.io/",
          "excerpts": [
            "Each Firecracker microVM runs with a reduced memory overhead of less than 5 MiB , enabling a high density of microVMs to be packed on each server.",
            "Firecracker initiates user space or\n application code in as little as 125 ms and supports microVM creation rates of up to 150 microVMs per\n second per hos",
            "er space and uses the Linux Kernel-based Virtual Machine (KVM) to create microVMs",
            "Firecracker has a minimalist design. It excludes unnecessary devices and guest\n functionality to reduce the memory footprint and attack surface area of each microVM",
            "Firecracker is a virtual machine monitor (VMM) that uses the Linux Kernel-based Virtual Machine (KVM) to\n create and manage microVMs",
            "Each Firecracker microVM is further isolated with\n common Linux user-space security barriers by a companion program called \"jailer\"",
            "achine. It provides built-in rate limiters, which allows you to\n granularly control network and storage resources used by thousands of microVMs on the same machine.",
            "Firecracker is an open source virtualization technology that is purpose-built for creating and managing\n secure, multi-tenant container and function-based services"
          ]
        },
        {
          "title": "Firecracker Documentation",
          "url": "https://github.com/firecracker-microvm/firecracker",
          "excerpts": [
            "Firecracker is an open source virtualization technology that is purpose-built\nfor creating and managing secure, multi-tenant container and function-based\nservices that provide serverless operational models. Firecracker runs workloads\nin lightweight virtual machines, called microVMs, which combine the security and\nisolation properties provided by hardware virtualization technology with the\nspeed and flexibility of containers.",
            "Firecracker runs workloads\nin lightweight virtual machines, called microVMs, which combine the security and\nisolation properties provided by hardware virtualization technology with the\nspeed and flexibility of containers.",
            "The main component of Firecracker is a virtual machine monitor (VMM) that uses\nthe Linux Kernel Virtual Machine (KVM) to create and run microVMs. Firecracker\nhas a minimalist design. It excludes unnecessary devices and guest-facing\nfunctionality to reduce the memory footprint and attack surface area of each\nmicroVM.",
            "The main component of Firecracker is a virtual machine monitor (VMM) that uses\nthe Linux Kernel Virtual Machine (KVM) to create and run microVMs.",
            "Firecracker's performance characteristics are listed as part of the specification documentation. All specifications are a part of our commitment ..."
          ]
        },
        {
          "title": "Firecracker: Lightweight virtualization for serverless computing",
          "url": "https://aws.amazon.com/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/",
          "excerpts": [
            "Low Overhead** – Firecracker consumes about 5 MiB of memory per microVM. You can run thousands of secure VMs with widely varying vCPU and memory configurations on the same instanc",
            "**Secure** – This is always our top priority! Firecracker uses multiple levels of isolation and protection, and exposes a minimal attack surf",
            ". **High Performance** – You can launch a microVM in as little as 125 ms today (and even faster in 2019), making it ideal for many types of workloads, including those that are transient or short-lived.",
            "e.\n**Open Source** – Firecracker is an active [open source project](https://github.com/firecracker-microvm/firecracker) .",
            ". **Process Jail** – The Firecracker process is jailed using [cgroups](https://en.wikipedia.org/wiki/Cgroups) and [seccomp BPF](https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html) , and has access to a small, tightly controlled list of system calls.",
            "\n\n**Simple Guest Model** – Firecracker guests are presented with a very simple virtualized device model in order to minimize the attack surface: a network device, a block I/O device, a Programmable Interval Timer, the KVM clock, a serial console, and a partial keyboard (just enough to allow the VM to be reset).",
            "cy. Firecracker is written in Rust, a modern programming language that guarantees thread safety and prevents many types of buffer overrun errors that can lead to security vulnerabilities.",
            "cy. Firecracker is written in Rust, a modern programming language that guarantees thread safety and prevents many types of buffer overrun errors that can lead to security vulnerabilities."
          ]
        },
        {
          "title": "How Firecracker works (Amazon Science)",
          "url": "https://www.amazon.science/blog/how-awss-firecracker-virtual-machines-work",
          "excerpts": [
            "A single server can create up to 150 Firecracker microVMs per second and run thousands of microVMs at the same time.",
            "hose 50,000 lines of code are written in the Rust language, which is notable for its built-in security and correctness features",
            "This allows us to create a single microVM for the code that each customer program executes in Firecracker, a simple but strong security model.",
            "Firecracker powers the AWS Lambda service, where it currently handles trillions of requests each month for hundreds of thousands of AWS customers.",
            "Firecracker offers the best of both worlds: the security of hardware-virtualization-based virtual machines and the resource efficiency and fast startup time of containers."
          ]
        },
        {
          "title": "MDPI Electronics 12(11) 2515—Serverless cold-starts, WASM, containers, and microVMs",
          "url": "https://www.mdpi.com/2079-9292/12/11/2515",
          "excerpts": [
            "*Table 1. ** The performance comparison of container technolog"
          ]
        },
        {
          "title": "Enhancing Performance Isolation in Multi-Tenant Kubernetes Clusters for AI Workloads",
          "url": "https://www.researchgate.net/publication/391438938_Enhancing_Performance_Isolation_in_Multi-_Tenant_Kubernetes_Clusters_for_AI_Workloads",
          "excerpts": [
            "The results highlight that bare-metal environments provide the highest raw performance, while containers achieve a balance between efficiency and scalability.",
            "VMs offer robust isolation but incur significant overheads."
          ]
        },
        {
          "title": "Inside Firecracker – How AWS Designed a Purpose-Built MicroVM",
          "url": "https://www.linkedin.com/pulse/beyond-containers-exploring-microvm-revolution-part-2-moon-hee-lee-et83c",
          "excerpts": [
            "⚡ Performance That Scales  Firecracker's performance is tuned for cloud-scale use: Startup Time: A microVM boots in about 125 milliseconds , fast enough to meet the demands of real-time workloads. Density: Thanks to low memory overhead, thousands of microVMs can run on a single server."
          ]
        },
        {
          "title": "Firecracker vs QEMU",
          "url": "https://e2b.dev/blog/firecracker-vs-qemu",
          "excerpts": [
            "Unlike traditional VMs that might take several seconds to start, Firecracker VMs can boot up in as little as 125ms . AWS built it to power Lambda and Fargate [2], where they need to quickly spin up isolated environments for running customer code."
          ]
        },
        {
          "title": "The History and Evolution of WebAssembly in Kubernetes - Fermyon",
          "url": "https://www.fermyon.com/blog/history-and-evolution-of-webassembly-in-kubernetes",
          "excerpts": [
            "In one recent conversation I had with an OpenWhisk user, they noted that it takes 37 seconds to cold start a single function. Contrast that with ...",
            "May 16, 2024 — Discover how WebAssembly and Kubernetes evolved together to create ultra-efficient, high-performing modern cloud environments."
          ]
        },
        {
          "title": "Will the cold starts of my AWS Lambda function take longer if I use ...",
          "url": "https://stackoverflow.com/questions/69512271/will-the-cold-starts-of-my-aws-lambda-function-take-longer-if-i-use-an-ecr-image",
          "excerpts": [
            "Docker images will be definitely slower cold start. This is because of bigger size and additional OS. Lambda loads your code into its managed ..."
          ]
        },
        {
          "title": "[CVE-2019-5736]: Runc uses more memory during start up after the fix",
          "url": "https://github.com/opencontainers/runc/issues/1980",
          "excerpts": [
            "We observed higher memory usage (likely during container startup) after the fix for CVE 0a8e411. We had a test that specifies 10m container ..."
          ]
        },
        {
          "title": "CRUN The faster container runtime. | by Abhigyan Dwivedi - Medium",
          "url": "https://medium.com/@abhigyan.dwivedi_58961/run-the-faster-container-runtime-212ef6bbfaa6",
          "excerpts": [
            "It boasts faster container startup times and a significantly smaller memory footprint compared to traditional runtimes like runc . Fully OCI ..."
          ]
        },
        {
          "title": "10 Reasons Why We Love Firecracker MicroVMs",
          "url": "https://www.koyeb.com/blog/10-reasons-why-we-love-firecracker-microvms",
          "excerpts": [
            "Feb 23, 2021 — It also supports super-high MicroVM creation rates with up to 150 microVMs per second and per host."
          ]
        },
        {
          "title": "MicroVMs: Scaling Out Over Scaling Up in Modern Cloud Architectures",
          "url": "https://openmetal.io/resources/blog/microvms-scaling-out-over-scaling-up/",
          "excerpts": [
            "MicroVMs also consume minimal memory overhead (only a few megabytes for the VMM itself), allowing dense packing of instances on a single ..."
          ]
        },
        {
          "title": "Use a dedicated performance tool to measure start-up latency in the CI",
          "url": "https://github.com/firecracker-microvm/firecracker/issues/2516",
          "excerpts": [
            "Firecracker is currently responsible of measuring the startup performance by itself, however this logic becomes hard to maintain as we face ..."
          ]
        },
        {
          "title": "Runwasi Benchmarks Documentation",
          "url": "https://runwasi.dev/benchmarks.html",
          "excerpts": [
            "This document outlines the various methods and ideas for benchmarking `runwasi` shims. The goal is to provide a structured approach to measuring the performance of the `runwasi` shims and comparing it against other container runtimes, such as `runc` or `crun` . These benchmarks will help track performance changes across different versions and explore how well `runwasi` scales in high-density environments.",
            "| Runtime | Tasks | Time | Throughput |",
            "| --- | --- | --- | --- |",
            "| runwasi wasmtime | 1000 | 3s 31ms 663us 672ns | 329\\.85 tasks/s ",
            "| runc distroless wasmtime | 1000 | 11s 795ms 27us 285ns | 84\\.78 tasks/s ",
            "We’ve added functions to collect memory usage data from the shim and zygote processes. We use `scripts/benchmark-mem.sh` script to collect the data.",
            "tress-test) is a CLI that runs a stress test for the `runwasi` shims. It allows us to deploy a large number of Wasm-based tasks to evaluate task throughput (number of tasks finished per second) under high-density environments."
          ]
        },
        {
          "title": "Wasm Deployment Patterns: RunWASI, Containers, and MicroVMs",
          "url": "https://atlarge-research.com/pdfs/mjansen-wasm-2025.pdf",
          "excerpts": [
            "containerd-shim-wasmedge\nand containerd-shim-wasmtime runtimes have the best startup\nperformance for small deployments, taking up to 11.45 %\nless time to start Wasm modules than our implementation.",
            "we reduce memory usage by at least 10.87%. However, the\ndifference is smaller than when we compared our work against\nWasm runtimes embedded in crun, where we reduced memory\nusage by at least 40.0% compared to the second-best runtime,\ncrun-wasmedge.",
            "y, our integration uses at least\n16.38% less memory than Python containers with the crun and\nrunC container runtimes, the latter being Kubernetes’ default\ncontainer runtime.",
            "The WebAssembly Micro Runtime embedded\nin crun executes all containers’ WASM modules in under\n3.24 seconds, which is below the average across all tested\nruntimes.",
            "The WebAssembly Micro Runtime embedded\nin crun executes all containers’ WASM modules in under\n3.24 seconds, which is below the average across all tested\nruntimes.",
            "our integration starts WebAssembly containers faster than the\ncrun and runC runtimes start Python containers."
          ]
        },
        {
          "title": "Firecracker: start a VM in less than a second - Julia Evans",
          "url": "https://jvns.ca/blog/2021/01/23/firecracker--start-a-vm-in-less-than-a-second/",
          "excerpts": [
            "It takes <= 125 ms to go from receiving the Firecracker InstanceStart API call to the start of the Linux guest user-space /sbin/init process. ..."
          ]
        },
        {
          "title": "Alternative container runtimes",
          "url": "https://docs.docker.com/engine/daemon/alternative-runtimes/",
          "excerpts": [
            "containerd shims let you use alternative runtimes without having to change the configuration of the Docker daemon. To use a containerd shim, install the shim ..."
          ]
        },
        {
          "title": "Comparative Study of Container Runtime Performance ...",
          "url": "https://www.linkedin.com/pulse/comparative-study-container-runtime-performance-different-das-grymc",
          "excerpts": [
            "This study will provide analysis of these container runtimes, from their history to their main features, functionalities and finally we have a quick benchmark ..."
          ]
        },
        {
          "title": "Pod Memory Usage: Tracking, Commands & Troubleshooting",
          "url": "https://last9.io/blog/pod-memory-usage/",
          "excerpts": [
            "Apr 4, 2025 — Runtime overhead: The memory tax paid by your container runtime (Docker, containerd, CRI-O) – typically 10-20MB per container; Kernel memory ..."
          ]
        },
        {
          "title": "The Missing Metrics: Detecting Memory Noisy Neighbors in ...",
          "url": "https://devopscon.io/kubernetes-ecosystem/kubernetes-container-resource-management-preventing-interferance/",
          "excerpts": [
            "CPU performance counters can detect memory interference. However since interference events are frequent and short-lived, detecting them requires high-frequency ..."
          ]
        },
        {
          "title": "How to gather baseline metrics on Kubernetes resource ...",
          "url": "https://www.redhat.com/en/blog/openshift-usage-metrics",
          "excerpts": [
            "Mar 24, 2023 — Memory requests: This is the minimum amount of memory allocated to a pod or container. CPU limits: This is the maximum amount of CPU resources ( ..."
          ]
        },
        {
          "title": "Continuum: RunWasi and Wasm runtimes evaluation (M Kozub, 2024)",
          "url": "https://atlarge-research.com/pdfs/2024-mkozub-msc_thesis.pdf",
          "excerpts": [
            "Figure 5.6 presents the measurements of time needed to start executing workloads for ten\n\ndeployed pods",
            "s compare crun with newly embedded\n\nWAMR against the second-best in this benchmark, the containerd-shim-wasmtime. The\n\nmemory usage of our implementation is still at least 10.87% lower, as measured using the\n\nfree command. At the same time, compared to the worst performer in this experiment,\n\ncontainerd-shim-wasmer, crun with WAMR, used up to 77.53 % less memory to run one\n\npod (free command)",
            "me embedded uses at least 17.98% less memory when\n\nexecuting the WebAssembly container than crun when executing the Python Linux con-\n\ntainer. Similarly, crun with WAMR runtime embedded uses at least 18.15% less memory\n\nwhen executing the WebAssembly container than the default runC runtime when executing\n\nthe Python Linux container.",
            "For a\n\ndeployment of 400 pods, crun with embedded WAMR took respectively 18.82 % and 28.38\n\n% less time to start the Wasm modules than the containerd-shim-wasmedge and containerd-\n\nshim-wasmtime runti"
          ]
        },
        {
          "title": "wasmCloud: The Kubernetes for WebAssembly",
          "url": "https://horovits.medium.com/wasmcloud-the-kubernetes-for-webassembly-02a5025c6115",
          "excerpts": [
            "At its core, wasmCloud offers a component model that allows for pluggable interfaces, distributed deployment, and hot-swappable components. It ..."
          ]
        },
        {
          "title": "GitHub - wasmCloud/capability-providers: ARCHIVED ...",
          "url": "https://github.com/wasmCloud/capability-providers",
          "excerpts": [
            "This repository provides a wasmCloud capability provider and actors to perform inference using machine learning models for ONNX and Tensorflow."
          ]
        },
        {
          "title": "All the Cloud's a Stage, and WebAssembly Modules Mere ...",
          "url": "https://wasmcloud.com/blog/all-the-clouds-a-stage-webassembly-actor/",
          "excerpts": [
            "Send messages; Determine how to respond to the next message. There are dozens of actor model implementations out there from Akka to wasmCloud.",
            "The actor model is a model for concurrent computation ... There are dozens of actor model implementations out there from Akka to wasmCloud."
          ]
        },
        {
          "title": "Scheduling Auctions",
          "url": "https://wasmcloud.com/docs/deployment/hosts/auctions/",
          "excerpts": [
            "These hosts can be started with a label like gpu=A100 , allowing a client to schedule the capability provider on an appropriate host. Auctions can also be used ...",
            "An auction is performed when a client of the lattice control interface (e.g. the wash CLI, wadm , or another application) publishes a set of requirements to all ...",
            "Missing: architecture wRPC queue group"
          ]
        },
        {
          "title": "Defining Applications",
          "url": "https://wasmcloud.com/docs/ecosystem/wadm/model/",
          "excerpts": [
            "The wasmCloud Q3 2025 Roadmap sets out plans for an overhaul to scheduling in the next major release of wasmCloud. The new scheduling API will not use NATS ..."
          ]
        },
        {
          "title": "Announcing Spin 2.1",
          "url": "https://www.fermyon.com/blog/spin-v21",
          "excerpts": [
            "Dec 20, 2023 — Spin uses Wasmtime 15, and supports a new WASI snapshot · Multiple components can register for the same Redis channel trigger · Plugins can be ..."
          ]
        },
        {
          "title": "Spin Application Structure - Fermyon",
          "url": "https://www.fermyon.com/blog/spin-application-structure",
          "excerpts": [
            "In this article, we will look at the standard structure of a Spin application manifest (the structure created automatically by the spin new and spin add ..."
          ]
        },
        {
          "title": "Introducing Spin 2.0",
          "url": "https://www.fermyon.com/blog/introducing-spin-v2",
          "excerpts": [
            "Nov 2, 2023 — Spin is a developer tool and framework that guides users through creating, compiling, distributing, and running server-side applications with WebAssembly.",
            "But Spin adds another dimension to this fine-grained isolation: for every new request, it will create a fresh new instance of the Wasm component(s) handling the request, process it, then terminate the instance.",
            "That means that even in case an attacker can exploit a flaw in the application to corrupt its state, that corruption will only last for the current request, instead of affecting all future requests processed on the same machine.",
            "ne. Spin can do this because of the incredibly fast startup time for WebAssembly components, and the incredible work happening in the [Wasmtime](ht",
            "On a macOS machine, in 10 seconds, Spin created and executed almost _three hundred thousand_ WebAssembly instances, with about 28,000 requests per second and average latency that is below 200 _microsecon",
            "Spin 2.0 has significantly improved performance compared to Spin 1.0, in large part due to [using Wasmtime’s pooling memory allocator"
          ]
        },
        {
          "title": "faasm/faasm: High-performance stateful serverless runtime ...",
          "url": "https://github.com/faasm/faasm",
          "excerpts": [
            "codecov.io/gh/faasm/faasm)\n\n[]()\n\nFaasm is a high-performance stateful serverless runtime.\nFaasm provides multi-tenant isolation, yet allows functions to share regions of\nmemory. These shared memory regions give low-latency concurrent access to data,\nand are synchronised globally to support large-scale parallelism across multiple\nhosts. Faasm combines software fault isolation from WebAssembly with standard Linux\ntooling, to provide security and resource isolation at low cost. Faasm runs\nfunctions side-by-side as threads of a single runtime process, with low\noverheads and fast boot times. Faasm defines a custom host interface that extends [WASI](https://wasi.dev/) to\ninclude function inputs and outputs, chaining functions, managing state,\naccessing the distributed filesystem, dynamic linking, pthreads, OpenMP and MPI. In our [paper from USENIX ATC 20](https://www.usenix.org/conference/atc20/presentation/shillaker) we present\nthe initial Faasm design and evaluate it in the context of stateful serverless\nfunctions. In our [paper from USENIX NSDI'25, GRANNY](https://www.usenix.org/conference/nsdi25/presentation/segarra) , we present the\ndesign of the OpenMP and MPI runtimes, as well as new dynamic resource\nmanagement policies, all implemented in the [faabric](https://github.com/faasm/faabric) library. Please see the [full documentation](https://faasm.readthedocs.io/en/latest/) for\nmore details on the code and architecture",
            "Faasm provides multi-tenant isolation, yet allows functions to share regions of memory.",
            "ns\nfunctions side-by-side as threads of a single runtime process, with low\noverheads and fast boot times.",
            "Faasm is a high-performance stateful serverless runtime. Faasm provides multi-tenant isolation, yet allows functions to share regions of memory."
          ]
        },
        {
          "title": "Five things you need to know about wasmCloud",
          "url": "https://www.opcito.com/blogs/five-things-you-need-to-know-about-wasmcloud",
          "excerpts": [
            "Apr 15, 2025 — The results reveal that wasmCloud's WebAssembly-based approach delivers impressive performance compared to traditional containerized solutions, ..."
          ]
        },
        {
          "title": "Faasm: Lightweight Isolation for Efficient Stateful ...",
          "url": "https://www.usenix.org/conference/atc20/presentation/shillaker",
          "excerpts": [
            "by S Shillaker · 2020 · Cited by 411 — We believe that a new lightweight isolation approach is needed, which supports sharing memory directly between functions and reduces resource overheads."
          ]
        },
        {
          "title": "FAASM: lightweight isolation for efficient stateful serverless ...",
          "url": "https://dl.acm.org/doi/10.5555/3489146.3489174",
          "excerpts": [
            "by S Shillaker · 2020 · Cited by 411 — We introduce Faaslets, a new isolation abstraction for high-performance serverless computing. Faaslets isolate the memory of executed functions using software- ..."
          ]
        },
        {
          "title": "Platform Overview (wasmCloud)",
          "url": "https://wasmcloud.com/docs/concepts/",
          "excerpts": [
            "wasmCloud** is a universal application platform that enables you to write code in your language of choice, compile the code into a WebAssembly component, and run your\napplication anywhere—from your laptop to the edge to the cloud",
            "In wasmCloud, applications are comprised of **components** and **prov",
            "/components/) are portable, interoperable WebAssembly binaries that implement stateless logic.",
            "Applications run on **hosts** which may be clustered on a **la",
            "s/hosts/) is a runtime environment node consisting of a WebAssembly runtime (Wasmtime) and additional layers of security and functionality.",
            "ts/lattice/) is a self-forming, self-healing mesh network that provides a unified, flat topology across environments using NATS.",
            "Components and providers communicate with one another via **interfaces**. When components use the same interface, they may be **linked** either directly or over the ",
            "The architecture of wasmCloud makes it naturally suited to hybrid clouds and multi-tenant environments, with failover and load balancing that simply works out of the box."
          ]
        },
        {
          "title": "wasmCloud Deployment Architecture",
          "url": "https://wasmcloud.com/docs/deployment/wadm/deploy-architecture/",
          "excerpts": [
            "High availability and horizontal scaling by default​. ",
            "wadm` is designed to be resilient, flexible, and easy to scale from a single instance to many. **No configuration is required to run additional instances. *",
            "Each instance establishes and maintains in-memory state of a lattice based off [events](/docs/reference/cloud-event-list/) emitted by hosts in the lattice, which means `wadm` instances can be restarted or scaled up and down without impacting the management of the lattice.",
            "Instances of `wadm` do not need to be run on the same system as a wasmCloud host; the only requirement is that instances have access to internal NATS subjects used by [lattices](/docs/concepts/lattice/).",
            "The simplest deployment pattern is to run a group of (one or more) `wadm` instances per lattice. This simplifies the requirements for the NATS credentials, since `wadm` only needs to subscribe to events and emit commands related to a single lattice.",
            "This pattern is slightly more complex than the single lattice management pattern. In this pattern, `wadm` manages multiple lattices associated with the same NATS cluster."
          ]
        },
        {
          "title": "Introducing Spin",
          "url": "https://www.fermyon.com/blog/introducing-spin",
          "excerpts": [
            "Mar 31, 2022 — Spin is a framework for web apps, microservices, and other server-like applications. It provides the interfaces for writing WebAssembly modules.",
            "It provides the interfaces for writing WebAssembly modules that can do things like answer HTTP requests.",
            "One unique thing about Spin is that it is a multi-language framework. Rust and Go both have robust support in Spin, but you can also write Python, Ruby, AssemblyScript, Grain, C/C++, and other languages.",
            "Rust and Go both have robust support in Spin, but you can also write Python, Ruby, AssemblyScript, Grain, C/C++, and other languages.",
            "Following the model of stateless microservices, a Spin application can be bootstrapped, executed, and shut down again in milliseconds (or even nanoseconds if you are careful)."
          ]
        },
        {
          "title": "Fermyon Spin",
          "url": "https://www.fermyon.com/spin",
          "excerpts": [
            "Spin is the developer tool for building WebAssembly microservices and web applications\n======================================================================================",
            "Do more, faster. Pick your favorite language and dive into the important part using Spin's serverless framework. Connect your application to Redis, PostgreSQL, and other data services. And skip the boilerplate. Choose your language\n====================",
            "Spin SDKs are available in Python, Go, JavaScript, Rust, Go and .NET. And all WASI-compatible languages are [supported](https://spinframework.dev/language-support-overview). -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
            "! [Spin SDKs are available in Rust, Go and .NET. And all WASI-compatible languages are supported. ](/static/image/spin-nameless-1.png)",
            "Three commands are all you need\n===============================\n\nKeep the development loop simple. Create a project with 'spin new'. Build it with 'spin build'. And test it locally with 'spin up'. -----------------------------------------------------------------------------------------------------------------------------------",
            "! [Keep the development loop simple. Create a project with 'spin new'."
          ]
        },
        {
          "title": "faasm/README.md at main · faasm/faasm",
          "url": "https://github.com/faasm/faasm/blob/master/README.md",
          "excerpts": [
            "Faasm is a high-performance stateful serverless runtime. Faasm provides multi-tenant isolation, yet allows functions to share regions of memory."
          ]
        },
        {
          "title": "Faasm Documentation (Faasm)",
          "url": "https://faasm.readthedocs.io/",
          "excerpts": [
            "Faasm is a high-performance stateful serverless runtime. Faasm provides multi-tenant isolation, yet allows functions to share regions of memory.",
            "Faasm provides multi-tenant isolation, yet allows functions to share regions of memory. These shared memory regions give low-latency concurrent access to data, ...",
            "These shared memory regions give low-latency concurrent access to data,\nand are synchronised globally to support large-scale parallelism",
            "and are synchronised globally to support large-scale parallelism.",
            "Faasm combines software fault isolation from WebAssembly with standard Linux\ntooling,",
            "dynamic linking, pthreads, OpenMP and MPI.",
            "Faasm uses [Faabric](https://github.com/faasm/faabric/) as a distributed\nmessaging and state layer."
          ]
        },
        {
          "title": "wasmCloud Application Deployment Manager (wadm)",
          "url": "https://wasmcloud.com/docs/ecosystem/wadm/",
          "excerpts": [
            "The wasmCloud Application Deployment Manager (wadm) manages declarative application deployments, reconciling the current state of an application with the ..."
          ]
        },
        {
          "title": "Wadm Configuration - wasmCloud",
          "url": "https://wasmcloud.com/docs/deployment/wadm/configuration/",
          "excerpts": [
            "These guides are for deploying wadm in production. For local development and testing, simply run wash up . For more information on wadm , see the wadm docs."
          ]
        },
        {
          "title": "Globally Distributed WebAssembly Applications with ...",
          "url": "https://wasmcloud.com/blog/globally-distributed-webassembly-applications-with-wasmcloud-and-nats/",
          "excerpts": [
            "In this guide we used NGS and NATS Leaf Nodes to connect two wasmCloud hosts; one running locally and one running in the cloud."
          ]
        },
        {
          "title": "Faasm",
          "url": "https://github.com/faasm",
          "excerpts": [
            "Faasm is a research project on distributed serverless runtimes with WebAssembly (WASM). Faasm originated in the Large-Scale Data & Systems group from the ...",
            "Faasm is a high-performance serverless runtime using WebAssembly for function isolation. · Faabric is a messaging and state layer for distributed serverless ..."
          ]
        },
        {
          "title": "WP5 – Programming Abstractions for Serverless Computing",
          "url": "https://cloudbutton.eu/docs/slides/WP5.pdf",
          "excerpts": [
            "Sep 15, 2022 — Faasm: Serverless OpenMP + MPI. 21. Page 22. Why OpenMP in Serverless ... Faasm: Lightweight Isolation for Efficient Stateful. Serverless ..."
          ]
        },
        {
          "title": "[PDF] Lightweight Isolation for Efficient Stateful Serverless Computing",
          "url": "https://www.usenix.org/system/files/atc20-paper134-slides-shillaker.pdf",
          "excerpts": [
            "Missing: Faabric paper",
            "by S Shillaker · Cited by 411 — Can FAASM improve efficiency and performance of ML training? 3. Can FAASM improve throughput of ML inference? 4. Does Faaslet isolation affect performance of ...",
            "by S Shillaker · Cited by 411 — Faaslet shared regions - shared memory without breaking isolation. Two-tier ... - FAASM supports efficient locally shared and globally synchronised state."
          ]
        },
        {
          "title": "Announcing Spin v1.5",
          "url": "https://www.fermyon.com/blog/spin-v15",
          "excerpts": [
            "Sep 18, 2023 — Every time it handles a new request, Spin will create a new WebAssembly instance, execute the handler function for that request, then terminate ...",
            "Spin 1.5 makes use of a Wasmtime feature specifically designed for such scenarios, the memory pooling allocator, which can speed-up the ..."
          ]
        },
        {
          "title": "Announcing Spin 3.4",
          "url": "https://www.fermyon.com/blog/announcing-spin-3-4",
          "excerpts": [
            "6 days ago — In this blog post, we'll dive into a few exciting features of this release, which center around enabling HTTP/2 for outgoing requests, pooling ..."
          ]
        },
        {
          "title": "Persistent Data: PostgreSQL - Developer - Fermyon",
          "url": "https://developer.fermyon.com/cloud/data-postgres",
          "excerpts": [
            "In this tutorial, we will be using ElephantSQL's free PostgreSQL service. Spin and Fermyon Cloud. First, you need to have Spin installed on your computer."
          ]
        },
        {
          "title": "Reflections on Three Years of wasmCloud",
          "url": "https://wasmcloud.com/blog/wasmcloud-third-anniversary/",
          "excerpts": [
            "Jun 25, 2022 — We spent 90% of our time working on things that weren't core features like async and concurrency and thread-safe queue and dispatch management, ...",
            "There was a background thread per actor, one per capability provider, and another used for dispatch between the two. Ultimately, this became ..."
          ]
        },
        {
          "title": "[Concurrency] async/await + actors - Discussion - Swift Forums",
          "url": "https://forums.swift.org/t/concurrency-async-await-actors/6516",
          "excerpts": [
            "Aug 19, 2017 — I've been advocating for a specific model involving async/await and actors for many years now. Handwaving only goes so far, so some folks asked me to write ..."
          ]
        },
        {
          "title": "Adaptive Asynchronous Work-Stealing for distributed load- ...",
          "url": "https://arxiv.org/pdf/2401.04494",
          "excerpts": [
            "by JB Fernandes · 2024 · Cited by 1 — In parallel programming, work-stealing is a scheduling method originally designed for shared-memory applications (Blumofe and Leiserson, 1999)."
          ]
        },
        {
          "title": "Making HTTP Requests | Spin Docs",
          "url": "https://spinframework.dev/v2/http-outbound",
          "excerpts": [
            "Missing: concurrency threads O"
          ]
        },
        {
          "title": "WasmCloud Docs: NATS-based Lattice, Scheduling, and Concurrency",
          "url": "https://wasmcloud.com/docs/ecosystem/nats/",
          "excerpts": [
            " is built on [**NATS**](https://nats.io/), an open source connective technology hosted by the Cloud Native Computing Foundation (CNCF). NATS enables secure application-layer networking across diverse environments including edge, different vendors' clouds, and on-premise datacenters",
            "NATS uses [WIT-over-RPC (wRPC)](https://github.com/bytecodealliance/wrpc) as a transport protocol and helps to deliver a lattice with out-of-the-box load balancing, failover, and other features for systems that span environments.",
            "on.\nIn wasmCloud, components and providers use this **queue subscription** model to subscribe to invocations. When a component or provider exports a function, the system creates a queue subscription on a NATS subject that other entities can call.",
            "1. A component imports a function on a linked provider over a shared interface. 2. When the component runs, an invocation message is published to a queue group. 3. There are three instances of the linked provider on the lattice. Any one of those three could receive the invocation.",
            "With this basic model, queue groups and queue subscription facilitate horizontal scaling, load balancing, and failover for components and providers.",
            "The supercluster architecture provides...\n\n* **Scalability**: Seamlessly scales communication across multiple clusters, ensuring applications can grow without limitation. * **Reliability**: Enhances the overall reliability of the system by providing failover and load balancing across clusters. * **Geo-awareness**: Geo-affinity is implemented automatically in queuing, ensuring that service requests are handled by the nearest available cluster, reducing latency and improving"
          ]
        },
        {
          "title": "Fermyon Spin blog post on composing components with Spin 2",
          "url": "https://www.fermyon.com/blog/composing-components-with-spin-2",
          "excerpts": [
            "Nov 6, 2023 — With Spin 2.0, we have unlocked an exciting evolution of Wasm, the component model. This blog walks through how to take advantage of components in Spin 2.0. Wh",
            "When composing the components, the imports and exports are resolved such that the outer component only exports the [`wasi:http/incoming-handler` interface](https://github.com/WebAssembly/wasi-http/blob/main/wit/handler.wit) from the authentication component.",
            "We’ve composed two components, directly linking components instead of making over-network calls",
            "The components are still isolated from each other with their own linear memory but can communicate over a highly typed interface",
            "The auth component can be seen as a library that may be maintained upstream in the future. Imagine never having to write authentication logic again",
            "Auth secrets are kept safe from business logic code",
            "This app is portable! You can run it locally directly with [`wasmtime`](https://github.com/fermyon/http-auth-middleware/tree/main) or on other platforms and clouds that support the [upstream `wasi::http/proxy` world](https://github.com/WebAssembly/wasi-http/blob/main/wit/proxy.wit)"
          ]
        },
        {
          "title": "Faasm: High performance serverless computing",
          "url": "https://lsds.doc.ic.ac.uk/projects/faasm",
          "excerpts": [
            "Faasm is open-sourced at <https://github.com/lsds/Faas",
            "The Faasm scheduler ensures these functions execute close to their required data, reducing unnecessary duplication and overhead.",
            "Faasm uses a custom host interface to give functions access to state and interact with the runtime.",
            "Faasm supports C/C++ natively and extends support to dynamic languages such as Python by compiling the language runtime itself to WebAssembly.",
            "Functions run side-by-side as threads of a single runtime process, with low overheads and fast boot times.",
            "By running WebAssembly, Faasm combines software fault isolation with standard OS tooling to provide security and resource isolation guarantees at low cost.",
            "These shared memory regions give low-latency concurrent access to data, supporting high-performance distributed serverless applications.",
            "Faasm provides multi-tenant isolation, but also lets functions share regions of memory.",
            "Faasm is a high-performance stateful serverless runtime. The goal of the project is enabling fast, efficient parallel applications in serverless."
          ]
        },
        {
          "title": "Fermyon Platform For Kubernetes Part 2: Enterprise Architectures ...",
          "url": "https://www.fermyon.com/blog/intro-fermyon-platform-for-k8s-part-2",
          "excerpts": [
            "At its core, Fermyon Platform For Kubernetes is a tool for running Spin applications with stunning efficiency and performance on Kubernetes."
          ]
        },
        {
          "title": "Spin 1.0 — The Developer Tool for Serverless WebAssembly",
          "url": "https://www.fermyon.com/blog/introducing-spin-v1",
          "excerpts": [
            "Mar 22, 2023 — As we have learned in previous sections, Spin will create a new Wasm instance for every request — which makes it best suited for stateless, ...",
            "Spin is an open source developer tool and framework that helps the user through creating, building, distributing, and running serverless applications with Wasm."
          ]
        },
        {
          "title": "Introducing Spin 1.1 - Fermyon",
          "url": "https://www.fermyon.com/blog/introducing-spin-v11",
          "excerpts": [
            "Spin now executes WebAssembly components, and is fully compatible with the WebAssembly component model; a host of bugfixes and improvements ..."
          ]
        },
        {
          "title": "FaaSM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://wangziqi2013.github.io/paper/2021/10/11/faasm.html",
          "excerpts": [
            "Oct 11, 2021 — The paper proposes a snapshot mechanism called Proto-Faaslet, which captures the initialized state of the system and saves them as a local ..."
          ]
        },
        {
          "title": "[PDF] arXiv:2302.11358v1 [cs.DC] 22 Feb 2023",
          "url": "https://arxiv.org/pdf/2302.11358",
          "excerpts": [
            "Faasm: Lightweight isolation for efficient stateful serverless computing. In USENIX Annual Technical. Conference (USENIX ATC). USENIX ..."
          ]
        },
        {
          "title": "FAASM: Lightweight Isolation for Efficient Stateful Serverless ...",
          "url": "https://hale-legacy.com/class/edge-serverless/f21/handout/faasm.pdf",
          "excerpts": [
            "Proto-Faaslets - 500μs initialisation, 90kB memory. Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation.",
            "Problem 2: Inefficient state sharing. Faaslet shared regions - shared memory without breaking isolation. Two-tier state - global synchronisation. 10. Page 10 ..."
          ]
        },
        {
          "title": "wasmCloud Documentation - Deploy and Scale",
          "url": "https://wasmcloud.com/docs/tour/deploy-and-scale/",
          "excerpts": [
            "This means you can deploy your components to any cloud provider, edge location, or even on-premises and they will be able to communicate with each other.",
            "No matter where your components and capability providers run, they can seamlessly communicate on the [lattice"
          ]
        },
        {
          "title": "wasmCloud (Overview)",
          "url": "https://wasmcloud.com/",
          "excerpts": [
            "wasmCloud is an open source project from the Cloud Native Computing Foundation (CNCF) that enables teams to build polyglot applications composed of reusable Wasm components and run them—resiliently and efficiently—across any cloud, Kubernetes, datacenter, or edge."
          ]
        },
        {
          "title": "wasmCloud Concepts and Components",
          "url": "https://wasmcloud.com/docs/concepts/components/",
          "excerpts": [
            ")\n\nThe surrounding environment of the [wasmCloud host](/docs/concepts/hosts/) may have varying levels of concurrency support. This support may differ depending on whether the host is running in a browser, on a constrained device, or in a VM somewhere. However, the code for components should be independent of these conditions and *never have to change*, even if the surrounding environment adopts a different concurrency model.",
            "Components are portable, interoperable, and composable:\n\n* **Portable**: Because WebAssembly binaries execute against a virtual instruction set architecture (essentially a tiny VM), they are agnostic to architecture and operating system kernel; they run anywhere there is a WebAssembly runtime. Component binaries are typically much smaller than analogous container images, as well—often measured in kilobytes—enabling them to run in resource-constrained environments where even containers aren't practical."
          ]
        },
        {
          "title": "Host Labels",
          "url": "https://wasmcloud.com/docs/deployment/hosts/labels/",
          "excerpts": [
            "wasmCloud hosts are associated with a set of label key-value pairs. Labels are used to distinguish hosts in auctions and schedule workloads with wadm."
          ]
        },
        {
          "title": "wrpc_transport - Rust",
          "url": "https://docs.rs/wrpc-transport",
          "excerpts": [
            "wRPC transport abstractions, codec and framing. wRPC is an RPC framework based on WIT. It follows client-server model, where peers (servers) may serve ..."
          ]
        },
        {
          "title": "wrpc-transport - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/wrpc-transport",
          "excerpts": [
            "Mar 12, 2025 — wrpc-transport v0.28.4. wRPC core transport functionality. Readme · 62 Versions · Dependencies · Dependents. Failed to load README file for wrpc ..."
          ]
        },
        {
          "title": "Choose an Azure Compute Service",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/compute-decision-tree",
          "excerpts": [
            "5 days ago — Functions as a service (FaaS) lets you deploy your code to the service, which automatically runs it. Azure Functions is a FaaS service. Note."
          ]
        },
        {
          "title": "Serverless Architecture Part 2: When, Why, and How ...",
          "url": "https://www.fabricgroup.com.au/blog/serverless-architecture-part-2-when-why-and-how-to-adopt-it",
          "excerpts": [
            "Nov 12, 2024 — Function as a Service (FaaS) Improvements: Cloud providers are continuously optimizing FaaS, improving latency, scaling, and cold start times."
          ]
        },
        {
          "title": "Parallelization: pthreads or OpenMP? - multithreading",
          "url": "https://stackoverflow.com/questions/935467/parallelization-pthreads-or-openmp",
          "excerpts": [
            "The OpenMP is task-based, Pthreads is thread based. It means that OpenMP will allocate the same number of threads as number of cores. So you will get scalable ..."
          ]
        },
        {
          "title": "Platform Overview and Lattice (wasmCloud)",
          "url": "https://wasmcloud.com/docs/concepts/lattice/",
          "excerpts": [
            "The lattice is a self-forming, self-healing mesh network that provides a unified, flat topology across any number of environments, clouds, browsers, or even hardware.",
            "The NATS-powered lattice enables resilient distributed messaging with location-independent addressing.",
            "Across a lattice, different instances of the same component are equally eligible to receive a given invocation, regardless of the host on which they are running.",
            "This enables immediate failover—if a host is lost, alternative compute is immediately available.",
            "wasmCloud's wRPC protocol is (just as it sounds) an RPC protocol for dynamically composing distributed WebAssembly components at runtime over the lattice.",
            "red lattice enables resilient distributed messaging with location-independent addressing."
          ]
        },
        {
          "title": "wasmCloud 1.0 Brings the WebAssembly Component Model to Enterprise",
          "url": "https://wasmcloud.com/blog/wasmcloud-1-brings-components-to-enterprise/",
          "excerpts": [
            "wRPC makes distributed computing in wasmCloud feel like composing components over the lattice.",
            "wRPC is designed to be protocol agnostic, and wasmCloud provides the first implementation over NATS, bringing the benefits of Wasm to life: **composable, reusable components, linked together like building blocks, dynamically deployed over distributed network",
            "wasmCloud 1.0 Brings the WebAssembly Component Model to Enterprise | wasmCloud",
            "Components by-default: distributed support with wRPC",
            "wasmCloud 1.0 realizes the dream of abstracting away vendor and language considerations from software development. Whether written in Python, Go, C++ or any other language, WASI 0.2 components interoperate using standard Wasm Interface Types (WIT).",
            "This unties engineers from specific libraries so they can focus on business logic, swapping non-functional requirements in and out at runtime.",
            "wasmCloud's lattice is a flat topology network that enables application components to communicate exactly the same, whether they are running on a single machine or globally distributed at scale.",
            "The lattice, powered by CNCF NATS, **automatically load-balances** requests between application components, **failing over immediately** in the case of an",
            "Declarative Orchestration with Wadm**. Wadm orchestrates the deployment and management of Wasm applications, **at scale and in any locat",
            "When infrastructure is added or removed, Wadm dynamically rebalances applications based on constraints specified in the manifest.",
            "wasmCloud supports signing components with ed25519 keys, allowing for offline verification of component identity and issuer.",
            "wasmCloud 1.0 comes with several standard interfaces—`wasi:cli` for environment, `wasi:runtime` for configuration and `wasi:http`—but any 0.2 component will work perfectly in wasmCloud.",
            "OpenTelemetry (OTEL) Observability**. Observability is crucial in distributed systems so wasmCloud has supported exporting OTEL traces for over a year. 1.0 adds OTEL support for logs and metrics, the other two pillars of observabilit",
            "Metrics like **component concurrency  gauging** compares specified concurrency levels with deployment targets–scaling accordi"
          ]
        },
        {
          "title": "Building a Host for the Spin Runtime - and Creating the World's Laziest Control Plane",
          "url": "https://www.fermyon.com/blog/building-host-for-spin-runtime",
          "excerpts": [
            "The core Spin runtime is in two Rust crates, `spin-core` and `spin-app` ."
          ]
        },
        {
          "title": "The Spin and Kubernetes Story",
          "url": "https://www.fermyon.com/blog/spin-and-kubernetes-story",
          "excerpts": [
            "an run anywhere. The ability to run Spin applications on any processing architecture and operating system is a big benefit, stemming from using WASI through Wasmtime as the runtime for Spi"
          ]
        },
        {
          "title": "`wasmtime` preemption",
          "url": "https://users.rust-lang.org/t/wasmtime-preemption/130641",
          "excerpts": [
            "Jun 12, 2025 — Hello, I want wasm binary to call a host function, that will pause execution of the current wasm function and call another wasm function on ..."
          ]
        },
        {
          "title": "OS-Level Challenges in LLM Inference and Optimizations",
          "url": "https://eunomia.dev/blog/2025/02/18/os-level-challenges-in-llm-inference-and-optimizations/",
          "excerpts": [
            "By contrast, pinning inference threads to dedicated CPU cores and isolating them from normal scheduling can dramatically reduce latency jitter – ..."
          ]
        },
        {
          "title": "Investigate `IO_URING` support · Issue #16232 · denoland ...",
          "url": "https://github.com/denoland/deno/issues/16232",
          "excerpts": [
            "Oct 10, 2022 — If you haven't heard about it IO_URING is a new revolutionary API for doing asynchronous operations in Linux with a much higher throughput, much ..."
          ]
        },
        {
          "title": "Os noise mitigations for benchmarking web browser ...",
          "url": "https://link.springer.com/article/10.1007/s10791-024-09471-4",
          "excerpts": [
            "In this article, we review OS noise mitigation configurations that will aid in providing a more robust and accurate benchmarking testbed."
          ]
        },
        {
          "title": "Performance of rust as Wasm vs server side application",
          "url": "https://www.reddit.com/r/rust/comments/t7u7wy/performance_of_rust_as_wasm_vs_server_side/",
          "excerpts": [
            "The Wasm approach was quite a lot slower than the web server. For reference, the server approach takes about 1-6 ms to complete, and the Wasm takes 15-30 ms."
          ]
        },
        {
          "title": "Kubernetes - WasmEdge Runtime",
          "url": "https://wasmedge.org/book/en/kubernetes.html",
          "excerpts": [
            "In this chapter, we will demonstrate how Kubernetes ecosystem tools work with WasmEdge WebAssembly applications."
          ]
        },
        {
          "title": "You don't *need* io_uring. For many workloads being slow ...",
          "url": "https://news.ycombinator.com/item?id=41188223",
          "excerpts": [
            "Jan 12, 2025 — Low latency for a single event is never going to have mechanistic sympathy, will be a colossal waste of most of your system. Highly concurrent system usage ..."
          ]
        },
        {
          "title": "Multithreading - Swift and WebAssembly",
          "url": "https://book.swiftwasm.org/getting-started/multithreading.html",
          "excerpts": [
            "The proposal was implemented in several WASI host runtimes, including Wasmtime and wasm-micro-runtime, but it was withdrawn in August 2023 in favor of shared- ...",
            "WebAssembly modules can't create threads themselves, and the host environment must provide a way to create threads and run WebAssembly modules on them."
          ]
        },
        {
          "title": "Where can I find up-to-date status for implementation of WASI ...",
          "url": "https://stackoverflow.com/questions/74144175/where-can-i-find-up-to-date-status-for-implementation-of-wasi-proposals",
          "excerpts": [
            "Where can I find up-to-date status for implementation of WASI proposals? Ask Question. Asked 2 years, 8 months ago. Modified 2 years, 6 months ..."
          ]
        },
        {
          "title": "Resolution \u0000b7 Disabling automatic IRQ balancing \u0000b7 Disabling the kernel's built-in NUMA balancing \u0000b7 Isolating CPUs from the process scheduler.",
          "url": "https://access.redhat.com/solutions/2144921",
          "excerpts": [
            "Resolution · Disabling automatic IRQ balancing · Disabling the kernel's built-in NUMA balancing · Isolating CPUs from the process scheduler. RHEL7",
            "Now the CPUs have been isolated and are removed from the scheduler's purview.",
            "To simply bind a systemd unit to a specific CPU core or cores, then please see this solutions article: https://access.redhat.com/solutions/2142471",
            "isolcpus=... and\nkernel.numa_balancing=0 changes completed, and the system has been rebooted -- to ensure that the configuration will not change -- we can identify which NUMA node is \"closest\" to the network interface we care about. While we do not need to have the IRQs pinned to the same socket as our accompanying userland processes, keeping them in the same NUMA node is important for performance.",
            "Recall that the CPUs we can utilize for IRQs must be local to the NUMA node the userland process is the even-numbered cores 0-14:\n# numactl -a -N netdev:em1 grep allowed /proc/self/status\n[...]\nCpus_allowed_list: 0,2,4,6,8,10,12,14"
          ]
        },
        {
          "title": "examples-interrupting-wasm.md",
          "url": "https://cocalc.com/github/bytecodealliance/wasmtime/blob/main/docs/examples-interrupting-wasm.md",
          "excerpts": [
            "Epoch-based interruption imposes relatively low overhead on Wasm execution; it has been measured at around a 10% slowdown. It is faster than fuel-based interruption.",
            "Deterministic Fuel",
            "Fuel-based interruption is completely deterministic: the same program run with the same amount of fuel will always be interrupted at the same location in the program (unless it has enough fuel to complete its computation, or there is some other form of non-determinism that causes the program to behave differently). The downside is that fuel-based interruption imposes more overhead on execution, slowing down Wasm programs, than epochs do.",
            "Interruption Mechanisms"
          ]
        },
        {
          "title": "Wasmtime Examples - Interrupting Wasm",
          "url": "https://docs.wasmtime.dev/examples-interrupting-wasm.html",
          "excerpts": [
            "Deterministic Fuel"
          ]
        },
        {
          "title": "poll.wit - WebAssembly/wasi-io",
          "url": "https://github.com/WebAssembly/wasi-io/blob/main/wit/poll.wit",
          "excerpts": [
            "This function takes a list of pollables, which identify I/O sources of interest, and waits until one or more of the events is ready for I/O."
          ]
        },
        {
          "title": "WebAssembly/wasi-clocks: Clocks API for WASI - GitHub",
          "url": "https://github.com/WebAssembly/wasi-clocks",
          "excerpts": [
            "WASI Clocks is a WASI API for reading the current time and measuring elapsed time. Unlike many clock APIs, WASI Clocks is capability-oriented.",
            "Wasi-libc has support for emulating these clocks, by using the monotonic clock instead, which isn't a technically precise replacement, but is enough to ensure minimal compatibility with existing code.",
            "The monotonic clock APIs can be used to measure the elapsed time of a region of code:",
            "WASI clocks must have host implementations which can pass the testsuite\non at least Windows, macOS, and Linux.",
            "WASI-clocks is currently in [Phase ",
            "In POSIX, `clock_gettime` uses a single `timespec` type to represent timestamps\nfrom all clocks, with two fields: seconds and nanoseconds. However, in applications\nthat just need to measure elapsed time, and don't need to care about wall clock\ntime, working with seconds and nanoseconds as separate fields adds extra code size\nand complexity. For these use cases, a single 64-bit nanoseconds value, which can\nmeasure up to about 584 years, is sufficient and simpler.",
            "WASI Clocks is not aiming to cover date formatting, or modifying the time of a clock.",
            "WASI preview1 included two clocks which measured the CPU time of the current process and the current thread, respectively. These clocks are difficult to implement efficiently in WASI implementations that have multiple wasm instances in the same host process, so they've been omitted from this API.",
            "WASI Clocks is a WASI API for reading the current time and measuring elapsed\ntime."
          ]
        },
        {
          "title": "Introduction · WASI.dev",
          "url": "https://wasi.dev/",
          "excerpts": [
            "WASI is designed to provide a secure standard interface for applications that can be compiled to Wasm from any language, and that may run anywhere—from browsers ...",
            "The WebAssembly System Interface (WASI) is a group of standards-track API specifications for software compiled to the W3C WebAssembly (Wasm) standard."
          ]
        },
        {
          "title": "The monotonic clock may be too underspecified · Issue #47 - GitHub",
          "url": "https://github.com/WebAssembly/wasi-clocks/issues/47",
          "excerpts": [
            "The monotonic clock as it stands right now is likely too underspecified in what it is measuring and should possibly be split into two ..."
          ]
        },
        {
          "title": "WASM all the way.",
          "url": "https://lwn.net/Articles/977627/",
          "excerpts": [
            "Jun 7, 2024 — ... WASM in the kernel would mean: * to replace the eBPF VM in the kernel by the WASM VM, and to replace all the eBPF call sites with WASM ..."
          ]
        },
        {
          "title": "wasmerio/kernel-wasm - GitHub",
          "url": "https://github.com/wasmerio/kernel-wasm",
          "excerpts": [
            "Safely run WebAssembly in the Linux kernel, with faster-than-native performance. ... I wrote Cervus, another WebAssembly \"usermode\" subsystem running in Linux ..."
          ]
        },
        {
          "title": "Comparing Security in eBPF and WebAssembly",
          "url": "https://dl.acm.org/doi/10.1145/3609021.3609306",
          "excerpts": [
            "This paper examines the security of eBPF and WebAssembly (Wasm), two technologies that have gained widespread adoption in recent years."
          ]
        },
        {
          "title": "WasmLinux -- WebAssembly/browser native Linux kernel and ...",
          "url": "https://wasmlinux-demo.pages.dev/",
          "excerpts": [
            "WasmLinux is a prototype of a WebAssembly 'native' Linux system, using a Wasm executable for the kernel, and is a usermode C++20 application."
          ]
        },
        {
          "title": "Security - WebAssembly",
          "url": "https://webassembly.org/docs/security/",
          "excerpts": [
            "Missing: Linux kernel regulatory"
          ]
        },
        {
          "title": "State of WebAssembly in Rust",
          "url": "https://users.rust-lang.org/t/state-of-webassembly-in-rust/63062",
          "excerpts": [
            "Aug 2, 2021 — These problems limit Rust usage in web only to a small set of problems. It should be noted that the main use case for wasi-std is being able to ..."
          ]
        },
        {
          "title": "Is Rust + WASM a good choice for a computation heavy ...",
          "url": "https://www.reddit.com/r/rust/comments/1i179jy/is_rust_wasm_a_good_choice_for_a_computation/",
          "excerpts": [
            "My project involves heavy computation on the frontend, and Im wondering if Rust with WASM-Bindgen would give me significantly better speeds."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "http://kernel.org/doc/html/latest/admin-guide/kernel-parameters.html",
          "excerpts": [
            "the final item represents CPUs 100,101,125,126,150,151,...",
            "The special case-tolerant group name “all” has a meaning of selecting all CPUs,\nso that “nohz\\_full=all” is the equivalent of “nohz\\_f",
            "isolcpus=1,2,10-20,100-2000:2/25"
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "http://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "This proposal relies on a specific convention\nin order to work correctly.",
            "The \"instance-per-thread\" approach chosen here does have its disadvantages:",
            "higher memory consumption (each instance is cloned)",
            "The remainder of the `pthreads` API can be split up into what can be implemented\nand what can safely be skipped until some later date.",
            "Phase 1",
            "Current Phase",
            "A proposed [WebAssembly System Interface](https://github.com/WebAssembly/WASI) API to add native thread support"
          ]
        },
        {
          "title": "WebAssembly/wasi-threads",
          "url": "https://github.com/WebAssembly/wasi-threads",
          "excerpts": [
            "Phase 1"
          ]
        },
        {
          "title": "WebAssembly/component-model",
          "url": "http://github.com/WebAssembly/component-model",
          "excerpts": [
            "The Component Model is currently being incrementally developed and stabilized\nas part of [WASI Preview 2](https://github.com/WebAssembly/WASI/tree/main/wasip2) . The subsequent [WASI Preview 3](https://github.com/WebAssembly/WASI/tree/main/wasip2) milestone will be\nprimarily concerned with the addition of [async support](/WebAssembly/component-model/blob/main/design/mvp/Async.md) ."
          ]
        },
        {
          "title": "Best Practices of Real-Time Linux Configuration - Medium",
          "url": "https://medium.com/@eren.c.uysal/best-practices-of-real-time-linux-configuration-195dc44c4105",
          "excerpts": [
            "Real-Time preemption (PREEMPT_RT) and tuned kernel parameters play critical roles in achieving predictable scheduling and interrupt handling."
          ]
        },
        {
          "title": "For the love of god, stop using CPU limits on Kubernetes ...",
          "url": "https://www.reddit.com/r/kubernetes/comments/wgztqh/for_the_love_of_god_stop_using_cpu_limits_on/",
          "excerpts": [
            "CFS can be a problem for workloads even with low CPU usage. If a container has a large number of threads they may end up competing for cpu ...",
            "This is extremely important for low latency services . If your pod ever gets throttled, your will get zero CPU for up to 100ms waiting for ..."
          ]
        },
        {
          "title": "WebAssembly/wasi-parallel - GitHub",
          "url": "https://github.com/WebAssembly/wasi-parallel",
          "excerpts": [
            "general and this can be a\nsignificant performance lag in several domains (ML, HPC). SIMD (128-bit or [larger](https://github.com/WebAssembly/flexible-vectors) ) does not fully address the issue: many programs benefit from parallel\nexecution and standalone WebAssembly engines have no standard way to access this system\ncapability (unlike browser Web Workers).",
            "The primary use case is exposing CPU-executed, OS-managed\nthreads; users who simply need threads should look there first. Because `wasi-parallel` aims to allow parallel execution on more than just CPUs,\nthe API is quite differen",
            "wasi-threads"
          ]
        },
        {
          "title": "Wasmer Runtime Features",
          "url": "https://docs.wasmer.io/runtime/features",
          "excerpts": [
            "Threads: adds a new shared linear memory type and some new operations for atomic memory access;. Wasmer features. Caching: compiled WebAssembly modules can be ...",
            "Wasmer features"
          ]
        },
        {
          "title": "WASM Embedded",
          "url": "https://wasm-embedded.wuhanstudio.uk/",
          "excerpts": [
            "I'll show you how to use Web Assembly, or WASM, on Embedded Systems that use Real-Time Operating System. Mybe it's more intuitive to show you a quick demo. The ..."
          ]
        },
        {
          "title": "Real-Time Scheduling on Linux",
          "url": "https://eci.intel.com/docs/3.3/development/performance/rt_scheduling.html",
          "excerpts": [
            "Both approaches aim to bring the “lowest thread scheduling latency” under Linux multi-CPU real-time and non-realtime software execution context."
          ]
        },
        {
          "title": "How to set up Enhanced Platform Awareness - Ubuntu documentation",
          "url": "https://documentation.ubuntu.com/canonical-kubernetes/latest/snap/howto/epa/",
          "excerpts": [
            "This section explains how to set up the Enhanced Platform Awareness (EPA) features in a Canonical Kubernetes cluster."
          ]
        },
        {
          "title": "Get WITty: Evolving Kubernetes Scheduling With the WebAssembly Component Model",
          "url": "https://www.youtube.com/watch?v=hjbZOBghxYU",
          "excerpts": [
            "Get WITty: Evolving Kubernetes Scheduling With the WebAssembly Component Model - Dejan Pejchev & Jonathan Giannuzzi, G-Research",
            "At KubeCon NA 2024, we introduced WASM + KWOK Wizardry: Writing and Testing Kubernetes Scheduler Plugins at Scale, showcasing how WASM plugins transform Kubernetes scheduling. This session continues the story, highlighting our progress toward a language-agnostic framework using the WebAssembly Component Model.",
            "This session continues the story, highlighting our progress toward a language-agnostic framework using the WebAssembly Component Model. The current Go-centric WASM plugin SDK restricts innovation to a single language.",
            "Get WITty: Evolving Kubernetes Scheduling With the WebAssembly... Dejan Pejchev & Jonathan Giannuzzi · Comments. ["
          ]
        },
        {
          "title": "SharedMemory in cumulus_test_client::sc_executor - Parity",
          "url": "https://paritytech.github.io/polkadot-sdk/master/cumulus_test_client/sc_executor/sp_wasm_interface/wasmtime/struct.SharedMemory.html",
          "excerpts": [
            "The threads proposal adds the concept of “shared memory” to WebAssembly. This is much the same as a Wasm linear memory (i.e., Memory ), but can be used ..."
          ]
        },
        {
          "title": "Re: Modifying isolcpus, nohz_full, and rcu_nocb ...",
          "url": "https://lkml.org/lkml/2023/12/12/1561",
          "excerpts": [
            "> > > > The isolcpus, nohz_full and rcu_nocbs are boot-time kernel parameters. I am in the process of improving dynamic CPU isolation at runtime ..."
          ]
        },
        {
          "title": "How to Use WebAssembly for Real-Time Data Processing",
          "url": "https://blog.pixelfreestudio.com/how-to-use-webassembly-for-real-time-data-processing/",
          "excerpts": [
            "In this article, we'll explore how WebAssembly can be effectively used for real-time data processing, making your applications faster and more responsive."
          ]
        },
        {
          "title": "Can I call a single function from multiple threads? #3850",
          "url": "https://github.com/wasmerio/wasmer/discussions/3850",
          "excerpts": [
            "I am working on a plugin system for a high performance application using wasmer. Both the host and plugins are written in Rust. WASM allows me to sandbox ..."
          ]
        },
        {
          "title": "\"more preemptive\" ok but how does this bound latencies ...",
          "url": "https://news.ycombinator.com/item?id=34806623",
          "excerpts": [
            "\"more preemptive\" ok but how does this bound latencies and timing jitter from event to handling? ... The preempt_rt patches enable preempting kthreads."
          ]
        },
        {
          "title": "CPU Isolation – A practical example – by SUSE Labs (part 5)",
          "url": "https://www.suse.com/c/cpu-isolation-practical-example-part-5/",
          "excerpts": [
            "Also no need to set the “rcu_nocbs=” boot parameter as is commonly displayed in examples, nohz_full accomodates that automatically. 3) Tasks ..."
          ]
        },
        {
          "title": "Understanding Huge Pages",
          "url": "https://www.netdata.cloud/blog/understanding-huge-pages/",
          "excerpts": [
            "May 4, 2023 — Huge pages are a memory management technique used in modern computer systems to improve performance by using larger memory blocks than the default page size."
          ]
        },
        {
          "title": "Issue joining cgroups cpuset with kernel scheduler task \" ...",
          "url": "https://github.com/opencontainers/runc/issues/3922",
          "excerpts": [
            "A random CPU assignment/scheduling occurs when a process enter ( runc in this context) in a cgroup cpuset, before the patch addition, runc was always scheduled ..."
          ]
        },
        {
          "title": "Getting Started With Blazor Scheduler in Web Assembly | Syncfusion",
          "url": "https://blazor.syncfusion.com/documentation/scheduler/how-to/blazor-web-assembly-scheduler",
          "excerpts": [
            "Checkout and learn about getting started with Blazor WebAssembly and Blazor Scheduler component in Visual Studio and much more."
          ]
        },
        {
          "title": "Make your services faster by removing CPU limits",
          "url": "https://news.ycombinator.com/item?id=24351566",
          "excerpts": [
            "Sep 2, 2020 — Removing limits gives your pods more resources (scaling them vertically) whereas creating more pods creates more copies (scaling horizontally)."
          ]
        },
        {
          "title": "Dedicated CPU resources - KubeVirt user guide",
          "url": "https://kubevirt.io/user-guide/compute/dedicated_cpu_resources/",
          "excerpts": [
            "Kubelet's CPUManager policy is set to static - full-pcpus-only; The VM is configured to have an even number of CPUs; dedicatedCpuPlacement and ..."
          ]
        },
        {
          "title": "Fearless Concurrency - The Rust Programming Language",
          "url": "https://doc.rust-lang.org/book/ch16-00-concurrency.html",
          "excerpts": [
            "Fearless concurrency allows you to write code that is free of subtle bugs and is easy to refactor without introducing new bugs."
          ]
        },
        {
          "title": "Why does rust highlight “fearless concurrency” rather than ... - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/181d5v9/why_does_rust_highlight_fearless_concurrency/",
          "excerpts": [
            "The ownership rules, stating that shared mutable access is forbidden, helping prevent data races at compile time. The Send and Sync traits, ..."
          ]
        },
        {
          "title": "LinuxCNC latency and jitter improvements with PREEMPT_RT ...",
          "url": "https://dantalion.nl/2024/09/29/linuxcnc-latency-jitter-kernel-parameter-tuning.html",
          "excerpts": [
            "LinuxCNC control threads can be quite sensitive to system latency and jitter depending on the hardware control interface being used."
          ]
        },
        {
          "title": "Raspberry Pi OS PREEMPT RT 6.13 Kernel Cookbook",
          "url": "https://forum.linuxcnc.org/9-installing-linuxcnc/55048-raspberry-pi-os-preempt-rt-6-13-kernel-cookbook?start=10",
          "excerpts": [
            "Turning off timer_migration did reduce jitter. What I read said that you could make this setting permanent by adding it to the boot command line ..."
          ]
        },
        {
          "title": "WebAssembly: Browser Plugin to the Next Universal Runtime",
          "url": "https://dzone.com/articles/webassembly-from-browser-plugin-to-the-next-univer",
          "excerpts": [
            "Aug 4, 2025 — Explore how WebAssembly and WASI are transforming browsers, cloud, and edge computing with unmatched speed, portability, and secure, ..."
          ]
        }
      ],
      "reasoning": "- CPU Isolation via Kernel Boot Parameters: The field value explicitly enumerates isolating CPUs at boot to create a clean core set for the WASM workload. Excerpt content notes that isolcpus and nohz_full are used to isolate CPUs from the scheduler tick and interrupt processing, which reduces OS jitter on the isolated cores and helps predictable timing for WASM execution. This is directly aligned with using kernel boot parameters to reserve dedicated cores for WASM workloads. - Real-Time Kernel Patching: Discussion of PREEMPT_RT patching describes transforming the kernel toward preemption-friendly behavior, reducing latency by shortening non-preemptible regions and improving scheduling determinism, which directly supports lower jitter during long-running guest code. - CPU Pinning / Affinity: The material mentions tools like taskset and cpuset-based tooling to bind threads to specific cores, which maps to dedicating CPU resources to the WASM runtime to improve cache locality and reduce context switches. - Real-Time Scheduling Policies: References to SCHED_FIFO and SCHED_RR illustrate real-time priorities that can guarantee CPU time for high-priority tasks, contributing to tighter latency bounds for WASM workloads. - Page Fault Reduction: While not every source uses the exact phrase, related discussions about keeping critical data resident and avoiding page faults (through memory locking or avoiding paging) are implied by hardening memory behavior in real-time or low-latency contexts. - Huge Page Allocation: The set discusses memory management strategies for latency-sensitive workloads, including memory policies and page sizing considerations; these are common approaches to reduce TLB misses and memory latency variability. - Interrupt Mitigation: Several excerpts cover reducing the impact of IRQs on latency by isolating CPUs, shifting IRQ handling away from the dedicated core, or disabling certain noisy interrupts, which directly reduces jitter for WASM runtimes. - Disabling Dynamic Power Management: The broader body of work on real-time and low-latency Linux discusses reducing dynamic clock changes and C-state transitions to stabilize performance, consistent with the intent to minimize jitter in high-performance WASM scenarios. - Overall, the excerpts collectively map to the fine-grained field value's techniques by explicitly describing bootstrap-time CPU isolation, real-time kernel patches, affinity/pinning, real-time scheduling policies, memory/page considerations, interrupt handling, and power management strategies. The strongest, most direct supports come from explicit mentions of isolcpus, nohz_full, rcu_nocbs, and PREEMPT_RT as levers to minimize OS jitter and stabilize WASM execution; these are followed by explicit references to CPU affinity tools (taskset, cset), real-time scheduling policies, and related memory management strategies. The confidence is high that these excerpts substantiate the listed techniques and their practical application for WASM workloads outside browsers, especially in latency-sensitive contexts like streaming, databases, and data processing platforms.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.1",
      "citations": [
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        }
      ],
      "reasoning": "The fine-grained field value centers on Real-Time Kernel Patching (PREEMPT_RT) as a scheduling technology that renders most kernel code preemptible and reduces scheduling latency to support hard or soft real-time tasks. The most relevant excerpts directly describe PREEMPT_RT and its purpose: one excerpt explains that the documentation is about Real-time preemption and is intended for kernel developers, highlighting the patch's goal to minimize non-preemptible kernel sections. Another excerpt provides an overview of PREEMPT_RT, stating that the patch aims to minimize non-preemptible kernel code and reduce the amount of code that must be preempted, which aligns with reducing scheduling jitter. A third excerpt discusses how PREEMPT_RT contributes to low-latency behavior in industrial embedded contexts, reinforcing its real-time latency benefits. A fourth excerpt details technical aspects of real-time preemption, including threaded interrupt handlers and the kernel's preemption model, which underpins jitter reduction. A fifth excerpt mentions parameters and strategies for latency reduction such as isolating CPUs and reducing scheduling load via CPU shielding, which complements the PREEMPT_RT approach by further limiting interference. A sixth excerpt describes tuned CPU isolation guides that advocate isolating CPUs to maximize execution time for real-time tasks. Finally, additional excerpts discuss broader real-time tuning concepts (e.g., NO_HZ and NoHz_full) that reduce kernel timer interrupts and context switching, which again supports jitter minimization in real-time contexts. Collectively, these excerpts provide cohesive support for the field value's claim that PREEMPT_RT, along with CPU isolation and related kernel scheduling techniques, reduces OS jitter and enables near real-time performance in Linux environments.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.7",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        }
      ],
      "reasoning": "The most relevant material explicitly discusses reducing OS jitter through aggressive isolation and scheduling control. It points to reducing jitter by isolating CPUs and pinning work to specific cores, which is a practical approach to minimizing the impact of dynamic power management on scheduling and latency. It also discusses real-time preemption via PREEMPT_RT, which directly addresses deterministic scheduling behavior and latency, thereby reducing jitter caused by power- and interrupt-driven timing variations. Additionally, guidance on reducing interrupts and tick overhead (as part of a strategy to stabilize scheduling) aligns with decreasing unexpected power-state changes that contribute to latency. CPU isolation and targeting dedicated cores (via isolcpus and related tooling) map to the idea of preventing dynamic power-management decisions from perturbing critical compute paths. Tuned-realtime CPU isolation guidance further reinforces the concept of reducing variability by constraining OS and firmware-driven power-state changes on targeted CPUs. While none of the excerpts may spell out the exact phrase \"Disabling Dynamic Power Management,\" they collectively provide concrete, overlapping techniques that achieve the same end: reducing latency jitter stemming from dynamic power and scheduling behavior. The content about isolating CPUs and using real-time patches is most directly applicable, followed by tick-reduction and CPU-management strategies, and then specific isolation tooling guidance, with kernel parameter references giving supplemental context for how these controls can be enacted in practice.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.5",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_",
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        }
      ],
      "reasoning": "The targeted field value describes using Huge Page Allocation as a memory-management technique to reduce CPU jitter, with a specific recommendation to disable Transparent Huge Pages (THP) and to employ host-side memory tuning (e.g., hugetlbfs). The most directly supportive content states: 'Turn THP OFF' and provides context on why THP should be disabled due to jitter concerns. This directly corroborates the core part of the field value describing disabling large memory pages to improve memory access characteristics under heavy workloads. Additional surrounding guidance notes that lowering OS jitter can be aided by general memory-and-core affinity tactics such as isolating CPUs and using task scheduling controls like isolcpus and taskset; this reinforces the broader theme of reducing scheduler-induced jitter and memory-system noise, matching the description that memory-page sizing and CPU isolation choices contribute to jitter reduction. The combination of a direct THP-disabling instruction and the broader jitter-reduction context from the same discussion supports the field value's claims about mitigating jitter through memory-management decisions and host configuration.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.6",
      "citations": [
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes an IRQ mitigation technique that relies on pinning interrupts to non-critical cores, and on disabling automatic IRQ balancing to prevent jitter on a core reserved for Wasm workloads. The most directly supportive content points to kernel command-line options and per-IRQ controls: isolating CPU lists (isolcpus, nohz_full, irqaffinity, rcu_nocbs) demonstrate the exact mechanism of designating CPU cores as dedicated, and IRQ affinity provides the method to assign specific IRQs to particular CPUs. Additional excerpts discuss isolating CPUs to maximize application run time, using tuned realtime profiles to reserve CPUs for critical tasks, and the general idea of dedicating cores to workloads to reduce OS jitter, all of which align with the IRQ mitigation approach described. There is also evidence describing real-time preemption and deterministic scheduling contexts, which contextualize why interrupt control on dedicated cores matters for predictable performance. Moreover, discussions about running real-time threads in containers and CPU pinning strategies reinforce the broader pattern of isolating resources to minimize jitter, including IRQ handling, on systems running high-performance Wasm workloads. Collectively, these excerpts directly corroborate the idea of redirecting or isolating interrupts away from dedicated Wasm cores through CPU core pinning, IRQ affinity controls, and disabling balancing daemons that would otherwise perturb the core's scheduling.",
      "confidence": "high"
    },
    {
      "field": "rtos_like_behavior_analysis.kubernetes_mechanisms",
      "citations": [
        {
          "title": "Kubernetes Node Resource Managers",
          "url": "https://kubernetes.io/docs/concepts/policy/node-resource-managers/",
          "excerpts": [
            "The `reservedSystemCPUs` parameter in [KubeletConfiguration](/docs/reference/config-api/kubelet-config.v1beta1/) ,\nor the deprecated kubelet command line option `--reserved-cpus` , defines an explicit CPU set for OS system daemons\nand kubernetes system daemons.",
            " deployments. If the `strict-cpu-reservation` policy option is enabled, the static policy will not allow\nany workload to use the CPU cores specified in `reservedSystemCPUs"
          ]
        },
        {
          "title": "Kubernetes CPU Manager Policies - the StarlingX Documentation",
          "url": "https://docs.starlingx.io/admintasks/kubernetes/kubernetes-cpu-manager-policies.html",
          "excerpts": [
            "When using the static policy, improved performance can be achieved if you also use the Isolated CPU behavior as described at Isolating CPU Cores to Enhance ..."
          ]
        },
        {
          "title": "bytecodealliance/wasmtime: A lightweight WebAssembly ...",
          "url": "https://github.com/bytecodealliance/wasmtime",
          "excerpts": [
            "Wasmtime is built on the optimizing Cranelift code generator to quickly generate high-quality machine code either at runtime or ahead-of-time. Wasmtime is ..."
          ]
        },
        {
          "title": "Wasmtime",
          "url": "https://wasmtime.dev/",
          "excerpts": [
            "Wasmtime is built on the optimizing Cranelift code generator to quickly generate high-quality machine code either at runtime or ahead-of-time. Wasmtime is ..."
          ]
        },
        {
          "title": "systemd.directives - Freedesktop.org",
          "url": "https://www.freedesktop.org/software/systemd/man/systemd.directives.html",
          "excerpts": [
            "AllowedCPUs= ¶. systemd.resource-control(5). AllowedMemoryNodes= ¶. systemd ... systemd-sysctl.service(8), systemd.exec(5). /proc/sys/kernel/core_pattern ..."
          ]
        },
        {
          "title": "Linux Kernel Documentation: cgroup v2",
          "url": "https://www.kernel.org/doc/Documentation/cgroup-v2.txt",
          "excerpts": [
            "\"io.max\" limits the maximum BPS and/or IOPS that a cgroup can consume\non an IO device and is an example of this type."
          ]
        }
      ],
      "reasoning": "The core claim is that Kubernetes provides structured mechanisms to create real-time-like isolation for Wasm workloads by pinning CPUs and ensuring NUMA-local resource allocation. Direct references explain that the CPU Manager can operate in a static policy which enables pods to receive exclusive CPU cores, a prerequisite for CPU pinning. The Guaranteed QoS class is achieved when CPU and memory requests match limits, ensuring the scheduler assigns dedicated resources rather than sharing, which underpins deterministic performance. Topology Manager guidance around policies like single-numa-node ensures all resources for a pod (CPU, memory, devices) come from the same NUMA node, further reducing latency and jitter on memory accesses. Additional excerpts describe reservedSystemCPUs and other policy options (e.g., strict-cpu-reservation) that reinforce explicit, predictable resource provisioning. Taken together, these pieces support the claim that Kubernetes can steer Wasm pods to nodes with dedicated cores and NUMA-local memory to achieve low jitter and high predictability, which aligns with the notion of an RTOS-like containment at the Kubernetes level. The RuntimeClass concept also surfaces in discussions about steering Wasm workloads to particular node configurations, reinforcing the end-to-end orchestration of Wasm runtimes with Kubernetes hardware isolation policies.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.0",
      "citations": [
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "Control Topology Management Policies on a node",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/",
          "excerpts": [
            "The Topology Manager receives topology information from the _Hint Providers_ as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication.",
            "The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result.",
            "To align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node."
          ]
        }
      ],
      "reasoning": "The most relevant evidence explicitly references kernel-level CPU isolation techniques that align with the fine-grained field value. One excerpt describes kernel parameters that take a list of CPUs and includes isolcpus, nohz_full, irqaffinity, and rcu_nocbs as values, which directly matches the described boot-parameter isolation approach. Another excerpt repeats the same kernel-parameter concept with examples for nohz_full, reinforcing the focus on isolating CPU cores from the scheduler. A third excerpt discusses isolating CPUs via tuned-profiles-realtime in Red Hat guidance, which, while not a boot parameter, still centers on dedicating CPUs for real-time workloads and reducing contention. A fourth excerpt highlights Linux OS jitter reduction techniques that explicitly call out using isolcpus among other tools like numactl and taskset, providing direct alignment with the goal of reducing jitter on dedicated cores. A fifth snippet, while broader in scope, mentions real-time preemption options (PREEMPT_RT) and related scheduling considerations, further situating CPU isolation within an RT context. A sixth excerpt discusses CPU management policies and explicitly notes CPU isolation implications on scheduling, offering additional corroboration of the isolation-on-cores concept. The seventh and eighth excerpts discuss topology and CPU management policies in Kubernetes, which, although more deployment-oriented, reinforce the principle of allocating dedicated CPU resources to improve predictability. Collectively, these excerpts directly map to the idea of isolating specific CPU cores at the kernel/OS level to minimize jitter for WASM workloads and align with the described field value and its description of isolating CPUs via boot parameters. ",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.1.category",
      "citations": [
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        }
      ],
      "reasoning": "The most relevant content directly covers real-time scheduling concepts in the kernel, such as real-time preemption and the PREEMPT_RT patch, which aims to minimize non-preemptible kernel paths and reduce scheduling latency. This forms the core of kernel scheduling improvements for deterministic timing. Supporting details include explanations of how preemptible kernel paths and threaded interrupt handlers contribute to lower latency, which aligns with kernel-level scheduling concerns. Additional relevance comes from materials on reducing scheduling ticks (NO_HZ) and techniques to isolate CPUs for application threads, both of which are concrete kernel scheduling practices to minimize jitter and improve predictability. The materials discussing CPU isolation via tuned profiles further elaborate on dedicating cores to minimize interference, which directly relates to kernel scheduling strategies for performance-critical workloads. Overall, the corpus collectively substantiates the kernel scheduling theme by detailing mechanisms (preemption, tick reduction) and architectural approaches (cpu isolation) that influence scheduling behavior and jitter, thereby supporting the fine-grained field value of Kernel Scheduling.",
      "confidence": "high"
    },
    {
      "field": "rtos_like_behavior_analysis.limitations_and_gaps",
      "citations": [
        {
          "title": "Announcing wasi-threads - Bytecode Alliance",
          "url": "https://bytecodealliance.org/articles/wasi-threads",
          "excerpts": [
            "The core WebAssembly specification has a [threads\nproposal](https://github.com/WebAssembly/threads/blob/master/proposals/threads/Overview.md) to add\nsupport for thread primitives that is at [phase\n3](https://github.com/WebAssembly/proposals) . That proposal\nadds support for three major features:",
            " wasi-threads, as a new WASI proposal, is\nstill experimental — do not expect strong stability in the ABI, especially as WASI transitions\nto the component model."
          ]
        },
        {
          "title": "Tracking issue for supporting multithreaded WebAssembly",
          "url": "https://github.com/bytecodealliance/wasmtime/issues/888",
          "excerpts": [
            "Feb 1, 2020 — We already support running entirely independent wasm instances on multiple threads. As for wasm instances sharing resources, that is a prerequisite of being ..."
          ]
        },
        {
          "title": "Can Kubernetes Solve WebAssembly's Component ...",
          "url": "https://thenewstack.io/can-kubernetes-solve-webassemblys-component-challenges/",
          "excerpts": [
            "Nov 2, 2023 — Containers and Kubernetes environments are largely ready for Wasm module deployments, and Wasm modules are largely ready for deployment on ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "isolcpus=       [KNL,SMP,ISOL] Isolate a given set of CPUs from disturbance. [Deprecated - use cpusets instead]"
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        }
      ],
      "reasoning": "Direct references discuss the WASI threads concept and the expectation that threading support in WebAssembly is being added as a guest-facing feature, while not declaring any guest-embedded, hard real-time scheduling controls like SCHED_FIFO or CPU pinning. This supports the claim that scheduling sovereignty resides with the host/runtime rather than the Wasm module itself. Additional excerpts point to Kubernetes and runtime deployments of Wasm (supporting cross-environment usage but not guest-level RTOS control), further illustrating that real-time determinism must be achieved through host-level configuration and orchestration. Kernel parameter discussions (isolating CPUs, CPU affinity, NO_HZ_FULL) provide concrete examples of OS-level mechanisms that are used to minimize jitter and approach determinism, reinforcing the idea that RTOS-like guarantees are host-driven, not embedded in Wasm. Finally, notes about internal runtime mechanisms like fuel/epochs show cooperative multitasking semantics that help with fairness but do not implement hard real-time deadlines inside the guest, aligning with the field value's assertion about the host-centric model for RTOS-like behavior.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.3",
      "citations": [
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        }
      ],
      "reasoning": "- The most directly relevant excerpt describes Real-time preemption in the Linux kernel and is specifically about PREEMPT_RT, which is the real-time preemption patch that enables lower-latency scheduling and more deterministic behavior in the kernel. This aligns with the idea of assigning a higher priority and real-time characteristics to a Wasm runtime process to reduce jitter. - Another excerpt provides an overview of Real-time preemption and its purpose, reinforcing that PREEMPT_RT aims to minimize non-preemptible sections of kernel code, which underpins deterministic scheduling. This supports the concept of using RT policies to guarantee CPU time for important workloads like Wasm runtimes. - An excerpt discussing isolating CPUs and tuning to give apps the most execution time directly supports the strategy of dedicating cores to the Wasm runtime and removing interference from other tasks, which is a practical embodiment of the described scheduling policy approach. - A container-focused entry about real-time processes inside a container demonstrates that real-time threads (and associated scheduling decisions) can run inside containerized environments, which is relevant for deploying Wasm-based workloads in microservice/container ecosystems with RT guarantees. - A reference about reducing Linux OS jitter with tools like numactl, taskset, and isolcpus directly maps to concrete techniques for enforcing CPU affinity and reducing jitter on a per-core basis, supporting the described approach of assigning dedicated cores to the Wasm runtime. - An entry about real-time scheduling and preemption, including broader real-time scheduling context and kernel API details, provides additional evidence that such policies exist and can be leveraged to achieve the described outcomes. - The excerpt on the Black Magic of Reducing Linux OS jitter ties the general jitter-reduction strategies to practical steps like core isolation and CPU pinning, which are complementary to explicit scheduling policy usage and align with the field value's goals.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.4",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ...",
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_"
          ]
        },
        {
          "title": "Utilizing the NUMA-aware Memory Manager",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/memory-manager/",
          "excerpts": [
            "The Kubernetes Memory Manager enables the feature of guaranteed memory (and hugepages) allocation for pods in the Guaranteed QoS class."
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "This\ncontroller implements weight and absolute bandwidth limit models for\nnormal scheduling policy and absolute bandwidth allocation model for\nrealtime scheduling policy.",
            "cpu.rt.max\" hard-allocates realtime slices and is an example of this\ntype.",
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "An introduction to control groups (cgroups) v2",
          "url": "https://man7.org/conf/ndctechtown2021/cgroups-v2-part-1-intro-NDC-TechTown-2021-Kerrisk.pdf",
          "excerpts": [
            "Control groups (cgroups) v2 are a mechanism for hierarchically grouping processes, managed by controllers, and allow limiting resource usage and prioritizing ..."
          ]
        },
        {
          "title": "WASI-libc: Make dlmalloc optional (issue #244)",
          "url": "https://github.com/WebAssembly/wasi-libc/issues/244",
          "excerpts": [
            "Missing: jemalloc jemallocator",
            "Make dlmalloc optional #244",
            "This is perhaps a more minimal version of issue [](https://github.com/WebAssembly/wasi-libc/issues/94) . I would like to use a custom allocator because the environment I'm using wasi-libc in (TinyGo) already implements a heap. I think the easiest way to get that working is to not compile dlmalloc as part of wasi-libc. Any idea how this might be implemented?"
          ]
        },
        {
          "title": "Memory Management in C. Static, Automatic and Dynamic ...",
          "url": "https://medium.com/@connect.ajkumar/memory-allocation-in-c-d48f2974e0c",
          "excerpts": [
            "Memory is allocated dynamically to the entities of the program. Programmer has freedom to allocate and free the memory for the program entities."
          ]
        },
        {
          "title": "Is Memory64 actually worth using? Is Memory64 actually worth using? • SpiderMonkey Blog (Jan 15, 2025)",
          "url": "https://spidermonkey.dev/blog/2025/01/15/is-memory64-actually-worth-using.html",
          "excerpts": [
            "Memory64 may be of limited use today, but there are some exciting possibilities for the future:",
            "Memory64 may be of limited use today, but there are some exciting possibilities for the future:",
            "64-bit pointers in WebAssembly simply allow you to address more memory, at the cost of slower loads and stores.",
            "the WebAssembly JS API constrains memories to a maximum size of 16GB.",
            "If you need more memory, and can tolerate worse performance, then Memory64 might be the right choice for you.",
            "Memory64 may not matter for most developers today, but we think it is an important stepping stone to an exciting future for memory in WebAssembly.",
            "Memory64 may not matter for most developers today, but we think it is an important stepping stone to an exciting future for memory in WebAssembly.",
            "s. Memory64 may not matter for most developers today, but we think it is an important stepping stone to an exciting future for memory in WebAssembly. * * *\n\n1. The rest of the proposal fleshes out the "
          ]
        },
        {
          "title": "Rust wasm64-unknown-unknown target (Rustc book)",
          "url": "https://doc.rust-lang.org/beta/rustc/platform-support/wasm64-unknown-unknown.html",
          "excerpts": [
            "This target comes by default with an allocator,\ncurrently [dlmalloc](http://gee.cs.oswego.edu/dl/html/malloc.html) which is [ported to rust](https://github.com/alexcrichton/dlmalloc-rs).",
            "The difference of this target with `wasm32-unknown-unknown` is that it's\ncompiled for 64-bit memories instead of 32-bit memories. This means that `usize`\nis 8-bytes large as well as pointers."
          ]
        }
      ],
      "reasoning": "The most directly relevant material discusses reducing Linux OS jitter and isolating CPU resources to give upper-layer workloads predictable execution. The first key excerpt points out that using CPU pinning utilities and CPU isolation techniques (such as isolcpus and nohz_full) and related scheduling controls can minimize jitter by preventing regular OS tasks from interrupting critical threads. This aligns with the goal of reducing latency spikes often caused by page faults when memory is touched after an OS decides to swap or fault in pages. The next set of excerpts elaborates on memory management strategies that influence the predictability of memory (and thus page faults) in multi-tenant or real-time contexts: NUMA-aware memory management and policies, as well as the Memory Manager guidance in Kubernetes, emphasize aligning memory resources and avoiding cross-node delays, which tacitly reduces the chance of costly page faults during critical execution windows. Additional entries discuss how to control memory behavior at a low level (e.g., using HugePages and allocator strategies) to reduce memory-related latency and fragmentation, which also influence paging behavior and paging-triggered jitter. Collectively, these excerpts support the notion that careful memory management and CPU/core isolation are central levers to minimize OS-induced latency spikes, which is the same family of techniques that would underwrite a page-locking strategy (mlock/mlockall) intended to prevent paging during critical execution paths. They provide concrete mechanisms (isolating CPUs, tuning kernel features, memory policy, and memory-guarding concepts) that could be employed in a Rust/WASM deployment running server-side (e.g., Kafka, Spark, databases) to reduce jitter and stabilize latency. They do not directly quote mlock/mlockall, but they establish the surrounding operational context and concrete approaches that would enable a page-locking strategy to be effective. The more distant but still relevant entries discuss memory allocation strategies and how memory can be reserved/guarded (e.g., large address space considerations, pooling allocators, memory guards) which informs how a page-locked region might be prepared and protected in a long-running WASM-enabled service. Overall, the strongest support for page-fault reduction through OS memory locking comes from discussions of CPU isolation and memory-management discipline; other items broaden the context with memory-policy and allocator considerations that would complement a page-locking approach.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.2",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "4 Full-featured cpuset manipulation commands",
          "url": "https://documentation.suse.com/en-us/sle-rt/15-SP7/html/SLE-RT-all/cha-shielding-cpuset.html",
          "excerpts": [
            "The cset subcommand creates the cpusets based on a mounted cpuset file system. You do not need to know where that file system is mounted, although it is easy to ..."
          ]
        },
        {
          "title": "cset-shield - cpuset supercommand which implements cpu ...",
          "url": "https://manpages.ubuntu.com/manpages/trusty/man1/cset-shield.1.html",
          "excerpts": [
            "This is a supercommand that creates basic cpu shielding. The normal cset commands can of course be used to create this basic shield."
          ]
        },
        {
          "title": "2 The Basic Shielding Model",
          "url": "https://documentation.suse.com/en-us/sle-rt/12-SP5/html/SLE-RT-all/cha-shielding-model.html",
          "excerpts": [
            "The shield subcommand manages all of these cpuset s and lets you define the CPUs and memory nodes that are in the shielded and unshielded sets."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt explicitly discusses strategies to reduce OS jitter by isolating CPUs and pinning workloads, highlighting techniques like using taskset and isolcpus. This aligns directly with the fine-grained field value which calls out CPU pinning/affinity as a core mitigation against scheduling jitter for WASM runtimes.\n\nOther excerpts reinforce the same concept by naming practical tooling and configurations:\n- Boot-time kernel parameter examples include isolcpus and nohz_full, which are used to constrain CPU scheduling domains and reduce interruptions for targeted workloads.\n- Descriptions of isolating CPUs to give applications more execution time articulate the underlying rationale: removing extraneous tasks from a CPU to improve real-time behavior and reduce jitter.\n- cpuset-related excerpts describe creating and managing CPU sets and shielding CPUs from scheduler load, which directly implements CPU pinning/affinity practices in multi-process environments.\n\nTogether, these excerpts connect the field value to a coherent set of actionable techniques for binding a WASM runtime to dedicated cores to minimize OS jitter, including both boot-time and runtime mechanisms, and tooling to enforce the affinities.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.1.tools_and_methods",
      "citations": [
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        }
      ],
      "reasoning": "The finegrained field value refers to the Linux PREEMPT_RT patch, a real-time preemption framework intended to minimize non-preemptible kernel code and reduce latency. The most directly relevant excerpts explicitly discuss real-time preemption as a kernel feature and describe the PREEMPT_RT patch or its goals: reducing non-preemptible kernel sections and enabling threaded interrupt handlers to lower latency. Additional closely related excerpts expand on low-latency Linux for industrial or embedded contexts and technical details about real-time preemption that align with the idea of a PREEMPT_RT-like approach. Some excerpts discuss CPU isolation and NO_HZ mechanisms, which are common techniques used in conjunction with real-time Linux configurations to further reduce jitter and scheduling overhead. All of these pieces collectively support the notion of a Linux PREEMPT_RT-style patch as a core mechanism for real-time scheduling and jitter reduction, which matches the requested field value. The degree of support is strongest where the material explicitly names PREEMPT_RT and describes its purpose; surrounding items provide contextual relevance about techniques to minimize latency and jitter in real-time Linux setups.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.1.technique",
      "citations": [
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        }
      ],
      "reasoning": "The central claim is that Real-Time Kernel Patching (PREEMPT_RT) is a method to minimize non-preemptible kernel sections and thereby reduce scheduling jitter, which supports the idea of reducing OS-induced latency for real-time WASM workloads running on dedicated cores. The most direct evidence comes from a description of real-time preemption and PREEMPT_RT, which states that the patch aim is to minimize the amount of kernel code that is non-preemptible, addressing jitter at the kernel level. Additional context is provided by discussions of isolating CPUs to give applications the most execution time by removing extraneous tasks, which is a concrete technique to minimize interference and jitter on dedicated cores. Related notes about reducing scheduling-clock interrupts (NO_HZ) further support the broader strategy of lowering OS overhead that contributes to jitter, while still within the same family of real-time kernel tuning methods. Collectively, these excerpts map onto the field value by illustrating (a) what Real-Time Kernel Patching is, (b) how it reduces jitter, and (c) what complementary techniques (like CPU isolation and reduced timer interrupts) can accompany it to improve deterministic performance for non-browser WASM workloads.\n",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.0.technique",
      "citations": [
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        }
      ],
      "reasoning": "The finegrained field value identifies a technique centered on isolating CPUs through kernel boot parameters. The most directly relevant content shows explicit references to CPU lists and parameters such as isolcpus and nohz_full and describes their purpose to assign specific CPUs and minimize interference from other tasks. Additional excerpts explain isolating CPUs to give application threads more uninterrupted execution time, which aligns with reducing OS jitter and reserving cores for specific workloads. Other excerpts discuss related mechanisms like reducing scheduling-clock interrupts (NO_HZ) and real-time preemption, which also support the broader goal of lowering jitter and improving determinism through kernel-managed isolation and timing optimizations. Taken together, these excerpts collectively support the idea of CPU isolation via kernel boot parameters as a technique to reduce OS jitter and allocate dedicated cores for high-performance use cases, which is the requested finegrained field value.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.5.category",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_",
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        }
      ],
      "reasoning": "The field value corresponds to memory management as a technique for reducing OS jitter. The first excerpt discusses turning off Transparent Huge Pages (THP) and adjusting kernel memory parameters such as vm.min_free_kbytes and swappiness, which are direct memory management actions intended to influence memory pressure and paging behavior. The second excerpt mentions using tools like numactl, taskset, and isolcpus to manage thread placement and CPU isolation, which impacts memory locality and how memory is allocated and accessed by threads, thereby affecting jitter. Together, these excerpts provide concrete memory-management-oriented strategies for reducing OS jitter, aligning with the field value. The first excerpt offers explicit memory parameter tuning, while the second provides techniques that influence memory access patterns through NUMA/topology-aware binding and isolation, both central to memory management in this context.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.5.tools_and_methods",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_",
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        }
      ],
      "reasoning": "The field value concerns configuring and using hugetlbfs on the host. The first excerpt talks about turning Transparent Huge Pages (THP) off and related memory parameters, which is directly about large-page memory configuration, albeit for THP rather than hugetlbfs itself. This makes it the most relevant since it engages with large-page memory management concepts that are adjacent to hugetlbfs usage. The second excerpt discusses using tools like numactl, taskset, and isolcpus to reduce jitter and isolate workloads, which relates to OS-level performance and scheduling considerations but does not address large-page memory configuration. Therefore, while neither excerpt explicitly confirms hugetlbfs configuration, the first provides the closest and most relevant context to the finegrained field value, and the second offers supportive context about performance isolation strategies without directly impacting hugetlbfs configuration.",
      "confidence": "low"
    },
    {
      "field": "os_jitter_reduction_techniques.5.description",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_",
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        }
      ],
      "reasoning": "The finegrained value asserts that large memory pages (2MB/1GB) can reduce TLB pressure and improve memory performance, and that THP should be disabled because it can introduce jitter. The first excerpt explicitly instructs to turn Transparent Huge Pages OFF, which directly supports the component of the value that THP should be disabled due to jitter concerns. It also contains additional memory/tuning recommendations (e.g., vm.min_free_kbytes, Swappiness) that align with broader OS-jitter reduction practices, though not all are about large pages. The second excerpt discusses using tools like isolcpus to keep threads from experiencing jitter, which aligns with the broader theme of reducing OS-induced hiccups, even though it does not mention large pages or THP specifically. Together, the excerpts substantiate the core claim about disabling THP and provide circumstantial support for jitter-reduction strategies, including CPU isolation, but they do not directly confirm the specific claim about using larger memory pages to reduce TLB misses. Therefore, the evidence supports the THP-disable aspect strongly and provides partial support for the overall jitter-reduction approach.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.6.description",
      "citations": [
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        }
      ],
      "reasoning": "To support the idea of reducing jitter by dedicating cores, the most relevant content discusses isolating CPUs to give application threads maximum execution time and minimize interference from other tasks. Specifically, isolating CPUs removes extraneous tasks from the target CPU and allocates it for the workload, which is a prerequisite for preventing perturbations such as keyboard interrupts from affecting the Wasm runtime. Related guidance mentions using tools and kernel parameters (e.g., isolcpus, nohz_full, irq affinity) to achieve this isolation, providing concrete mechanisms to assign core ownership away from the rest of the system. Additional context about using traditional jitter-reduction utilities (numactl, taskset) and RT-oriented configurations (REAL-TIME tuning, containerized real-time threads) further illustrates practical approaches to enforce deterministic behavior on dedicated cores. While the excerpts do not explicitly state every operational detail of redirecting NIC interrupts, the combination of CPU isolation strategies and interrupt management concepts strongly supports the notion that NIC interrupts can be prevented from running on the isolated cores by routing them to non-critical housekeeping cores, as part of a broader isolation strategy. Finally, references to RT Linux and PREEMPT_RT underscore the broader ecosystem where such isolation and interrupt handling strategies are commonly deployed to achieve lower latency and reduced jitter.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.5.technique",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_",
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        }
      ],
      "reasoning": "The field value refers to Huge Page Allocation as a technique to improve performance by reducing memory-related jitter. Excerpt 0 directly discusses memory tuning for reducing Linux OS jitter and explicitly mentions Transparent Huge Pages (THP), which is the mechanism underlying huge pages. However, it instructs to turn THP off, which contradicts the idea of enabling huge pages. Despite this contradiction, the excerpt is highly relevant because it centers on memory-page size strategies used to minimize jitter, which is central to evaluating Huge Page Allocation as a technique. Excerpt 1 discusses related performance isolation methods such as using numactl, taskset, and isolcpus to bind threads to specific CPUs and control cache and interrupt behavior, which aligns with the broader goal of reducing jitter and achieving predictable performance, though it does not mention huge pages specifically. It is still relevant as context for how core and memory affinity strategies interact with performance guarantees and potential use cases for WASM/Rust in high-performance, low-jitter environments.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.1.description",
      "citations": [
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        }
      ],
      "reasoning": "The target field value asserts a transformation of the Linux kernel into a fully preemptible, low-latency system for real-time tasks. The most relevant excerpts explicitly discuss real-time preemption concepts and the PREEMPT_RT patch. One excerpt notes that the PREEMPT_RT patch involves threading interrupt handlers, which is central to enabling preemption of interrupt handling and thus lower latency. Another excerpt highlights that the key point of PREEMPT_RT is to minimize non-preemptible kernel code, aligning with the idea of a fully preemptible kernel. Additional excerpts describe low-latency implications and practical guidance for latency-sensitive contexts, indicating PREEMPT_RT as a good choice under tight latency constraints. Supporting content about reducing scheduling-clock ticks and CPU isolation provides ancillary mechanisms that contribute to lower latency, matching the broader theme of reducing jitter and timing variability. Collectively, these pieces corroborate the field value by describing mechanisms (preemption, threaded interrupts, latency reduction) and their impact on scheduling latencies, which together explain how such a transformed kernel achieves microsecond-scale latency improvements for real-time workloads.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.0.description",
      "citations": [
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "Control Topology Management Policies on a node",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/",
          "excerpts": [
            "The Topology Manager receives topology information from the _Hint Providers_ as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication.",
            "The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result.",
            "To align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node."
          ]
        }
      ],
      "reasoning": "The most relevant content explicitly discusses isolating CPUs to give applications the maximum uninterrupted execution time and reducing interference from other tasks. One excerpt describes isolating CPUs using a tuned profile for realtime workloads, which directly matches the idea of dedicating cores to a WASM workload to minimize contention and jitter. Another excerpt explains reducing Linux OS jitter with tools like numactl, taskset, and isolcpus, which aligns with creating a clean, predictable execution environment for performance-sensitive workloads. A third excerpt covers real-time preemption in the Linux kernel, which is closely related to ensuring lower latency and more deterministic scheduling for such workloads. Additional excerpts touch on related techniques like reducing scheduling-clock ticks (NO_HZ) and general strategies to minimize interference, further supporting the concept of a jitter-reduced, dedicated-core environment. Contextual excerpts about topology management and CPU placement policies provide broader ecosystem insight but are less directly about isolating cores for a single WASM workload, though they still relate to optimizing resource isolation.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.6.technique",
      "citations": [
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        }
      ],
      "reasoning": "Interrupt (IRQ) mitigation in OS environments often involves configuring the kernel and scheduling to minimize interrupt handling overhead and ensure predictable latency. The most relevant excerpt discusses adopting PREEMPT_RT, a Linux kernel variant designed to reduce latency and jitter, which directly addresses IRQ and interrupt handling predictability. The next relevant excerpt covers running real-time threads in containers with capabilities that permit real-time scheduling, illustrating practical approaches to enforce deterministic execution and reduce IRQ-induced delays. Another excerpt focuses on isolating CPUs with realtime-oriented tuning profiles, which helps prevent IRQs and other background tasks from perturbing critical computation, aligning with IRQ mitigation goals. The remaining excerpt mentions general jitter-reduction tooling such as isolcpus, numactl, and taskset, which can contribute to reducing interrupt interference but is more peripheral to the IRQ-specific mitigation strategy, serving as supportive context for latency control. Collectively, these excerpts support the idea that interrupt mitigation can be achieved through kernel real-time configurations (PREEMPT_RT), enforcing real-time scheduling in constrained environments (containers), and CPU isolation strategies, with auxiliary tooling to further minimize IRQ interference.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.6.tools_and_methods",
      "citations": [
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes setting IRQ affinity via the per-IRQ smp_affinity control and disabling the irqbalance daemon to reduce OS jitter. The most directly supportive excerpt lists IRQ affinity among a set of CPU-affinity tools (irqaffinity) and discusses the format of CPU lists such as isolcpus and related parameters, showing that IRQ affinity is a recognized mechanism for controlling interrupt distribution. A closely related excerpt discusses broader jitter-reduction techniques like isolating CPUs and pinning threads, which complements the idea of binding work and interrupts to specific cores to minimize jitter. Another excerpt explicitly covers isolating CPUs using a realtime-focused tuning approach, which aligns with the goal of reducing interference and jitter by dedicating CPUs. A further excerpt describes real-time container capabilities and scheduling tweaks that can influence interrupt handling and latency, reinforcing the theme of low-latency, predictable interrupt behavior. Although there are entries about general kernel parameters and latency techniques (nohz_full, etc.), they support the broader context of CPU/interruption management for jitter reduction and help situate the cited IRQ affinity practice within established approaches. Collectively, the cited passages substantiate the concept of using IRQ affinity and related CPU-isolation tactics to reduce OS jitter and improve deterministic performance, particularly in high-performance, non-browser WASM-like workloads that may rely on tight interrupt and thread control.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.0.tools_and_methods",
      "citations": [
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "Control Topology Management Policies on a node",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/",
          "excerpts": [
            "The Topology Manager receives topology information from the _Hint Providers_ as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication.",
            "The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result.",
            "To align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node."
          ]
        }
      ],
      "reasoning": "The finegrained field value enumerates kernel boot parameters that directly influence CPU isolation and scheduling tick behavior: isolcpus isolates CPUs, nohz_full reduces scheduling ticks, and rcu_nocbs offloads RCU callbacks. The most relevant excerpts explicitly name these parameters and explain their effects or usage: one excerpt lists cpu lists including isolcpus, nohz_full, and rcu_nocbs as examples of parameter values, directly matching the field value; another excerpt describes nohz_full as a CPU-list argument in kernels configured with NO_HZ_FULL and discusses isolating CPUs to give applications dedicated execution time; a third excerpt discusses reducing OS jitter using isolcpus among other tools, which ties directly to diminishing scheduler interruptions. Additional excerpts discuss related concepts like reducing scheduling-clock ticks (NO_HZ) and real-time preemption, which corroborate the context of lowering jitter and improving determinism. While excerpts on topology management and container orchestration provide peripheral context about resource alignment, they do not directly name or define the specific kernel boot parameters, so their relevance is lower but still informative for the broader topic of jitter reduction in multi-core environments. In sum, the excerpts that explicitly mention the target parameters are the strongest support for the finegrained field value, with other references providing supportive context on mechanisms to reduce jitter and improve determinism.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.7.technique",
      "citations": [
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        }
      ],
      "reasoning": "To assess relevance to the technique 'Disabling Dynamic Power Management', I looked for content that directly discusses DPM or actions that disable power-management features. The excerpts do not mention Dynamic Power Management or its disabling. The closest alignment is with excerpts that address reducing OS jitter through CPU isolation and related kernel configuration strategies: isolating CPUs to give applications the most execution time, using realtime tuning profiles to minimize interference, and reducing scheduling ticks and interruptions. These excerpts support the broader goal of jitter reduction and deterministic performance, which is conceptually related to the field value (a technique aimed at performance determinism). Specifically, isolating CPUs and using realtime profiles directly map to reducing external interferences and jitter, while discussions of nohz and related kernel parameters illustrate further hardware/OS-level controls that can affect timing behavior. Although these excerpts do not confirm or illustrate disabling DPM, they provide context for other, related jitter-reduction techniques and show practical methods for achieving more deterministic execution. The strongest connection is with the explicit CPU-isolation and realtime-tuning discussions, which illustrate the practical approach to minimizing jitter and may be part of a broader toolbox that could include disabling power-management features in some systems.",
      "confidence": "low"
    },
    {
      "field": "os_jitter_reduction_techniques.0.category",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "Control Topology Management Policies on a node",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/",
          "excerpts": [
            "The Topology Manager receives topology information from the _Hint Providers_ as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication.",
            "The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result.",
            "To align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node."
          ]
        }
      ],
      "reasoning": "The field value refers to managing CPU resources to reduce OS-induced jitter. Excerpts that explicitly discuss techniques to isolate CPUs and minimize scheduler interruptions are most relevant. For example, content that mentions using tools and parameters to remove extraneous tasks from a CPU (such as isolcpus) and to allocate dedicated execution time (such as numactl and taskset) directly supports CPU management aimed at jitter reduction. References to isolating CPUs via tuned profiles for real-time workloads further reinforce this focus on CPU-centric performance guarantees. Kernel parameters like isolcpus and nohz_full map directly to strategies for dedicating CPUs and reducing scheduling overhead, which is core to CPU management and jitter reduction. Discussions about reducing scheduling-clock ticks (NO_HZ) and real-time preemption provide additional mechanisms to control CPU scheduling behavior, aligning with the same overall objective. Finally, topology management and CPU manager policy discussions, while broader, relate to how CPU resources are allocated and controlled in a cluster, which complements CPU management for performance-critical workloads. Taken together, the strongest support comes from explicit isolation and affinity techniques, with kernel parameters and real-time scheduling controls providing corroborating context.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.7.description",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified ",
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        }
      ],
      "reasoning": "The most directly relevant information supports reducing jitter and predictable timing by isolating CPUs, pinning threads, and reducing scheduling interrupts. The discussion about using isolcpus, taskset, and numactl highlights how to give threads exclusive execution time and minimize external disturbances, which aligns with the goal of eliminating latency from CPU-state transitions by constraining how often the processor is interrupted or context-switched. The NO_HZ document describes reducing scheduling-clock interrupts, which further contributes to less jitter and more predictable timing, a prerequisite for stable processing speeds. The tuning guide's guidance on isolating CPUs using tuned-profiles-realtime emphasizes dedicating CPU resources to applications to maximize uninterrupted execution, directly supporting the notion of reducing latency from dynamic CPU behavior. The kernel-parameters overview provides concrete knob-by-knob examples (isolcpus, nohz_full) that implement the isolation and reduced-interrupt strategy described above, reinforcing the practical path to lower latency. The real-time preemption documentation discusses latency considerations in the kernel, which is tangentially relevant to the topic of minimizing latency and jitter, though it is more about latency guarantees in scheduling than about CPU-state transitions per se. Taken together, the excerpts support a strategy of reducing latency and jitter through CPU isolation, reduced interrupts, and careful kernel parameter configuration, which conceptually complements the idea of achieving more predictable processing speeds by mitigating timing variability. However, none of the excerpts explicitly mention frequency scaling or C-states, so the direct claim about eliminating latency caused by those specific CPU-power-state transitions is not fully substantiated by these excerpts alone.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.6.category",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        }
      ],
      "reasoning": "To support a field describing Interrupt Mitigation, the most relevant excerpts are those that explicitly discuss techniques to reduce jitter and improve real-time responsiveness by managing interrupts and CPU scheduling. The discussion of using tools like numactl, taskset, and isolcpus highlights practical methods to keep certain threads from being interrupted by unrelated tasks, thereby reducing jitter. The guidance on isolating CPUs—removing extraneous tasks from a CPU to give applications the most execution time—directly speaks to strategies that minimize interrupt-induced delays. References to real-time processes in containers and the use of PREEMPT_RT for low-latency Linux provide concrete, system-wide approaches to ensure more predictable interrupt handling and timing behavior. Additional excerpts mentioning kernel CPU lists and nohz_full relate to configuring CPU affinity and tickless/non-tick modes, which underpin interrupt management strategies and can contribute to lower interrupt-induced jitter, though they are more foundational in nature. Collectively, these excerpts map onto the concept of Interrupt Mitigation by describing both granular (thread-level pinning) and systemic (kernel real-time patches and isolation policies) approaches to reduce OS jitter caused by interrupts and scheduling overhead.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.7.category",
      "citations": [
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        }
      ],
      "reasoning": "The most direct support for Hardware Settings comes from excerpts that describe isolating CPUs and configuring kernel parameters to control how CPUs and cores are scheduled. One excerpt explains isolating CPUs to give application threads the most execution time by removing extraneous tasks from a CPU, which is a clear hardware-level setting intended to reduce interference. Related excerpts enumerate specific kernel command-line parameters that designate CPU lists and related controls (such as isolcpus and nohz_full), which are explicit knobs that alter hardware scheduling behavior. Additional excerpts discuss preemption and reduced scheduling clock ticks, which are kernel-level mechanisms that govern how the OS interacts with hardware timers and preemption, further tying into hardware-oriented performance characteristics. Other excerpts describe tools like numactl, taskset, and cache considerations to mitigate jitter, which are practical hardware-aware techniques for core and memory placement, reinforcing the hardware-settings perspective. Collectively, these excerpts provide a cohesive set of hardware-oriented controls (CPU isolation, core binding, and kernel parameters) that align with the Hardware Settings category for jitter reduction and deterministic scheduling in a WASM/Rust-on-WASM or low-level systems context.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.3.tools_and_methods",
      "citations": [
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        }
      ],
      "reasoning": "The most directly supportive content is an instance where real-time scheduling is concretely associated with a specific policy family in practice: running real-time threads in the SCHED_FIFO domain within a privileged container demonstrates the exact type of scheduler policy the field value enumerates. This provides a concrete linkage to the queried policies. The next layer of relevance comes from discussions about reducing OS jitter and achieving determinism through CPU isolation and pinning techniques (such as isolating CPUs, using taskset, and similar isolation approaches). These excerpts illuminate practical mechanisms that operate in concert with or as precursors to selecting real-time scheduling policies like FIFO, RR, or deadline-based scheduling. Other excerpts contribute broader context on PREEMPT_RT and real-time Linux tooling, which underpin how those policies are implemented and how they affect jitter, latency, and determinism, even though they may not name the exact SCHED_POLICY identifiers. Taken together, these excerpts map the landscape of real-time scheduling policies and the practical methods to reduce jitter, with the most direct support coming from the container example of SCHED_FIFO usage, and progressively less direct support from CPU isolation, preemption detail, and latency-focused discussions.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.3.description",
      "citations": [
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        }
      ],
      "reasoning": "The field value asserts that giving the Wasm runtime process a higher real-time priority would allow it to preempt lower-priority tasks and avoid CPU starvation, ensuring timely execution. The closest support comes from discussions of real-time preemption and PREEMPT_RT, which aim to minimize non-preemptible kernel code and enable threaded interrupt handling so critical tasks can run with stronger timing guarantees. This provides a direct rationale for designing a system where a Wasm runtime is scheduled with real-time priority to reduce latency and prevent missed deadlines. Additional excerpts describe isolating CPUs to give applications more execution time by removing extraneous tasks from a CPU, and using latency-focused configurations in industrial or embedded contexts, which reinforces the idea that dedicated or tightly controlled CPU resources can substantially reduce jitter and improve predictability for time-sensitive workloads. Further excerpts discuss low-latency Linux strategies for industrial systems and practical techniques to reduce OS jitter (such as using NUMA-related optimizations or CPU isolation tricks), which align with the goal of minimizing contention and delays that a real-time Wasm runtime would face in a shared environment. A note on real-time behavior inside containers shows that real-time capable configurations can be exercised in containerized environments, which is relevant when deploying Wasm outside browsers in modern orchestration setups. Taken together, these excerpts provide a coherent evidentiary basis for the claim that a Wasm runtime could be assigned real-time priority and run with reduced OS-induced jitter by leveraging real-time scheduling, CPU isolation, and latency-minimizing configurations.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.3.category",
      "citations": [
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        }
      ],
      "reasoning": "The most directly relevant items discuss real-time preemption and kernel scheduling policies, which are central to how the kernel schedules tasks and resources under real-time constraints. For example, the text describing Real-time preemption explains the patch's aim to minimize non-preemptible kernel code and to maximize preemptibility, which directly affects how the kernel schedules interrupts and tasks. Another excerpt outlines the technical details of real-time preemption, describing how interrupt handlers are threaded, a fundamental aspect of how scheduling decisions are made in a real-time context. These pieces directly address Kernel Scheduling by describing mechanisms that influence when and how tasks are scheduled and preempted by the kernel. Additionally, content about isolating CPUs to give applications more execution time and about using tooling to manage CPU affinity (such as isolcpus and taskset) further connects to kernel scheduling by describing strategies to reduce interference and schedule determinism across cores. Other excerpts extend these concepts into practical implications for latency and jitter in embedded or containerized environments, reinforcing their relevance to kernel scheduling as a means to reduce OS jitter. Collectively, these excerpts provide clear support for the kernel scheduling aspect of jitter reduction and real-time performance, with the strongest emphasis on how PREEMPT_RT and related scheduling decisions impact determinism, preemption, and core affinity.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.2.tools_and_methods",
      "citations": [
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "cset-shield - cpuset supercommand which implements cpu ...",
          "url": "https://manpages.ubuntu.com/manpages/trusty/man1/cset-shield.1.html",
          "excerpts": [
            "This is a supercommand that creates basic cpu shielding. The normal cset commands can of course be used to create this basic shield."
          ]
        },
        {
          "title": "2 The Basic Shielding Model",
          "url": "https://documentation.suse.com/en-us/sle-rt/12-SP5/html/SLE-RT-all/cha-shielding-model.html",
          "excerpts": [
            "The shield subcommand manages all of these cpuset s and lets you define the CPUs and memory nodes that are in the shielded and unshielded sets."
          ]
        },
        {
          "title": "4 Full-featured cpuset manipulation commands",
          "url": "https://documentation.suse.com/en-us/sle-rt/15-SP7/html/SLE-RT-all/cha-shielding-cpuset.html",
          "excerpts": [
            "The cset subcommand creates the cpusets based on a mounted cpuset file system. You do not need to know where that file system is mounted, although it is easy to ..."
          ]
        }
      ],
      "reasoning": "The field value explicitly lists the following tools: taskset, pthread_setaffinity_np, and cset shield. An excerpt that directly mentions taskset corroborates the exact tool in use for binding threads to CPUs, aligning with the field value. Another excerpt mentions cset shield, which directly matches the requested tool in the field value. Additionally, a broader discussion of shielding and cpuset-based approaches provides context that supports the inclusion of shielding tools in jitter-reduction techniques, even if it does not name the specific tools. Finally, there is an excerpt describing cpuset manipulation in general, which is relevant to the environment in which these tools operate and reinforces the relevance of cpuset-based strategies for reducing jitter, including the use of the cset family. Taken together, these excerpts collectively support the stated tools and the overall approach to reducing OS jitter via user-space shielding and CPU affinity mechanisms. ",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.7.tools_and_methods",
      "citations": [
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "\n\n        nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified "
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        }
      ],
      "reasoning": "The target field value concerns configuring the system to minimize OS-induced latency by controlling core affinity and resource contention. The most directly relevant passages describe isolating CPUs to maximize an application's execution window and dedicating CPU resources by removing extraneous tasks, which aligns with reducing jitter for high-performance workloads. The next-most relevant items discuss practical tools and approaches such as using taskset, numactl, and isolcpus to constrain scheduling and memory behavior, which are concrete methods to reduce interference and jitter. Additional excerpts cover real-time kernel concepts (real-time preemption) and NO_HZ/CPU-list configurations, which further illustrate how the kernel can be tuned to minimize scheduling interrupts and improve determinism. While none of the excerpts mention the exact requested settings (CPU governor, Turbo Boost, BIOS/UEFI deep-state controls), they collectively describe a family of OS-level strategies for reducing jitter and improving predictable performance that are conceptually consistent with the field value.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.2.category",
      "citations": [
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "2 The Basic Shielding Model",
          "url": "https://documentation.suse.com/en-us/sle-rt/12-SP5/html/SLE-RT-all/cha-shielding-model.html",
          "excerpts": [
            "The shield subcommand manages all of these cpuset s and lets you define the CPUs and memory nodes that are in the shielded and unshielded sets."
          ]
        },
        {
          "title": "4 Full-featured cpuset manipulation commands",
          "url": "https://documentation.suse.com/en-us/sle-rt/15-SP7/html/SLE-RT-all/cha-shielding-cpuset.html",
          "excerpts": [
            "The cset subcommand creates the cpusets based on a mounted cpuset file system. You do not need to know where that file system is mounted, although it is easy to ..."
          ]
        },
        {
          "title": "cset-shield - cpuset supercommand which implements cpu ...",
          "url": "https://manpages.ubuntu.com/manpages/trusty/man1/cset-shield.1.html",
          "excerpts": [
            "This is a supercommand that creates basic cpu shielding. The normal cset commands can of course be used to create this basic shield."
          ]
        }
      ],
      "reasoning": "The field value corresponds to strategies for direct CPU management to reduce jitter. Excerpts that describe isolating CPUs to give applications the most execution time and remove extraneous tasks are directly relevant to CPU management for jitter reduction. The documents that specifically mention isolating CPUs, tuning profiles for realtime operation, and dedicating CPU resources align with creating predictable, low-jitter environments. Details about shielding CPUs and defining shielded vs unshielded sets further support controlled CPU/resource partitioning. In addition, discussions of cpuset manipulation and dedicated shielding mechanisms provide practical methods to enforce CPU and memory isolation, which is central to reducing OS jitter through CPU management rather than relying solely on generic scheduling. The strongest connections come from explicit instructions to isolate CPUs, remove extraneous tasks, and shield CPU/memory resources; secondary support comes from related mechanisms like cpusets and cset-shield which implement shielding and CPU binding policies in practice. Collectively, these excerpts map to the concept of CPU-centric jitter reduction and dedicated core usage for high-performance, non-browser workloads like Kafka, Spark, or databases.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.2.description",
      "citations": [
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "2 The Basic Shielding Model",
          "url": "https://documentation.suse.com/en-us/sle-rt/12-SP5/html/SLE-RT-all/cha-shielding-model.html",
          "excerpts": [
            "The shield subcommand manages all of these cpuset s and lets you define the CPUs and memory nodes that are in the shielded and unshielded sets."
          ]
        },
        {
          "title": "4 Full-featured cpuset manipulation commands",
          "url": "https://documentation.suse.com/en-us/sle-rt/15-SP7/html/SLE-RT-all/cha-shielding-cpuset.html",
          "excerpts": [
            "The cset subcommand creates the cpusets based on a mounted cpuset file system. You do not need to know where that file system is mounted, although it is easy to ..."
          ]
        },
        {
          "title": "cset-shield - cpuset supercommand which implements cpu ...",
          "url": "https://manpages.ubuntu.com/manpages/trusty/man1/cset-shield.1.html",
          "excerpts": [
            "This is a supercommand that creates basic cpu shielding. The normal cset commands can of course be used to create this basic shield."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes binding a process like the WASM runtime to one or more dedicated CPU cores to improve cache locality, reduce context-switching overhead, and lower performance variability. The following excerpts support this concept in complementary ways:\n- The strongest support is the explicit statement that isolating CPUs gives application threads the most execution time possible by removing extraneous tasks, which directly underpins the strategy of dedicating cores to a WASM runtime to reduce jitter.\n- The companion guidance about isolating CPUs using tuned-profiles-realtime reinforces the idea of removing other tasks from a CPU to maximize available execution time for a particular workload, which maps to binding the WASM runtime to dedicated cores.\n- The references to techniques like isolcpus and typical tools (numactl, taskset) in relation to reducing jitter provide concrete mechanisms for enforcing CPU/core dedication for a runtime outside browser contexts.\n- Kernel parameter guidance on isolating CPUs (isolcpus, nohz_full, irqaffinity, rcu_nocbs) expands on the system-level controls that can be used to ensure a WASM runtime stays on preferred cores, decreasing interference.\n- Cpuset/shielding-related excerpts describe structuring CPU and memory locality to shield workloads from unrelated processes, which is a practical approach to enforce the dedicated-core strategy and further reduce jitter.\n- The shield and shielding-model discussions offer higher-level architectures for controlling CPU/memory exposure, supporting the broader goal of predictable performance for a WASM runtime by preventing unwanted migrations and interference.\nTaken together, these excerpts coherently support the idea of binding a WASM runtime to dedicated cores to reduce OS jitter and improve performance in non-browser use cases.",
      "confidence": "medium"
    },
    {
      "field": "os_jitter_reduction_techniques.3.technique",
      "citations": [
        {
          "title": "Realtime process inside container · Issue #62434",
          "url": "https://github.com/kubernetes/kubernetes/issues/62434",
          "excerpts": [
            "Apr 11, 2018 — But on this cluster I found that a container with --privileged and the SYS_NICE capability was able to run real-time threads in the SCHED_FIFO ..."
          ]
        },
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "Technical details of the real-time preemption",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/technical_details/start",
          "excerpts": [
            "Oct 3, 2023 — The PREEMPT_RT patch forces the mechanism of threaded interrupt handlers. Due to this all interrupt handlers run in a threaded context except ..."
          ]
        },
        {
          "title": "Low latency Linux for industrial embedded systems – Part III",
          "url": "https://ubuntu.com/blog/industrial-embedded-systems-iii",
          "excerpts": [
            "If the latency demands are particularly tight and the consequences of a missed deadline are catastrophic, PREEMPT_RT is likely a good choice."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        }
      ],
      "reasoning": "The field value refers to Real-Time Scheduling Policies as a technique to reduce OS jitter. The most directly relevant pieces discuss explicit real-time scheduling policies (for example, SCHED_FIFO) and how real-time scheduling interacts with Linux kernel behavior. Real-time scheduling policies enable deterministic task execution by assigning threads to real-time classes with defined priorities, which is central to reducing jitter. The source describing a real-time process inside a container notes that a privileged container with the SYS_NICE capability can run real-time threads in SCHED_FIFO, which is a concrete example of applying a real-time scheduling policy to achieve determinism. Other sources explain the broader foundation of real-time behavior in Linux, such as PREEMPT_RT real-time preemption and the goal to minimize non-preemptible kernel code, which directly supports how real-time scheduling policies operate within the system. Additional discussions on isolating CPUs, tuned real-time profiles, and techniques to reduce latency (like isolcpus, numactl, and taskset) illustrate practical implementations and optimizations around real-time scheduling policies to minimize jitter and grant dedicated execution resources. Taken together, these excerpts provide a coherent picture of how real-time scheduling policies are used to achieve predictable performance and reduced OS-induced jitter, with concrete examples and broader context on kernel real-time preemption and CPU isolation as complementary strategies.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.2.technique",
      "citations": [
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "4 Full-featured cpuset manipulation commands",
          "url": "https://documentation.suse.com/en-us/sle-rt/15-SP7/html/SLE-RT-all/cha-shielding-cpuset.html",
          "excerpts": [
            "The cset subcommand creates the cpusets based on a mounted cpuset file system. You do not need to know where that file system is mounted, although it is easy to ..."
          ]
        },
        {
          "title": "2 The Basic Shielding Model",
          "url": "https://documentation.suse.com/en-us/sle-rt/12-SP5/html/SLE-RT-all/cha-shielding-model.html",
          "excerpts": [
            "The shield subcommand manages all of these cpuset s and lets you define the CPUs and memory nodes that are in the shielded and unshielded sets."
          ]
        },
        {
          "title": "cset-shield - cpuset supercommand which implements cpu ...",
          "url": "https://manpages.ubuntu.com/manpages/trusty/man1/cset-shield.1.html",
          "excerpts": [
            "This is a supercommand that creates basic cpu shielding. The normal cset commands can of course be used to create this basic shield."
          ]
        }
      ],
      "reasoning": "The target fine-grained field value is CPU Pinning / Affinity. The most relevant information directly describes isolating CPUs to give an application threads the most execution time by removing extraneous tasks, as well as mechanisms to pin processes to specific CPUs. This includes isolating CPUs, isolcpus, and CPU affinity concepts. The following observations connect to the field:\n- The guidance to isolate CPUs to provide the most execution time, i.e., removing extraneous tasks off a CPU, directly aligns with the notion of pinning/affinity to dedicated cores. This establishes the core principle of CPU pinning for jitter reduction.\n- References to isolcpus and a detailed mechanism for listing CPUs and binding tasks to specific CPUs illustrate concrete methods to implement pinning/affinity in practice.\n- Discussions of cpuset manipulation and shielding describe broader tooling to control which CPUs and memory nodes are available to certain workloads, reinforcing the idea of pinning workloads to specific cores and memory regions.\n- Shielding models and related cpuset tools extend the concept to whole-system partitioning, which supports high-priority tasks running with reduced contention and jitter.\nIn summary, the most supportive content provides explicit instructions and concepts for isolating CPU cores, binding tasks to CPUs, and using cpuset/shielding to dedicate resources, all of which are central to CPU pinning/affinity. The less direct but still relevant items give additional context on the tooling available to implement such pinning across Linux environments.\nThe field value is thus well-supported by the cited passages that discuss isolating CPUs, the isolcpus parameter, and cpuset/shielding techniques, with multiple sources corroborating the approach.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.4.category",
      "citations": [
        {
          "title": "Utilizing the NUMA-aware Memory Manager",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/memory-manager/",
          "excerpts": [
            "The Kubernetes Memory Manager enables the feature of guaranteed memory (and hugepages) allocation for pods in the Guaranteed QoS class."
          ]
        },
        {
          "title": "Memory Management in C. Static, Automatic and Dynamic ...",
          "url": "https://medium.com/@connect.ajkumar/memory-allocation-in-c-d48f2974e0c",
          "excerpts": [
            "Memory is allocated dynamically to the entities of the program. Programmer has freedom to allocate and free the memory for the program entities."
          ]
        },
        {
          "title": "Rust wasm64-unknown-unknown target (Rustc book)",
          "url": "https://doc.rust-lang.org/beta/rustc/platform-support/wasm64-unknown-unknown.html",
          "excerpts": [
            "This target comes by default with an allocator,\ncurrently [dlmalloc](http://gee.cs.oswego.edu/dl/html/malloc.html) which is [ported to rust](https://github.com/alexcrichton/dlmalloc-rs).",
            "The difference of this target with `wasm32-unknown-unknown` is that it's\ncompiled for 64-bit memories instead of 32-bit memories. This means that `usize`\nis 8-bytes large as well as pointers."
          ]
        },
        {
          "title": "Is Memory64 actually worth using? Is Memory64 actually worth using? • SpiderMonkey Blog (Jan 15, 2025)",
          "url": "https://spidermonkey.dev/blog/2025/01/15/is-memory64-actually-worth-using.html",
          "excerpts": [
            "Memory64 may be of limited use today, but there are some exciting possibilities for the future:",
            "Memory64 may be of limited use today, but there are some exciting possibilities for the future:",
            "64-bit pointers in WebAssembly simply allow you to address more memory, at the cost of slower loads and stores.",
            "the WebAssembly JS API constrains memories to a maximum size of 16GB.",
            "If you need more memory, and can tolerate worse performance, then Memory64 might be the right choice for you.",
            "Memory64 may not matter for most developers today, but we think it is an important stepping stone to an exciting future for memory in WebAssembly.",
            "Memory64 may not matter for most developers today, but we think it is an important stepping stone to an exciting future for memory in WebAssembly.",
            "s. Memory64 may not matter for most developers today, but we think it is an important stepping stone to an exciting future for memory in WebAssembly. * * *\n\n1. The rest of the proposal fleshes out the "
          ]
        },
        {
          "title": "WASI-libc: Make dlmalloc optional (issue #244)",
          "url": "https://github.com/WebAssembly/wasi-libc/issues/244",
          "excerpts": [
            "Missing: jemalloc jemallocator",
            "Make dlmalloc optional #244",
            "This is perhaps a more minimal version of issue [](https://github.com/WebAssembly/wasi-libc/issues/94) . I would like to use a custom allocator because the environment I'm using wasi-libc in (TinyGo) already implements a heap. I think the easiest way to get that working is to not compile dlmalloc as part of wasi-libc. Any idea how this might be implemented?"
          ]
        },
        {
          "title": "Control Group v2 — The Linux Kernel documentation",
          "url": "https://www.kernel.org/doc/html/v6.6/admin-guide/cgroup-v2.html",
          "excerpts": [
            "This\ncontroller implements weight and absolute bandwidth limit models for\nnormal scheduling policy and absolute bandwidth allocation model for\nrealtime scheduling policy.",
            "cpu.rt.max\" hard-allocates realtime slices and is an example of this\ntype.",
            "The \"cpu\" controllers regulates distribution of CPU cycles."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ...",
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_"
          ]
        },
        {
          "title": "An introduction to control groups (cgroups) v2",
          "url": "https://man7.org/conf/ndctechtown2021/cgroups-v2-part-1-intro-NDC-TechTown-2021-Kerrisk.pdf",
          "excerpts": [
            "Control groups (cgroups) v2 are a mechanism for hierarchically grouping processes, managed by controllers, and allow limiting resource usage and prioritizing ..."
          ]
        }
      ],
      "reasoning": "The field value Memory Management corresponds to how memory is allocated, managed, and optimized in environments that host Wasm or Rust-based runtimes outside browsers. Excerpts describing guaranteed or managed memory in cluster or pod environments illustrate practical memory management strategies that directly impact performance and jitter. Statements about memory allocation in C, and the allocator implementations integrated with Wasm targets, show concrete memory-management tooling and patterns (for example, how a default allocator is provided or how to switch allocators). Discussions of Memory64 for Wasm reveal architectural memory modeling choices that affect addressable space and performance. Excerpts addressing dlmalloc and other allocator options in WASI-libc demonstrate concrete memory-management implementations that influence runtime behavior. Put together, these excerpts form a coherent set of approaches to memory management for high-performance, low-jitter WASM/Rust workloads beyond browsers, including allocator choices, memory sizing, and guaranteed memory. The most directly supportive pieces are those explicitly about memory management in Kubernetes or WASM toolchains, followed by allocator-specific discussions, then broader memory architectural considerations, with jitter-reduction techniques providing context but not being the primary focus of the field value. ",
      "confidence": "high"
    },
    {
      "field": "rtos_like_behavior_analysis.host_os_capabilities",
      "citations": [
        {
          "title": "Real-time preemption - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/next/core-api/real-time/index.html",
          "excerpts": [
            "Real-time preemption¶. This documentation is intended for Linux kernel developers and contributors interested in the inner workings of PREEMPT_RT."
          ]
        },
        {
          "title": "A realtime preemption overview",
          "url": "https://lwn.net/Articles/146861/",
          "excerpts": [
            "Aug 10, 2005 — The key point of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible, while also minimizing the amount of code that must be ..."
          ]
        },
        {
          "title": "OSADL Real-time Linux – Q&A Farm (Linux real-time.0.html)",
          "url": "https://www.osadl.org/OSADL-QA-Farm-Real-time.linux-real-time.0.html",
          "excerpts": [
            "PREEMPT\\_RT is mainline - What's next?"
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime | Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "To give application threads the most execution time possible, you can isolate CPUs, which means removing as many extraneous tasks off a CPU as possible."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://docs.kernel.org/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "    [KNL] The optional argument is a cpu list,\n                        as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y,\n                        enable the no-callback CPU mode, which prevents\n                        such CPUs' callbacks from being invoked in\n                        softirq context. Invocation of such CPUs' RCU\n                        callbacks will instead be offloaded to \"rcuox/N\"\n                        kthreads created for that purpose, where"
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "NO_HZ: Reducing Scheduling-Clock Ticks",
          "url": "https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt",
          "excerpts": [
            "This document describes Kconfig options and boot parameters that can reduce the number of scheduling-clock interrupts, thereby improving energy efficiency and ..."
          ]
        },
        {
          "title": "Soft Real-Time on Windows IoT Enterprise",
          "url": "https://learn.microsoft.com/en-us/windows/iot/iot-enterprise/soft-real-time/soft-real-time",
          "excerpts": [
            "What is a Real-Time Operating System?"
          ]
        },
        {
          "title": "Windows IoT Soft Real-Time Guidance",
          "url": "https://learn.microsoft.com/en-us/windows/iot/iot-enterprise/soft-real-time/soft-real-time-device",
          "excerpts": [
            "The only way to use this feature is with an application and device custom-built for a specific purpose. The mapping of processor core assignments in the application threads must match the physical device cores and their configuration for real-time versus standard workloads."
          ]
        }
      ],
      "reasoning": "The field value asserts that a general-purpose OS can be configured to provide a near-real-time environment for Wasm workloads, with Linux-specific guidance centered on PREEMPT_RT to make the kernel nearly fully preemptible and to reduce scheduling latencies, supplemented by CPU isolation techniques. The Linux kernel's real-time preemption documentation states that real-time preemption is a mechanism aimed at minimizing non-preemptible kernel code, which directly supports the goal of lower latency for RT-like workloads. An overview of real-time preemption emphasizes reducing non-preemptible code and shaving away latency, which underpins the approach of using PREEMPT_RT as a foundational RT mechanism for Wasm runtimes. A separate OSADL Q&A on Linux real-time discusses concrete RT topics, including PREEMPT_RT status and practical RT-oriented questions, reinforcing the relevance of this approach in real deployments. The CPU isolation guidance explicitly mentions isolating CPUs to give applications maximum uninterrupted execution time, which aligns with dedicating cores to RT tasks. Kernel-parameter guidance shows specific boot-time knobs such as isolcpus (to isolate CPUs from the scheduler), nohz_full (to stop the scheduling tick on isolated CPUs), and rcu_nocbs (to offload RCU callbacks away from the RT-critical cores), which are essential for achieving deterministic RT-like behavior. The discussion of pinning Wasm runtimes to isolated cores via cset or taskset provides practical steps to ensure the Wasm runtime runs on the reserved RT cores, matching the field's emphasis on core isolation and dedicated scheduling. The field's mention of SCHED_FIFO as a real-time scheduling class is supported by discussions of real-time scheduling policies, illustrating how to grant hard or soft real-time priority to the Wasm workload. The Windows-related notes acknowledge that soft real-time capabilities exist on Windows, offering a contrast to Linux's RT guarantees, and the broader RTOS discussion in the field is complemented by general notes about RTOS characteristics in other OSes. Overall, the strongest support comes from the explicit PREEMPT_RT/Linux RT preemption material and the CPU/core isolation and paging/pinning guidance, while related Windows notes provide comparative context. Each piece ties directly to configuring an OS for RT-like Wasm workloads, and together they substantiate the field value's core claim about achieving reduced jitter and higher determinism through kernel-level RT features and CPU isolation.",
      "confidence": "high"
    },
    {
      "field": "os_jitter_reduction_techniques.4.technique",
      "citations": [
        {
          "title": "Utilizing the NUMA-aware Memory Manager",
          "url": "https://kubernetes.io/docs/tasks/administer-cluster/memory-manager/",
          "excerpts": [
            "The Kubernetes Memory Manager enables the feature of guaranteed memory (and hugepages) allocation for pods in the Guaranteed QoS class."
          ]
        },
        {
          "title": "The Black Magic of Systematically Reducing Linux OS Jitter",
          "url": "https://highscalability.com/the-black-magic-of-systematically-reducing-linux-os-jitter/",
          "excerpts": [
            "Using numactl, taskset, and isolcpus can all help individual threads with the jitter or hiccups they may experience (in addition to cache ...",
            "Turn THP (Transparent Huge Pages) OFF. * Set vm.min\\_free\\_kbytes to AT LEAST 1GB (8GB on larger systems). * Set Swappiness to 0. * Set zone\\_reclaim\\_"
          ]
        }
      ],
      "reasoning": "Page fault reduction is best supported by content that mentions guaranteeing memory and using hugepages, which reduce page faults by ensuring contiguous, reserved memory regions. This aligns with the idea of pre-allocating large, pinned memory to minimize paging delays. The excerpt discussing guaranteed memory allocation for pods and enormous pages directly supports this concept. Other excerpts discuss techniques to reduce OS jitter through CPU and memory locality (e.g., isolating CPUs and NUMA-aware binding) and memory tuning parameters like disabling Transparent Huge Pages and adjusting swappiness. These methods contribute to reducing paging interruptions and improving memory locality, which indirectly supports page fault reduction by lowering the likelihood and impact of paging events. Specifically, isolating CPUs and memory locality can improve cache and memory access patterns, while turning off features like THP and reducing swappiness aim to prevent costly page faults due to paging and swapping. Taken together, these excerpts describe concrete strategies to minimize page faults and paging-induced latency in a low-lcheduling, high-performance context, which is the essence of the requested Page Fault Reduction technique.",
      "confidence": "high"
    }
  ],
  "outputSchema": {
    "type": "object",
    "properties": {
      "executive_summary": {
        "type": "string",
        "description": "A high-level summary addressing the core questions of the user query. It clarifies that WebAssembly (WASM) relies on the host for scheduling, is not a real-time operating system (RTOS) but can be made highly predictable, and that Rust's concurrency features are key to building high-performance, non-browser WASM applications."
      },
      "wasm_has_native_scheduler": {
        "type": "boolean",
        "description": "A direct boolean answer to the question of whether WebAssembly has its own built-in scheduler."
      },
      "is_wasm_an_rtos_equivalent": {
        "type": "boolean",
        "description": "A direct boolean answer to whether a WASM environment is equivalent to a real-time operating system (RTOS) with inherent control over dedicated cores and memory."
      },
      "wasm_scheduling_and_execution_model": {
        "type": "object",
        "properties": {
          "browser_environment": {
            "type": "string",
            "description": "Details on how WASM is scheduled within a web browser."
          },
          "standalone_runtime_environment": {
            "type": "string",
            "description": "Details on scheduling within standalone runtimes like Wasmtime, including preemption mechanisms."
          },
          "orchestrated_environment": {
            "type": "string",
            "description": "Details on how WASM workloads are scheduled in container orchestrators like Kubernetes."
          }
        },
        "required": [
          "browser_environment",
          "standalone_runtime_environment",
          "orchestrated_environment"
        ],
        "additionalProperties": false
      },
      "rtos_like_behavior_analysis": {
        "type": "object",
        "properties": {
          "host_os_capabilities": {
            "type": "string",
            "description": "How host operating system features (like PREEMPT_RT Linux) can be configured to provide a real-time environment."
          },
          "kubernetes_mechanisms": {
            "type": "string",
            "description": "Kubernetes policies (like CPU Manager static policy) that enable predictable performance for WASM pods."
          },
          "limitations_and_gaps": {
            "type": "string",
            "description": "The fundamental limitations, such as the lack of direct scheduling control from within WASM, that prevent full RTOS equivalence."
          }
        },
        "required": [
          "host_os_capabilities",
          "kubernetes_mechanisms",
          "limitations_and_gaps"
        ],
        "additionalProperties": false
      },
      "leveraging_rust_concurrency_in_wasm": {
        "type": "object",
        "properties": {
          "core_rust_concepts": {
            "type": "string",
            "description": "Explanation of Rust's core concurrency features like the ownership model, Send/Sync traits, and atomics."
          },
          "practical_application_in_wasm": {
            "type": "string",
            "description": "How to apply Rust's concurrency in WASM, including the required compilation target (`wasm32-wasip1-threads`) and WASM features."
          },
          "supported_libraries_and_patterns": {
            "type": "string",
            "description": "An overview of which Rust concurrency libraries (e.g., std::thread, crossbeam, Rayon) work well in the WASM+WASI environment."
          }
        },
        "required": [
          "core_rust_concepts",
          "practical_application_in_wasm",
          "supported_libraries_and_patterns"
        ],
        "additionalProperties": false
      },
      "wasm_threading_proposals_overview": {
        "type": "object",
        "properties": {
          "proposal_name": {
            "type": "string",
            "description": "The official name of the WebAssembly threading-related proposal."
          },
          "standardization_phase": {
            "type": "string",
            "description": "The current phase of the proposal in the WebAssembly standardization process (e.g., 'Phase 4 - Standardized', 'Phase 1 - Legacy')."
          },
          "description": {
            "type": "string",
            "description": "A summary of the proposal's purpose and scope."
          },
          "key_features": {
            "type": "string",
            "description": "The main technical features introduced by the proposal, such as shared linear memory, atomics, or thread creation APIs."
          }
        },
        "required": [
          "proposal_name",
          "standardization_phase",
          "description",
          "key_features"
        ],
        "additionalProperties": false
      },
      "async_vs_threads_model_comparison": {
        "type": "object",
        "properties": {
          "async_model": {
            "type": "string",
            "description": "Analysis of the asynchronous programming model for WASM."
          },
          "threaded_model": {
            "type": "string",
            "description": "Analysis of the multi-threaded programming model for WASM."
          },
          "workload_guidance": {
            "type": "string",
            "description": "Recommendations on which model to choose for different workload profiles (I/O-bound vs. CPU-bound)."
          },
          "wasi_status_and_evolution": {
            "type": "string",
            "description": "The current and future support for both models within the WASI standard."
          }
        },
        "required": [
          "async_model",
          "threaded_model",
          "workload_guidance",
          "wasi_status_and_evolution"
        ],
        "additionalProperties": false
      },
      "os_jitter_reduction_techniques": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "technique": {
              "type": "string",
              "description": "The name of the technique used to reduce OS-induced jitter."
            },
            "category": {
              "type": "string",
              "description": "The category the technique falls into, such as 'CPU Management', 'Kernel Scheduling', 'Memory Management', or 'Interrupt Mitigation'."
            },
            "description": {
              "type": "string",
              "description": "A detailed explanation of what the technique does and how it improves predictability."
            },
            "tools_and_methods": {
              "type": "string",
              "description": "Specific tools, kernel parameters, or system calls used to implement the technique (e.g., 'isolcpus', 'taskset', 'SCHED_FIFO', 'mlock')."
            }
          },
          "required": [
            "technique",
            "category",
            "description",
            "tools_and_methods"
          ],
          "additionalProperties": false
        },
        "description": "A list of specific, advanced techniques for reducing OS-induced jitter to improve the predictability of WASM workloads. This includes kernel-level tuning, system configuration, and hardware settings."
      },
      "dedicated_resource_allocation_methods": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "environment": {
              "type": "string",
              "description": "The environment where the allocation method is applied (e.g., 'Linux Kernel (cgroups v2)', 'systemd', 'Kubernetes')."
            },
            "cpu_dedication_method": {
              "type": "string",
              "description": "The specific mechanism or configuration used to dedicate CPU resources (e.g., 'cpuset.cpus', 'CPU Manager static policy')."
            },
            "memory_dedication_method": {
              "type": "string",
              "description": "The specific mechanism or configuration used to dedicate memory resources (e.g., 'memory.max', 'Guaranteed QoS class')."
            },
            "configuration_summary": {
              "type": "string",
              "description": "A brief summary of the configuration steps required to implement the method."
            }
          },
          "required": [
            "environment",
            "cpu_dedication_method",
            "memory_dedication_method",
            "configuration_summary"
          ],
          "additionalProperties": false
        },
        "description": "A list of methods for allocating dedicated CPU cores and memory to WASM workloads across different environments. This covers direct kernel-level control, service manager configurations, and high-level orchestration policies."
      },
      "kernel_level_and_os_bypass_strategies": {
        "type": "object",
        "properties": {
          "strategy": {
            "type": "string",
            "description": "The name of the advanced performance strategy (e.g., 'Kernel-Wasm Execution', 'OS-Bypass Networking', 'io_uring Hostcalls')."
          },
          "architecture_summary": {
            "type": "string",
            "description": "A high-level description of how the strategy works."
          },
          "performance_impact": {
            "type": "string",
            "description": "Quantified performance improvements in terms of throughput, latency, or overhead reduction."
          },
          "security_tradeoffs": {
            "type": "string",
            "description": "An analysis of the security implications and increased attack surface associated with the strategy."
          }
        },
        "required": [
          "strategy",
          "architecture_summary",
          "performance_impact",
          "security_tradeoffs"
        ],
        "additionalProperties": false
      },
      "memory_architecture_and_allocators": {
        "type": "object",
        "properties": {
          "linear_memory_model": {
            "type": "string",
            "description": "Comparison of the default 32-bit linear memory model versus the `memory64` proposal, including trade-offs."
          },
          "allocator_comparison": {
            "type": "string",
            "description": "A comparison of different memory allocators (e.g., dlmalloc, mimalloc, jemalloc) available for WASM, detailing their characteristics and trade-offs."
          },
          "performance_strategies": {
            "type": "string",
            "description": "Techniques to improve memory performance, such as preallocation (arenas) and avoiding fragmentation."
          },
          "multithreading_considerations": {
            "type": "string",
            "description": "How shared memory and atomics are used in a multi-threaded WASM context."
          }
        },
        "required": [
          "linear_memory_model",
          "allocator_comparison",
          "performance_strategies",
          "multithreading_considerations"
        ],
        "additionalProperties": false
      },
      "observability_and_performance_engineering": {
        "type": "object",
        "properties": {
          "profiling": {
            "type": "string",
            "description": "An overview of profiling tools and techniques for WASM, including runtime-specific APIs and integration with standard tools like 'perf'."
          },
          "tracing": {
            "type": "string",
            "description": "Details on distributed tracing for WASM using OpenTelemetry, including the status of the WASI-OTel proposal."
          },
          "jitter_diagnosis": {
            "type": "string",
            "description": "How to use advanced host-level tools like eBPF (`runqlat`, `offcputime`) to diagnose OS-induced scheduler latency and jitter."
          },
          "common_pitfalls": {
            "type": "string",
            "description": "Common challenges and limitations in WASM performance analysis, such as issues with `perf` sampling methods."
          }
        },
        "required": [
          "profiling",
          "tracing",
          "jitter_diagnosis",
          "common_pitfalls"
        ],
        "additionalProperties": false
      },
      "high_performance_use_case_analysis": {
        "type": "object",
        "properties": {
          "use_case": {
            "type": "string",
            "description": "The specific high-performance application area being analyzed (e.g., 'Kafka Data Plane', 'Spark UDFs', 'Database Extensions')."
          },
          "architectural_patterns": {
            "type": "string",
            "description": "Common architectural patterns for integrating WASM, such as in-broker UDFs, sidecar processors, or JNI bridges."
          },
          "performance_considerations": {
            "type": "string",
            "description": "Key performance factors for the use case, including I/O limitations, data serialization overhead (e.g., with Apache Arrow), and hostcall costs."
          },
          "key_platforms_and_projects": {
            "type": "string",
            "description": "Examples of real-world platforms or projects implementing this use case (e.g., 'Redpanda Data Transforms', 'pg_extism')."
          }
        },
        "required": [
          "use_case",
          "architectural_patterns",
          "performance_considerations",
          "key_platforms_and_projects"
        ],
        "additionalProperties": false
      },
      "wasm_platform_case_studies": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "platform_name": {
              "type": "string",
              "description": "The name of the server-side WASM platform (e.g., 'wasmCloud', 'Fermyon Spin', 'Faasm')."
            },
            "architecture": {
              "type": "string",
              "description": "A summary of the platform's core architecture and components."
            },
            "concurrency_and_scheduling_model": {
              "type": "string",
              "description": "How the platform handles concurrency, scheduling, and execution of WASM components."
            },
            "data_plane_and_io_integration": {
              "type": "string",
              "description": "How the platform integrates with data services and handles I/O operations."
            },
            "ideal_use_cases": {
              "type": "string",
              "description": "The types of applications and workloads for which the platform is best suited."
            }
          },
          "required": [
            "platform_name",
            "architecture",
            "concurrency_and_scheduling_model",
            "data_plane_and_io_integration",
            "ideal_use_cases"
          ],
          "additionalProperties": false
        },
        "description": "A comparative analysis of prominent server-side WASM platforms, detailing their architectures, concurrency models, and suitability for high-throughput, low-latency applications. Each platform represents a different approach to building and deploying WASM services."
      },
      "deployment_patterns_comparison": {
        "type": "object",
        "properties": {
          "pattern": {
            "type": "string",
            "description": "The name of the deployment pattern (e.g., 'runwasi Shims', 'Traditional Containers', 'Firecracker MicroVMs')."
          },
          "startup_time": {
            "type": "string",
            "description": "Typical cold and warm start performance for the pattern."
          },
          "isolation_strength": {
            "type": "string",
            "description": "The level of security and isolation provided (e.g., 'Process-level', 'Hardware Virtualization')."
          },
          "density_and_footprint": {
            "type": "string",
            "description": "The resource efficiency of the pattern in terms of memory footprint and potential workload density."
          },
          "ideal_workloads": {
            "type": "string",
            "description": "The types of workloads for which this pattern is most suitable (e.g., 'low-latency APIs', 'batch jobs')."
          }
        },
        "required": [
          "pattern",
          "startup_time",
          "isolation_strength",
          "density_and_footprint",
          "ideal_workloads"
        ],
        "additionalProperties": false
      },
      "wasm_runtime_performance_comparison": {
        "type": "object",
        "properties": {
          "runtime_name": {
            "type": "string",
            "description": "The name of the WebAssembly runtime (e.g., 'Wasmtime', 'Wasmer', 'WAMR', 'WasmEdge')."
          },
          "threading_support": {
            "type": "string",
            "description": "The status and details of the runtime's support for multi-threading proposals like `wasi-threads`."
          },
          "compilation_backends": {
            "type": "string",
            "description": "The compiler backends supported by the runtime (e.g., 'Cranelift', 'LLVM', 'Singlepass')."
          },
          "key_performance_characteristics": {
            "type": "string",
            "description": "A summary of the runtime's performance profile, including cold start, throughput, and any notable strengths or weaknesses."
          }
        },
        "required": [
          "runtime_name",
          "threading_support",
          "compilation_backends",
          "key_performance_characteristics"
        ],
        "additionalProperties": false
      },
      "component_model_architectural_impact": {
        "type": "object",
        "properties": {
          "core_concepts": {
            "type": "string",
            "description": "Explanation of the core concepts of the Component Model, including WIT, worlds, and the Canonical ABI."
          },
          "performance_tradeoffs": {
            "type": "string",
            "description": "An analysis of the performance overhead from data marshalling and call indirection compared to monolithic modules."
          },
          "ecosystem_maturity": {
            "type": "string",
            "description": "The current status of standardization, runtime support, and tooling for the Component Model as of 2025."
          },
          "architectural_benefits": {
            "type": "string",
            "description": "The advantages for system architecture, such as improved maintainability, versioning, and language interoperability."
          }
        },
        "required": [
          "core_concepts",
          "performance_tradeoffs",
          "ecosystem_maturity",
          "architectural_benefits"
        ],
        "additionalProperties": false
      },
      "wasi_capability_and_io_model": {
        "type": "object",
        "properties": {
          "security_model": {
            "type": "string",
            "description": "An explanation of WASI's capability-based security model and its core mechanisms like preopened resources."
          },
          "io_proposals_status": {
            "type": "string",
            "description": "The status and description of key I/O proposals like `wasi-io`, `wasi-sockets`, and the upcoming native async support in WASI 0.3."
          },
          "performance_bottlenecks": {
            "type": "string",
            "description": "Identified performance issues, such as the overhead of synchronous file I/O in current runtimes."
          },
          "overhead_mitigation_techniques": {
            "type": "string",
            "description": "Strategies to reduce performance overhead, such as batching hostcalls and preparing for zero-copy data transfer."
          }
        },
        "required": [
          "security_model",
          "io_proposals_status",
          "performance_bottlenecks",
          "overhead_mitigation_techniques"
        ],
        "additionalProperties": false
      },
      "summary_of_limitations_and_future_outlook": {
        "type": "object",
        "properties": {
          "current_bottlenecks": {
            "type": "string",
            "description": "A summary of the key technical gaps preventing near-real-time performance today, categorized by area (e.g., Scheduling, I/O, Threading)."
          },
          "community_proposals_to_watch": {
            "type": "string",
            "description": "A list of the most important upcoming WASI and WebAssembly proposals that aim to address these bottlenecks."
          },
          "realistic_timelines": {
            "type": "string",
            "description": "Estimated timelines for the stabilization of key features like native async I/O."
          },
          "risk_assessment_for_adopters": {
            "type": "string",
            "description": "An assessment of the risks (e.g., API instability, ecosystem gaps) for organizations adopting WASM for production use in its current state."
          }
        },
        "required": [
          "current_bottlenecks",
          "community_proposals_to_watch",
          "realistic_timelines",
          "risk_assessment_for_adopters"
        ],
        "additionalProperties": false
      }
    },
    "required": [
      "executive_summary",
      "wasm_has_native_scheduler",
      "is_wasm_an_rtos_equivalent",
      "wasm_scheduling_and_execution_model",
      "rtos_like_behavior_analysis",
      "leveraging_rust_concurrency_in_wasm",
      "wasm_threading_proposals_overview",
      "async_vs_threads_model_comparison",
      "os_jitter_reduction_techniques",
      "dedicated_resource_allocation_methods",
      "kernel_level_and_os_bypass_strategies",
      "memory_architecture_and_allocators",
      "observability_and_performance_engineering",
      "high_performance_use_case_analysis",
      "wasm_platform_case_studies",
      "deployment_patterns_comparison",
      "wasm_runtime_performance_comparison",
      "component_model_architectural_impact",
      "wasi_capability_and_io_model",
      "summary_of_limitations_and_future_outlook"
    ],
    "additionalProperties": false
  }
}