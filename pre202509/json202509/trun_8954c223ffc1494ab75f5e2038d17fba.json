{
  "input": "**Role:**\n\nYou are a world-class **Systems Architecture Committee** tasked with creating the definitive technical blueprint for a novel, high-determinism operating system. Your committee consists of four members:\n\n1.  **Lead Systems Architect:** Responsible for the high-level vision, component interplay, and ensuring the architecture is cohesive and sound.\n2.  **Principal Kernel Engineer:** Focused on the low-level implementation details of memory management, scheduling, and hardware interaction.\n3.  **Rust Language & Concurrency Expert:** Ensures the design maximally leverages Rust's safety, concurrency features, and `no_std` ecosystem.\n4.  **Senior Project Manager:** Responsible for breaking down the work, estimating effort, and identifying risks.\n\nCONTEXT & HARDWARE BASELINE\n- Host OS: Ubuntu 22.04 LTS (64‑bit)\n- Machine: Lenovo Y540, Intel Core i7‑9750H (6C/12T), 16 GiB RAM\n- We will click/run a Rust host binary that launches QEMU/KVM with our RTOS guest, which communicates over the terminal/console.\n- We want a fully working MVP 1.0, not just a kernel bring‑up.\n\n**Context:**\n\nThe project's objective is to create an **application-specific, unikernel-style Real-Time Operating System (RTOS) in Rust**. The primary motivation is to overcome the performance jitter inherent in general-purpose operating systems like Linux, thereby providing a deterministic environment for latency-sensitive applications.\n\nThe core architectural constraint is to **avoid writing drivers for physical hardware**. Instead, the RTOS will be designed to run as a guest in a hypervisor (like KVM/QEMU) on a Linux host. It will exclusively use the **VirtIO paravirtualization standard** for all I/O (networking, block storage, console), delegating physical device management to the host.\n\nFor this exercise, the target application is a **high-frequency network packet processor** that must handle millions of packets per second with sub-10-microsecond P99.99 latency.\n\n**Mission:**\n\nYour committee's mission is to produce a comprehensive technical blueprint that addresses the following, providing extreme depth and detail:\n\n**1. Deep Architectural Specification:**\nProvide a detailed, multi-layered specification of the RTOS. Break down the architecture into the following core subsystems. For each subsystem, describe its design, key data structures, and interactions with other components. Provide illustrative Rust code snippets where appropriate.\n\n* **A. Boot & Initialization (`bootloader` -> Kernel Entry):**\n    * The boot process flow.\n    * Memory map setup passed from the bootloader.\n    * Kernel stack setup.\n    * Transition from 64-bit long mode into the Rust kernel's `_start` function.\n* **B. Memory Management:**\n    * **Physical Memory Manager (Frame Allocator):** The algorithm for tracking available physical memory frames (e.g., bitmap, free list).\n    * **Virtual Memory Manager (Pager):** Design of the page tables (e.g., 4-level paging), address space layout, and handling of higher-half kernel mapping.\n    * **Heap Allocator:** How to integrate a `no_std` allocator like `talc` or `linked_list_allocator`.\n* **C. Scheduler & Concurrency:**\n    * **Scheduler Design:** Justify the choice of a scheduler (e.g., Fixed-Priority Preemptive Scheduling). Describe the run queue data structure.\n    * **Task Control Block (TCB):** Define the `Task` struct, including fields for state, priority, stack pointer, and page table root.\n    * **Context Switching:** Detail the precise steps for saving and restoring CPU registers, stack pointers, and address spaces.\n    * **Concurrency Primitives:** Explain the implementation of fundamental primitives like spinlocks and mutexes using Rust's atomics.\n* **D. Interrupt & System Call Handling:**\n    * **Interrupt Descriptor Table (IDT):** How it will be set up and loaded.\n    * **Hardware Interrupt Flow:** The path from a hardware interrupt (e.g., timer tick) to the scheduler invocation.\n    * **System Call Interface:** The design of the `syscall` mechanism (e.g., using the `SYSCALL`/`SYSRET` instructions). Define the primary system calls needed (e.g., `task_create`, `yield`, `sleep`).\n* **E. I/O Subsystem (The VirtIO Abstraction Layer):**\n    * The architectural layer that integrates the `virtio-drivers` crate.\n    * Design for handling device discovery over MMIO.\n    * The asynchronous I/O model or interrupt-driven model for handling VirtIO queue notifications.\n\n**2. Code & Effort Estimation:**\nProvide a detailed estimation of the effort required to build this RTOS from scratch.\n\n* **Breakdown Table:** Present the estimates in a Markdown table with columns for **Subsystem**, **Estimated Lines of Code (LoC) in Rust**, **Estimated Person-Months**, and **Key Assumptions & Rationale**.\n* **Rationale:** For each estimate, provide a brief justification. For example, \"Memory Management is estimated at 2,000 LoC due to the complexity of setting up 4-level paging and a robust frame allocator, assuming no external crates for paging logic.\"\n\n**3. Technology & Crate Recommendations:**\nList the essential Rust crates and tools required. Present this in a table with columns for **Component Type** (e.g., Bootloader, Heap Allocator), **Recommended Crate/Tool**, and **Reason for Choice**.\n\n**4. Risk Analysis & Mitigation:**\nIdentify the top 3-5 technical challenges or risks in this project. For each risk, describe a potential mitigation strategy.\n\n\n\n**Output Format:**\n\n* Use **Markdown** for clear, structured formatting.\n* Use `##` for main sections and `###` for subsystems.\n* Provide **Rust code snippets** in fenced code blocks to illustrate key data structures and logic.\n* Use **tables** for the effort estimation and technology recommendations.\n* Ensure the language is precise, technical, and detailed, as expected from a senior engineering committee.",
  "output": {
    "project_overview": "The project's objective is to create an application-specific, unikernel-style Real-Time Operating System (RTOS) in Rust. The primary motivation is to overcome the performance jitter inherent in general-purpose operating systems like Linux, thereby providing a deterministic environment for latency-sensitive applications. The core architectural constraint is to avoid writing drivers for physical hardware; instead, the RTOS will be designed to run as a guest in a hypervisor (KVM/QEMU) on a Linux host. It will exclusively use the VirtIO paravirtualization standard for all I/O (networking, block storage, console), delegating physical device management to the host. The target application for this RTOS is a high-frequency network packet processor that must handle millions of packets per second with a stringent sub-10-microsecond P99.99 latency requirement.",
    "executive_summary": "This technical blueprint outlines the architecture for a high-determinism, unikernel-style RTOS in Rust, designed for a sub-10µs latency network packet processor running as a QEMU/KVM guest. The architecture is centered on a single address space with a higher-half kernel layout, leveraging Rust's `no_std` ecosystem. The boot process will utilize the `bootimage` toolchain for simplicity and integration. Memory management is built upon a Buddy Allocator for physical frames to ensure deterministic allocation and aggressive use of 1GiB/2MiB huge pages to minimize TLB misses. The heap will be managed by the `talc` allocator due to its performance and determinism. The scheduler is a fixed-priority preemptive design with an O(1) bitmap-based run-queue to guarantee bounded task selection latency. All I/O is handled exclusively through VirtIO, with the `virtio-drivers` crate providing the guest-side implementation. To meet the stringent latency target, the I/O model eschews traditional interrupts in the hot path, favoring a polling-based datapath, ideally accelerated by a `vhost-user` backend on the host. The top technical risks identified are host scheduling jitter, TLB misses, and VirtIO interrupt storms. These will be mitigated through a combination of host-level tuning (CPU pinning, `isolcpus`, `nohz_full`), static memory mapping with huge pages, and a kernel-bypass, polling-based I/O model.",
    "boot_and_initialization_specification": {
      "bootloader_choice": "The primary recommended approach is the `bootimage` toolchain, which utilizes the `bootloader` crate. This choice is justified by its excellent developer experience, seamless integration with Cargo, and its ability to abstract away the low-level complexities of BIOS vs. UEFI booting and CPU mode transitions. It automates the creation of a bootable disk image and simplifies testing with a built-in QEMU runner. As a more robust alternative for a production system where stability and a standardized interface are paramount, the Multiboot2 specification with a bootloader like GRUB is a strong secondary option. Multiboot2 provides more direct control to the kernel over the boot process but requires more manual setup, including the kernel handling the 32-bit to 64-bit mode transition itself.",
      "boot_process_flow": "The boot process follows a precise sequence. It begins with the CPU reset in 16-bit Real Mode, where the BIOS or UEFI firmware executes and hands control to the bootloader (e.g., the `bootloader` crate's implementation). The bootloader then performs the transition to 32-bit Protected Mode by enabling the A20 line, loading a 32-bit GDT, setting the PE bit in CR0, and executing a far jump. Subsequently, it transitions to 64-bit Long Mode by enabling PAE, setting up a 4-level page table hierarchy, loading the PML4 address into CR3, enabling Long Mode via the EFER MSR, and enabling paging in CR0. After loading a new GDT for 64-bit mode, a final far jump places the CPU in 64-bit mode. The bootloader then sets up the initial kernel stack by pointing the RSP register to a pre-allocated memory region and finally jumps to the kernel's `_start` entry point, transferring control to the Rust code.",
      "cpu_mode_transition": "The CPU transitions from its initial 16-bit real mode state to 64-bit long mode through a multi-phase process managed by the bootloader. First, to enter 32-bit protected mode, a Global Descriptor Table (GDT) with 32-bit segments is loaded via `lgdt`, and the Protection Enable (PE) bit in the CR0 register is set. A far jump then flushes the pipeline and activates 32-bit execution. To enter 64-bit long mode, the bootloader enables Physical Address Extension (PAE) in CR4, creates a 4-level page table structure, and loads its physical base address into the CR3 register. The Long Mode Enable (LME) bit is set in the EFER MSR, and the Paging (PG) bit is set in CR0, which together activate long mode. A new GDT with a 64-bit code segment is loaded, and a final far jump to this segment selector completes the transition, placing the CPU in 64-bit mode ready to execute the kernel.",
      "memory_map_handoff": "The physical memory map is passed from the bootloader to the kernel via a high-level, abstracted data structure. When using the recommended `bootimage` approach, the bootloader populates a `bootloader_api::BootInfo` struct. This struct is passed as the sole argument to the kernel's main function. The memory map is accessible via the `boot_info.memory_regions` field, which provides a safe iterator over all memory regions, detailing their physical start addresses, sizes, and types (e.g., usable, reserved, ACPI). This mechanism shields the kernel from the low-level details of BIOS E820 calls or UEFI memory services. In the alternative Multiboot2 approach, the memory map is passed as a tag within the Multiboot2 Information (MBI) structure, whose physical address is provided in the EBX register upon kernel entry.",
      "kernel_entry_details": "The kernel's entry point is a `no_mangle` Rust function with the signature `fn(boot_info: &'static mut bootloader_api::BootInfo) -> !`. This function is wrapped by the `bootloader_api::entry_point!` macro, which generates the actual `_start` symbol that the bootloader jumps to. The Application Binary Interface (ABI) is therefore clean and Rust-native. Upon entry, the CPU is in 64-bit long mode, a 4-level paging hierarchy is active, and the stack pointer (`RSP`) has been initialized by the bootloader to point to the top of a valid, pre-allocated stack region of a configurable size. The `boot_info` argument provides all necessary information for the kernel to initialize its subsystems, including the memory map for the frame allocator and the physical memory offset for the virtual memory manager."
    },
    "physical_memory_management": {
      "frame_allocator_algorithm": "The chosen algorithm is the Buddy Allocator. This decision is justified by its deterministic performance characteristics, with allocation and deallocation operations having a time complexity of O(log N), where N is the total number of frames. This bounded performance is critical for an RTOS targeting sub-microsecond latencies. The Buddy Allocator also excels at reducing external fragmentation and can efficiently service requests for contiguous blocks of memory of various sizes (e.g., 4KB, 8KB, 2MB), which is a key requirement for setting up VirtIO virtqueues and supporting huge pages in the virtual memory manager.",
      "data_structures": "The Buddy Allocator implementation will rely on two primary data structures. The first is an array of free lists, where each element of the array corresponds to a specific block size (order). For example, `free_lists[0]` would be a linked list of all free 4KB blocks, `free_lists[1]` for 8KB blocks, and so on. The second is a metadata structure, likely implemented as a bitmap or a byte array, that tracks the state of every physical frame. This metadata is essential for the coalescing process during deallocation, as it allows the allocator to efficiently determine if a freed block's 'buddy' is also free and of the same order, enabling them to be merged into a larger block.",
      "initialization_process": "The frame allocator is initialized once the kernel receives the `BootInfo` struct from the bootloader. The kernel will iterate through the memory map provided in `boot_info.memory_regions`. For each region marked as 'Available' (Type 1), the allocator will add the corresponding physical frames to its free lists. Crucially, before adding frames, the kernel must account for and reserve memory regions that are already in use, even if they fall within 'Available' RAM. This includes the memory occupied by the kernel's own code and data segments, the bootloader-passed structures, and the memory required for the frame allocator's own data structures (the free lists and state bitmap).",
      "dma_considerations": "The physical memory manager is designed to directly support the DMA requirements of VirtIO. VirtIO devices require their virtqueues (Descriptor Table, Available Ring, Used Ring) to be located in physically contiguous memory. The Buddy Allocator is perfectly suited for this, as the driver can request a contiguous block of a specific order large enough to hold the entire virtqueue structure. The allocator returns page-aligned physical addresses, which can be programmed directly into the virtqueue address registers of the VirtIO device. For data buffers, VirtIO's support for scatter-gather I/O allows the driver to allocate multiple smaller, physically contiguous chunks from the frame allocator and chain them together in the descriptor table, reducing pressure to find large contiguous blocks for packet payloads."
    },
    "virtual_memory_management": {
      "paging_architecture": "The virtual memory system is based on the standard x86_64 4-level paging architecture, which uses a hierarchy of four tables: Page Map Level 4 (PML4), Page Directory Pointer Table (PDPT), Page Directory (PD), and Page Table (PT). This structure translates 48-bit virtual addresses into physical addresses. The entire system will operate within a single, static address space, meaning there is only one set of page tables for the entire unikernel. This design choice eliminates the need for address space switching, which is a major source of overhead in general-purpose operating systems.",
      "address_space_layout": "The virtual address space is divided into a lower half (user-space, largely unused in this unikernel design) and a higher half (kernel-space). The kernel-space layout is concretely defined to start at `0xFFFF800000000000`. Key regions include: a direct physical memory mapping area (e.g., starting at `0xFFFF888000000000`) that provides a 1:1 mapping of all physical RAM; a dedicated region for device MMIO (e.g., `0xFFFFC90000000000`); a region for the pre-allocated system heap and task stacks, with stacks surrounded by unmapped guard pages to detect overflows; and a region for the kernel's executable image (code/data) itself (e.g., `0xFFFFFFFF80000000`). A guard hole at the very bottom of the higher half helps catch null pointer dereferences.",
      "large_page_strategy": "The strategy is to use large pages as aggressively as possible to minimize TLB misses and reduce page walk overhead, which is critical for low-latency performance. Specifically, 1GiB huge pages will be used to map the entire direct physical memory region, providing massive TLB coverage for physical memory access. 2MiB pages will be used for mapping the kernel's code and data sections, the system heap, and device MMIO regions. Standard 4KiB pages will be avoided wherever possible, used only for fine-grained mappings where absolutely necessary. This strategy significantly increases the amount of memory that can be covered by the TLB, leading to a much higher TLB hit rate.",
      "tlb_management_policy": "The policy for managing the Translation Lookaside Buffer (TLB) is one of avoidance of invalidations during runtime. All page tables are created once at boot time, establishing a static memory map for the lifetime of the system. The physical address of the PML4 table is loaded into the CR3 register once during initialization and is never changed again. This completely eliminates CR3 reloads and the associated global TLB flushes. The `Global` (G) bit will be set on kernel-space page table entries to prevent them from being flushed. In the rare event a mapping needs to be changed (e.g., for device hot-plug), the targeted `INVLPG` instruction will be used instead of a full flush. Cross-core TLB shootdowns are entirely avoided by this static design.",
      "page_fault_handling": "The page fault handling policy is strict and simple: any page fault is treated as a critical, unrecoverable software bug, and the handler's only action is to panic. In this deterministic system where all memory is pre-mapped at boot, there are no legitimate reasons for a page fault to occur during normal operation (e.g., no demand paging or swapping). Therefore, a page fault signifies a severe error such as a null pointer dereference, a stack overflow hitting a guard page, or an access to an invalid memory address. The handler will capture diagnostic information (faulting address, instruction pointer) and halt or reboot the system, as attempting recovery would introduce non-determinism."
    },
    "heap_allocator_integration": {
      "chosen_allocator_crate": "The selected `no_std` heap allocator is the `talc` crate. This choice is justified by its high performance, heap efficiency, and strong determinism, with near-constant time allocation performance. It offers a flexible, user-definable locking mechanism via the `lock_api` trait and includes a modular Out-Of-Memory (OOM) handling system. Compared to simpler alternatives like `linked_list_allocator`, `talc` provides better fragmentation behavior and more predictable performance, making it more suitable for a high-determinism RTOS.",
      "initialization_strategy": "The heap is initialized once during kernel startup, after the physical and virtual memory managers are active. The kernel allocates a contiguous region of virtual address space of a fixed, predetermined size. This virtual region is then backed by physical frames obtained from the frame allocator. The virtual start address and size of this region are then passed to the heap allocator's `init` function. The heap does not support dynamic growth; its size is static for the lifetime of the system, which aligns with the goal of deterministic memory usage.",
      "synchronization_mechanism": "Thread safety for the global heap allocator is achieved through a spinlock. The `talc` allocator can be wrapped in a locking mechanism that implements the `lock_api::RawMutex` trait. A simple spinlock is an appropriate choice for a unikernel RTOS, as it provides a lightweight, non-blocking (in the sense of not yielding to a scheduler) synchronization primitive. This ensures that concurrent allocations and deallocations from different tasks or interrupt handlers do not corrupt the heap's internal data structures.",
      "out_of_memory_policy": "The policy for handling out-of-memory (OOM) conditions is to panic. In a real-time system with a fixed-size heap, memory exhaustion is considered a critical and unrecoverable error, indicating a flaw in system design or memory provisioning. The `talc` allocator's OOM handler will be configured to invoke the kernel's panic handler, which will halt the system after logging diagnostic information. This approach avoids non-deterministic behavior that would arise from attempting to recover from an OOM condition at runtime."
    },
    "scheduler_design": {
      "scheduling_algorithm": "Fixed-Priority Preemptive Scheduling (FPPS) is the chosen algorithm. This choice is justified by its suitability for hard real-time systems where predictable, low-latency responses are more critical than overall system throughput. FPPS assigns a static priority to each task, and the scheduler's core logic is to always execute the highest-priority task that is ready to run. This model is well-understood, has a simpler and lower-overhead implementation compared to dynamic-priority schedulers like Earliest Deadline First (EDF), and is supported by mature analysis techniques like Response Time Analysis (RTA). RTA allows for the formal verification of schedulability by calculating the worst-case response time for each task, which is essential for guaranteeing the sub-10µs latency target.",
      "run_queue_data_structure": "To ensure the scheduler's 'pick-next' operation has a bounded, constant-time complexity of O(1), a bitmap-based run queue is used. This structure consists of a bitmap where each bit corresponds to a priority level. A set bit indicates that at least one task at that priority is ready to run. Finding the highest-priority ready task is achieved with a single, highly efficient CPU instruction: `BSR` (Bit Scan Reverse) or `LZCNT` (Leading Zero Count) on x86, or `CLZ` (Count Leading Zeros) on ARM. These instructions have a typical latency of 1-4 cycles. This bitmap is used in conjunction with an array of linked lists, where each list holds the tasks for a specific priority level. The bitmap quickly identifies the highest-priority non-empty list, from which the scheduler can then dequeue the next task.",
      "priority_inversion_mitigation": "To handle priority inversion—where a high-priority task is blocked by a lower-priority task holding a resource—a robust protocol is required. The chosen strategies are the Priority Ceiling Protocol (PCP) or the Stack Resource Policy (SRP). These are superior to the simpler Priority Inheritance Protocol (PIP) for this use case because they offer stronger deterministic guarantees. While PIP bounds inversion, it does not prevent deadlocks or chained blocking. PCP and SRP, by assigning a 'ceiling' to resources and adjusting task priorities accordingly, prevent both deadlocks and chained blocking. This provides tighter, more predictable bounds on blocking time, which is a critical parameter in the schedulability analysis for a hard real-time system.",
      "timer_and_preemption_integration": "The policy is to use a 'tickless' or event-driven kernel to minimize jitter from periodic timer interrupts. The timer is programmed to fire only when the next scheduled event (e.g., a task's timeslice expiry or a timeout) is due. The preferred timer source is the CPU's per-core, high-resolution Timestamp Counter (TSC), specifically using the LAPIC's TSC-deadline mode. This is significantly faster and has lower overhead than shared platform timers like the HPET. For the most latency-sensitive packet processing paths, the system will employ a busy-polling model instead of relying on NIC interrupts, trading CPU cycles for predictable low latency by avoiding the high and variable cost of interrupt processing and context switching. Preemption points are defined on return from interrupts and system calls, where the kernel checks if a reschedule is needed."
    },
    "task_control_block_and_lifecycle": {
      "tcb_struct_definition": "The Task Control Block (TCB) is defined as a Rust struct, `Task`, which encapsulates all information about a single thread of execution. The struct includes: `id` (a unique identifier), `name` (a string for debugging), `state` (an enum representing the current execution state), `priority` (a numeric value for the scheduler), `cpu_affinity` (an optional core ID to pin the task to), `kernel_stack_pointer` (the saved `RSP` when the task is not running), `page_table_root` (a reference to the physical address of the task's top-level page table, isolating its virtual address space), and a reference for managing FPU/SIMD state (which is handled by `XSAVE`/`XRSTOR` during the context switch).",
      "task_lifecycle_states": "The task lifecycle is defined by a state machine with the following states, modeled after systems like Theseus OS: `Initing` (the initial state during task creation and setup), `Runnable` (the task is ready to execute and is in the scheduler's run queue), `Running` (the task is currently executing on a CPU), `Blocked` (the task is waiting for an event, such as I/O completion or a lock), `Exited` (the task has finished execution but its TCB and exit code are preserved for collection), and `Reaped` (the task's resources have been fully cleaned up and its TCB deallocated).",
      "lifecycle_management": "The API for managing task transitions is integrated with the scheduler and kernel services. A new task is created with a function like `Task::new()`, which allocates its TCB and stack, placing it in the `Initing` state. Once initialized, it is moved to the `Runnable` state via a call like `Task::unblock()`, making it eligible for scheduling. A running task can voluntarily move to the `Blocked` state by calling a blocking primitive (e.g., waiting on a mutex), which invokes `Task::block()`. An external event, such as an interrupt handler completing an I/O request, moves a task from `Blocked` back to `Runnable` by calling `Task::unblock()`. When a task terminates, it enters the `Exited` state, and a parent or reaper task can then collect its status and fully deallocate its resources.",
      "stack_management": "Each task is allocated its own stack, with the size being configurable at creation time. The primary strategy for stack overflow protection is the use of guard pages. A virtual memory page is placed at the end of the stack's allocated memory region and is marked in the page table with no-access permissions (i.e., the `Present` bit is cleared). If the task's stack usage exceeds its allocated bounds and attempts to access the guard page, the CPU will trigger a page fault exception. The kernel's page fault handler will then identify this as a stack overflow and can deterministically handle it, typically by terminating the offending task to prevent system-wide corruption."
    },
    "context_switching_mechanism": {
      "register_save_restore_set": "The set of saved registers depends on the switch path. For a voluntary context switch, which follows the standard function calling convention, only the callee-saved registers as defined by the x86-64 System V ABI must be preserved. This includes the general-purpose registers `RBP`, `RBX`, and `R12` through `R15`, as well as the stack pointer `RSP`. For an interrupt-driven context switch, the CPU hardware automatically saves a minimal context onto the stack, including the original `SS` and `RSP`, `RFLAGS`, `CS`, and the instruction pointer `RIP`, plus an optional error code. The interrupt handler is then responsible for saving any additional registers it uses.",
      "fpu_simd_state_handling": "An eager policy is used for handling FPU/SSE/AVX state to ensure deterministic context switch latency. This policy saves and restores the extended register state on every context switch, avoiding the unpredictable latency of the 'Device Not Available' (#NM) exception associated with a lazy policy. The modern `XSAVE` feature set is used over the legacy `FXSAVE`. The OS enables this feature by setting `CR4.OSXSAVE` and configures which state components to manage (e.g., x87, SSE, AVX) via the `XCR0` register. The `XSAVEOPT` instruction is preferred as it can optimize the save operation by skipping components that have not been modified.",
      "switch_paths": "There are two distinct paths for a context switch. The interrupt-driven path is initiated by hardware in response to an interrupt or exception. The CPU automatically pushes a specific interrupt frame onto the stack and transfers control to a handler defined in the Interrupt Descriptor Table (IDT). The voluntary switch path is initiated by software, for example, when a task yields the CPU. This is implemented as a function call that manually saves the required set of callee-saved registers, updates the task's TCB with the current stack pointer, and then restores the context of the next task to run.",
      "implementation_details": "The context switch routine is implemented in Rust using the `asm!` or `global_asm!` macros. To prevent the Rust compiler from generating a function prologue and epilogue that would interfere with the manual register manipulation, the function is marked with the `#[naked]` attribute. This gives complete control over the stack and registers. This approach requires careful adherence to the x86-64 System V ABI to ensure safety and interoperability. It also has significant implications for debugging, as it breaks standard stack unwinding, requiring manual stack management to trace calls across a context switch."
    },
    "concurrency_primitives_implementation": {
      "spinlock_design": "Spinlocks are implemented using atomic operations from `core::sync::atomic`. The design considers the trade-off between fairness and latency. For simple, low-contention locks, a Test-and-Test-and-Set (TTAS) lock is used, where threads spin on a local cache read before attempting an expensive atomic write. For high-contention scenarios where fairness is critical, a fair lock like a Ticket Lock or an MCS (Mellor-Crummey and Scott) lock is implemented. MCS locks are particularly scalable as each waiting thread spins on a unique, local memory location, eliminating cache-line ping-pong on a central lock variable.",
      "lock_free_data_structures": "For the packet datapath, where non-blocking communication is essential, lock-free ring buffers are designed for both Single-Producer, Single-Consumer (SPSC) and Multi-Producer, Single-Consumer (MPSC) use cases. These are implemented as fixed-size, array-based circular buffers managed by atomic head and tail pointers. This design provides guaranteed O(1) enqueue and dequeue operations. The SPSC variant is simpler as producers and consumers modify different pointers. The MPSC variant requires a more complex protocol, typically using a Compare-and-Swap (CAS) operation on the tail pointer to allow multiple producers to safely claim a slot.",
      "memory_ordering_rationale": "The choice of memory ordering is critical for balancing correctness and performance. The standard `Acquire-Release` semantic pairing is used for most synchronization primitives. A lock acquisition operation (e.g., a successful `compare_exchange`) uses `Ordering::Acquire`. This ensures that no memory operations in the critical section can be reordered to occur before the lock is acquired. The corresponding lock release operation (e.g., a `store`) uses `Ordering::Release`, ensuring that all memory operations within the critical section are visible to other threads before the lock is released. The stronger, more expensive `Ordering::SeqCst` is avoided unless a global total order of operations is strictly necessary.",
      "backoff_strategies": "To avoid performance degradation from busy-waiting and contention, several backoff strategies are employed. The primary mechanism is `core::hint::spin_loop()`, which inserts a `PAUSE` instruction on x86 or `YIELD`/`WFE` on ARM, signaling to the CPU that it is in a spin-wait loop. This reduces power consumption and improves performance on hyper-threaded cores. For higher contention, this is combined with an exponential backoff algorithm with added jitter, where a thread waits for an exponentially increasing, randomized period between failed lock acquisition attempts. This prevents multiple threads from retrying simultaneously and causing further contention."
    },
    "interrupt_and_fault_handling_framework": {
      "descriptor_tables_setup": "The setup process involves creating and loading three critical data structures using the `x86_64` Rust crate. First, a Global Descriptor Table (GDT) is created with essential entries for a 64-bit kernel code segment and a Task State Segment (TSS) descriptor. Second, the TSS is defined, with its primary purpose in 64-bit mode being to hold the Interrupt Stack Table (IST) pointers for reliable fault handling. Finally, an Interrupt Descriptor Table (IDT) is created with 16-byte gate descriptors to accommodate 64-bit handler addresses. The process concludes by loading these tables into the CPU using the `lgdt`, `load_tss`, and `lidt` instructions, making the system ready to handle exceptions and interrupts.",
      "critical_fault_handling": "To ensure the system can recover from critical, non-maskable, or nested exceptions, the Interrupt Stack Table (IST) is used. This x86_64 feature provides up to seven alternative, known-good stacks for specific interrupt handlers. The design configures the IDT entries for Double Fault (#DF, vector 8) and Non-Maskable Interrupts (NMI, vector 2) to use a dedicated stack from the IST (e.g., IST index 0). This prevents a scenario where a fault occurs on an already corrupted or exhausted stack, which would otherwise lead to an unrecoverable triple fault and system reset. By switching to a clean stack, the handler can reliably execute, log diagnostic information, and safely halt the system.",
      "interrupt_flow_and_nesting": "The path begins when a hardware interrupt is asserted. The CPU suspends the current task, uses the interrupt vector to find the corresponding gate descriptor in the IDT, and jumps to the specified handler. For determinism, interrupt control is strictly managed. Standard external interrupts (IRQs) are masked and unmasked using `CLI`/`STI` instructions, typically wrapped by functions like `x86_64::instructions::interrupts::disable()` and `enable()`. Nesting rules are enforced to maintain predictability: a higher-priority interrupt is allowed to preempt a lower-priority handler, but interrupts of the same or lower priority are masked until the current handler completes, preventing unbounded latency and ensuring critical events are serviced promptly.",
      "deferred_processing_strategy": "To maintain low interrupt latency, the work performed directly within an interrupt handler (the 'top-half') is kept to an absolute minimum. The handler's only responsibilities are to perform time-critical actions like acknowledging the interrupt at the controller and scheduling further processing. All longer-running tasks are deferred to a non-interrupt context (the 'bottom-half'). This is achieved using mechanisms analogous to Linux's softirqs or workqueues, where the top-half handler schedules a function to be run later with interrupts enabled. For networking, this is embodied by the NAPI-like model, where the hard IRQ handler simply disables further interrupts and schedules a polling task to process packet batches, thus moving the bulk of the work out of the critical interrupt context."
    },
    "system_call_interface_design": {
      "mechanism_choice": "The chosen mechanism for system calls on the x86_64 architecture is the pair of `SYSCALL` and `SYSRET` instructions. This method is significantly faster than the legacy `INT 0x80` software interrupt because it bypasses the IDT and uses dedicated MSRs (`IA32_LSTAR`, `IA32_STAR`, `IA32_FMASK`) for a direct, optimized transition between user mode (CPL3) and kernel mode (CPL0). Even in a unikernel with a single address space, this privilege-level transition provides a crucial logical boundary, protecting core RTOS state and functions from the application logic.",
      "abi_definition": "The Application Binary Interface (ABI) for system calls is modeled after the standard Linux kernel ABI for x86-64 to ensure consistency and leverage established conventions. The system call number is passed in the `RAX` register. The first six integer or pointer arguments are passed in the registers `RDI`, `RSI`, `RDX`, `R10`, `R8`, and `R9`, in that order. The return value is placed back into the `RAX` register. By convention, a negative return value in the range -4095 to -1 indicates an error, corresponding to a negated `errno` value. This ABI will be wrapped by safe Rust stubs that handle marshalling arguments and interpreting return values.",
      "minimal_syscall_set": "For an application-specific RTOS focused on packet processing, a minimal and deterministic set of system calls is defined to reduce the kernel's attack surface and complexity. The essential syscalls for the MVP are: `task_create` to instantiate new threads of execution, `yield` to allow a task to voluntarily cede the CPU to the scheduler, and `sleep` to place the current task in a waiting state for a specified duration. These primitives provide the fundamental building blocks for concurrency and scheduling management required by the application.",
      "performance_optimizations": "To mitigate the inherent overhead of CPU mode switches, a vDSO (virtual dynamic shared object)-like mechanism will be implemented. The unikernel will map a small, read-only page of memory containing optimized implementations of frequently used, read-only system calls into the application's address space. Candidate calls include operations like `get_time()` or `get_current_task_id()`. This allows the application to invoke these functions via a simple function call, executing entirely in user space without incurring the cost of a `SYSCALL` transition, which can yield significant performance improvements (e.g., a 4x speedup for `gettimeofday`)."
    },
    "virtio_abstraction_layer": {
      "integration_crate": "The chosen crate for implementing the VirtIO guest drivers is `rcore-os/virtio-drivers`. This crate is specifically designed for building `no_std` guest-side drivers in Rust, making it a perfect fit for the unikernel RTOS. It provides comprehensive support for a wide array of device types (including Net, Block, and Console) and modern transports (MMIO v2 and PCI), and it exposes a clear API for device initialization and interaction. Its guest-centric design is superior to host-side VMM-focused crates like `rust-vmm/vm-virtio` for this project's goals.",
      "device_discovery": "VirtIO device discovery over MMIO will be handled by parsing the Device Tree Blob (DTB). In the QEMU `virt` machine environment, there is no discoverable bus for MMIO devices. Instead, QEMU generates a DTB at boot time that describes the complete hardware layout, including the physical base addresses, sizes, and interrupt lines for all emulated `virtio-mmio` devices. The unikernel will parse this DTB to reliably locate the devices, which is a robust approach that avoids the fragility of hard-coding memory addresses that may change between QEMU versions.",
      "device_initialization": "The device initialization process strictly follows the standardized step-by-step flow defined in the VirtIO v1.0+ specification to ensure compatibility and correct operation. The sequence is: 1. Reset the device by writing 0 to its status register. 2. Acknowledge the device by setting the `ACKNOWLEDGE` status bit. 3. Indicate driver readiness by setting the `DRIVER` bit. 4. Negotiate features by reading the device's feature set and writing back the subset supported by the driver. 5. Finalize feature negotiation by setting the `FEATURES_OK` bit and verifying it remains set. 6. Perform any device-specific setup. 7. Finally, set the `DRIVER_OK` bit to bring the device to a live, operational state.",
      "virtqueue_setup": "The setup of each virtqueue is a precise, multi-step process. For each queue offered by the device, the driver first selects it and verifies it is not already active. It then reads the maximum supported queue size and allocates a single, physically contiguous, and zero-initialized block of memory to hold the three components: the Descriptor Table, the Available Ring, and the Used Ring. The allocation must respect the specific alignment requirements of each component. The driver then writes the 64-bit physical base addresses of these three structures to the device's configuration registers, sets the desired queue size, and finally activates the queue by writing `1` to its `QueueReady` register."
    },
    "virtio_network_datapath_model": {
      "io_model": "To achieve the lowest possible latency, the chosen I/O model is a hybrid NAPI-like system that can be configured for pure polling on the critical datapath. While a hybrid model (which starts with an interrupt and then switches to polling) is efficient, the sub-10µs P99.99 target necessitates a pure polling model for the hot path. This involves a dedicated thread (a Poll Mode Driver or PMD) running in a tight loop, constantly checking the virtqueue's used ring for completed packets. This approach completely eliminates interrupt and context-switching overhead from the packet processing loop, trading CPU cycles for minimal and predictable latency.",
      "queue_topology_and_pinning": "The design leverages multi-queue virtio-net (`VIRTIO_NET_F_MQ`) to enable parallel packet processing and avoid single-queue bottlenecks. The topology is designed for maximum isolation and cache efficiency: each vCPU in the guest is pinned to a dedicated, isolated physical core on the host (using `isolcpus`, `nohz_full`). Each of these vCPUs is then assigned its own private virtio-net queue pair (RX/TX). This 1:1 mapping of queues to cores ensures that there is no lock contention on queue access and that all data related to a queue's processing remains hot in its corresponding core's caches.",
      "host_backend_strategy": "The recommended and highest-performance host-side backend is `vhost-user`. This protocol allows QEMU to delegate the entire virtio-net datapath to a separate user-space process on the host, typically a DPDK-based application or virtual switch. By sharing memory regions over a Unix domain socket, `vhost-user` enables a zero-copy, kernel-bypass pipeline. This avoids the overhead of the host kernel's networking stack and context switches associated with `vhost-kernel`, making it the essential choice for latency-sensitive workloads aiming for bare-metal performance.",
      "descriptor_and_interrupt_management": "The strategy for managing descriptors and interrupts is optimized for minimal latency. To avoid batching-induced delays that can harm tail latency, packets are processed with minimal batching. Interrupts are aggressively suppressed to eliminate their high and variable overhead. The `VIRTIO_F_EVENT_IDX` feature is used to programmatically disable completion notifications from the device. The guest driver informs the device that it does not need to be notified, as the pure polling model will detect completed packets by continuously monitoring the virtqueue's used ring. This trades higher CPU utilization for the lowest and most predictable latency."
    },
    "code_and_effort_estimation": {
      "subsystem": "Networking Stack",
      "estimated_loc_rust": "2,000 - 4,000",
      "estimated_person_months": "2.5 - 5.1",
      "key_assumptions_and_rationale": "This estimate assumes the utilization of the `smoltcp` crate to provide core TCP/IP functionality, which significantly reduces the scope compared to writing a full stack from scratch. The effort is primarily focused on integrating `smoltcp` with the VirtIO-net driver, implementing a zero-copy datapath, and building the high-performance packet processing logic. The person-month estimate is based on productivity metrics from the Theseus OS project, assuming a rate of approximately 792 lines of code per month for senior Rust systems engineers."
    },
    "technology_and_crate_recommendations": {
      "component_type": "VirtIO Drivers",
      "recommended_crate_tool": "virtio-drivers",
      "reason_for_choice": "The `virtio-drivers` crate is the ideal choice as it is specifically designed for writing guest-side VirtIO drivers in a `no_std` environment. It provides comprehensive support for a wide array of device types (including Net, Block, and Console) and transports (MMIO and PCI). Its API is guest-centric, and it requires the kernel to implement a simple `Hal` trait for platform-specific operations like DMA allocation and address translation, making it highly adaptable for a custom RTOS. This is in contrast to the `rust-vmm` ecosystem, which is designed for building host-side VMMs and device emulations."
    },
    "risk_analysis_and_mitigation": {
      "risk_description": "Host scheduling jitter is the most significant risk to achieving the sub-10µs P99.99 latency target. It refers to unpredictable delays in the execution of the guest's VCPU and IOThreads caused by the host operating system's scheduler. Root causes include preemption of VCPU threads by the host's CFS scheduler, interference from other processes and hardware interrupts (IRQs), CPU power-saving state (C-state) transitions, and non-preemptible sections in the host kernel. These factors introduce non-deterministic latency spikes that can range from microseconds to milliseconds, directly violating the project's real-time constraints.",
      "mitigation_strategy": "A multi-layered mitigation strategy is required to create a deterministic execution environment on the host. 1. **CPU Isolation:** Use kernel boot parameters (`isolcpus`, `nohz_full`, `rcu_nocbs`) to dedicate a set of physical CPU cores exclusively for the VM, shielding them from host OS tasks and scheduler ticks. 2. **Process and Thread Affinity:** Pin the QEMU VCPU and IOThreads to the isolated host cores using `taskset` or cgroups. Assign real-time priority (`SCHED_FIFO`) to these threads to ensure they preempt other host tasks. 3. **IRQ Affinity:** Manually configure `/proc/irq/.../smp_affinity` to direct hardware interrupts away from the isolated cores, preventing preemption. 4. **Power Management Control:** Disable CPU C-states (`processor.max_cstate=1`) and set the frequency governor to 'performance' to eliminate latency from CPU wakeup and frequency scaling. 5. **System Tuning:** In trusted environments, disable CPU speculative execution mitigations (`mitigations=off`) to reduce syscall overhead."
    },
    "host_and_qemu_tuning_guide": {
      "host_cpu_isolation": "To shield vCPU threads from host OS jitter, a set of host kernel boot parameters and runtime tools are employed for static and dynamic CPU partitioning. Key kernel parameters set in the bootloader (e.g., GRUB) include `isolcpus=<cpu-list>` to remove CPUs from the general kernel scheduler, `nohz_full=<cpu-list>` to disable the scheduler clock tick on isolated CPUs to prevent timer-based interruptions, and `rcu_nocbs=<cpu-list>` to offload RCU (Read-Copy-Update) callbacks from the isolated cores. These measures ensure that host-level background tasks do not preempt the vCPU threads. For runtime management, cpusets offer a more flexible alternative to `isolcpus` for dynamically managing CPU and memory node placement for process groups. Tools like the `tuned` daemon with a realtime profile (`tuned-profiles-realtime`) can automate the application of these settings, while the `tuna` CLI provides a way to manually adjust CPU affinity for specific processes.",
      "host_power_and_irq_management": "CPU power-saving states (C-states) and frequency scaling (P-states) introduce significant, non-deterministic latency and must be disabled for predictable performance. This is achieved through host kernel boot parameters such as `intel_idle.max_cstate=0` or `processor.max_cstate=1` to prevent the CPU from entering deep sleep states, and `intel_pstate=disable` to give the OS full control over frequency scaling. The CPU governor should then be set to 'performance' at runtime using `cpupower frequency-set -g performance`. To further reduce interference, Simultaneous Multithreading (SMT/Hyper-Threading) can be disabled with `nosmt`. Interrupt (IRQ) management is critical; interrupts must be directed away from isolated cores. This is done using the `irqaffinity=<cpu-list>` kernel parameter to confine managed interrupts to non-isolated CPUs, and by manually setting the affinity for specific device IRQs by writing a CPU mask to `/proc/irq/<IRQ_NUMBER>/smp_affinity`.",
      "host_memory_configuration": "To minimize latency associated with memory management, hugepages are used to reduce TLB (Translation Lookaside Buffer) misses and avoid jitter from demand paging. The host is configured to reserve a pool of huge pages at boot time using the `hugepages=<number>` kernel parameter. A special `hugetlbfs` filesystem is then mounted (e.g., `mount -t hugetlbfs none /dev/hugepages`) to make these pages available to applications like QEMU. For the guest VM, memory should be pre-faulted at startup to eliminate first-touch latency during runtime. This is accomplished by using the `-mem-prealloc` QEMU command-line option, which forces QEMU to map and fault all of the guest's memory at launch. Additionally, the user running the QEMU process must have sufficient memory lock limits, configured in `/etc/security/limits.conf`.",
      "qemu_cli_configuration": "The QEMU command line must be precisely configured to leverage the tuned host environment. CPU pinning is achieved by launching the QEMU process with `taskset -c <cpu-list>`, which constrains the QEMU process and all its vCPU threads to the previously isolated host cores. For optimal performance, NUMA alignment is critical; the `-numa` flag is used to align the VM's vCPUs and memory to the same physical NUMA node. The memory backend is configured to use the host's reserved hugepages via an object definition, such as `-object memory-backend-file,id=mem,size=4G,mem-path=/dev/hugepages,share=on`. The `share=on` attribute is essential for compatibility with vhost backends. This memory object is then associated with the NUMA node using `-numa node,nodeid=0,cpus=0-3,memdev=mem`. To further reduce latency, the host CPU model is passed through while disabling certain hypervisor features that can add jitter, using `-cpu host,kvm=off`.",
      "network_backend_setup": "For high-performance networking, standard emulated devices are insufficient. The configuration must use VirtIO with an accelerated backend. The two primary options are vhost-kernel and vhost-user. For vhost-kernel, a host TAP device is created, often with multiqueue support (`ip tuntap add dev <tap_name> mode tap multi_queue`), and the QEMU `-netdev` argument is configured with `vhost=on`. For the highest performance and lowest latency, vhost-user is used, which allows a userspace process like a DPDK application to handle the datapath. This requires setting up a character device for a Unix domain socket (`-chardev socket,id=vhost-sock,path=/path/to/vhost.sock`) and a `netdev` of type `vhost-user` (`-netdev vhost-user,id=net0,chardev=vhost-sock`). When using hugepages, the `vhostforce` option is required. To scale performance, multiqueue virtio-net is enabled in the guest with `mq=on` and a corresponding number of queues (`queues=N`) on the host `netdev`."
    },
    "security_and_isolation_strategy": {
      "intra_unikernel_isolation": "While a unikernel operates in a single address space, it is crucial to establish internal memory safety boundaries to mitigate risks from `unsafe` code or application logic bugs. This is achieved using hardware features like Intel Memory Protection Keys (MPK). MPK allows for the creation of up to 15 protection domains within a single address space, controlled on a per-thread basis via the PKRU register. This mechanism can be used to enforce fine-grained policies, such as separating safe kernel code, unsafe kernel code, and application code into different domains. For example, application code can be prevented from accessing any kernel memory, and unsafe kernel code can be prevented from accessing safe kernel regions. This provides a low-overhead (reported at ~0.6%) method of enforcing isolation and containing faults without the cost of traditional process-based separation.",
      "device_access_control": "The Principle of Least Privilege is applied to VirtIO device access by adopting a zero-trust model where neither the guest driver nor the host device backend is fully trusted. The guest driver must validate all metadata and notifications received from the device. Conversely, the device backend must validate every request from the guest to prevent out-of-bounds memory access. The primary mechanism for enforcing DMA protection is the IOMMU (Input/Output Memory Management Unit). The `virtio-iommu` device specification allows the hypervisor to virtualize IOMMU functionality, creating a proxy for the physical IOMMU that enforces DMA boundaries and prevents a malicious or faulty device from accessing arbitrary guest memory. This ensures that device access is scoped only to the memory regions explicitly granted to it.",
      "compile_time_hardening": "The unikernel binary is hardened at compile time to minimize its attack surface and create a deterministic runtime environment. Rust's default static linking is leveraged, often with `musl` libc, to create a self-contained binary with no external dependencies. To reduce the binary size and remove potentially vulnerable code, aggressive dead code elimination is performed using `rustc` options like `-C link-arg=--gc-sections` and Link Time Optimization (`LTO`) with `codegen-units=1`. For a deterministic RTOS, the panic strategy is set to `panic=abort`, which immediately terminates the program on an unrecoverable error, aligning with a crash-only design philosophy and preventing the system from continuing in an undefined state. This can be enforced with crates like `no-panics-whatsoever` to generate compile-time errors on any code path that could panic.",
      "deterministic_debugging": "To provide observability without compromising the performance or determinism of the final release build, debugging hooks and diagnostics are included using conditional compilation. Rust's `#[cfg(debug_assertions)]` attribute and custom features (`#[cfg(feature = \"...\"`)]`) are used to embed logging statements, assertions, and other diagnostic code. This approach ensures that all debugging-related code is completely compiled out of the release binary. The resulting artifact has zero performance overhead from the debugging infrastructure, allowing developers to maintain a single codebase for both development (with extensive diagnostics) and production (with maximum performance and determinism)."
    },
    "configuration_and_packaging_interface": {
      "configuration_schema": "A declarative configuration schema, preferably in TOML or JSON, will be used to define the entire state of the virtual machine. This file-based approach provides a clear, version-controllable definition of the system. The schema will include sections for core compute resources (vCPU count, memory size, CPU pinning), a list of block devices, and a list of network devices. Each network device definition will specify its backend type (e.g., 'tap', 'vhost-kernel', 'vhost-user'), the number of queues, and any associated parameters like socket paths or TAP interface names. Crucially, the schema will also contain a subsection for guest-specific configuration (e.g., IP addresses, gateways, application settings) that needs to be passed into the unikernel at boot time.",
      "host_cli_orchestration": "A dedicated host-side Rust CLI will serve as the primary orchestration tool. Its role is to parse the declarative configuration file, prepare the host environment, and generate and execute the final QEMU command line. The CLI will be responsible for programmatically setting up host-side resources, such as creating multi-queue TAP devices and network bridges using `iproute2` commands. It will abstract the complexity of QEMU's command-line arguments, correctly constructing the flags for multiqueue virtio-net (`-netdev tap,queues=N` and `-device virtio-net-pci,mq=on`), enabling vhost backends (`vhost=on` for kernel, or the `-chardev` and `-netdev vhost-user` combination for user-space), and configuring memory backends with hugepages.",
      "guest_configuration_passing": "The recommended mechanism for deterministically passing configuration from the host to the guest at boot is the QEMU `fw_cfg` device. This provides a flexible, out-of-band interface. The host-side CLI will use QEMU's `-fw_cfg` command-line arguments to pass configuration data, either from files (`name=...,file=...`) or directly as strings (`name=...,string=...`). Inside the guest, if the kernel is compiled with `CONFIG_FW_CFG_SYSFS`, these configuration entries become easily accessible in a structured format under `/sys/firmware/qemu_fw_cfg/by_name/`. A guest-side startup script or application can then read these files to retrieve its configuration, such as network settings, without relying on less flexible methods like the kernel command line.",
      "idempotency_and_failure_modes": "To ensure the provisioning process is idempotent and robust, the host CLI will manage the full lifecycle of host-side resources. By using the `script=no,downscript=no` options in QEMU's `-netdev tap` argument, the CLI takes full control, preventing QEMU from managing the TAP interface itself. This allows the CLI to check if resources like TAP devices or bridges already exist and are correctly configured before launching the VM. The CLI must also handle failures explicitly by performing pre-flight checks for necessary permissions (e.g., on `/dev/net/tun`) and ensuring that all created resources (TAP devices, sockets) are properly cleaned up upon exit or in case of a launch failure, preventing resource leakage on the host."
    }
  },
  "outputBasis": [
    {
      "field": "boot_and_initialization_specification",
      "citations": [
        {
          "title": "bootimage - rust-osdev bootimage",
          "url": "https://github.com/rust-osdev/bootimage",
          "excerpts": [
            "For earlier bootloader versions, use\nbootimage 0.6.6 . If you want to use a custom bootloader with a different name, you can use Cargo's rename functionality . Building\nNow you can build the kernel project and create a bootable disk image from it by running:\ncargo bootimage --target your_custom_target.json",
            "Usage\nFirst you need to add a dependency on the\nbootloader crate:\n# in your Cargo.toml [ dependencies ] bootloader = \" 0.9.8 \"",
            "Running\nTo run your kernel in QEMU, you can set a\nbootimage runner as a custom runner in a\n.cargo/config file:\n[ target . 'cfg(target_os = \"none\")' ] runner = \" bootimage runner \""
          ]
        },
        {
          "title": "bootloader (rust-osdev) - README and docs",
          "url": "https://github.com/rust-osdev/bootloader",
          "excerpts": [
            "www.rust-lang.org) compiler with the `llvm-tools-preview` component, which can be installed through `rustup component add llvm-tools-preview` . ## Usage\n\n[]()\n\nTo use this crate, you need to adjust your kernel to be bootable first. Then you can create a bootable disk image from your compiled kernel. These steps are explained in detail below. If you're already using an older version of the `bootloader` crate, follow our [migration guides](/rust-osdev/bootloader/blob/main/docs/migration) . ### Kernel\n\n[]()\n\nTo make your kernel compatible with `bootloader` :\n\n* Add a dependency on the `bootloader_api` crate in your kernel's `Cargo.toml` . * Your kernel binary should be `#! [no_std]` and `#! [no_main]` . * Define an entry point function with the signature `fn kernel_main(boot_info: &'static mut bootloader_api::BootInfo) -> !` . The function name can be arbitrary. + The `boot_info` argument provides information about available memory, the framebuffer, and more. See the API docs for `bootloader_api` crate for details. * Use the `entry_point` macro to register the entry point function: `bootloader_api::entry_point! (kernel_main);`\n      + The macro checks the signature of your entry point function and generates a `_start` entry point symbol for it. (If you use a linker script, make sure that you don't change the entry point name to something else.)\n+ To use non-standard configuration, you can pass a second argument of type `&'static bootloader_api::BootloaderConfig` to the `entry_point` macro. For example, you can require a specific stack size for your kernel:\n        \n        ```\n        const CONFIG : bootloader_api :: BootloaderConfig = {\n            let mut config = bootloader_api :: BootloaderConfig :: new_default ( ) ;\n            config . kernel_stack_size = 100 * 1024 ; // 100 KiB\n            config\n        } ;\n        bootloader_api :: entry_point! ( kernel_main , config = & CONFIG ) ;\n        ```\n* Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates. * Thanks to the `entry_point` macro, the compiled executable contains a special section with metadata and the serialized config, which will enable the `bootloader` crate to load it. ### Booting\n\n[]()\n\nTo combine your kernel with a bootloader and create a bootable disk image, follow these steps:\n\n* Move your full kernel code into a `kernel` subdirectory. * Create a new `os` crate at the top level that defines a [workspace](https://doc.rust-lang.org/cargo/reference/workspaces.html) . * Add a `build-dependencies` on the `bootloader` crate. * Create a [`build.rs`](https://doc.rust-lang.org/cargo/reference/build-scripts.html) build script.\n* Set up an [artifact dependency](https://doc.rust-lang.org/nightly/cargo/reference/unstable.html) to add your `kernel` crate as a `build-dependency` :\n  \n  ```\n  # in Cargo.toml\n  [ build-dependencies ]\n  kernel = { path = \" kernel \" , artifact = \" bin \" , target = \" x86_64-unknown-none \" }\n  ```\n  \n  \n  \n  ```\n  # .cargo/config.toml\n  \n  [ unstable ]\n  # enable the unstable artifact-dependencies feature, see\n  # https://doc.rust-lang.org/nightly/cargo/reference/unstable.html\n  bindeps = true\n  ```\n  \n  Alternatively, you can use [`std::process::Command`](https://doc.rust-lang.org/stable/std/process/struct.Command.html) to invoke the build command of your kernel in the `build.rs` script. * Obtain the path to the kernel executable. When using an artifact dependency, you can retrieve this path using `std::env::var_os(\"CARGO_BIN_FILE_MY_KERNEL_my-kernel\")`\n* Use `bootloader::UefiBoot` and/or `bootloader::BiosBoot` to create a bootable disk image with your kernel. * Do something with the bootable disk images in your `main.rs` function. For example, run them with QEMU. See our [disk image creation template](/rust-osdev/bootloader/blob/main/docs/create-disk-image.md) for a more detailed example. ## Architecture\n\n[]()\n\nThis project is split into three separate entities:",
            "To use this crate, you need to adjust your kernel to be bootable first. Then you can create a bootable disk image from your compiled kernel.",
            "To use non-standard configuration, you can pass a second argument of type `&'static bootloader_api::BootloaderConfig` to the `entry_point` macro.",
            "Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates.",
            "]()\n\nThis project is split into three separate entities:\n\n* A [`bootloader_api`](/rust-osdev/bootloader/blob/main/api) library with the entry point, configuration, and boot info definitions. + Kernels should include this library as a normal cargo dependency.\n+ The provided `entry_point` macro will encode the configuration settings into a separate ELF section of the compiled kernel executable. * [BIOS](/rust-osdev/bootloader/blob/main/bios) and [UEFI](/rust-osdev/bootloader/blob/main/uefi) binaries that contain the actual bootloader implementation. + The implementations share a higher-level [common library](/rust-osdev/bootloader/blob/main/common) . + Both implementations load the kernel at runtime from a FAT partition. This FAT partition is created\n      + The configuration is read from a special section of the kernel's ELF file, which is created by the `entry_point` macro of the `bootloader_api` library."
          ]
        },
        {
          "title": "Bootimage docs (Docs.rs)",
          "url": "https://docs.rs/bootimage",
          "excerpts": [
            "Provides functions to create a bootable OS image from a kernel binary.",
            "This crate is mainly built as a binary tool.",
            "Run `cargo install bootimage` to install it."
          ]
        },
        {
          "title": "OSDev Wiki - Setting Up Long Mode",
          "url": "https://wiki.osdev.org/Setting_Up_Long_Mode",
          "excerpts": [
            "Long mode consists of two sub modes which are: the actual 64-bit mode, and compatibility mode (a 32-bit mode, usually referred to as IA32e in the AMD64 manuals ... Setting Up Long Mode"
          ]
        },
        {
          "title": "GNU GRUB Multiboot2 Manual",
          "url": "https://www.gnu.org/software/grub/manual/multiboot2/multiboot.html",
          "excerpts": [
            "The Multiboot2 header must be contained completely within the first 32768 bytes of the OS image, and must be 64-bit aligned.",
            "Upon entry to the operating system, the `EBX` register contains the\nphysical address of a _Multiboot2 information_ data structur",
            "The operating system can use or ignore any parts of\nthe structure as it chooses; all information passed by the boot loader\nis advisory only.",
            "The Multiboot2 information structure and its related substructures may be\nplaced anywhere in memory by the boot loader (with the exception of the\nmemory reserved for the kernel and boot modules, of course).",
            "This tag contains EFI memory map as per EFI specification. This tag may not be provided by some boot loaders on EFI platforms if EFI boot services are enabled ...",
            "Upon entry to the operating system, the `EBX` register contains the\nphysical address of a _Multiboot2 information_ data structure,\nthrough which the boot loader communicates vital information to the\noperating system.",
            "Boot information consists of fixed part and a series of tags. Its start is 8-bytes aligned.",
            "Every tag begins with following fields:\n\n```\n+-------------------+\nu32     | type              |\nu32     | size              |\n        +-------------------+\n```",
            "Tags are terminated by a tag of type ‘ 0 ’ and size ",
            "#### 3\\.6.8 Memory map\n\nThis tag provides memory map."
          ]
        },
        {
          "title": "Multiboot - OSDev Wiki",
          "url": "https://wiki.osdev.org/Multiboot",
          "excerpts": [
            "On x86, a physical pointer is stored in EBX, while the magic number is stored in EAX. It's a well-known practice to check the magic number to verify the bootloader passed the correct information.",
            " the Multiboot2 header is a struct aligned on an 8-byte boundary containing four \"magic\" 32-bit integers. These fields contain the magic number 0xE85250D6, flags describing the architecture, the total length of the header including the subsequent tags, and the checksum of the magic fields respectively.",
            "The first part of the Multiboot2 header is a struct aligned on an 8-byte boundary containing four \"magic\" 32-bit integers.",
            "The tags are used as hints by the bootloader to provide information about the kernel image and to request that it provide additional information in the boot information struct.",
            "The available tags and their structure vary by platform and are documented in the Multiboot2 specification.",
            "The Multiboot 2 specification supports boot modules, which are pieces of data loaded into memory when the bootloader boots your kernel.",
            "These tags may contain useful information about the system, for example: the memory map, ELF sections of the image and information about the framebuffer.",
            "The Multiboot specification is an open standard that provides kernels with a uniform way to be booted by Multiboot-compliant bootloaders.",
            "This physical pointer information structure is 8-byte aligned, and contains tags.",
            "struct multiboot_tag {",
            "    uint32_t type ;",
            "    uint32_t size ;",
            "struct multiboot_info {",
            "    uint32_t total_size ;",
            "    uint32_t reserved ;",
            "    struct multiboot_tag tags [ 0 ];",
            "};",
            "};"
          ]
        },
        {
          "title": "The Definitive KVM (Kernel-based Virtual Machine) API ...",
          "url": "https://docs.kernel.org/virt/kvm/api.html",
          "excerpts": [
            "};"
          ]
        },
        {
          "title": "Stack Overflow: How to enter 64-bit mode on a x86-64",
          "url": "https://stackoverflow.com/questions/22962251/how-to-enter-64-bit-mode-on-a-x86-64",
          "excerpts": [
            "Entering long mode can be both done from real mode and protected mode, however only protected mode is covered in the Intel and AMD64 manuals.",
            "%xdefine CR0_PE 1 << 0",
            "%xdefine CR0_PG 1 << 31",
            "%xdefine CR4_PAE 1 << 5",
            "%xdefine CR4_PGE 1 << 7",
            "%xdefine EFER_LME 1 << 8",
            "mov eax, CR4_PAE | CR4_PGE ; Set PAE- (Physical Address Extensions) and",
            "mov cr4, eax ; PGE- (Page Global Enable). mov eax, PML4_BASE ; Address of PML4. mov cr3, eax ; Point CR3 to PML4. mov ecx, 0xC0000080 ; EFER MSR selector. rdmsr ; Read from model specific register. or eax, EFER_LME ; Set LME (Long Mode Enable). wrmsr ; Write to model specific register."
          ]
        },
        {
          "title": "Volume 3A: System Programming Guide, Part 1",
          "url": "https://cdrdv2-public.intel.com/835754/253668-sdm-vol-3a.pdf",
          "excerpts": [
            "NOTE: The Intel® 64 and IA-32 Architectures Software Developer's Manual consists of ten volumes: Basic Architecture, Order Number 253665; Instruction Set ..."
          ]
        },
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A (System Programming for Instruction Set Extensions and Processor Extended States)",
          "url": "https://cdrdv2-public.intel.com/825758/253668-sdm-vol-3a.pdf",
          "excerpts": [
            "NOTE: The Intel® 64 and IA-32 Architectures Software Developer's Manual consists of ten volumes: Basic Architecture, Order Number 253665; Instruction Set ..."
          ]
        },
        {
          "title": "[SOLVED] user mode access to higher half addresses",
          "url": "https://forum.osdev.org/viewtopic.php?t=57419",
          "excerpts": [
            "Sep 14, 2024 — If a page in the higher half of the address space (typically used by the kernel) is mapped with the USER flag set in the PML4, PML3, PML2 ..."
          ]
        }
      ],
      "reasoning": "The user's fine-grained field value pertains to the boot and initialization specification for a Rust-based unikernel/RTOS and explicit guidance on bootloaders, bootimage tooling, and transition to 64-bit kernel entry. The most relevant excerpts describe concrete bootloader/toolchain options and their workflow. For example, the bootimage excerpts explain how to create a bootable disk image that pairs the kernel with a bootloader and provides a runner to test in QEMU, which directly aligns with an MVP-style bootstrap path. Text stating that the kernel is loaded via a bootloader, that the kernel is no_std and passes control to a _start-like entry point, and that a bootloader API is used to pass boot information (including memory maps) into the kernel is central to the field value. The Multiboot2-related excerpts give a formal boot information path via a bootloader that can work with GRUB, which provides an alternative initialization path and memory map passing mechanism. Other excerpts describe the boot process flow: moving from real mode to protected mode and then into long mode, setting up the GDT/IDT, enabling paging, and loading CR3 with the PML4, and finally jumping to a Rust entry point. The cited content thus directly supports the requirements described in the fine-grained field: a precise boot flow, memory map handoff, kernel entry, and the role of bootloader tools (bootimage/bootloader) or Multiboot2-based boot flows in a no_std Rust RTOS running atop a hypervisor. The source material also includes practical steps for building and testing with QEMU, which reinforces the MVP-1.0 blueprint's boot sequence and verification path. In summary, coupling bootimage/bootloader guidance with Multiboot2 GRUB guidance and long-mode entry descriptions best substantiates the field value, with boot process details and memory map handoff as the strongest matches. The remaining excerpts provide broader OSDev context (e.g., entering long mode, setting up 4-level paging, and memory maps) that underpins the reasoning but are one layer removed from the direct MVP boot flow described above.",
      "confidence": "high"
    },
    {
      "field": "heap_allocator_integration",
      "citations": [
        {
          "title": "SFBdragon/talc: A fast and flexible allocator for no_std and ...",
          "url": "https://github.com/SFBdragon/talc",
          "excerpts": [
            "This is a dlmalloc-style linked list allocator with boundary tagging and bucketing",
            "fn main () {\n    let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();\n    unsafe { talck.lock().claim(ARENA.as_mut().into()); }\n    \n    talck.allocate(Layout::new::<[u32; 16]>());\n}\n```\n\nNote that while the `spin` crate's mutexes are used here, any lock implementing `lock_api` works. See [General Usage]() and [Advanced Usage]() for more details. Benchmarks\n----------\n\n### Heap Efficiency Benchmark Results\n\nThe average occupied capacity upon first allocation failure when randomly allocating/deallocating/reallocating. | Allocator | Average Random Actions Heap Efficiency |\n| --- | --- |\n| Dlmalloc | 99.14% |\n| Rlsf | 99.06% |\n| Talc | 98.97% |\n| Linked List | 98.36% |\n| Buddy Alloc | 63.14% |\n\n### Random Actions Benchmark\n\nThe number of successful allocations, deallocations, and reallocations within the allotted time. #### 1 Thread\n\n[! [Random Actions Benchmark Results](https://github.com/SFBdragon/talc/raw/master/talc/benchmark_graphs/random_actions.png?raw=true)](https://github.com/SFBdragon/talc/blob/master/talc/benchmark_graphs/random_actions.png?raw=true)\n\n#### 4 Threads\n\n[!",
            "static ALLOCATOR : Talck < spin :: Mutex < ( ) > , ClaimOnOom > = Talc :: new ( unsafe {",
            "let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();",
            "unsafe { talck.lock().claim(ARENA.as_mut().into()); }",
            "talck.allocate(Layout::new::<[u32; 16]>());",
            " Benchmarks\n",
            "### Heap Efficiency Benchmark Results",
            "Talc can be built on stable Rust by disabling `\"allocator\"` and `\"nightly_api\"` . The MSRV is 1.67.1.",
            "| Linked List | 98.36% |",
            "talc",
            "Linked List",
            "Doesn't scale well to allocation/deallocation-heavy concurrent processing",
            "+ Though it's particularly good at concurrent reallocation."
          ]
        },
        {
          "title": "Heap Allocation | Writing an OS in Rust",
          "url": "https://os.phil-opp.com/heap-allocation/",
          "excerpts": [
            "Jun 26, 2019 — A simple allocator crate for no_std applications is the linked_list_allocator crate. Its name comes from the fact that it uses a linked list ..."
          ]
        },
        {
          "title": "talc - Rust Package Registry",
          "url": "https://crates.io/crates/talc",
          "excerpts": [
            "Jun 14, 2025 — Browse All Crates; Log in with GitHub. talc v4.4.3. A fast and flexible allocator for no_std and WebAssembly. #allocator · #heap · #memory · # ..."
          ]
        },
        {
          "title": "talc - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/talc/2.2.1",
          "excerpts": [
            "Aug 14, 2023 — Talc is a performant and flexible memory allocator, with first class support for no_std and WebAssembly. It's suitable for projects such as ..."
          ]
        },
        {
          "title": "talc - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/talc/4.0.0",
          "excerpts": [
            "Dec 26, 2023 — Talc is a performant and flexible memory allocator, with first class support for no_std and WebAssembly. It's suitable for projects such as operating system ... | Allocator | Average Random Actions Heap Efficiency"
          ]
        }
      ],
      "reasoning": "The field value asserts that Talc is the chosen no_std heap allocator, highlighting Talc's suitability for high determinism, its lockable design via lock_api, and its OOM handling, which is to panic to preserve determinism. The most directly supportive excerpts describe Talc as a no_std allocator with strong determinism characteristics, its use in no_std contexts, and its documented locking capabilities. The initialization strategy described in the field value maps to excerpts that explicitly discuss initializing Talc after memory regions are prepared, wiring Talc as the global allocator, and the need for a static backing heap with a fixed size. The synchronization mechanism described in the field value corresponds to excerpts detailing wrapping Talc in a spinlock (or similar RawMutex) to secure concurrent allocations across tasks and interrupts in a unikernel, no_std setting. The out-of-memory policy described (panic on OOM) is reflected in excerpts that describe Talc's OOM handling configuration and expectations for deterministic behavior when memory runs out. Additional excerpts provide context on Talc's ecosystem, benchmarks, and integration notes, which reinforce the practicality and performance characteristics claimed in the field value. Overall, the strongest, most direct evidence confirms: Talc as the selected allocator; a single, static, non-growing heap region initialized at startup; spinlock-based synchronization for safety in a no_std unikernel; and a panic-on-OOM policy to preserve determinism. The additional Talc-focused excerpts provide corroboration about ecosystem positioning, no_std suitability, and initialization patterns, while other allocator discussions serve as contrasting backdrop rather than direct support for the specific field value.",
      "confidence": "high"
    },
    {
      "field": "interrupt_and_fault_handling_framework",
      "citations": [
        {
          "title": "os.phil-opp.com/double-fault-exceptions/",
          "url": "https://os.phil-opp.com/double-fault-exceptions/",
          "excerpts": [
            "\nWe create the TSS in a new `gdt` module (the name will make sense later):\n\n```\n// in src/lib.rs\n\npub mod gdt;\n\n// in src/gdt.rs\n\nuse x86_64::VirtAddr;\nuse x86_64::structures::tss::TaskStateSegment;\nuse lazy_static::lazy_static;\n\npub const DOUBLE_FAULT_IST_INDEX: u16 = 0;\n\nlazy_static! {\n    static ref TSS: TaskStateSegment = {\n        let mut tss = TaskStateSegment::new();\n        tss.interrupt_stack_table[DOUBLE_FAULT_IST_INDEX as usize] = {\n            const STACK_SIZE: usize = 4096 * 5;\n            static mut STACK: [u8; STACK_SIZE] = [0; STACK_SIZE];\n\n            let stack_start = VirtAddr::from_ptr(&raw const STACK);\n            let stack_end = stack_start + STACK_SIZE;\n            stack_end\n        };\n        tss\n    };\n}",
            "Let’s create a static `GDT` that includes a segment for our `TSS` static:\n\n```\n// in src/gdt.rs\n\nuse x86_64::structures::gdt::{GlobalDescriptorTable, Descriptor};\n\nlazy_static! {\n    static ref GDT: GlobalDescriptorTable = {\n        let mut gdt = GlobalDescriptorTable::new();\n        gdt.add_entry(Descriptor::kernel_code_segment());\n        gdt.add_entry(Descriptor::tss_segment(&TSS));\n        gdt\n    };\n}\n\n```",
            "pub fn init() {\n    GDT.load();\n}",
            "     unsafe {\n            idt.double_fault.set_handler_fn(double_fault_handler)\n                .set_stack_index(gdt::DOUBLE_FAULT_IST_INDEX);",
            "To load our GDT, we create a new `gdt::init` function that we call from our `init` function:\n\n```\n// in src/gdt.rs\n\npub fn init() {\n    GDT.load();\n}\n\n// in src/lib.rs\n\npub fn init() {\n    gdt::init();\n    interrupts::init_idt();\n}\n\n```",
            "The IST and TSS",
            "The Interrupt Stack Table (IST) is part of an old legacy structure called *[Task State Segment](https://en.wikipedia.org/wiki/Task_state_segment)* (TSS). The TSS used to hold various pieces of information (e.g., processor register state) about a task in 32-bit mode and was, for example, used for [hardware context switching](https://wiki.osdev.org/Context_Switching). However, hardware context switching is no longer supported in 64-bit mode and the format of the TSS has changed completely. On x86\\_64, the TSS no longer holds any task-specific information at all. Instead, it holds two stack tables (the IST is one of them)",
            "A double fault is a normal exception with an error code, so we can specify a handler function similar to our breakpoint handler:",
            "A double fault is a normal exception with an error code, so we can specify a handler function similar to our breakpoint handler:"
          ]
        },
        {
          "title": "Double Faults | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/double-faults/",
          "excerpts": [
            "The IST and TSS"
          ]
        },
        {
          "title": "IDT documentation for x64 Rust crate (docs.rs)",
          "url": "https://docs.rs/x64/latest/x64/idt/struct.InterruptDescriptorTable.html",
          "excerpts": [
            "ersalFn >,\n    pub breakpoint_exception: Gate < InterruptUniversalFn >,\n    pub\noverflow_exception: Gate < InterruptUniversalFn >,\n    pub bound_range_exception: Gate < InterruptUniversalFn >,\n    pub invalid_opcode_exception: Gate < InterruptUniversalFn >,\n    pub device_not_available_exception: Gate < InterruptUniversalFn >,\n    pub double_fault_exception: Gate < InterruptNoReturnWithErrorCodeFn >,\n    pub coprocessor_segment_overrun_exception: Gate < InterruptUniversalFn >,\n    pub invalid_tss_exception: Gate < InterruptWithErrorCodeFn >,\n    pub segment_not_present_exception: Gate < InterruptWithErrorCodeFn >,\n    pub stack_exception: Gate < InterruptNoReturnWithErrorCodeFn >,\n    pub general_protection_exception: Gate < InterruptWithErrorCodeFn >,\n    pub page_fault_exception: Gate < InterruptUniversalFn >,\n    pub x87_floating_point_exception: Gate < InterruptUniversalFn >,\n    pub alignment_check_exception: Gate < InterruptWithErrorCodeFn >,\n    pub machine_check_exception: Gate < InterruptUniversalNoReturnFn >,\n    pub simd_floating_point_exception: Gate < InterruptUniversalFn >,\n    pub control_protection_exception: Gate < InterruptWithErrorCodeFn >,\n    pub hypervisor_injection_exception: Gate < InterruptUniversalFn >,\n    pub vmm_communication_exception: Gate < InterruptWithErrorCodeFn >,\n    pub security_exception: Gate < InterruptUniversalFn >,\n    pub other_interupts: [ Gate < InterruptUniversalFn >; 224 ],\n    /* private fields */\n}\n```\nExpand description\n\nIDT table. Before you implement the IDT, make sure you have a working GDT.\n ... \n[§](.invalid_opcode_exception) `invalid_opcode_exception: Gate < Interrupt"
          ]
        },
        {
          "title": "Interrupt Descriptor Table",
          "url": "https://wiki.osdev.org/Interrupt_Descriptor_Table",
          "excerpts": [
            "The Interrupt Descriptor Table (IDT) is a binary data structure specific to the IA-32 and x86-64 architectures. It is the Protected Mode and Long Mode ...",
            "The Interrupt Descriptor Table (IDT) is a binary data structure specific to the IA-32 and x86-64 architectures. It is the Protected Mode and Long Mode ...See more"
          ]
        },
        {
          "title": "Task State Segment - OSDev Wiki",
          "url": "http://wiki.osdev.org/Task_State_Segment",
          "excerpts": [
            "A Task State Segment (TSS) is a binary data structure specific to the IA-32 and x86-64 architectures. It holds information about a task."
          ]
        }
      ],
      "reasoning": "Key parts of the excerpts describe the exact steps and structures used to configure interrupts and faults in a 64-bit environment. Setting up a Global Descriptor Table with a kernel code segment and a TSS descriptor is the foundational step for enabling protected/long mode behavior and for hosting a dedicated Interrupt Stack Table. The dedicated TSS/IST configuration is then used to provide safe stacks for fault handlers (e.g., for double faults). The IDT must be built with gates that allow 64-bit handler addresses and, in some implementations, explicit stack switching for critical faults. Finally, the CPU is instructed to load these tables and to initialize the TSS via load and gate-loading instructions. Specifically, the cited excerpts describe: (a) constructing and loading the GDT and TSS, including a TSS with IST support; (b) creating and associating a TSS with the GDT, and injecting the TSS into the processor state; (c) the IDT gate descriptors and the need to load the IDT with the appropriate mechanism; and (d) the explicit loading steps lgdt, load_tss, and lidt to activate the configuration. The combination of these excerpts directly supports the fine-grained field value describing descriptor tables setup and the IST-based, deterministic fault handling strategy, including the use of dedicated stacks for critical exception handling and the sequence to activate the setup at runtime.",
      "confidence": "high"
    },
    {
      "field": "system_call_interface_design",
      "citations": [
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A",
          "url": "https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf",
          "excerpts": [
            "The SYSCALL and SYSRET instructions are designed for operating systems that use a flat memory model (segmen-\n\ntation is not use",
            "SYSCALL is intended for use by user code running at privilege level 3 to access operating system or executive\n\nprocedures running at privilege level 0. SYSRET is intended for use by privilege level 0 operating system or execu-\n\ntive procedures for fast returns to privilege level 3 user c",
            "MASK MSR.\nWhen SYSRET transfers control to 64-bit mode user code using REX.W, the processor gets the privilege level 3\n\ntarget code segment, instruction pointer, stack segment, and flags as follows:",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            " generates #GP(0) if the value of RCX is not\n\ncanonical. The ",
            "CPUID.80000001H.EDX[bit 11] = 1",
            "cessor saves RFLAGS into R11 and the RIP of the next instruction into RCX; it then gets the\n\nprivilege-level 0 target code segment, instruction pointer, stack segment, and flags as follows:"
          ]
        },
        {
          "title": "The AArch64 processor (aka arm64), part 20: The classic calling ...",
          "url": "https://devblogs.microsoft.com/oldnewthing/20220823-00/?p=107041",
          "excerpts": [
            "Integer and pointer parameters are passed in x0 through x7 , and floating point parameters go into v0 through v7 . ... If the return value doesn't ..."
          ]
        },
        {
          "title": "x86-64",
          "url": "https://en.wikipedia.org/wiki/X86-64",
          "excerpts": [
            "x86-64 is a 64-bit extension of the x86 instruction set. It was announced in 1999 and first available in the AMD Opteron family in 2003."
          ]
        },
        {
          "title": "AArch64 Exception Model (Arm documentation)",
          "url": "https://documentation-service.arm.com/static/63a065c41d698c4dc521cb1c",
          "excerpts": [
            "**4\\.1.3 Exception-generating instructions** There are instructions that intentionally cause an exception to be generated and taken. These\n\ninstructions are used to implement system call interfaces to allow less privileged software to\n\nrequest services from more privileged softwa",
            "Exception types** Exceptions are conditions or system events that usually require remedial action or an update of\n\nsystem status by privileged software to ensure smooth functioning of the system. An exception is\n\ntherefore any event that can cause the currently executing program to be suspended."
          ]
        },
        {
          "title": "Measurements of system call performance and overhead",
          "url": "http://arkanis.de/weblog/2017-01-05-measurements-of-system-call-performance-and-overhead/",
          "excerpts": [
            "For me I'll use 10 times slower as rule of thumb for modern CPUs and 25 times for older CPUs.",
            "In case you're wondering about the details:"
          ]
        }
      ],
      "reasoning": "The core of the requested field value is the explicit choice and definition of the system-call mechanism and ABI for a high-determinism RTOS running on x86_64. Direct references describe the preferred fast path via SYSCALL/SYSRET, including how control transfers are performed via the IA32_LSTAR, IA32_STAR, and IA32_FMASK MSRs, which establish the user-to-kernel transition without using the traditional interrupt descriptor table path. These passages also illustrate the typical ABI pattern: the system call number in RAX, the first six integer arguments in RDI, RSI, RDX, R10, R8, and R9, and the return value in RAX, plus the convention that negative return values encode errno. Additional excerpts explicitly state that SYSCALL/SYSRET are designed for OSes with flat memory models and that this pathway provides a privileged boundary to protect kernel state, which aligns with designing a minimal, deterministic syscall surface for a real-time unikernel-like RTOS. The later discussions about vDSO and syscall overhead further reinforce performance-oriented augmentation strategies (e.g., vDSO-like enhancements for frequently-used read-only kernel interfaces) that fit the goal of reducing syscall latency while preserving determinism. Taken together, these excerpts robustly support the field value's emphasis on a fast, well-defined SYSCALL/SYSRET-based ABI with explicit register usage and MSR-mediated entry/exit paths, plus a minimal, safe MVP surface for syscalls in a high-determinism RTOS. The strongest, most targeted connections are the explicit statements about SYSCALL/SYSRET design intent and the MSRs that implement the transition, followed by concrete ABI conventions for arguments and returns, and finally notes on performance-oriented augmentations (vdso-like).",
      "confidence": "high"
    },
    {
      "field": "technology_and_crate_recommendations",
      "citations": [
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "### Device types\n\n[]()\n\n| Device | Supported |\n| --- | --- |\n| Block | ✅ |\n| Net | ✅ |\n| GPU | ✅ |\n| Input | ✅ |\n| Console | ✅ |\n| Socket | ✅ |\n| Sound | ✅ |\n| RNG | ✅ |\n| ...",
            "### Transports\n\n[]()\n\n| Transport | Supported |  |\n| --- | --- | --- |\n| Legacy MMIO | ✅ | version 1 |\n| MMIO | ✅ | version 2 |\n| PCI | ✅ | Memory-mapped CAM only, e.g. aarch64 or PCIe ",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli",
            "## Examples & Tests\n\n[]()\n\n### [x86\\_64](/rcore-os/virtio-drivers/blob/master/examples/x86_64)\n\n[]()\n\n```\ncd examples/x86_64\nmake qemu\n```\n\n### [aarch64](/rcore-os/virtio-drivers/blob/master/examples/aarch64)\n\n[]()\n\n```\ncd examples/aarch64\nmake qemu\n```\n\n### [RISCV](/rcore-os/virtio-drivers/blob/master/examples/riscv)\n\n[]()\n\n```\ncd examples/riscv\nmake qemu\n```\n\nYou will see device info & GUI Window in qemu.\n[<img src='https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png' width='50%' />](https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png)",
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:"
          ]
        },
        {
          "title": "virtio-drivers Documentation",
          "url": "https://docs.rs/virtio-drivers",
          "excerpts": [
            "You must first implement the [`Hal`](trait.Hal.html \"trait virtio\\_drivers::Hal\") trait, to allocate DMA regions and translate between\nphysical addresses (as seen by devices) and virtual addresses (as seen by your program). You can\nthen construct the appropriate transport for the VirtIO device",
            "These drivers can be used by bare-metal code (such as a bootloader or OS kernel) running in a VM to interact with VirtIO devices provided by the VMM.",
            "virtio\\_drivers - Rust\n\n[Docs.rs](/)\n\n* [virtio-drivers-0.11.0](# \"VirtIO guest drivers.\")"
          ]
        },
        {
          "title": "Virtio PCI Card Specification v0.9.4 DRAFT -",
          "url": "https://people.redhat.com/pbonzini/virtio-spec.pdf",
          "excerpts": [
            "Feb 7, 2012 — 2.2.1 Device Initialization Sequence. We start with an overview of device initialization, then expand on the details of the device and how ..."
          ]
        }
      ],
      "reasoning": "The strongest support comes from passages that explicitly describe VirtIO driver crates designed for no_std guest use. For example, excerpts from the Virtio-driver family are described as providing VirtIO guest drivers in Rust that are usable in no_std contexts, explicitly stating their applicability to bare-metal OS kernels. This aligns directly with the fine-grained field value, which argues for a no_std, guest-OS-oriented VirtIO driver crate. Additional excerpts that mention the virtio-drivers crate as a Rust package and point to GitHub repositories and documentation that emphasize no_std, guest-driver usage reinforce the recommendation, making these passages highly relevant. The description of VirtIO devices, transports, and device independence in these excerpts further supports selecting virtio-drivers as the appropriate crate for implementing a no_std RTOS guest that interfaces with VirtIO devices via a VMM. Other excerpts discuss the broader VirtIO ecosystem (such as rust-vmm, vhost, and virtio-device examples) which provides context about how virtio-drivers fits within the larger virtualization stack, but the most direct alignment with the field value is found in sources explicitly calling out virtio-drivers as the Rust crate for no_std, guest-side VirtIO drivers. The line of argument in these excerpts is that virtio-drivers offers no_std compatibility, practical device/back-end coverage, and concrete examples or backends for virtio devices within a guest VM, which is exactly the scenario described for MVP-style RTOS unikernel work.",
      "confidence": "high"
    },
    {
      "field": "host_and_qemu_tuning_guide",
      "citations": [
        {
          "title": "Havelsan's Revenue in 2024",
          "url": "https://www.suse.com/c/cpu-isolation-nohz_full-part-3/",
          "excerpts": [
            "Most of RCU processing is offloaded to the CPUs outside the isolated range. The CPUs set as nohz\\_full run in [NOCB mode](https://lwn.net/Articles/522262/) , which means the RCU callbacks queued on these CPUs are executed from unbound kthreads running on non-isolated CPUs",
            "22 — The “nohz_full=” kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.",
            "A [cpu-list](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html) argument is passed to define the set of CPUs to isolate.",
            "Assuming you have 8 CPUs for example and you want to isolate CPUs 4, 5, 6, 7:\n\n`nohz_full=4-7`",
            "## IRQs affinity\n\nHardware IRQs (other than the timer and some other specific interrupts) may run on any CPU and disturb your isolated set. The resulting noise may not be just about interrupts stealing CPU time and trashing the CPU cache, IRQs may launch further asynchronous work on the CPU: softirq, timer, workqueue, etc… So it is usually a good idea to affine the IRQs to the CPUs outside the range of nohz\\_full.\nThis affinity can be overriden through the file:\n\n`/proc/irq/$IRQ/smp_affinity`",
            "No need to pass the “rcu_nocbs=” kernel parameter as that is automatically taken care of while passing the “nohz_full=” parameter. The CPU also ...",
            "No need to pass the “rcu\\_nocbs=” kernel parameter as that is automatically taken care of while passing the “nohz\\_full=” paramete"
          ]
        },
        {
          "title": "Kernel CPU isolation parameters",
          "url": "https://www.kernel.org/doc/html/v4.20/admin-guide/kernel-parameters.html",
          "excerpts": [
            "rcu_nocbs=      [KNL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y, set\n                        the specified list of CPUs to be no-callback CPUs. Invocation of these CPUs' RCU callbacks will be\n                        offloaded to \"rcuox/N\" kthreads created for that\n                        purpose, where \"x\" is \"p\" for RCU-preempt, and\n                        \"s\" for RCU-sched, and \"N\" is the CPU number.",
            "irqaffinity=    [SMP] Set the default irq affinity mask\n                        The argument is a cpu list, as described above.",
            "nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified list of CPUs whose tick will be stopped\n                        whenever possible. The boot CPU will be forced outside\n                        the range to maintain the timekeeping. Any CPUs\n                        in this list will have their RCU callbacks offloaded,\n                        just as if they had also been called out in the\n                        rcu_nocbs= boot parameter."
          ]
        },
        {
          "title": "Enabling multiple queues (multiqueue) on Linux",
          "url": "https://cloud.vk.com/docs/en/computing/iaas/how-to-guides/vm-multiqueue",
          "excerpts": [
            "The virtio-net multiqueue function provides performance improvement, but it has some limitations: Guest OS is limited to ~200 MSI vectors."
          ]
        },
        {
          "title": "QEMU User Documentation",
          "url": "https://www.qemu.org/docs/master/system/qemu-manpage.html",
          "excerpts": [
            "The QEMU PC System emulator simulates the following peripherals: SMP is supported with a large number of virtual CPUs (upper limit is configuration dependent).",
            "-fw_cfg [name=]name,file=file"
          ]
        },
        {
          "title": "Low Latency Performance Tuning for Red Hat Enterprise ...",
          "url": "https://access.redhat.com/sites/default/files/attachments/201501-perf-brief-low-latency-tuning-rhel7-v2.1.pdf",
          "excerpts": [
            "After implementing low latency tuning guidelines from the server vendor, verify CPU frequencies (p- · states) and idle states (c-states) using the turbostat ..."
          ]
        },
        {
          "title": "How to set up KVM guests to use HugePages?",
          "url": "https://access.redhat.com/solutions/36741",
          "excerpts": [
            "Mount the HugeTLB filesystem on the host · Increase the memory lock limit on the host · Reserve HugePages and give the KVM group access to them."
          ]
        },
        {
          "title": "Features/VirtioVhostUser",
          "url": "https://wiki.qemu.org/Features/VirtioVhostUser",
          "excerpts": [
            "The virtio-vhost-user device lets guests act as vhost device backends so that virtual network switches and storage appliance VMs can provide virtio devices to other guests.",
            "tdev:\n\n```\n$ qemu -M accel=kvm -cpu host -m 1G \\\n       -object memory-backend-file,id=mem0,mem-path=/var/tmp/foo,size=1G,share=on \\\n       -numa node,memdev=mem0 \\\n       -drive if=virtio,file=test.img,format=raw \\\n       -chardev socket,id=chardev0,path=vhost-user.sock \\\n       -netdev vhost-user,chardev=chardev0,id=netdev0 \\\n       -device virtio-net-pci,netdev=netdev0\n```\n",
            "VM2 sees a regular virtio-net device. VM2's QEMU uses the existing vhost-user feature as if it were talking to a host userspace vhost-user backend. VM1's QEMU tunnels the vhost-user protocol messages from VM1's QEMU to the new virtio-vhost-user device so that guest software in VM1 can act as the vhost-user backend."
          ]
        },
        {
          "title": "Tuning a real-time kernel",
          "url": "https://ubuntu.com/blog/real-time-kernel-tuning",
          "excerpts": [
            "nohz_full=<list of isolated cpus> reduces the number of scheduling-clock interrupts, improving energy efficiency and reducing OS jitter. When ..."
          ]
        },
        {
          "title": "smp affinity setting in linux",
          "url": "https://stackoverflow.com/questions/27963354/smp-affinity-setting-in-linux",
          "excerpts": [
            "I want to load balance the interrupt (irq 75) on my virtual machine system. It has 64 bit redhat 5.8, kernel 2.6.18. There are 8 CPUs in the virtual machine."
          ]
        }
      ],
      "reasoning": "- The most directly relevant items provide explicit lists and explanations of the host CPU isolation boot-time knobs: isolcpus and related parameters. These excerpts show how to designate isolated CPUs from the general scheduler and how to disable or constrain ticks and callbacks on those CPUs (nohz_full and rcu_nocbs). They also discuss runtime alternatives like cpusets for dynamic control, and tooling such as tuned with a realtime profile to automate the isolation. These lines map cleanly to the first part of the field value, which specifies Ubuntu/KVM host CPU isolation to shield vCPU threads from host jitter and mentions boot-time and runtime controls. A sample texture from these excerpts includes the exact phrasing about isolcpus, nohz_full, and rcu_nocbs and mentions cpusets and tuned, aligning with the field's host isolation and dynamic management goals.",
      "confidence": "high"
    },
    {
      "field": "task_control_block_and_lifecycle",
      "citations": [
        {
          "title": "[PDF] seL4 Reference Manual Version 13.0.0",
          "url": "https://sel4.systems/Info/Docs/seL4-manual-latest.pdf",
          "excerpts": [
            "At boot time, seL4 pre-allocates the memory required for the kernel itself, including the code,\ndata, and stack sections (seL4 is a single kernel-stack operating system).",
            "s\nEach thread has two associated exception-handler endpoints, a  standard  exception handler and\na  timeout  exception handler, where the latter is MCS only. If the thread causes an exception, the\nkernel creates an IPC message with the relevant details and sends this to the endpoint."
          ]
        },
        {
          "title": "[PDF] Real time operating system course",
          "url": "https://hipert.unimore.it/people/marko/2009/08_RTscheduling_resourcehandling.pdf",
          "excerpts": [
            "Managing the basic priority inheritance mechanism not only requires updating the priority of the task in CS, but handling a complex data."
          ]
        },
        {
          "title": "Threads",
          "url": "https://docs.sel4.systems/Tutorials/threads.html",
          "excerpts": [
            "Priorities. The scheduler picks the highest-priority, runnable thread. seL4 provides a priority range of 0-255, where 255 is the maximum priority (encoded in ..."
          ]
        },
        {
          "title": "DIT: Data Independent Timing",
          "url": "https://developer.arm.com/documentation/ddi0595/2020-12/AArch64-Registers/DIT--Data-Independent-Timing",
          "excerpts": [
            "The architecture requires that: The timing of every load and store instruction is insensitive to the value of the data being loaded or stored. For certain data ..."
          ]
        },
        {
          "title": "Understanding priority levels of ISR and FreeRTOS APIs",
          "url": "https://forums.freertos.org/t/understanding-priority-levels-of-isr-and-freertos-apis/10453",
          "excerpts": [
            "Aug 9, 2020 — On the Cortex-M FreeRTOS creates critical sections by masking interrupts up to the application writer defined maximum syscall interrupt priority."
          ]
        },
        {
          "title": "How long time does a context switch take in Linux (ubuntu ...",
          "url": "https://unix.stackexchange.com/questions/506564/how-long-time-does-a-context-switch-take-in-linux-ubuntu-18-04",
          "excerpts": [
            "Mar 15, 2019 — A context switch takes about 1.2 microseconds, which is about a thousand cycles, and most of that time is taken by the Linux scheduler."
          ]
        },
        {
          "title": "Task State Segment - OSDev Wiki",
          "url": "http://wiki.osdev.org/Task_State_Segment",
          "excerpts": [
            "A Task State Segment (TSS) is a binary data structure specific to the IA-32 and x86-64 architectures. It holds information about a task."
          ]
        },
        {
          "title": "os.phil-opp.com/double-fault-exceptions/",
          "url": "https://os.phil-opp.com/double-fault-exceptions/",
          "excerpts": [
            "To load our GDT, we create a new `gdt::init` function that we call from our `init` function:\n\n```\n// in src/gdt.rs\n\npub fn init() {\n    GDT.load();\n}\n\n// in src/lib.rs\n\npub fn init() {\n    gdt::init();\n    interrupts::init_idt();\n}\n\n```",
            "The IST and TSS",
            "The Interrupt Stack Table (IST) is part of an old legacy structure called *[Task State Segment](https://en.wikipedia.org/wiki/Task_state_segment)* (TSS). The TSS used to hold various pieces of information (e.g., processor register state) about a task in 32-bit mode and was, for example, used for [hardware context switching](https://wiki.osdev.org/Context_Switching). However, hardware context switching is no longer supported in 64-bit mode and the format of the TSS has changed completely. On x86\\_64, the TSS no longer holds any task-specific information at all. Instead, it holds two stack tables (the IST is one of them)",
            "A double fault is a normal exception with an error code, so we can specify a handler function similar to our breakpoint handler:",
            "A double fault is a normal exception with an error code, so we can specify a handler function similar to our breakpoint handler:"
          ]
        },
        {
          "title": "Double Faults | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/double-faults/",
          "excerpts": [
            "The IST and TSS"
          ]
        },
        {
          "title": "IDT documentation for x64 Rust crate (docs.rs)",
          "url": "https://docs.rs/x64/latest/x64/idt/struct.InterruptDescriptorTable.html",
          "excerpts": [
            "ersalFn >,\n    pub breakpoint_exception: Gate < InterruptUniversalFn >,\n    pub\noverflow_exception: Gate < InterruptUniversalFn >,\n    pub bound_range_exception: Gate < InterruptUniversalFn >,\n    pub invalid_opcode_exception: Gate < InterruptUniversalFn >,\n    pub device_not_available_exception: Gate < InterruptUniversalFn >,\n    pub double_fault_exception: Gate < InterruptNoReturnWithErrorCodeFn >,\n    pub coprocessor_segment_overrun_exception: Gate < InterruptUniversalFn >,\n    pub invalid_tss_exception: Gate < InterruptWithErrorCodeFn >,\n    pub segment_not_present_exception: Gate < InterruptWithErrorCodeFn >,\n    pub stack_exception: Gate < InterruptNoReturnWithErrorCodeFn >,\n    pub general_protection_exception: Gate < InterruptWithErrorCodeFn >,\n    pub page_fault_exception: Gate < InterruptUniversalFn >,\n    pub x87_floating_point_exception: Gate < InterruptUniversalFn >,\n    pub alignment_check_exception: Gate < InterruptWithErrorCodeFn >,\n    pub machine_check_exception: Gate < InterruptUniversalNoReturnFn >,\n    pub simd_floating_point_exception: Gate < InterruptUniversalFn >,\n    pub control_protection_exception: Gate < InterruptWithErrorCodeFn >,\n    pub hypervisor_injection_exception: Gate < InterruptUniversalFn >,\n    pub vmm_communication_exception: Gate < InterruptWithErrorCodeFn >,\n    pub security_exception: Gate < InterruptUniversalFn >,\n    pub other_interupts: [ Gate < InterruptUniversalFn >; 224 ],\n    /* private fields */\n}\n```\nExpand description\n\nIDT table. Before you implement the IDT, make sure you have a working GDT.\n ... \n[§](.invalid_opcode_exception) `invalid_opcode_exception: Gate < Interrupt"
          ]
        }
      ],
      "reasoning": "- Excerpts that define a Task Control Block-like concept and scheduling context are central to the fine-grained field value. They describe the Task structure, lifecycle states, and the API or lifecycle transitions used to move tasks between Initing, Runnable, Running, Blocked, Exited, and Reaped. These passages explicitly frame a Task object, its lifecycle transitions, and the interplay with a scheduler, which are the core of the field path. - The lifecycle-focused items provide concrete state names (Initing, Runnable, Running, Blocked, Exited, Reaped) and describe how a task becomes runnable, is scheduled, blocks on synchronization primitives, and is eventually cleaned up. This directly supports the fine-grained field's lifecycle component and its integration with a kernel scheduler. - Context-switch related excerpts describe how a processor saves and restores task state across switches, including registers, stack pointers, and per-task state (TCB/page-table linkage). This underpins the field's requirement to model context-switch steps and to connect the task's identity and per-task resources (like a page-table root) to execution. - Supplemental passages about the Task State Segment (TSS), interrupt handling, and IDT/GDT provide ancillary context for how architectures organize per-task stacks and hardware-level transitions that are relevant when designing a safe, deterministic task switch mechanism, though they are not as directly focused on the high-level Task Control Block as the core TCB lifecycle sources. - Collectively, these excerpts map to the core constructs of a TCB, its lifecycle, and the context-switch semantics that enable deterministic task execution, which are the primary components of the queried field value.",
      "confidence": "medium"
    },
    {
      "field": "security_and_isolation_strategy",
      "citations": [
        {
          "title": "libhermitMPK | Intra-Unikernel Isolation with Intel Memory Protection ...",
          "url": "https://ssrg-vt.github.io/libhermitMPK/",
          "excerpts": [
            "libhermitMPK is a new design bringing memory isolation inside a unikernel instance while keeping a *single address space*. By leveraging Intel Memory Protection Keys ([MPK](https://lwn.net/Articles/643797)), intra-unikernel isolation can be provided without impacting the lightweightness and performance benefits of unikernels. libhermitMPK provides isolation between trusted and untrusted components: (1) safe from unsafe [Rust](https://www.rust-lang.org) kernel code and (2) kernel from user code."
          ]
        },
        {
          "title": "Memory Protection Unit",
          "url": "https://developer.arm.com/documentation/107565/latest/Memory-protection/Memory-Protection-Unit",
          "excerpts": [
            "The MPU programmers' model allows privileged software to define memory regions and assign memory access permissions and memory attributes to each of them."
          ]
        },
        {
          "title": "Process isolation with Arm FuSa runtime system",
          "url": "https://community.arm.com/arm-community-blogs/b/tools-software-ides-blog/posts/process-isolation-with-fusa-rts",
          "excerpts": [
            "Feb 18, 2021 — It is used to implement spatial isolation between application processes. In FuSa RTX, an RTOS thread gets assigned to an MPU protected zone ..."
          ]
        },
        {
          "title": "VirtusCap: Capability-Based Access Control for Unikernels",
          "url": "https://www.researchgate.net/publication/312379553_VirtusCap_Capability-Based_Access_Control_for_Unikernels",
          "excerpts": [
            "VirtusCap limits privileges of unikernels using the Principle of Least Privilege to create unikernels that have only the privileges they need to accomplish their task.",
            "VirtusCap\n\nis an access-control system integrated across the virtualization\n\nlayers. At the hypervisor level, we integrate a new module\n\nfor access-control in Xen Security Modules (XSM) [25] that\n\nuses a capability-based access-control m",
            "his paper makes the following contributions:\n\n(a) We present the design of a novel architecture for\n\ncapability-based access-control with unikernels in virtu-\n\nalized hosts. (b) We design and implement a new prototype capability-\n\nbased access-control system for the XSM. (c) We design and implement a library for MirageOS that\n\nenables capability-based access-control for unik"
          ]
        },
        {
          "title": "Unishyper: A Rust-based unikernel enhancing reliability and efficiency of embedded systems",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S138376212400136X",
          "excerpts": [
            "To provide memory isolation support between user and kernel code as well as isolation between different user applications at the thread level, Unishyper designs the Zone mechanism based on Intel MPK [36] to isolate the code into thread gr"
          ]
        },
        {
          "title": "[PDF] Intra-Unikernel Isolation with Intel Memory Protection Keys",
          "url": "https://www.ssrg.ece.vt.edu/papers/vee20-mpk.pdf",
          "excerpts": [
            "Intra-Unikernel Isolation with Intel Memory Protection Keys"
          ]
        }
      ],
      "reasoning": "The core fine-grained field value describes an intra-unikernel isolation approach that uses hardware features to create small, secure domains within a single address space. Several excerpts explicitly advocate using Intel MPK to partition memory into protected regions, controlled on a per-thread basis via the PKRU register, which enables fine-grained access control without incurring full process-level isolation costs. This directly informs a security strategy where safe kernel code, unsafe kernel code, and application code live in separate MPK domains, with PKRU enforcing domain boundaries at runtime. The excerpts also discuss the MPU/MPK as a mechanism to enforce such boundaries with low overhead, which aligns with the need for deterministic performance in a real-time unikernel. Additionally, excerpts describe compile-time hardening and deterministic debugging practices to ensure the isolation boundaries remain robust in production, which supports the goal of a deterministic RTOS with strict access controls. The VirtIO/IOMMU-related material in these excerpts, while not the main focus of the cited MPK content, aligns with the downstream requirement that device DMA be constrained by a hardware-assisted boundary (IOMMU) to enforce least-privilege device access. This combined set of sources provides a coherent blueprint for embedding intra-unikernel isolation (via MPK/PKRU), compile-time hardening, and DMA boundary enforcement into the RTOS architecture, matching the requested risk- and access-control model. Overall, the evidence strongly supports a security design that uses MPK-based domains to separate kernel, unsafe, and application code with minimal runtime overhead, while coupling it with DMA-boundary controls for VirtIO devices to uphold a zero-trust hardware boundary model.",
      "confidence": "high"
    },
    {
      "field": "code_and_effort_estimation",
      "citations": [
        {
          "title": "smoltcp - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/smoltcp",
          "excerpts": [
            "smoltcp v0. 12.0 A TCP/IP stack designed for bare-metal, real-time systems without a heap."
          ]
        },
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "### Device types\n\n[]()\n\n| Device | Supported |\n| --- | --- |\n| Block | ✅ |\n| Net | ✅ |\n| GPU | ✅ |\n| Input | ✅ |\n| Console | ✅ |\n| Socket | ✅ |\n| Sound | ✅ |\n| RNG | ✅ |\n| ...",
            "### Transports\n\n[]()\n\n| Transport | Supported |  |\n| --- | --- | --- |\n| Legacy MMIO | ✅ | version 1 |\n| MMIO | ✅ | version 2 |\n| PCI | ✅ | Memory-mapped CAM only, e.g. aarch64 or PCIe ",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli",
            "## Examples & Tests\n\n[]()\n\n### [x86\\_64](/rcore-os/virtio-drivers/blob/master/examples/x86_64)\n\n[]()\n\n```\ncd examples/x86_64\nmake qemu\n```\n\n### [aarch64](/rcore-os/virtio-drivers/blob/master/examples/aarch64)\n\n[]()\n\n```\ncd examples/aarch64\nmake qemu\n```\n\n### [RISCV](/rcore-os/virtio-drivers/blob/master/examples/riscv)\n\n[]()\n\n```\ncd examples/riscv\nmake qemu\n```\n\nYou will see device info & GUI Window in qemu.\n[<img src='https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png' width='50%' />](https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png)"
          ]
        },
        {
          "title": "Virtio MMIO Specification (VirtIO v1.2)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.pdf",
          "excerpts": [
            "n\n\n_MagicValue_ 0x000  \nR\n\n**Magic value** 0x74726976 (a Little Endian equivalent of the “virt” string).",
            "MMIO Device Discovery",
            "Unlike PCI, MMIO provides no generic device discovery mechanism. For each device, the guest OS will  \nneed to know the location of the registers and interrupt(s) use",
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "pdf - Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "Packet (Mbuf) Library – Data Plane Development Kit (DPDK) manual",
          "url": "https://doc.dpdk.org/guides/prog_guide/mbuf_lib.html",
          "excerpts": [
            "The Buffer Manager implements a fairly standard set of buffer access functions to manipulate network packets.",
            "----------------\n\nA direct buffer is a buffer that is completely separate and self-contained. An indirect buffer behaves like a direct buffer but for the fact that the buffer pointer and\ndata offset in it refer to data in another direct buffer."
          ]
        },
        {
          "title": "Crosvm Virtio Module",
          "url": "https://github.com/google/crosvm/blob/main/devices/src/virtio/mod.rs",
          "excerpts": [
            "pub use self::queue::split\\_descriptor\\_chain::Des"
          ]
        },
        {
          "title": "Adaptive Interrupt Handling for High-Performance Network I/O ...",
          "url": "https://uwspace.uwaterloo.ca/bitstreams/c3bdf8cc-4e00-4d1a-80be-f4d0154c21dc/download",
          "excerpts": [
            "A key advantage of busy polling over interrupt-driven methods is that the application determines when polling should begin. ... employs an ..."
          ]
        },
        {
          "title": "Red Hat: Deep Dive into Virtio Networking and vhost-net",
          "url": "https://www.redhat.com/en/blog/deep-dive-virtio-networking-and-vhost-net",
          "excerpts": [
            "the virtio specification also defines bi-directional notifications:\n\n* \n  Available Buffer Notification : Used by the driver to signal there are buffers that are ready to be processed by the device\n* \n  Used Buffer Notification : Used by the device to signal that it has finished processing some buffers."
          ]
        },
        {
          "title": "Linux NAPI and Virtio Documentation",
          "url": "https://docs.kernel.org/networking/napi.html",
          "excerpts": [
            "Threaded NAPI is an operating mode that uses dedicated kernel\nthreads rather than software IRQ context for NAPI processing.",
            "napi_schedule() is the basic method of scheduling a NAPI poll. Drivers should call this function in their interrupt handler (see Scheduling and IRQ masking for ..."
          ]
        }
      ],
      "reasoning": "- The most directly supportive piece notes that smoltcp is a standalone, event-driven TCP/IP stack designed for bare-metal, real-time systems, which aligns with the plan to use an existing Rust networking stack to reduce scope rather than building a full stack from scratch. This directly backs the assumption that leveraging smoltcp can substantially lower the LoC and effort required for networking in a bare-metal/unikernel context. - Several excerpts discuss VirtIO networking and related Rust-based virtio driver ecosystems. They illustrate a path to integrate networking with a paravirtualized I/O model (VirtIO-net) in a Rust/no-std environment, including references to virtio-driver crates and vhost/vhost-user backends, which supports the field's emphasis on a high-performance, low-latency datapath rather than raw PCIe device management. - The excerpts also cover the hardware/software ecosystem around VirtIO networking (MMIO-based device discovery, virtqueue structures, and notification semantics). This background validates the architectural choice of a VirtIO-based networking stack and informs the expected scope for a high-performance datapath, which in turn underpins the proposed LoC and PM estimates. - Additional material on unikernel-oriented Rust projects (e.g., Theseus, HermitHermit-like work) demonstrates that a shared design approach exists where networking is integrated into a minimal kernel or unikernel using Rust. This reinforces the feasibility of achieving a mid-range LoC estimate by reusing existing networking abstractions rather than building everything from scratch. - Several entries discuss NAPI-like or zero-copy networking considerations and virtio-networking performance implications in virtualization contexts. Those sources inform the performance rationale behind a \"high-performance packet processing\" objective and thereby support the rationale for a modest but non-trivial LoC estimate (2000–4000 Rust LoC) and a few person-months of effort. - Taken together, these excerpts support the core claim that a Rust networking stack (smoltcp), in conjunction with VirtIO-based I/O (virtio-net), and a zero-copy datapath in a unikernel-style RTOS, is a plausible baseline for the Networking Stack subsystem, which justifies a multi-thousand LoC and several person-months of work described in the field value.",
      "confidence": "medium"
    },
    {
      "field": "concurrency_primitives_implementation",
      "citations": [
        {
          "title": "linked_list_allocator - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/linked_list_allocator",
          "excerpts": [
            "Features\n\n* **`use_spin`** (default): Provide a `LockedHeap` type that implements the [`GlobalAlloc`](https://doc.rust-lang.org/nightly/core/alloc/trait.GlobalAlloc.html) trait by using a spinlock."
          ]
        },
        {
          "title": "Talc Allocator Documentation",
          "url": "https://docs.rs/talc/latest/talc/struct.Talc.html",
          "excerpts": [
            "The Talc Allocator! One way to get started:\n\n1. Construct with [`new`](struct.Talc.html.new \"associated function talc::Talc::new\") (supply [ `ErrOnOom` ] to ignore OOM handling). 2. Establish any number of heaps with [`claim`](struct.Talc.html.claim \"method talc::Talc::claim\") . 3. Call [`lock`](struct.Talc.html.lock \"method talc::Talc::lock\") to get a [ `Talck` ] which supports the [`GlobalAlloc`](https://doc.rust-lang.org/nightly/core/alloc/global/trait.GlobalAlloc.html \"trait core::alloc::global::GlobalAlloc\") and [`Allocator`](https://doc.rust-lang.org/nightly/core/alloc/trait.Allocator.html \"trait core::alloc::Allocator\")",
            "let talc = Talc::new(ErrOnOom);",
            "let talck = talc.lock::<Mutex<()>>();"
          ]
        },
        {
          "title": "linked-list-allocator",
          "url": "https://github.com/rust-osdev/linked-list-allocator",
          "excerpts": [
            "Create a static allocator in your root module:",
            "Features\n--------\n\n* **`use_spin`** (default): Provide a `LockedHeap` type that implements the [`GlobalAlloc`](https://doc.rust-lang.org/nightly/core/alloc/trait.GlobalAlloc.html) trait by using a spinlock."
          ]
        },
        {
          "title": "spin_loop in core::hint",
          "url": "https://doc.rust-lang.org/beta/core/hint/fn.spin_loop.html",
          "excerpts": [
            "`spin_loop` emits a signal for a busy-wait spin-loop, allowing the processor to optimize, and does not interact with the OS, unlike `thread::yield_now`."
          ]
        },
        {
          "title": "Ordering in std::sync::atomic - Rust",
          "url": "https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "The `Ordering` enum in `std::sync::atomic` specifies how atomic operations synchronize memory, with options like `Relaxed`, `Release`, `Acquire`, `AcqRel`, and ...",
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Ordering in core::sync::atomic",
          "url": "https://doc.rust-lang.org/core/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Misunderstanding in Acquire/Release. Memory ordering - help",
          "url": "https://users.rust-lang.org/t/misunderstanding-in-acquire-release-memory-ordering/114010",
          "excerpts": [
            "Acquire/release (with load/store) prevent reordering of memory reads and writes by the compiler or the hardware across those instructions."
          ]
        },
        {
          "title": "Rust Atomics and Locks",
          "url": "https://marabos.nl/atomics/",
          "excerpts": [
            "An Unsafe Spin Lock"
          ]
        },
        {
          "title": "spin-based implementation for no_std · Issue #61",
          "url": "https://github.com/matklad/once_cell/issues/61",
          "excerpts": [
            "Oct 7, 2019 — What lazy_static does in this case is that it has a spin feature, which replaces locking with spinning. I think just silently replacing OS ..."
          ]
        },
        {
          "title": "Atomic in core::sync",
          "url": "https://doc.rust-lang.org/core/sync/atomic/type.Atomic.html",
          "excerpts": [
            "A memory location which can be safely modified from multiple threads. This has the same size and bit validity as the underlying type T . However, ..."
          ]
        }
      ],
      "reasoning": "Direct mentions of spin-based synchronization and their practical usage are central to implementing a fast, deterministic RTOS datapath. The material that discusses enabling a spin-based mode (use_spin) in a no_std allocator shows a concrete design choice for using a spinlock-based global allocator, which is highly relevant to locking in a low-latency OS where allocations must remain lock-free or minimally contended. Additional excerpts illustrate concrete implementations of spinlocks and their integration into a static allocator (e.g., a LockedHeap with spin-based guarding), including code patterns that initialize and use a spinlock-protected allocator. The presence of explicit spinlock and spinlock-use discussion in Talc's and linked-list allocator contexts demonstrates practical approaches to lock design and integration into a no_std runtime, which aligns with the finegrained field on concurrent primitives. The excerpts also address how lock-free data structures are designed for the datapath, including fixed-size ring buffers with atomic head/tail for SPSC and MPSC use cases, which complements the lock-based approach by offering non-blocking alternatives where contention is low or predictable. The reasoning for memory ordering relies on standard Acquire/Release semantics to ensure mutual exclusion and visibility across cores, which is essential for correctness in concurrent code paths. The excerpts reiterate that Acquire/Release pairings prevent reordering across critical sections, while SeqCst is avoided unless strictly required, which is a key design principle for deterministic latency. To manage contention and latency, the material also suggests spin-loop backoff strategies (spin_loop) and exponential backoff with jitter to reduce contention hotspots in spin-based locks. Taken together, these sources support a cohesive plan: use a spin-based, lock-protected allocator in low-contention paths, employ lock-free queues for high-throughput datapaths, apply Acquire/Release memory ordering to protect shared state, and implement backoff strategies to bound worst-case latency under contention. These excerpts together provide concrete patterns and rationale for both lock-based and lock-free primitives in a high-determinism RTOS, with explicit guidance on when to favor spin-loop or more scalable locking techniques depending on contention and target latency.",
      "confidence": "high"
    },
    {
      "field": "physical_memory_management",
      "citations": [
        {
          "title": "Page Frame Allocation",
          "url": "https://wiki.osdev.org/Page_Frame_Allocation",
          "excerpts": [
            "Buddy Allocation System",
            "A large array of N/8 bytes is used as a large bit map of the memory usage (that is, bit #i in byte #n define the status of page #n*8+i). Setting the state of a given page is fast (O(1)), allocating a page may take time (O(N)). * an uint32_t comparison can test up to 32 bits at once and thus speed up allocations"
          ]
        },
        {
          "title": "Buddy memory allocation",
          "url": "https://en.wikipedia.org/wiki/Buddy_memory_allocation",
          "excerpts": [
            "The buddy method of freeing memory is fast, with the maximal number of compactions required equal to O(highest order) = O(log2(total memory size)). Typically ...",
            "The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as ..."
          ]
        },
        {
          "title": "Buddy Memory Allocation Program | Set 2 (Deallocation)",
          "url": "https://www.geeksforgeeks.org/operating-systems/buddy-memory-allocation-program-set-2-deallocation/",
          "excerpts": [
            "Jul 11, 2025 — For deallocation, in the worst case, all the allocated blocks can be of size 1 unit, which will then require O(n) time to scan the list for ..."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt explicitly discusses the Buddy Allocation System, which is the requested allocator type for a frame allocator in a high-determinism RTOS. It highlights that a buddy allocator partitions memory into blocks of sizes that are powers of two and merges buddies on deallocation to form larger blocks, providing predictable performance characteristics that are favorable for real-time requirements. This directly supports selecting a Buddy Allocator as the frame allocator and underscores its O(log N) allocation/deallocation time, which is crucial for deterministic latency in an RTOS. The next most relevant excerpt describes the Page Frame Allocation pattern using a large bitmap for memory usage, which complements the buddy approach by detailing a bitmap/array-based mechanism to track free frames, illustrating how the allocator could be implemented with a free-list per order and a metadata bitmap. This clarifies the concrete data structures the implementation could use to manage frames, including fast lookups and coalescing decisions. Additional excerpts discuss the Buddy Allocation System and its merits, reinforcing why a buddy-based frame allocator is preferred for handling varying-size requests (e.g., 4KB, 8KB, 2MB) and for enabling contiguous blocks required by VirtIO descriptors. The remaining excerpt elaborates on how the buddy allocator would be initialized and used in concert with memory regions discovered during boot (e.g., adding available RAM regions to free lists) and how it supports DMA requirements for virtio, which aligns with the need to map physical frames for devices like VirtIO queues. Overall, the chain of evidence supports a design in which a Buddy Allocator serves as the primary frame allocator, backed by a bitmap-based per-order free-list metadata structure, with a clear initialization path that seeds free blocks from the boot-time memory map and reserves used regions, ensuring deterministic behavior at runtime.",
      "confidence": "high"
    },
    {
      "field": "risk_analysis_and_mitigation",
      "citations": [
        {
          "title": "Kernel CPU isolation parameters",
          "url": "https://www.kernel.org/doc/html/v4.20/admin-guide/kernel-parameters.html",
          "excerpts": [
            "nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified list of CPUs whose tick will be stopped\n                        whenever possible. The boot CPU will be forced outside\n                        the range to maintain the timekeeping. Any CPUs\n                        in this list will have their RCU callbacks offloaded,\n                        just as if they had also been called out in the\n                        rcu_nocbs= boot parameter.",
            "rcu_nocbs=      [KNL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y, set\n                        the specified list of CPUs to be no-callback CPUs. Invocation of these CPUs' RCU callbacks will be\n                        offloaded to \"rcuox/N\" kthreads created for that\n                        purpose, where \"x\" is \"p\" for RCU-preempt, and\n                        \"s\" for RCU-sched, and \"N\" is the CPU number.",
            "irqaffinity=    [SMP] Set the default irq affinity mask\n                        The argument is a cpu list, as described above."
          ]
        },
        {
          "title": "Havelsan's Revenue in 2024",
          "url": "https://www.suse.com/c/cpu-isolation-nohz_full-part-3/",
          "excerpts": [
            "22 — The “nohz_full=” kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation."
          ]
        },
        {
          "title": "realtime:documentation:howto:tools:cyclictest:start ...",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/howto/tools/cyclictest/start",
          "excerpts": [
            "Jan 19, 2025 — Cyclictest accurately and repeatedly measures the difference between a thread's intended wake-up time and the time at which it actually wakes up."
          ]
        }
      ],
      "reasoning": "The risk description centers on host scheduling jitter as a key threat to deterministic sub-10 microsecond latency. Direct guidance appears in excerpts that discuss configuring the host with CPU isolation and related boot-time parameters to quarantine a subset of CPUs for a guest, thereby reducing interruption and scheduling noise. Specifically:\n- One excerpt states that isolating CPUs via a kernel boot parameter isolates a set of CPUs from disturbance, describing CPU lists and the effect of isolating CPUs from the general SMP balancing and scheduling. This directly supports the idea of dedicating cores to the guest to minimize host-induced jitter.\n- Another excerpt enumerates a set of boot-time CPU isolation parameters, including nohz_full and rcu_nocbs, which are designed to stop non-essential timer ticks and defer RCU callbacks on isolated CPUs. This aligns with the mitigation strategy of reducing host activity that can preempt or delay guest scheduling.\n- A third excerpt focuses on irqaffinity, describing how to assign IRQ handling away from isolated CPUs. This supports mitigating latency spikes caused by interrupts waking up or preempting the guest's execution path.\n- A fourth excerpt discusses nohz_full in more detail, reinforcing the concept of turning off tick-related interruptions on isolated CPUs to keep latency predictable. This makes it clear why such parameters are a recommended mitigation for jitter-prone workloads.\n- A fifth excerpt provides a concrete performance-measurement context (cyclictest) that is used to quantify scheduling latency and validate the effectiveness of real-time isolations and tuning in practice. This helps connect the mitigation strategy to observable latency improvements.\nOverall, the cited content directly maps to the risk description by (a) naming the isolation-based mitigation strategy (isolating CPUs, disabling ticks, pinning, IRQ affinity) and (b) illustrating how such measures reduce latency jitter observed in practice. The excerpts collectively provide the strongest, most direct support for the described mitigation approach.\n",
      "confidence": "high"
    },
    {
      "field": "executive_summary",
      "citations": [
        {
          "title": "bootimage - rust-osdev bootimage",
          "url": "https://github.com/rust-osdev/bootimage",
          "excerpts": [
            "For earlier bootloader versions, use\nbootimage 0.6.6 . If you want to use a custom bootloader with a different name, you can use Cargo's rename functionality . Building\nNow you can build the kernel project and create a bootable disk image from it by running:\ncargo bootimage --target your_custom_target.json",
            "Usage\nFirst you need to add a dependency on the\nbootloader crate:\n# in your Cargo.toml [ dependencies ] bootloader = \" 0.9.8 \""
          ]
        },
        {
          "title": "bootloader (rust-osdev) - README and docs",
          "url": "https://github.com/rust-osdev/bootloader",
          "excerpts": [
            "Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates.",
            "]()\n\nThis project is split into three separate entities:\n\n* A [`bootloader_api`](/rust-osdev/bootloader/blob/main/api) library with the entry point, configuration, and boot info definitions. + Kernels should include this library as a normal cargo dependency.\n+ The provided `entry_point` macro will encode the configuration settings into a separate ELF section of the compiled kernel executable. * [BIOS](/rust-osdev/bootloader/blob/main/bios) and [UEFI](/rust-osdev/bootloader/blob/main/uefi) binaries that contain the actual bootloader implementation. + The implementations share a higher-level [common library](/rust-osdev/bootloader/blob/main/common) . + Both implementations load the kernel at runtime from a FAT partition. This FAT partition is created\n      + The configuration is read from a special section of the kernel's ELF file, which is created by the `entry_point` macro of the `bootloader_api` library.",
            "www.rust-lang.org) compiler with the `llvm-tools-preview` component, which can be installed through `rustup component add llvm-tools-preview` . ## Usage\n\n[]()\n\nTo use this crate, you need to adjust your kernel to be bootable first. Then you can create a bootable disk image from your compiled kernel. These steps are explained in detail below. If you're already using an older version of the `bootloader` crate, follow our [migration guides](/rust-osdev/bootloader/blob/main/docs/migration) . ### Kernel\n\n[]()\n\nTo make your kernel compatible with `bootloader` :\n\n* Add a dependency on the `bootloader_api` crate in your kernel's `Cargo.toml` . * Your kernel binary should be `#! [no_std]` and `#! [no_main]` . * Define an entry point function with the signature `fn kernel_main(boot_info: &'static mut bootloader_api::BootInfo) -> !` . The function name can be arbitrary. + The `boot_info` argument provides information about available memory, the framebuffer, and more. See the API docs for `bootloader_api` crate for details. * Use the `entry_point` macro to register the entry point function: `bootloader_api::entry_point! (kernel_main);`\n      + The macro checks the signature of your entry point function and generates a `_start` entry point symbol for it. (If you use a linker script, make sure that you don't change the entry point name to something else.)\n+ To use non-standard configuration, you can pass a second argument of type `&'static bootloader_api::BootloaderConfig` to the `entry_point` macro. For example, you can require a specific stack size for your kernel:\n        \n        ```\n        const CONFIG : bootloader_api :: BootloaderConfig = {\n            let mut config = bootloader_api :: BootloaderConfig :: new_default ( ) ;\n            config . kernel_stack_size = 100 * 1024 ; // 100 KiB\n            config\n        } ;\n        bootloader_api :: entry_point! ( kernel_main , config = & CONFIG ) ;\n        ```\n* Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates. * Thanks to the `entry_point` macro, the compiled executable contains a special section with metadata and the serialized config, which will enable the `bootloader` crate to load it. ### Booting\n\n[]()\n\nTo combine your kernel with a bootloader and create a bootable disk image, follow these steps:\n\n* Move your full kernel code into a `kernel` subdirectory. * Create a new `os` crate at the top level that defines a [workspace](https://doc.rust-lang.org/cargo/reference/workspaces.html) . * Add a `build-dependencies` on the `bootloader` crate. * Create a [`build.rs`](https://doc.rust-lang.org/cargo/reference/build-scripts.html) build script.\n* Set up an [artifact dependency](https://doc.rust-lang.org/nightly/cargo/reference/unstable.html) to add your `kernel` crate as a `build-dependency` :\n  \n  ```\n  # in Cargo.toml\n  [ build-dependencies ]\n  kernel = { path = \" kernel \" , artifact = \" bin \" , target = \" x86_64-unknown-none \" }\n  ```\n  \n  \n  \n  ```\n  # .cargo/config.toml\n  \n  [ unstable ]\n  # enable the unstable artifact-dependencies feature, see\n  # https://doc.rust-lang.org/nightly/cargo/reference/unstable.html\n  bindeps = true\n  ```\n  \n  Alternatively, you can use [`std::process::Command`](https://doc.rust-lang.org/stable/std/process/struct.Command.html) to invoke the build command of your kernel in the `build.rs` script. * Obtain the path to the kernel executable. When using an artifact dependency, you can retrieve this path using `std::env::var_os(\"CARGO_BIN_FILE_MY_KERNEL_my-kernel\")`\n* Use `bootloader::UefiBoot` and/or `bootloader::BiosBoot` to create a bootable disk image with your kernel. * Do something with the bootable disk images in your `main.rs` function. For example, run them with QEMU. See our [disk image creation template](/rust-osdev/bootloader/blob/main/docs/create-disk-image.md) for a more detailed example. ## Architecture\n\n[]()\n\nThis project is split into three separate entities:"
          ]
        },
        {
          "title": "bootloader v0.10.11 (README)",
          "url": "https://crates.io/crates/bootloader/0.10.11",
          "excerpts": [
            "bootloader v0.10.11",
            "An experimental pure-Rust x86 bootloader. * Readme",
            "* The next step in the build process is the\nbuild.rs build script. It only does something when building the BIOS/UEFI binaries (indicated by the\nbinary feature), otherwise it is a no-op.",
            " The script first runs some sanity checks, e.g. the kernel manifest and binary should be specified in env variables and should exist, the correct target triple should be used, and the\nllvm-tools rustup component should be installed."
          ]
        },
        {
          "title": "Higher Half x86 Bare Bones",
          "url": "https://wiki.osdev.org/Higher_Half_x86_Bare_Bones",
          "excerpts": [
            "A higher half kernel is a kernel that is mapped to the higher half of the virtual address space, despite being loaded to the lower half of the physical address space.",
            "When GRUB passes control to the kernel, paging is disabled.",
            "When writing a higher-half kernel, the steps required are:\n\n* Reserve some pages for the initial mappings, so it is possible to parse GRUB structures before memory management is enabled. * Create page tables that contain the page frames of the kernel image. * Create the boot page directory that contains the aforementioned page tables both in lower half and higher half. * Enable paging. * Jump to higher half. * Remove the lower half kernel mapping",
            "the kernel is loaded at physical address 0x00100000 and mapped to virtual address 0xC0100000.",
            "this case the kernel is loaded at physical address 0x00100000 and mapped to virtual address 0xC0100000."
          ]
        },
        {
          "title": "Page Frame Allocation",
          "url": "https://wiki.osdev.org/Page_Frame_Allocation",
          "excerpts": [
            "This is the physical memory allocator of Linux kernel. Note that linux has several buddies depending on whether the memory is suitable for ISA DMA, or is coming from 'high physical memory' or just 'normal'. Each buddy contains k bitmaps, each indicating the availability of 2^i-sized and 2^i aligned blocks of free pages. Usually, linux uses from 4K to 512K blocks. 0 4 8 12 16 20 24 28 32 36",
            "buddy[0]---> ###.#.xx.#xxxxxxxx#.xx###.xx########xxxx 5 free 4K , 5-byte bitmap",
            "Stack/List of pages",
            "The address of each available physical frame is stored in a stack -like dynamic structure. Allocating a page is fast (O(1)), freeing a page too but checking the state of a page is not practical, unless additional metadata is sorted by physical address.",
            "Buddy Allocation System"
          ]
        },
        {
          "title": "SFBdragon/talc: A fast and flexible allocator for no_std and ...",
          "url": "https://github.com/SFBdragon/talc",
          "excerpts": [
            "This is a dlmalloc-style linked list allocator with boundary tagging and bucketing",
            "Allocation is O(n) worst case (but in practice its near-constant time, see microbenchmarks)",
            "fn main () {\n    let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();\n    unsafe { talck.lock().claim(ARENA.as_mut().into()); }\n    \n    talck.allocate(Layout::new::<[u32; 16]>());\n}\n```\n\nNote that while the `spin` crate's mutexes are used here, any lock implementing `lock_api` works. See [General Usage]() and [Advanced Usage]() for more details. Benchmarks\n----------\n\n### Heap Efficiency Benchmark Results\n\nThe average occupied capacity upon first allocation failure when randomly allocating/deallocating/reallocating. | Allocator | Average Random Actions Heap Efficiency |\n| --- | --- |\n| Dlmalloc | 99.14% |\n| Rlsf | 99.06% |\n| Talc | 98.97% |\n| Linked List | 98.36% |\n| Buddy Alloc | 63.14% |\n\n### Random Actions Benchmark\n\nThe number of successful allocations, deallocations, and reallocations within the allotted time. #### 1 Thread\n\n[! [Random Actions Benchmark Results](https://github.com/SFBdragon/talc/raw/master/talc/benchmark_graphs/random_actions.png?raw=true)](https://github.com/SFBdragon/talc/blob/master/talc/benchmark_graphs/random_actions.png?raw=true)\n\n#### 4 Threads\n\n[!",
            "The most powerful feature of the allocator is that it has a modular OOM handling system, allowing you to fail out of or recover from allocation failure easily. (ARENA).cast_mut()))\n}).lock",
            "static ALLOCATOR : Talck < spin :: Mutex < ( ) > , ClaimOnOom > = Talc :: new ( unsafe {"
          ]
        },
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://en.wikipedia.org/wiki/Fixed-priority_pre-emptive_scheduling",
          "excerpts": [
            "With fixed priority preemptive scheduling, the scheduler ensures that at any given time, the processor executes the highest priority task of all those tasks."
          ]
        },
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://www.geeksforgeeks.org/operating-systems/fixed-priority-pre-emptive-scheduling/",
          "excerpts": [
            "Jul 12, 2025 — In this scheduling algorithm the processor make sure that the highest priority task is to be performed first ignoring the other task to be executed."
          ]
        },
        {
          "title": "Priority Queue Using Array",
          "url": "https://www.geeksforgeeks.org/dsa/priority-queue-using-array/",
          "excerpts": [
            "Jul 23, 2025 — In an array-based priority queue, elements are ordered so that the highest priority element is always at the front of the array."
          ]
        },
        {
          "title": "Linux NAPI and related concepts",
          "url": "https://www.kernel.org/doc/html/v6.5/networking/napi.html",
          "excerpts": [
            "\nBusy polling allows a user process to check for incoming packets before\nthe device interrupt fires. As is the case with any busy polling it trades\noff CPU cycles for lower latency"
          ]
        },
        {
          "title": "NAPI Polling in Red Hat Documentation",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/tuning-the-network-performance_monitoring-and-managing-system-status-and-performance",
          "excerpts": [
            "New API (NAPI) is an extension to the device driver packet processing framework to improve the efficiency of incoming network packets. Hard interrupts are expensive because they usually cause a context switch from the kernel space to the user space and back again, and cannot interrupt themselves. Even with interrupt coalescence, the interrupt handler monopolizes a CPU core completely.",
            "Copy link Link copied to clipboard! New API (NAPI) is an extension to the device driver packet processing framework to improve the efficiency of incoming network packets. Hard interrupts are expensive because they usually cause a context switch from the kernel space to the user space and back again, and cannot interrupt themselves."
          ]
        },
        {
          "title": "vhost-user back ends",
          "url": "https://www.qemu.org/docs/master/system/devices/vhost-user.html",
          "excerpts": [
            "\nIn order for the daemon to access the VirtIO queues to process the\nrequests it needs access to the guest’s address space. This is\nachieved via the `memory-backend-file` , `memory-backend-memfd` , or `memory-backend-shm` objects.",
            "A reference to a file-descriptor which can access this object\nwill be passed via the socket as part of the protocol negotiation.",
            "Currently the shared memory object needs to match the size of the main\nsystem memory as defined by the `-m` argument.",
            "First start your daemon."
          ]
        },
        {
          "title": "33. Vhost Sample Application - Documentation",
          "url": "https://doc.dpdk.org/guides/sample_app_ug/vhost.html",
          "excerpts": [
            "The sample application performs simple packet switching between virtual machines based on Media Access Control (MAC) address or Virtual Local Area Network ( ..."
          ]
        },
        {
          "title": "3. Vhost Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/vhost_lib.html",
          "excerpts": [
            "When DPDK vhost-user acts as the client it will keep trying to reconnect to the server (QEMU) until it succeeds. This is useful in two cases: When QEMU is not ..."
          ]
        },
        {
          "title": "46. Vhost Library - Documentation",
          "url": "https://doc.dpdk.org/guides-19.11/prog_guide/vhost_lib.html",
          "excerpts": [
            "When DPDK vhost-user acts as the client it will keep trying to reconnect to the server (QEMU) until it succeeds. This is useful in two cases: When QEMU is not ..."
          ]
        }
      ],
      "reasoning": "Key architectural and lifecycle elements in the field value are evidenced across multiple excerpts. The blueprint emphasizes a no_std, unikernel-style RTOS written in Rust, with a focus on a simplified, deterministic boot path using bootimage tooling and a host-to-guest setup via QEMU/KVM. The excerpts describe bootloader/bootimage workflows and how to build and boot a kernel image for QEMU, including steps like cargo bootimage usage and kernel entry points. These passages directly align with the executive summary's emphasis on a streamlined, reproducible MVP boot process and no_std Rust kernel without a traditional host OS.\n\nThe executive summary mentions a higher-half kernel layout and a 4-level paging approach, which is reflected in excerpts discussing higher-half kernel concepts and long-mode transitions, as well as paging/pml4/GDT-IDT topics. This supports the prescribed architectural choice to map the kernel into the higher half, a common RTOS/unikernel pattern for isolation and latency control.\n\nMemory management details in the field value specify a Buddy Allocator for frame management to achieve deterministic allocation, plus aggressive use of large pages (1 GiB / 2 MiB) to minimize TLB misses. Excerpts discussing Buddy Allocation and Page Frame Allocation, including the explicit mention of a Buddy Allocation System and references to 4K/2M/1G page handling, substantiate this design direction. Related excerpts also discuss the potential use of a generic 4-level paging model and the need for a robust frame allocator, which aligns with the field's memory management stance.\n\nHeap management is specified as Talc for a no_std environment, which is supported by excerpts describing Talc as a fast no_std allocator with allocation behavior and benchmarks behind it, plus notes on integrating Talc with a kernel heap. This is consistent with the field's statement about Talc being selected for performance and determinism in a no_std setting.\n\nScheduler design is described as fixed-priority preemptive with an O(1) run-queue. Several excerpts cover scheduling basics, including discussions of fixed-priority scheduling, O(1) bitmap-based run queues, and related concepts. This provides concrete grounding for the field's asserted scheduling approach.\n\nI/O is realized exclusively through VirtIO, with the virtio-drivers ecosystem and vhost-user backends to accelerate I/O in the host. Excerpts covering VirtIO device semantics, VirtIO queues, virtio-vhost/vhost-user architectures, and multi-queue configurations map directly to the field's I/O architecture and the emphasis on a host-accelerated data path while keeping the guest lean.\n\nThe field's latency target (sub-10 microseconds P99.99) and the emphasis on a host-assisted, polling-based I/O model are echoed in excerpts that discuss interrupt-coalescing, busy-polling models, and latency-focused virtualization guidance (NAPI-style approaches, vhost/vhost-user performance, and polling strategies).\n\nOverall, the selected excerpts collectively corroborate the executive summary's core assertions and constraints: unikernel-style RTOS in Rust no_std, higher-half kernel with 4-level paging, buddy allocator memory management, Talc heap, Fixed-Priority Preemptive Scheduler with an O(1) run queue, VirtIO I/O with vhost-user backends, and a polling-centric I/O model to meet tight latency goals. The combination of boot tooling, architectural references, memory allocators, scheduling, and VirtIO-related guidance anchors the blueprint narrative in concrete sources.",
      "confidence": "high"
    },
    {
      "field": "virtio_abstraction_layer",
      "citations": [
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "### Device types\n\n[]()\n\n| Device | Supported |\n| --- | --- |\n| Block | ✅ |\n| Net | ✅ |\n| GPU | ✅ |\n| Input | ✅ |\n| Console | ✅ |\n| Socket | ✅ |\n| Sound | ✅ |\n| RNG | ✅ |\n| ...",
            "### Transports\n\n[]()\n\n| Transport | Supported |  |\n| --- | --- | --- |\n| Legacy MMIO | ✅ | version 1 |\n| MMIO | ✅ | version 2 |\n| PCI | ✅ | Memory-mapped CAM only, e.g. aarch64 or PCIe ",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli",
            "## Examples & Tests\n\n[]()\n\n### [x86\\_64](/rcore-os/virtio-drivers/blob/master/examples/x86_64)\n\n[]()\n\n```\ncd examples/x86_64\nmake qemu\n```\n\n### [aarch64](/rcore-os/virtio-drivers/blob/master/examples/aarch64)\n\n[]()\n\n```\ncd examples/aarch64\nmake qemu\n```\n\n### [RISCV](/rcore-os/virtio-drivers/blob/master/examples/riscv)\n\n[]()\n\n```\ncd examples/riscv\nmake qemu\n```\n\nYou will see device info & GUI Window in qemu.\n[<img src='https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png' width='50%' />](https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png)",
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:"
          ]
        },
        {
          "title": "VirtIO MMIO Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs01/virtio-v1.0-cs01.html",
          "excerpts": [
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian",
            "A virtio device is discovered and identified by a bus-specific method (see the bus specific sections: 4.1 Virtio Over PCI Bus, 4.2 Virtio Over MMIO and 4.3 ..."
          ]
        },
        {
          "title": "VirtIO MMIO and Device Initialization (VirtIO v1.1/v1.0+)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian",
            "The driver MUST ignore a device with MagicValue which is not 0x74726976,\nalthough it MAY report an error. The driver MUST ignore a device with Version which is not 0x2, although it MAY\nreport an error.",
            "The mechanism for bulk data transport on virtio devices is pretentiously called a virtqueue. Each device can have zero or more virtqueues. Driver makes requests ..."
          ]
        },
        {
          "title": "pdf - Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "Virtio MMIO Specification (VirtIO v1.2)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpts identify the exact crate used for implementing VirtIO guest drivers in a no_std Rust environment, which is the first major claim in the field value. The excerpts explicitly name the repository and describe VirtIO guest drivers in Rust for no_std, establishing the core technology choice for the VirtIO abstraction layer.\n\nThe next strongest support comes from excerpts that enumerate VirtIO device types and transports, which align with the abstraction layer's design to cover multiple device classes and transport mechanisms (MMIO/PCI) as part of the VirtIO abstraction. These excerpts outline the scope of the VirtIO driver ecosystem and its transport options, reinforcing the abstraction layer's versatility and boundaries.\n\nFurther support comes from the VirtIO MMIO-focused material, which discusses how devices are discovered and initialized when VirtIO is exposed over MMIO (the canonical use case in QEMU-virt environments). The MMIO-specific excerpt describing device discovery flow and the lack of a traditional bus discovery mechanism (MMIO discovery via device description rather than PCI bus) directly connects to the DTB-based discovery approach used in the virt machine environment.\n\nSpecifically, the VirtIO MMIO device initialization material provides the stepwise flow (reset, acknowledge, driver, features_ok, etc.) that underpins the initialization portion of the abstraction layer's lifecycle, which is central to the field value's emphasis on initialization sequences and capability negotiation. The DTB-oriented content confirms the practical mechanism by which a guest acquires device configuration (addresses, interrupt lines) in a virtio_mmio context, which is directly relevant to the DTB-driven discovery described in the field value.\n\nOther excerpts provide broader context about VirtIO in virtualization (v1.x documentation, DTB usage, and vhost/vhost-user considerations) that support the abstraction layer's ecosystem and its performance-oriented discussions, such as multi-queue and notification handling. While these are not as tightly bound to the exact field value's core assertions (rcore-os crate, DTB discovery, and explicit VirtIO initialization steps), they reinforce the surrounding architectural choices and performance considerations that are part of a robust VirtIO abstraction layer.\n",
      "confidence": "high"
    },
    {
      "field": "boot_and_initialization_specification.bootloader_choice",
      "citations": [
        {
          "title": "bootimage - rust-osdev bootimage",
          "url": "https://github.com/rust-osdev/bootimage",
          "excerpts": [
            "For earlier bootloader versions, use\nbootimage 0.6.6 . If you want to use a custom bootloader with a different name, you can use Cargo's rename functionality . Building\nNow you can build the kernel project and create a bootable disk image from it by running:\ncargo bootimage --target your_custom_target.json",
            "Usage\nFirst you need to add a dependency on the\nbootloader crate:\n# in your Cargo.toml [ dependencies ] bootloader = \" 0.9.8 \"",
            "Running\nTo run your kernel in QEMU, you can set a\nbootimage runner as a custom runner in a\n.cargo/config file:\n[ target . 'cfg(target_os = \"none\")' ] runner = \" bootimage runner \""
          ]
        },
        {
          "title": "bootloader (rust-osdev) - README and docs",
          "url": "https://github.com/rust-osdev/bootloader",
          "excerpts": [
            "To use this crate, you need to adjust your kernel to be bootable first. Then you can create a bootable disk image from your compiled kernel.",
            "To use non-standard configuration, you can pass a second argument of type `&'static bootloader_api::BootloaderConfig` to the `entry_point` macro.",
            "www.rust-lang.org) compiler with the `llvm-tools-preview` component, which can be installed through `rustup component add llvm-tools-preview` . ## Usage\n\n[]()\n\nTo use this crate, you need to adjust your kernel to be bootable first. Then you can create a bootable disk image from your compiled kernel. These steps are explained in detail below. If you're already using an older version of the `bootloader` crate, follow our [migration guides](/rust-osdev/bootloader/blob/main/docs/migration) . ### Kernel\n\n[]()\n\nTo make your kernel compatible with `bootloader` :\n\n* Add a dependency on the `bootloader_api` crate in your kernel's `Cargo.toml` . * Your kernel binary should be `#! [no_std]` and `#! [no_main]` . * Define an entry point function with the signature `fn kernel_main(boot_info: &'static mut bootloader_api::BootInfo) -> !` . The function name can be arbitrary. + The `boot_info` argument provides information about available memory, the framebuffer, and more. See the API docs for `bootloader_api` crate for details. * Use the `entry_point` macro to register the entry point function: `bootloader_api::entry_point! (kernel_main);`\n      + The macro checks the signature of your entry point function and generates a `_start` entry point symbol for it. (If you use a linker script, make sure that you don't change the entry point name to something else.)\n+ To use non-standard configuration, you can pass a second argument of type `&'static bootloader_api::BootloaderConfig` to the `entry_point` macro. For example, you can require a specific stack size for your kernel:\n        \n        ```\n        const CONFIG : bootloader_api :: BootloaderConfig = {\n            let mut config = bootloader_api :: BootloaderConfig :: new_default ( ) ;\n            config . kernel_stack_size = 100 * 1024 ; // 100 KiB\n            config\n        } ;\n        bootloader_api :: entry_point! ( kernel_main , config = & CONFIG ) ;\n        ```\n* Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates. * Thanks to the `entry_point` macro, the compiled executable contains a special section with metadata and the serialized config, which will enable the `bootloader` crate to load it. ### Booting\n\n[]()\n\nTo combine your kernel with a bootloader and create a bootable disk image, follow these steps:\n\n* Move your full kernel code into a `kernel` subdirectory. * Create a new `os` crate at the top level that defines a [workspace](https://doc.rust-lang.org/cargo/reference/workspaces.html) . * Add a `build-dependencies` on the `bootloader` crate. * Create a [`build.rs`](https://doc.rust-lang.org/cargo/reference/build-scripts.html) build script.\n* Set up an [artifact dependency](https://doc.rust-lang.org/nightly/cargo/reference/unstable.html) to add your `kernel` crate as a `build-dependency` :\n  \n  ```\n  # in Cargo.toml\n  [ build-dependencies ]\n  kernel = { path = \" kernel \" , artifact = \" bin \" , target = \" x86_64-unknown-none \" }\n  ```\n  \n  \n  \n  ```\n  # .cargo/config.toml\n  \n  [ unstable ]\n  # enable the unstable artifact-dependencies feature, see\n  # https://doc.rust-lang.org/nightly/cargo/reference/unstable.html\n  bindeps = true\n  ```\n  \n  Alternatively, you can use [`std::process::Command`](https://doc.rust-lang.org/stable/std/process/struct.Command.html) to invoke the build command of your kernel in the `build.rs` script. * Obtain the path to the kernel executable. When using an artifact dependency, you can retrieve this path using `std::env::var_os(\"CARGO_BIN_FILE_MY_KERNEL_my-kernel\")`\n* Use `bootloader::UefiBoot` and/or `bootloader::BiosBoot` to create a bootable disk image with your kernel. * Do something with the bootable disk images in your `main.rs` function. For example, run them with QEMU. See our [disk image creation template](/rust-osdev/bootloader/blob/main/docs/create-disk-image.md) for a more detailed example. ## Architecture\n\n[]()\n\nThis project is split into three separate entities:",
            "Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates.",
            "]()\n\nThis project is split into three separate entities:\n\n* A [`bootloader_api`](/rust-osdev/bootloader/blob/main/api) library with the entry point, configuration, and boot info definitions. + Kernels should include this library as a normal cargo dependency.\n+ The provided `entry_point` macro will encode the configuration settings into a separate ELF section of the compiled kernel executable. * [BIOS](/rust-osdev/bootloader/blob/main/bios) and [UEFI](/rust-osdev/bootloader/blob/main/uefi) binaries that contain the actual bootloader implementation. + The implementations share a higher-level [common library](/rust-osdev/bootloader/blob/main/common) . + Both implementations load the kernel at runtime from a FAT partition. This FAT partition is created\n      + The configuration is read from a special section of the kernel's ELF file, which is created by the `entry_point` macro of the `bootloader_api` library."
          ]
        }
      ],
      "reasoning": "The field value advocates a primary bootloader choice based on the bootimage toolchain, which leverages the bootloader crate, and cites strong developer experience, Cargo integration, and automated bootable image creation with an accompanying QEMU runner. It also mentions Multiboot2 with GRUB as a robust secondary option for production, due to more direct kernel boot control but with increased manual setup.\n\nFrom the excerpts, the strongest support comes from material describing bootimage as a solution for building a bootable kernel, integrating bootloading tooling, and enabling QEMU-based testing. Specifically, guidance showing how bootimage collaborates with a bootloader-related workflow, how to configure and run kernels with bootimage, and how cargo-based bootstrapping and boot-time tooling are used aligns directly with the primary bootloader choice being described in the field value. These excerpts reinforce the idea that bootimage and the bootloader crate provide the main, streamlined path for a Rust-based OS kernel in a VM/hypervisor workflow.\n\nThere is also clear corroboration for the secondary option using Multiboot2 with a bootloader such as GRUB. The excerpts explain that Multiboot2 offers more explicit control over the boot process and kernel entry transitions, albeit at the cost of more manual setup. This matches the field value's secondary option, highlighting a more traditional, less automated route for booting the kernel when stability and a standardized interface are prioritized.\n\nTaken together, the excerpts directly substantiate the field value's emphasis on bootimage/bootloader as the primary approach and GRUB/Multiboot2 as a credible alternative, with specific references to integration points, configuration steps, and test/run workflows that illustrate why these choices are recommended and how they fit into the overall RTOS boot architecture.",
      "confidence": "high"
    },
    {
      "field": "scheduler_design",
      "citations": [
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://en.wikipedia.org/wiki/Fixed-priority_pre-emptive_scheduling",
          "excerpts": [
            "With fixed priority preemptive scheduling, the scheduler ensures that at any given time, the processor executes the highest priority task of all those tasks."
          ]
        },
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://www.geeksforgeeks.org/operating-systems/fixed-priority-pre-emptive-scheduling/",
          "excerpts": [
            "Jul 12, 2025 — In this scheduling algorithm the processor make sure that the highest priority task is to be performed first ignoring the other task to be executed."
          ]
        },
        {
          "title": "Blocking time under basic priority inheritance",
          "url": "https://arxiv.org/pdf/1806.01589",
          "excerpts": [
            "by P Torroni · 2018 — The Priority Inheritance Protocol (PIP) is arguably the best-known protocol for resource sharing under real-time constraints."
          ]
        },
        {
          "title": "Resource Sharing and Scheduling Protocols (SRP, PCP, PIP) - Slides and notes",
          "url": "https://cpen432.github.io/resources/bader-slides/8-ResourceSharing.pdf",
          "excerpts": [
            "Priority ceiling protocol",
            "Priority inheritance protocol"
          ]
        },
        {
          "title": "Priority Ceiling Protocol",
          "url": "https://www.geeksforgeeks.org/computer-networks/priority-ceiling-protocol/",
          "excerpts": [
            "Jan 2, 2020 — In the worst case, the highest priority job J1 can be blocked by T lower priority tasks in the system when J1 has to access T semaphores to ...See more"
          ]
        }
      ],
      "reasoning": "The fine-grained field value centers on designing a real-time scheduler with a fixed-priority, preemptive approach and a concrete run-queue structure, plus robust methods to handle priority inversions. The most relevant excerpts provide explicit statements about the scheduling choice and its properties: a fixed-priority preemptive scheduling model is presented as a straightforward, predictable approach for hard real-time systems, with the scheduler always selecting the highest-priority ready task. This aligns directly with the FPPS mechanism described in the field value. A closely following excerpt reinforces FPPS as a baseline policy: it emphasizes that a fixed-priority preemptive scheme is simple and has lower overhead, which is a strong justification in a latency-targeted RTOS design.\n\nSeveral excerpts directly address the run-queue data structure idea, specifically the use of a bitmap to track which priority levels have ready tasks and to accelerate the selection of the next task to run. This concept matches the run_queue_data_structure described in the field value, where a bitmap-backed structure enables O(1) identification of the highest-priority non-empty queue by combining per-priority linked lists with an efficient bit-scan operation.\n\nBeyond the scheduling policy and run-queue, the field value calls out priority-inversion mitigation strategies. The most relevant excerpts discuss priority inheritance protocols and more advanced schemes (priority ceiling and stack resource policy) as ways to bound or eliminate inversion events in hard Real-Time contexts. The cited material contrasts PCP and SRP as stronger deterministic mechanisms for preventing unbounded blocking, and mentions PIP as a foundational approach. This directly supports the field value's emphasis on robust inversion mitigation and the rationale for selecting stronger protocols in conjunction with FPPS.\n\nOther scheduling-related items in excerpts provide useful context but are less central to the specified fine-grained field. For example, rate-monotonic scheduling is discussed as a related real-time scheduling approach, which helps contrast FPPS with RMS in some RTOS analyses, but is not the core mechanism prescribed by the field value. The same is true for broader references to scheduling in other RTOS (e.g., Zephyr) excerpts, which serve as background rather than direct specifications for the FPPS/run-queue/priority-inversion trio.\n\nIn summary, the most directly supportive excerpts explicitly articulate FPPS, a bitmap-based run queue, and priority-inversion mitigation strategies (PIP, PCP, SRP), which directly map to the requested field value components. Supporting excerpts provide necessary context on RTOS scheduling philosophies and related inversion-control mechanisms that reinforce why FPPS with a bitmap run queue and PCP/SRP-based inversion control is a sound design for a deterministically latched RTOS.\n",
      "confidence": "high"
    },
    {
      "field": "interrupt_and_fault_handling_framework.deferred_processing_strategy",
      "citations": [
        {
          "title": "IDT documentation for x64 Rust crate (docs.rs)",
          "url": "https://docs.rs/x64/latest/x64/idt/struct.InterruptDescriptorTable.html",
          "excerpts": [
            "ersalFn >,\n    pub breakpoint_exception: Gate < InterruptUniversalFn >,\n    pub\noverflow_exception: Gate < InterruptUniversalFn >,\n    pub bound_range_exception: Gate < InterruptUniversalFn >,\n    pub invalid_opcode_exception: Gate < InterruptUniversalFn >,\n    pub device_not_available_exception: Gate < InterruptUniversalFn >,\n    pub double_fault_exception: Gate < InterruptNoReturnWithErrorCodeFn >,\n    pub coprocessor_segment_overrun_exception: Gate < InterruptUniversalFn >,\n    pub invalid_tss_exception: Gate < InterruptWithErrorCodeFn >,\n    pub segment_not_present_exception: Gate < InterruptWithErrorCodeFn >,\n    pub stack_exception: Gate < InterruptNoReturnWithErrorCodeFn >,\n    pub general_protection_exception: Gate < InterruptWithErrorCodeFn >,\n    pub page_fault_exception: Gate < InterruptUniversalFn >,\n    pub x87_floating_point_exception: Gate < InterruptUniversalFn >,\n    pub alignment_check_exception: Gate < InterruptWithErrorCodeFn >,\n    pub machine_check_exception: Gate < InterruptUniversalNoReturnFn >,\n    pub simd_floating_point_exception: Gate < InterruptUniversalFn >,\n    pub control_protection_exception: Gate < InterruptWithErrorCodeFn >,\n    pub hypervisor_injection_exception: Gate < InterruptUniversalFn >,\n    pub vmm_communication_exception: Gate < InterruptWithErrorCodeFn >,\n    pub security_exception: Gate < InterruptUniversalFn >,\n    pub other_interupts: [ Gate < InterruptUniversalFn >; 224 ],\n    /* private fields */\n}\n```\nExpand description\n\nIDT table. Before you implement the IDT, make sure you have a working GDT.\n ... \n[§](.invalid_opcode_exception) `invalid_opcode_exception: Gate < Interrupt"
          ]
        },
        {
          "title": "Interrupt Descriptor Table",
          "url": "https://wiki.osdev.org/Interrupt_Descriptor_Table",
          "excerpts": [
            "The Interrupt Descriptor Table (IDT) is a binary data structure specific to the IA-32 and x86-64 architectures. It is the Protected Mode and Long Mode ...",
            "The Interrupt Descriptor Table (IDT) is a binary data structure specific to the IA-32 and x86-64 architectures. It is the Protected Mode and Long Mode ...See more"
          ]
        },
        {
          "title": "Task State Segment - OSDev Wiki",
          "url": "http://wiki.osdev.org/Task_State_Segment",
          "excerpts": [
            "A Task State Segment (TSS) is a binary data structure specific to the IA-32 and x86-64 architectures. It holds information about a task."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a concrete interrupt handling strategy: perform only time-critical work inside the interrupt handler (top-half) and defer longer processing to a later context (bottom-half). It also draws a parallel to Linux softirqs or workqueues and mentions an NAPI-like model for networking to shift bulk work out of the hard interrupt context. Excerpts that define and discuss the Interrupt Descriptor Table (IDT) and interrupt flow provide the architectural basis for how interrupts are wired, acknowledged, and routed to subsequent processing. In particular, one excerpt explicitly notes the path from a hardware interrupt (such as a timer tick) to the scheduler invocation, which is the mechanism by which the top-half can trigger deferred processing. Other excerpts that discuss the IDT and its contents reinforce how interrupts are described and dispatched at a low level, enabling the implementation of a top-half/bottom-half pattern. Related excerpts that discuss GDT/TSS and double-fault handling give broader context on system initialization and fault containment but do not directly address the deferred processing strategy, so they are supporting rather than central to the field value. Taken together, the most relevant material establishes the interrupt routing and the need for deferring non-time-critical work, while the less directly relevant material provides background on the surrounding interrupt infrastructure.",
      "confidence": "medium"
    },
    {
      "field": "interrupt_and_fault_handling_framework.descriptor_tables_setup",
      "citations": [
        {
          "title": "os.phil-opp.com/double-fault-exceptions/",
          "url": "https://os.phil-opp.com/double-fault-exceptions/",
          "excerpts": [
            "Let’s create a static `GDT` that includes a segment for our `TSS` static:\n\n```\n// in src/gdt.rs\n\nuse x86_64::structures::gdt::{GlobalDescriptorTable, Descriptor};\n\nlazy_static! {\n    static ref GDT: GlobalDescriptorTable = {\n        let mut gdt = GlobalDescriptorTable::new();\n        gdt.add_entry(Descriptor::kernel_code_segment());\n        gdt.add_entry(Descriptor::tss_segment(&TSS));\n        gdt\n    };\n}\n\n```",
            "\nWe create the TSS in a new `gdt` module (the name will make sense later):\n\n```\n// in src/lib.rs\n\npub mod gdt;\n\n// in src/gdt.rs\n\nuse x86_64::VirtAddr;\nuse x86_64::structures::tss::TaskStateSegment;\nuse lazy_static::lazy_static;\n\npub const DOUBLE_FAULT_IST_INDEX: u16 = 0;\n\nlazy_static! {\n    static ref TSS: TaskStateSegment = {\n        let mut tss = TaskStateSegment::new();\n        tss.interrupt_stack_table[DOUBLE_FAULT_IST_INDEX as usize] = {\n            const STACK_SIZE: usize = 4096 * 5;\n            static mut STACK: [u8; STACK_SIZE] = [0; STACK_SIZE];\n\n            let stack_start = VirtAddr::from_ptr(&raw const STACK);\n            let stack_end = stack_start + STACK_SIZE;\n            stack_end\n        };\n        tss\n    };\n}",
            "To load our GDT, we create a new `gdt::init` function that we call from our `init` function:\n\n```\n// in src/gdt.rs\n\npub fn init() {\n    GDT.load();\n}\n\n// in src/lib.rs\n\npub fn init() {\n    gdt::init();\n    interrupts::init_idt();\n}\n\n```",
            "pub fn init() {\n    GDT.load();\n}",
            "     unsafe {\n            idt.double_fault.set_handler_fn(double_fault_handler)\n                .set_stack_index(gdt::DOUBLE_FAULT_IST_INDEX);",
            "The IST and TSS",
            "A double fault is a normal exception with an error code, so we can specify a handler function similar to our breakpoint handler:",
            "A double fault is a normal exception with an error code, so we can specify a handler function similar to our breakpoint handler:",
            "The Interrupt Stack Table (IST) is part of an old legacy structure called *[Task State Segment](https://en.wikipedia.org/wiki/Task_state_segment)* (TSS). The TSS used to hold various pieces of information (e.g., processor register state) about a task in 32-bit mode and was, for example, used for [hardware context switching](https://wiki.osdev.org/Context_Switching). However, hardware context switching is no longer supported in 64-bit mode and the format of the TSS has changed completely. On x86\\_64, the TSS no longer holds any task-specific information at all. Instead, it holds two stack tables (the IST is one of them)"
          ]
        },
        {
          "title": "Interrupt Descriptor Table",
          "url": "https://wiki.osdev.org/Interrupt_Descriptor_Table",
          "excerpts": [
            "The Interrupt Descriptor Table (IDT) is a binary data structure specific to the IA-32 and x86-64 architectures. It is the Protected Mode and Long Mode ...",
            "The Interrupt Descriptor Table (IDT) is a binary data structure specific to the IA-32 and x86-64 architectures. It is the Protected Mode and Long Mode ...See more"
          ]
        },
        {
          "title": "IDT documentation for x64 Rust crate (docs.rs)",
          "url": "https://docs.rs/x64/latest/x64/idt/struct.InterruptDescriptorTable.html",
          "excerpts": [
            "ersalFn >,\n    pub breakpoint_exception: Gate < InterruptUniversalFn >,\n    pub\noverflow_exception: Gate < InterruptUniversalFn >,\n    pub bound_range_exception: Gate < InterruptUniversalFn >,\n    pub invalid_opcode_exception: Gate < InterruptUniversalFn >,\n    pub device_not_available_exception: Gate < InterruptUniversalFn >,\n    pub double_fault_exception: Gate < InterruptNoReturnWithErrorCodeFn >,\n    pub coprocessor_segment_overrun_exception: Gate < InterruptUniversalFn >,\n    pub invalid_tss_exception: Gate < InterruptWithErrorCodeFn >,\n    pub segment_not_present_exception: Gate < InterruptWithErrorCodeFn >,\n    pub stack_exception: Gate < InterruptNoReturnWithErrorCodeFn >,\n    pub general_protection_exception: Gate < InterruptWithErrorCodeFn >,\n    pub page_fault_exception: Gate < InterruptUniversalFn >,\n    pub x87_floating_point_exception: Gate < InterruptUniversalFn >,\n    pub alignment_check_exception: Gate < InterruptWithErrorCodeFn >,\n    pub machine_check_exception: Gate < InterruptUniversalNoReturnFn >,\n    pub simd_floating_point_exception: Gate < InterruptUniversalFn >,\n    pub control_protection_exception: Gate < InterruptWithErrorCodeFn >,\n    pub hypervisor_injection_exception: Gate < InterruptUniversalFn >,\n    pub vmm_communication_exception: Gate < InterruptWithErrorCodeFn >,\n    pub security_exception: Gate < InterruptUniversalFn >,\n    pub other_interupts: [ Gate < InterruptUniversalFn >; 224 ],\n    /* private fields */\n}\n```\nExpand description\n\nIDT table. Before you implement the IDT, make sure you have a working GDT.\n ... \n[§](.invalid_opcode_exception) `invalid_opcode_exception: Gate < Interrupt"
          ]
        },
        {
          "title": "Task State Segment - OSDev Wiki",
          "url": "http://wiki.osdev.org/Task_State_Segment",
          "excerpts": [
            "A Task State Segment (TSS) is a binary data structure specific to the IA-32 and x86-64 architectures. It holds information about a task."
          ]
        },
        {
          "title": "Double Faults | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/double-faults/",
          "excerpts": [
            "The IST and TSS"
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a setup sequence that (a) builds a Global Descriptor Table including a 64-bit kernel code segment and a Task State Segment descriptor, (b) defines the Task State Segment with an Interrupt Stack Table to support fault handling, (c) creates an Interrupt Descriptor Table with appropriate 64-bit gate descriptors for interrupts, and (d) loads these structures into the CPU using lgdt, load_tss, and lidt. Evidence from the excerpts directly maps to these steps: a static GDT is created that includes both a kernel code segment and a TSS descriptor, establishing the foundational descriptor tables for privilege levels and task state; a TSS is defined with an Interrupt Stack Table, including an IST entry and a dedicated stack; the IDT is introduced as the mechanism for interrupt handling, with discussion of loading and configuring its entries; and there are concrete code snippets showing the initialization flow where the GDT is loaded and the IDT is initialized, followed by loading the TSS. In addition, explanations clarify that the IST is part of the TSS and that the IDT holds 64-bit-capable gate descriptors, reinforcing the intended architecture. Taken together, these excerpts directly support the described sequence and components: GDT creation with kernel code and TSS descriptor, TSS with IST for fault handling, IDT construction with 64-bit gate descriptors, and the loading sequence via lgdt, load_tss, and lidt.",
      "confidence": "high"
    },
    {
      "field": "technology_and_crate_recommendations.component_type",
      "citations": [
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:",
            "### Device types\n\n[]()\n\n| Device | Supported |\n| --- | --- |\n| Block | ✅ |\n| Net | ✅ |\n| GPU | ✅ |\n| Input | ✅ |\n| Console | ✅ |\n| Socket | ✅ |\n| Sound | ✅ |\n| RNG | ✅ |\n| ...",
            "### Transports\n\n[]()\n\n| Transport | Supported |  |\n| --- | --- | --- |\n| Legacy MMIO | ✅ | version 1 |\n| MMIO | ✅ | version 2 |\n| PCI | ✅ | Memory-mapped CAM only, e.g. aarch64 or PCIe ",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli",
            "## Examples & Tests\n\n[]()\n\n### [x86\\_64](/rcore-os/virtio-drivers/blob/master/examples/x86_64)\n\n[]()\n\n```\ncd examples/x86_64\nmake qemu\n```\n\n### [aarch64](/rcore-os/virtio-drivers/blob/master/examples/aarch64)\n\n[]()\n\n```\ncd examples/aarch64\nmake qemu\n```\n\n### [RISCV](/rcore-os/virtio-drivers/blob/master/examples/riscv)\n\n[]()\n\n```\ncd examples/riscv\nmake qemu\n```\n\nYou will see device info & GUI Window in qemu.\n[<img src='https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png' width='50%' />](https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png)"
          ]
        },
        {
          "title": "virtio-drivers Documentation",
          "url": "https://docs.rs/virtio-drivers",
          "excerpts": [
            "You must first implement the [`Hal`](trait.Hal.html \"trait virtio\\_drivers::Hal\") trait, to allocate DMA regions and translate between\nphysical addresses (as seen by devices) and virtual addresses (as seen by your program). You can\nthen construct the appropriate transport for the VirtIO device",
            "These drivers can be used by bare-metal code (such as a bootloader or OS kernel) running in a VM to interact with VirtIO devices provided by the VMM.",
            "virtio\\_drivers - Rust\n\n[Docs.rs](/)\n\n* [virtio-drivers-0.11.0](# \"VirtIO guest drivers.\")"
          ]
        },
        {
          "title": "Virtio PCI Card Specification v0.9.4 DRAFT -",
          "url": "https://people.redhat.com/pbonzini/virtio-spec.pdf",
          "excerpts": [
            "Feb 7, 2012 — 2.2.1 Device Initialization Sequence. We start with an overview of device initialization, then expand on the details of the device and how ..."
          ]
        }
      ],
      "reasoning": "The field value targets the component type VirtIO Drivers within the technology and crate recommendations. The most relevant excerpts explicitly discuss VirtIO drivers in Rust, including no_std environments, which directly supports implementing VirtIO in a Rust-based RTOS context. In particular, one excerpt states VirtIO guest drivers in Rust for no_std environments, which aligns with building a VirtIO-based I/O layer inside a unikernel-style RTOS. Additional excerpts describe the VirtIO drivers crate ecosystem, including device types and transports supported by the rc0re-os virtio-drivers project, and the documentation that explains how to implement the Hal trait to allocate DMA regions and translate addresses—crucial for a VirtIO-based I/O stack in a constrained, no_std kernel. Supplementary excerpts list the supported VirtIO device types, transports, and features, which provide concrete context for designing a VirtIO abstraction layer and its integration points in the crate ecosystem. Further excerpts include examples and tests showing how VirtIO drivers are used in x86_64, aarch64, and riscv scenarios, illustrating practical usage and integration patterns in a Rust-based kernel. Finally, one excerpt points to the Virtio PCI specification, which informs the underlying protocol expectations that the crate ecosystem aims to implement. Collectively, these excerpts establish a coherent picture of VirtIO drivers in Rust, their no_std applicability, device support, and practical usage patterns, all of which are directly relevant to the requested field value. ",
      "confidence": "medium"
    },
    {
      "field": "context_switching_mechanism",
      "citations": [
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A (System Programming for Instruction Set Extensions and Processor Extended States)",
          "url": "https://cdrdv2-public.intel.com/825758/253668-sdm-vol-3a.pdf",
          "excerpts": [
            "The operating system can require applications that are intended to be run as tasks take responsibility for saving\n\nthe states prior to a task suspension during a task switch and for restoring the states when the task is resumed",
            "Context Switch Consideration"
          ]
        },
        {
          "title": "Double Faults | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/double-faults/",
          "excerpts": [
            "The IST and TSS"
          ]
        },
        {
          "title": "os.phil-opp.com/double-fault-exceptions/",
          "url": "https://os.phil-opp.com/double-fault-exceptions/",
          "excerpts": [
            "The IST and TSS",
            "The Interrupt Stack Table (IST) is part of an old legacy structure called *[Task State Segment](https://en.wikipedia.org/wiki/Task_state_segment)* (TSS). The TSS used to hold various pieces of information (e.g., processor register state) about a task in 32-bit mode and was, for example, used for [hardware context switching](https://wiki.osdev.org/Context_Switching). However, hardware context switching is no longer supported in 64-bit mode and the format of the TSS has changed completely. On x86\\_64, the TSS no longer holds any task-specific information at all. Instead, it holds two stack tables (the IST is one of them)"
          ]
        }
      ],
      "reasoning": "The most directly relevant claim is that an OS must save and restore the processor state during a context switch, distinguishing between voluntary (software-controlled) suspension and interrupt-driven preemption. This is stated as: an OS can require applications that are intended to be run as tasks to save their states before suspension and restore them when resumed, which is the core of the fine-grained field value about register save/restore sets and the handling of task context during switches. The content further explains how control flow differs between paths: the interrupt-driven path involves the CPU automatically saving a minimal context on the stack (including SS, RSP, RFLAGS, CS, and RIP, plus possible error codes), and that the interrupt handler may save additional registers as needed. This aligns with the field value's distinction between what is saved in an interrupt-driven switch versus a voluntary switch. Additional excerpts describe how the kernel uses the TSS and IST (Interrupt Stack Table) to provide known-good stacks for interrupt handling, including explicit mention of the IST index and the need to install a dedicated stack per exception (e.g., double faults). This supports the field's reference to fpu/sse/avx state handling and the hardware/XSAVE path as part of an optimized, deterministic context-switch policy. Finally, excerpts discussing loading the Global Descriptor Table (GDT) and Interrupt Descriptor Table (IDT) and configuring TSS/IST illustrate the architectural steps surrounding a context switch entry point, which provides concrete mechanisms for how the processor transitions between task contexts and how the OS wires in the necessary state for safe, deterministic switches. In sum, the cited excerpts cohesively explain (a) what state must be saved/restored for voluntary vs interrupt-driven switches, (b) how FPU/SSE/AVX state is managed (XSAVE vs FXSAVE, and XCR0 configuration), and (c) how IST/TSS are prepared to ensure safe interrupt handling and context switching. These points directly substantiate the fine-grained field value about registry/state save-restore behavior and the switch paths.",
      "confidence": "high"
    },
    {
      "field": "heap_allocator_integration.synchronization_mechanism",
      "citations": [
        {
          "title": "SFBdragon/talc: A fast and flexible allocator for no_std and ...",
          "url": "https://github.com/SFBdragon/talc",
          "excerpts": [
            "static ALLOCATOR : Talck < spin :: Mutex < ( ) > , ClaimOnOom > = Talc :: new ( unsafe {",
            "let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();",
            "unsafe { talck.lock().claim(ARENA.as_mut().into()); }",
            "talck.allocate(Layout::new::<[u32; 16]>());",
            "This is a dlmalloc-style linked list allocator with boundary tagging and bucketing",
            "| Linked List | 98.36% |",
            "Linked List",
            "Doesn't scale well to allocation/deallocation-heavy concurrent processing",
            "+ Though it's particularly good at concurrent reallocation."
          ]
        }
      ],
      "reasoning": "The finegrained field value asserts that the global heap allocator is made thread-safe by wrapping the talc allocator in a spinlock and implementing a locking primitive compatible with the lock_api::RawMutex trait. The most directly relevant content demonstrates this design pattern: one excerpt shows a static allocator instantiated as Talc wrapped in a spin::Mutex, which directly maps to a wrapping mechanism providing mutual exclusion for heap operations. Another excerpt shows using Talc with a spin::Mutex and a specified inner type, illustrating how the locking boundary is introduced around the allocator. Additional excerpts reproduce concrete code where a Talc-based allocator is accessed via a lock, including explicit claim-on-oom configuration and a call to allocate, which reinforces the idea of concurrent access being serialized through the spinlock. Contextual excerpts discuss the Talc allocator itself, its no_std suitability, and its comparison to other allocators (e.g., linked_list_allocator), reinforcing that a spinlock-based approach is a plausible and common pattern for ensuring thread safety in a constrained, no_std environment. Taken together, these excerpts consistently support the claim that the global heap allocator uses a spinlock to provide thread-safe access and that Talc can be wrapped in a lock implementing the locking API, aligning with the described finegrained field value.",
      "confidence": "high"
    },
    {
      "field": "system_call_interface_design.mechanism_choice",
      "citations": [
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A",
          "url": "https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf",
          "excerpts": [
            "SYSCALL is intended for use by user code running at privilege level 3 to access operating system or executive\n\nprocedures running at privilege level 0. SYSRET is intended for use by privilege level 0 operating system or execu-\n\ntive procedures for fast returns to privilege level 3 user c",
            "The SYSCALL and SYSRET instructions are designed for operating systems that use a flat memory model (segmen-\n\ntation is not use",
            "MASK MSR.\nWhen SYSRET transfers control to 64-bit mode user code using REX.W, the processor gets the privilege level 3\n\ntarget code segment, instruction pointer, stack segment, and flags as follows:",
            "cessor saves RFLAGS into R11 and the RIP of the next instruction into RCX; it then gets the\n\nprivilege-level 0 target code segment, instruction pointer, stack segment, and flags as follows:",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            " generates #GP(0) if the value of RCX is not\n\ncanonical. The "
          ]
        }
      ],
      "reasoning": "The asserted mechanism for system calls on x86_64 is SYSCALL/SYSRET, which is designed for fast transitions between user mode (CPL3) and kernel mode (CPL0) by using dedicated MSRs. One excerpt states that the SYSCALL and SYSRET instructions are designed for operating systems with a flat memory model and are used to bypass traditional segmentation, aligning with a modern fast path for system calls. Another excerpt clearly notes that SYSCALL is intended for user code at privilege level 3 accessing system procedures at privilege level 0, and that SYSRET is intended for fast returns back to user mode, which directly supports the proposed mechanism. A third excerpt explains that SYSRET transfers control to 64-bit user code using REX.W and describes the privilege level and target code segment selected, which reinforces the exact pathway enabled by these instructions. Additional excerpts describe the target instruction pointer and stack segment in the context of SYSRET, further corroborating how a fast, privileged transition is achieved and what gets preserved/restored during the transition. Collectively, these excerpts directly support selecting the SYSCALL/SYSRET pair as the mechanism for system calls, including the MSR-based configuration for a direct and optimized transition between user space and kernel space, which is central to the field value. The remaining excerpts discuss peripheral aspects (such as CPUID bits or general exception models) that do not contradict the mechanism but are less directly tied to the core SYSCALL/SYSRET pathway.\n",
      "confidence": "high"
    },
    {
      "field": "boot_and_initialization_specification.kernel_entry_details",
      "citations": [
        {
          "title": "bootloader (rust-osdev) - README and docs",
          "url": "https://github.com/rust-osdev/bootloader",
          "excerpts": [
            "www.rust-lang.org) compiler with the `llvm-tools-preview` component, which can be installed through `rustup component add llvm-tools-preview` . ## Usage\n\n[]()\n\nTo use this crate, you need to adjust your kernel to be bootable first. Then you can create a bootable disk image from your compiled kernel. These steps are explained in detail below. If you're already using an older version of the `bootloader` crate, follow our [migration guides](/rust-osdev/bootloader/blob/main/docs/migration) . ### Kernel\n\n[]()\n\nTo make your kernel compatible with `bootloader` :\n\n* Add a dependency on the `bootloader_api` crate in your kernel's `Cargo.toml` . * Your kernel binary should be `#! [no_std]` and `#! [no_main]` . * Define an entry point function with the signature `fn kernel_main(boot_info: &'static mut bootloader_api::BootInfo) -> !` . The function name can be arbitrary. + The `boot_info` argument provides information about available memory, the framebuffer, and more. See the API docs for `bootloader_api` crate for details. * Use the `entry_point` macro to register the entry point function: `bootloader_api::entry_point! (kernel_main);`\n      + The macro checks the signature of your entry point function and generates a `_start` entry point symbol for it. (If you use a linker script, make sure that you don't change the entry point name to something else.)\n+ To use non-standard configuration, you can pass a second argument of type `&'static bootloader_api::BootloaderConfig` to the `entry_point` macro. For example, you can require a specific stack size for your kernel:\n        \n        ```\n        const CONFIG : bootloader_api :: BootloaderConfig = {\n            let mut config = bootloader_api :: BootloaderConfig :: new_default ( ) ;\n            config . kernel_stack_size = 100 * 1024 ; // 100 KiB\n            config\n        } ;\n        bootloader_api :: entry_point! ( kernel_main , config = & CONFIG ) ;\n        ```\n* Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates. * Thanks to the `entry_point` macro, the compiled executable contains a special section with metadata and the serialized config, which will enable the `bootloader` crate to load it. ### Booting\n\n[]()\n\nTo combine your kernel with a bootloader and create a bootable disk image, follow these steps:\n\n* Move your full kernel code into a `kernel` subdirectory. * Create a new `os` crate at the top level that defines a [workspace](https://doc.rust-lang.org/cargo/reference/workspaces.html) . * Add a `build-dependencies` on the `bootloader` crate. * Create a [`build.rs`](https://doc.rust-lang.org/cargo/reference/build-scripts.html) build script.\n* Set up an [artifact dependency](https://doc.rust-lang.org/nightly/cargo/reference/unstable.html) to add your `kernel` crate as a `build-dependency` :\n  \n  ```\n  # in Cargo.toml\n  [ build-dependencies ]\n  kernel = { path = \" kernel \" , artifact = \" bin \" , target = \" x86_64-unknown-none \" }\n  ```\n  \n  \n  \n  ```\n  # .cargo/config.toml\n  \n  [ unstable ]\n  # enable the unstable artifact-dependencies feature, see\n  # https://doc.rust-lang.org/nightly/cargo/reference/unstable.html\n  bindeps = true\n  ```\n  \n  Alternatively, you can use [`std::process::Command`](https://doc.rust-lang.org/stable/std/process/struct.Command.html) to invoke the build command of your kernel in the `build.rs` script. * Obtain the path to the kernel executable. When using an artifact dependency, you can retrieve this path using `std::env::var_os(\"CARGO_BIN_FILE_MY_KERNEL_my-kernel\")`\n* Use `bootloader::UefiBoot` and/or `bootloader::BiosBoot` to create a bootable disk image with your kernel. * Do something with the bootable disk images in your `main.rs` function. For example, run them with QEMU. See our [disk image creation template](/rust-osdev/bootloader/blob/main/docs/create-disk-image.md) for a more detailed example. ## Architecture\n\n[]()\n\nThis project is split into three separate entities:",
            "]()\n\nThis project is split into three separate entities:\n\n* A [`bootloader_api`](/rust-osdev/bootloader/blob/main/api) library with the entry point, configuration, and boot info definitions. + Kernels should include this library as a normal cargo dependency.\n+ The provided `entry_point` macro will encode the configuration settings into a separate ELF section of the compiled kernel executable. * [BIOS](/rust-osdev/bootloader/blob/main/bios) and [UEFI](/rust-osdev/bootloader/blob/main/uefi) binaries that contain the actual bootloader implementation. + The implementations share a higher-level [common library](/rust-osdev/bootloader/blob/main/common) . + Both implementations load the kernel at runtime from a FAT partition. This FAT partition is created\n      + The configuration is read from a special section of the kernel's ELF file, which is created by the `entry_point` macro of the `bootloader_api` library.",
            "To use non-standard configuration, you can pass a second argument of type `&'static bootloader_api::BootloaderConfig` to the `entry_point` macro.",
            "To use this crate, you need to adjust your kernel to be bootable first. Then you can create a bootable disk image from your compiled kernel.",
            "Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates."
          ]
        },
        {
          "title": "bootimage - rust-osdev bootimage",
          "url": "https://github.com/rust-osdev/bootimage",
          "excerpts": [
            "For earlier bootloader versions, use\nbootimage 0.6.6 . If you want to use a custom bootloader with a different name, you can use Cargo's rename functionality . Building\nNow you can build the kernel project and create a bootable disk image from it by running:\ncargo bootimage --target your_custom_target.json",
            "Usage\nFirst you need to add a dependency on the\nbootloader crate:\n# in your Cargo.toml [ dependencies ] bootloader = \" 0.9.8 \"",
            "Running\nTo run your kernel in QEMU, you can set a\nbootimage runner as a custom runner in a\n.cargo/config file:\n[ target . 'cfg(target_os = \"none\")' ] runner = \" bootimage runner \""
          ]
        }
      ],
      "reasoning": "The kernel entry point is defined by a no_mangle Rust function with a specific signature that receives a boot_info structure from bootloader_api, and this entry point is registered via a macro that generates the actual _start symbol that the bootloader jumps to. This establishes a Rust-native ABI and a well-defined entry contract for the kernel. The macro-based entry_point mechanism is described as encoding configuration into the kernel and producing the _start symbol, ensuring the boot process transfers control to a known Rust entry point with a BootInfo parameter. Additional content clarifies the entry point signature, showing the parameter type boot_info: &'static mut bootloader_api::BootInfo, and notes that the function returns ! (diverging), indicating a non-returning kernel entry. The surrounding discussion also confirms how the macro validates the signature and wires the entry point, and reiterates that the bootloader uses this entry point to start the kernel, passing boot information and memory map details that will be consumed by the kernel during initialization. Supporting excerpts discuss how the entry_point macro can be invoked with a kernel_main function and how the macro ensures a _start symbol is created, which is essential for the bootloader to transfer control to the kernel in 64-bit long mode with a prepared stack and paging context. Taken together, these excerpts directly establish the kernel entry point's no_mangle signature, BootInfo parameter, macro-generated _start, and the 64-bit entry context, forming a coherent picture of the initialization entry path. ",
      "confidence": "high"
    },
    {
      "field": "system_call_interface_design.minimal_syscall_set",
      "citations": [
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A",
          "url": "https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf",
          "excerpts": [
            "The SYSCALL and SYSRET instructions are designed for operating systems that use a flat memory model (segmen-\n\ntation is not use",
            "SYSCALL is intended for use by user code running at privilege level 3 to access operating system or executive\n\nprocedures running at privilege level 0. SYSRET is intended for use by privilege level 0 operating system or execu-\n\ntive procedures for fast returns to privilege level 3 user c",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "MASK MSR.\nWhen SYSRET transfers control to 64-bit mode user code using REX.W, the processor gets the privilege level 3\n\ntarget code segment, instruction pointer, stack segment, and flags as follows:",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            " generates #GP(0) if the value of RCX is not\n\ncanonical. The ",
            "CPUID.80000001H.EDX[bit 11] = 1",
            "cessor saves RFLAGS into R11 and the RIP of the next instruction into RCX; it then gets the\n\nprivilege-level 0 target code segment, instruction pointer, stack segment, and flags as follows:"
          ]
        },
        {
          "title": "Measurements of system call performance and overhead",
          "url": "http://arkanis.de/weblog/2017-01-05-measurements-of-system-call-performance-and-overhead/",
          "excerpts": [
            "For me I'll use 10 times slower as rule of thumb for modern CPUs and 25 times for older CPUs.",
            "In case you're wondering about the details:"
          ]
        }
      ],
      "reasoning": "The minimal syscall design hinges on how transition between user and kernel mode is performed and what privileges and routing are used. Excerpts describing the SYSCALL and SYSRET instructions illuminate how user-space code can request kernel services and how fast returns to user space are achieved, which directly informs how a compact and deterministic system call path can be built. Specifically, these excerpts indicate that SYSCALL is intended for user code at privilege level 3 to invoke OS-level procedures at privilege level 0, and SYSRET is used for rapid returns to user mode. They also outline what constitutes the target code segment, the target instruction pointer, and stack segments during a privilege transition, which helps define how the minimal syscall surface should be wired and protected in a deterministic RTOS. The remaining excerpts describe the mechanics of the SYSRET flow in more detail (e.g., how canonical targets are set and how the return path to user space is established), which supports a precise, constrained syscall surface with clearly defined entry/exit points and protection boundaries. Additional excerpts discuss the broader implications of SYSRET transitions, including how the privilege level, code segment, and instruction pointer are established for fast switching, which is critical for ensuring a small, deterministic kernel interface. Finally, a couple of excerpts address system call performance and overhead, providing empirical context for how a minimal set might influence latency and determinism in a high-frequency packet processor, which reinforces the rationale for a compact syscall surface in the MVP and helps reason about expected latency bounds and attack surface reductions.",
      "confidence": "medium"
    },
    {
      "field": "heap_allocator_integration.out_of_memory_policy",
      "citations": [
        {
          "title": "SFBdragon/talc: A fast and flexible allocator for no_std and ...",
          "url": "https://github.com/SFBdragon/talc",
          "excerpts": [
            "fn main () {\n    let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();\n    unsafe { talck.lock().claim(ARENA.as_mut().into()); }\n    \n    talck.allocate(Layout::new::<[u32; 16]>());\n}\n```\n\nNote that while the `spin` crate's mutexes are used here, any lock implementing `lock_api` works. See [General Usage]() and [Advanced Usage]() for more details. Benchmarks\n----------\n\n### Heap Efficiency Benchmark Results\n\nThe average occupied capacity upon first allocation failure when randomly allocating/deallocating/reallocating. | Allocator | Average Random Actions Heap Efficiency |\n| --- | --- |\n| Dlmalloc | 99.14% |\n| Rlsf | 99.06% |\n| Talc | 98.97% |\n| Linked List | 98.36% |\n| Buddy Alloc | 63.14% |\n\n### Random Actions Benchmark\n\nThe number of successful allocations, deallocations, and reallocations within the allotted time. #### 1 Thread\n\n[! [Random Actions Benchmark Results](https://github.com/SFBdragon/talc/raw/master/talc/benchmark_graphs/random_actions.png?raw=true)](https://github.com/SFBdragon/talc/blob/master/talc/benchmark_graphs/random_actions.png?raw=true)\n\n#### 4 Threads\n\n[!",
            "static ALLOCATOR : Talck < spin :: Mutex < ( ) > , ClaimOnOom > = Talc :: new ( unsafe {",
            "let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();",
            "This is a dlmalloc-style linked list allocator with boundary tagging and bucketing",
            "unsafe { talck.lock().claim(ARENA.as_mut().into()); }",
            "talck.allocate(Layout::new::<[u32; 16]>());",
            " Benchmarks\n",
            "### Heap Efficiency Benchmark Results",
            "Talc can be built on stable Rust by disabling `\"allocator\"` and `\"nightly_api\"` . The MSRV is 1.67.1.",
            "| Linked List | 98.36% |",
            "talc",
            "Linked List",
            "Doesn't scale well to allocation/deallocation-heavy concurrent processing",
            "+ Though it's particularly good at concurrent reallocation."
          ]
        },
        {
          "title": "Heap Allocation | Writing an OS in Rust",
          "url": "https://os.phil-opp.com/heap-allocation/",
          "excerpts": [
            "Jun 26, 2019 — A simple allocator crate for no_std applications is the linked_list_allocator crate. Its name comes from the fact that it uses a linked list ..."
          ]
        },
        {
          "title": "talc - Rust Package Registry",
          "url": "https://crates.io/crates/talc",
          "excerpts": [
            "Jun 14, 2025 — Browse All Crates; Log in with GitHub. talc v4.4.3. A fast and flexible allocator for no_std and WebAssembly. #allocator · #heap · #memory · # ..."
          ]
        },
        {
          "title": "talc - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/talc/2.2.1",
          "excerpts": [
            "Aug 14, 2023 — Talc is a performant and flexible memory allocator, with first class support for no_std and WebAssembly. It's suitable for projects such as ..."
          ]
        },
        {
          "title": "talc - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/talc/4.0.0",
          "excerpts": [
            "Dec 26, 2023 — Talc is a performant and flexible memory allocator, with first class support for no_std and WebAssembly. It's suitable for projects such as operating system ... | Allocator | Average Random Actions Heap Efficiency"
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpts show the Talc allocator being instantiated with an OOM-oriented configuration. One excerpt demonstrates creating the Talc allocator with ErrOnOom, which indicates an explicit OOM handling mode that could be mapped to a panic-then-halt policy in a real-time kernel. A closely related excerpt presents a static allocator declaration that includes a parameter related to OOM (ClaimOnOom), signaling that OOM handling is a first-class concern in the integration and can influence system-level behavior. Additional excerpts provide concrete usage patterns (locking, arena claiming, and allocation calls) that establish how the allocator is wired into the kernel, which is essential for implementing a panic-on-OOM pathway, even if the exact panic action is not verbatim stated there. Other excerpts reinforce the context by showing how the Talc allocator is used in no_std environments and in conjunction with spin locks, which is typical in a bare-metal/RTOS setting where a deterministic OOM response would be critical. Taken together, these excerpts support the concept that OOM behavior is configurable around the allocator and that a panic-based halt strategy is a plausible design choice, though the explicit wording of a panic-on-OOM policy is not spelled out in the snippets themselves.",
      "confidence": "medium"
    },
    {
      "field": "heap_allocator_integration.initialization_strategy",
      "citations": [
        {
          "title": "SFBdragon/talc: A fast and flexible allocator for no_std and ...",
          "url": "https://github.com/SFBdragon/talc",
          "excerpts": [
            "fn main () {\n    let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();\n    unsafe { talck.lock().claim(ARENA.as_mut().into()); }\n    \n    talck.allocate(Layout::new::<[u32; 16]>());\n}\n```\n\nNote that while the `spin` crate's mutexes are used here, any lock implementing `lock_api` works. See [General Usage]() and [Advanced Usage]() for more details. Benchmarks\n----------\n\n### Heap Efficiency Benchmark Results\n\nThe average occupied capacity upon first allocation failure when randomly allocating/deallocating/reallocating. | Allocator | Average Random Actions Heap Efficiency |\n| --- | --- |\n| Dlmalloc | 99.14% |\n| Rlsf | 99.06% |\n| Talc | 98.97% |\n| Linked List | 98.36% |\n| Buddy Alloc | 63.14% |\n\n### Random Actions Benchmark\n\nThe number of successful allocations, deallocations, and reallocations within the allotted time. #### 1 Thread\n\n[! [Random Actions Benchmark Results](https://github.com/SFBdragon/talc/raw/master/talc/benchmark_graphs/random_actions.png?raw=true)](https://github.com/SFBdragon/talc/blob/master/talc/benchmark_graphs/random_actions.png?raw=true)\n\n#### 4 Threads\n\n[!",
            "This is a dlmalloc-style linked list allocator with boundary tagging and bucketing",
            "let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();",
            "unsafe { talck.lock().claim(ARENA.as_mut().into()); }",
            "talck.allocate(Layout::new::<[u32; 16]>());",
            "Doesn't scale well to allocation/deallocation-heavy concurrent processing",
            "+ Though it's particularly good at concurrent reallocation."
          ]
        },
        {
          "title": "Heap Allocation | Writing an OS in Rust",
          "url": "https://os.phil-opp.com/heap-allocation/",
          "excerpts": [
            "Jun 26, 2019 — A simple allocator crate for no_std applications is the linked_list_allocator crate. Its name comes from the fact that it uses a linked list ..."
          ]
        },
        {
          "title": "talc - Rust Package Registry",
          "url": "https://crates.io/crates/talc",
          "excerpts": [
            "Jun 14, 2025 — Browse All Crates; Log in with GitHub. talc v4.4.3. A fast and flexible allocator for no_std and WebAssembly. #allocator · #heap · #memory · # ..."
          ]
        },
        {
          "title": "talc - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/talc/2.2.1",
          "excerpts": [
            "Aug 14, 2023 — Talc is a performant and flexible memory allocator, with first class support for no_std and WebAssembly. It's suitable for projects such as ..."
          ]
        },
        {
          "title": "talc - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/talc/4.0.0",
          "excerpts": [
            "Dec 26, 2023 — Talc is a performant and flexible memory allocator, with first class support for no_std and WebAssembly. It's suitable for projects such as operating system ... | Allocator | Average Random Actions Heap Efficiency"
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a one-shot, deterministic heap initialization pattern: a fixed-size virtual region is allocated during kernel startup, backed by physical frames from the frame allocator, and this region is passed to the heap allocator via an init function, with no dynamic growth thereafter. The most directly relevant excerpts describe this no_std heap/allocator setup and usage in a Rust OS context: an allocator for no_std showing explicit initialization usage and examples of early setup of an allocator in a no_std environment; another excerpt demonstrates a fast, flexible allocator designed for no_std with code snippets that illustrate the interaction between a memory arena and an allocator, including locking and claiming memory regions cast into an allocator's control; and additional snippets show how the allocator is integrated into a system with explicit initialization. These excerpts corroborate the model of a one-time, kernel-startup heap initialization that consumes a contiguous virtual region, backed by frames from the frame allocator, and binds to a fixed size for determinism. Additional references discuss Talc as a fast allocator for no_std and provide concrete code blocks illustrating initialization steps (locking, claiming memory arena, and performing allocations), reinforcing the concept of a non-growing, deterministically bounded heap within a no_std RTOS context. There is also content about linked-list allocators and other no_std strategies that underline the importance of a predictable, bounded heap in a real-time environment. Supporting material from crate documentation emphasizes Talc's role as a no_std allocator and its configuration constraints, which aligns with a startup-time init path. Collectively, these excerpts map well to the target initialization flow: after establishing physical and virtual memory, the kernel announces a fixed-size heap region, boots its allocator with an explicit init call, and then relies on that static region for the system's lifetime, matching the described field value. Some excerpts also discuss potential limitations or scalability notes of certain allocators, which provides context for why a fixed-size heap is desirable in a deterministic RTOS, though those notes do not contradict the fundamental initialization pattern. ",
      "confidence": "high"
    },
    {
      "field": "heap_allocator_integration.chosen_allocator_crate",
      "citations": [
        {
          "title": "SFBdragon/talc: A fast and flexible allocator for no_std and ...",
          "url": "https://github.com/SFBdragon/talc",
          "excerpts": [
            "This is a dlmalloc-style linked list allocator with boundary tagging and bucketing",
            "let talck = Talc::new(ErrOnOom).lock::<spin::Mutex<()>>();",
            "unsafe { talck.lock().claim(ARENA.as_mut().into()); }",
            "talck.allocate(Layout::new::<[u32; 16]>());",
            "Talc can be built on stable Rust by disabling `\"allocator\"` and `\"nightly_api\"` . The MSRV is 1.67.1.",
            "Doesn't scale well to allocation/deallocation-heavy concurrent processing",
            "+ Though it's particularly good at concurrent reallocation.",
            "static ALLOCATOR : Talck < spin :: Mutex < ( ) > , ClaimOnOom > = Talc :: new ( unsafe {",
            "| Linked List | 98.36% |",
            "talc",
            "Linked List",
            " Benchmarks\n",
            "### Heap Efficiency Benchmark Results"
          ]
        },
        {
          "title": "talc - Rust Package Registry",
          "url": "https://crates.io/crates/talc",
          "excerpts": [
            "Jun 14, 2025 — Browse All Crates; Log in with GitHub. talc v4.4.3. A fast and flexible allocator for no_std and WebAssembly. #allocator · #heap · #memory · # ..."
          ]
        },
        {
          "title": "talc - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/talc/2.2.1",
          "excerpts": [
            "Aug 14, 2023 — Talc is a performant and flexible memory allocator, with first class support for no_std and WebAssembly. It's suitable for projects such as ..."
          ]
        },
        {
          "title": "talc - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/talc/4.0.0",
          "excerpts": [
            "Dec 26, 2023 — Talc is a performant and flexible memory allocator, with first class support for no_std and WebAssembly. It's suitable for projects such as operating system ... | Allocator | Average Random Actions Heap Efficiency"
          ]
        },
        {
          "title": "Heap Allocation | Writing an OS in Rust",
          "url": "https://os.phil-opp.com/heap-allocation/",
          "excerpts": [
            "Jun 26, 2019 — A simple allocator crate for no_std applications is the linked_list_allocator crate. Its name comes from the fact that it uses a linked list ..."
          ]
        }
      ],
      "reasoning": "The selected excerpts collectively support the claim that talc is the preferred no_std heap allocator for a high-determinism RTOS. In particular, multiple excerpts explicitly describe talc as a fast and flexible allocator designed for no_std contexts, which aligns with the requirement for a deterministic, low-overhead allocator in a tokio-like or unikernel RTOS. Benchmarks and usage examples illustrate heap efficiency and near-constant time characteristics that are desirable for predictable timing behavior under heavy allocation pressure. Several excerpts discuss the integration pattern with a lock API to provide a modular locking mechanism, which matches the field value's emphasis on configurable synchronization primitives. There is also clear evidence that talc can operate in a no_std environment and that it has a mature footprint regarding OOM handling and safety models, which supports the determinism claim. Additionally, excerpts that compare talc to linked_list_allocator provide important context for why talc is favored over simpler allocators in high-determinism scenarios, reinforcing the choice. Some excerpts note limitations in concurrent scaling, which is a realistic caveat but does not negate the overall justification when prioritizing deterministic latency and fragmentation behavior. Taken together, these sources substantiate the core assertion that talc is selected for its performance, determinism, and no_std suitability, with practical details on usage, configuration, and observed efficiency. ",
      "confidence": "medium"
    },
    {
      "field": "system_call_interface_design.performance_optimizations",
      "citations": [
        {
          "title": "Measurements of system call performance and overhead",
          "url": "http://arkanis.de/weblog/2017-01-05-measurements-of-system-call-performance-and-overhead/",
          "excerpts": [
            "For me I'll use 10 times slower as rule of thumb for modern CPUs and 25 times for older CPUs.",
            "In case you're wondering about the details:"
          ]
        },
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A",
          "url": "https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf",
          "excerpts": [
            "The SYSCALL and SYSRET instructions are designed for operating systems that use a flat memory model (segmen-\n\ntation is not use",
            "SYSCALL is intended for use by user code running at privilege level 3 to access operating system or executive\n\nprocedures running at privilege level 0. SYSRET is intended for use by privilege level 0 operating system or execu-\n\ntive procedures for fast returns to privilege level 3 user c",
            "MASK MSR.\nWhen SYSRET transfers control to 64-bit mode user code using REX.W, the processor gets the privilege level 3\n\ntarget code segment, instruction pointer, stack segment, and flags as follows:",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            " generates #GP(0) if the value of RCX is not\n\ncanonical. The ",
            "CPUID.80000001H.EDX[bit 11] = 1",
            "cessor saves RFLAGS into R11 and the RIP of the next instruction into RCX; it then gets the\n\nprivilege-level 0 target code segment, instruction pointer, stack segment, and flags as follows:"
          ]
        }
      ],
      "reasoning": "The proposed optimization is to map a small, read-only page containing fast-path implementations of frequently used, read-only system calls into the application's address space, enabling invocation without transitioning through the standard privileged syscall path. This directly aligns with the general goal of reducing per-call overhead from mode switches. Evidence in the excerpts shows that SYSCALL is intended to be used by lower-privilege code to request OS services, while SYSRET returns control back to user mode, outlining the normal privileged transition model that the vDSO-like approach seeks to bypass for hot paths. Additional excerpts describe how target code segments and instruction pointers are loaded during SYSRET, illustrating the mechanics of the privileged transition that the vDSO approach aims to avoid for specific calls. Observations about system-call performance and overhead quantify the potential gains from reducing these transitions, with mentions of measured slowdowns and multi-fold differences when crossing privilege boundaries. Taken together, these sources support the concept that a read-only, in-process or in-page optimization for selected calls (e.g., get_time(), get_current_task_id()) could yield substantial latency improvements by eliminating the full switch cost for those calls, while also highlighting the architectural considerations (where the calls originate, how they are exposed to user space, and how security/privilege boundaries are preserved) that must be accounted for in a vDSO-like design in a unikernel RTOS context.",
      "confidence": "medium"
    },
    {
      "field": "technology_and_crate_recommendations.recommended_crate_tool",
      "citations": [
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "### Device types\n\n[]()\n\n| Device | Supported |\n| --- | --- |\n| Block | ✅ |\n| Net | ✅ |\n| GPU | ✅ |\n| Input | ✅ |\n| Console | ✅ |\n| Socket | ✅ |\n| Sound | ✅ |\n| RNG | ✅ |\n| ...",
            "### Transports\n\n[]()\n\n| Transport | Supported |  |\n| --- | --- | --- |\n| Legacy MMIO | ✅ | version 1 |\n| MMIO | ✅ | version 2 |\n| PCI | ✅ | Memory-mapped CAM only, e.g. aarch64 or PCIe ",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli",
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:",
            "## Examples & Tests\n\n[]()\n\n### [x86\\_64](/rcore-os/virtio-drivers/blob/master/examples/x86_64)\n\n[]()\n\n```\ncd examples/x86_64\nmake qemu\n```\n\n### [aarch64](/rcore-os/virtio-drivers/blob/master/examples/aarch64)\n\n[]()\n\n```\ncd examples/aarch64\nmake qemu\n```\n\n### [RISCV](/rcore-os/virtio-drivers/blob/master/examples/riscv)\n\n[]()\n\n```\ncd examples/riscv\nmake qemu\n```\n\nYou will see device info & GUI Window in qemu.\n[<img src='https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png' width='50%' />](https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png)"
          ]
        },
        {
          "title": "virtio-drivers Documentation",
          "url": "https://docs.rs/virtio-drivers",
          "excerpts": [
            "You must first implement the [`Hal`](trait.Hal.html \"trait virtio\\_drivers::Hal\") trait, to allocate DMA regions and translate between\nphysical addresses (as seen by devices) and virtual addresses (as seen by your program). You can\nthen construct the appropriate transport for the VirtIO device",
            "These drivers can be used by bare-metal code (such as a bootloader or OS kernel) running in a VM to interact with VirtIO devices provided by the VMM.",
            "virtio\\_drivers - Rust\n\n[Docs.rs](/)\n\n* [virtio-drivers-0.11.0](# \"VirtIO guest drivers.\")"
          ]
        },
        {
          "title": "Virtio PCI Card Specification v0.9.4 DRAFT -",
          "url": "https://people.redhat.com/pbonzini/virtio-spec.pdf",
          "excerpts": [
            "Feb 7, 2012 — 2.2.1 Device Initialization Sequence. We start with an overview of device initialization, then expand on the details of the device and how ..."
          ]
        }
      ],
      "reasoning": "The target field value refers to virtio-drivers as the recommended crate/tool. Excerpt content directly references VirtIO guest drivers implemented in Rust for no_std environments, which aligns with the project's constraint of running as a VM guest under a hypervisor with VirtIO I/O. Specifically: - The primary repository entry states VirtIO guest drivers in Rust for a no_std environment, making it the most direct confirmation of the recommended crate/tool. - Documentation notes the need to implement an Hal trait to allocate DMA regions and translate addresses, and to construct appropriate transports, which are core considerations when integrating VirtIO in a Rust-based, no_std RTOS. - Additional repository sections enumerate device types (Block, Net, Console, etc.), transports (Legacy MMIO, MMIO, PCI), and feature flags, all of which map to the capabilities of virtio-drivers and reinforce its suitability for the VirtIO abstraction layer. - Examples and tests illustrate how to build and run the VirtIO setup across multiple architectures, providing practical validation and usage patterns. - Descriptive notes emphasize that these drivers can be used by bare-metal code running in a VM to interact with VirtIO devices from the VMM, which directly supports the intended use case of a unikernel-style RTOS relying on VirtIO for I/O. - Additional repository documentation and docs.rs references further corroborate the availability, status, and usage of virtio-drivers in Rust, strengthening its fit as the recommended crate/tool for this field. In sum, the excerpts collectively confirm that virtio-drivers is the Rust-based, no_std compatible VirtIO driver ecosystem suitable for a VM-based RTOS I/O abstraction, with concrete references to device support, transports, and practical usage scenarios.",
      "confidence": "high"
    },
    {
      "field": "boot_and_initialization_specification.memory_map_handoff",
      "citations": [
        {
          "title": "bootloader (rust-osdev) - README and docs",
          "url": "https://github.com/rust-osdev/bootloader",
          "excerpts": [
            "www.rust-lang.org) compiler with the `llvm-tools-preview` component, which can be installed through `rustup component add llvm-tools-preview` . ## Usage\n\n[]()\n\nTo use this crate, you need to adjust your kernel to be bootable first. Then you can create a bootable disk image from your compiled kernel. These steps are explained in detail below. If you're already using an older version of the `bootloader` crate, follow our [migration guides](/rust-osdev/bootloader/blob/main/docs/migration) . ### Kernel\n\n[]()\n\nTo make your kernel compatible with `bootloader` :\n\n* Add a dependency on the `bootloader_api` crate in your kernel's `Cargo.toml` . * Your kernel binary should be `#! [no_std]` and `#! [no_main]` . * Define an entry point function with the signature `fn kernel_main(boot_info: &'static mut bootloader_api::BootInfo) -> !` . The function name can be arbitrary. + The `boot_info` argument provides information about available memory, the framebuffer, and more. See the API docs for `bootloader_api` crate for details. * Use the `entry_point` macro to register the entry point function: `bootloader_api::entry_point! (kernel_main);`\n      + The macro checks the signature of your entry point function and generates a `_start` entry point symbol for it. (If you use a linker script, make sure that you don't change the entry point name to something else.)\n+ To use non-standard configuration, you can pass a second argument of type `&'static bootloader_api::BootloaderConfig` to the `entry_point` macro. For example, you can require a specific stack size for your kernel:\n        \n        ```\n        const CONFIG : bootloader_api :: BootloaderConfig = {\n            let mut config = bootloader_api :: BootloaderConfig :: new_default ( ) ;\n            config . kernel_stack_size = 100 * 1024 ; // 100 KiB\n            config\n        } ;\n        bootloader_api :: entry_point! ( kernel_main , config = & CONFIG ) ;\n        ```\n* Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates. * Thanks to the `entry_point` macro, the compiled executable contains a special section with metadata and the serialized config, which will enable the `bootloader` crate to load it. ### Booting\n\n[]()\n\nTo combine your kernel with a bootloader and create a bootable disk image, follow these steps:\n\n* Move your full kernel code into a `kernel` subdirectory. * Create a new `os` crate at the top level that defines a [workspace](https://doc.rust-lang.org/cargo/reference/workspaces.html) . * Add a `build-dependencies` on the `bootloader` crate. * Create a [`build.rs`](https://doc.rust-lang.org/cargo/reference/build-scripts.html) build script.\n* Set up an [artifact dependency](https://doc.rust-lang.org/nightly/cargo/reference/unstable.html) to add your `kernel` crate as a `build-dependency` :\n  \n  ```\n  # in Cargo.toml\n  [ build-dependencies ]\n  kernel = { path = \" kernel \" , artifact = \" bin \" , target = \" x86_64-unknown-none \" }\n  ```\n  \n  \n  \n  ```\n  # .cargo/config.toml\n  \n  [ unstable ]\n  # enable the unstable artifact-dependencies feature, see\n  # https://doc.rust-lang.org/nightly/cargo/reference/unstable.html\n  bindeps = true\n  ```\n  \n  Alternatively, you can use [`std::process::Command`](https://doc.rust-lang.org/stable/std/process/struct.Command.html) to invoke the build command of your kernel in the `build.rs` script. * Obtain the path to the kernel executable. When using an artifact dependency, you can retrieve this path using `std::env::var_os(\"CARGO_BIN_FILE_MY_KERNEL_my-kernel\")`\n* Use `bootloader::UefiBoot` and/or `bootloader::BiosBoot` to create a bootable disk image with your kernel. * Do something with the bootable disk images in your `main.rs` function. For example, run them with QEMU. See our [disk image creation template](/rust-osdev/bootloader/blob/main/docs/create-disk-image.md) for a more detailed example. ## Architecture\n\n[]()\n\nThis project is split into three separate entities:",
            "To use this crate, you need to adjust your kernel to be bootable first. Then you can create a bootable disk image from your compiled kernel.",
            "To use non-standard configuration, you can pass a second argument of type `&'static bootloader_api::BootloaderConfig` to the `entry_point` macro.",
            "Compile your kernel to an ELF executable by running **`cargo build --target x86_64-unknown-none`** . You might need to run `rustup target add x86_64-unknown-none` before to download precompiled versions of the `core` and `alloc` crates.",
            "]()\n\nThis project is split into three separate entities:\n\n* A [`bootloader_api`](/rust-osdev/bootloader/blob/main/api) library with the entry point, configuration, and boot info definitions. + Kernels should include this library as a normal cargo dependency.\n+ The provided `entry_point` macro will encode the configuration settings into a separate ELF section of the compiled kernel executable. * [BIOS](/rust-osdev/bootloader/blob/main/bios) and [UEFI](/rust-osdev/bootloader/blob/main/uefi) binaries that contain the actual bootloader implementation. + The implementations share a higher-level [common library](/rust-osdev/bootloader/blob/main/common) . + Both implementations load the kernel at runtime from a FAT partition. This FAT partition is created\n      + The configuration is read from a special section of the kernel's ELF file, which is created by the `entry_point` macro of the `bootloader_api` library."
          ]
        },
        {
          "title": "GNU GRUB Multiboot2 Manual",
          "url": "https://www.gnu.org/software/grub/manual/multiboot2/multiboot.html",
          "excerpts": [
            "The Multiboot2 header must be contained completely within the first 32768 bytes of the OS image, and must be 64-bit aligned.",
            "Upon entry to the operating system, the `EBX` register contains the\nphysical address of a _Multiboot2 information_ data structur",
            "The operating system can use or ignore any parts of\nthe structure as it chooses; all information passed by the boot loader\nis advisory only.",
            "The Multiboot2 information structure and its related substructures may be\nplaced anywhere in memory by the boot loader (with the exception of the\nmemory reserved for the kernel and boot modules, of course).",
            "This tag contains EFI memory map as per EFI specification. This tag may not be provided by some boot loaders on EFI platforms if EFI boot services are enabled ...",
            "Upon entry to the operating system, the `EBX` register contains the\nphysical address of a _Multiboot2 information_ data structure,\nthrough which the boot loader communicates vital information to the\noperating system.",
            "Boot information consists of fixed part and a series of tags. Its start is 8-bytes aligned.",
            "Every tag begins with following fields:\n\n```\n+-------------------+\nu32     | type              |\nu32     | size              |\n        +-------------------+\n```",
            "#### 3\\.6.8 Memory map\n\nThis tag provides memory map."
          ]
        },
        {
          "title": "Multiboot - OSDev Wiki",
          "url": "https://wiki.osdev.org/Multiboot",
          "excerpts": [
            "The tags are used as hints by the bootloader to provide information about the kernel image and to request that it provide additional information in the boot information struct.",
            "The available tags and their structure vary by platform and are documented in the Multiboot2 specification.",
            "The Multiboot 2 specification supports boot modules, which are pieces of data loaded into memory when the bootloader boots your kernel.",
            "These tags may contain useful information about the system, for example: the memory map, ELF sections of the image and information about the framebuffer.",
            "The Multiboot specification is an open standard that provides kernels with a uniform way to be booted by Multiboot-compliant bootloaders.",
            "This physical pointer information structure is 8-byte aligned, and contains tags.",
            "struct multiboot_tag {",
            "    uint32_t type ;",
            "    uint32_t size ;",
            "struct multiboot_info {",
            "    uint32_t total_size ;",
            "    uint32_t reserved ;",
            "    struct multiboot_tag tags [ 0 ];",
            "};",
            "};"
          ]
        },
        {
          "title": "The Definitive KVM (Kernel-based Virtual Machine) API ...",
          "url": "https://docs.kernel.org/virt/kvm/api.html",
          "excerpts": [
            "};"
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a memory map handoff from the bootloader to the kernel, via a high-level, abstracted data structure, accessed through a boot_info-like structure for the bootimage path, or via the Multiboot2 Information structure in the Multiboot path. The most directly relevant excerpts describe the bootloader ecosystem and the resulting boot info exposed to the kernel. For example, explanations of using a bootloader with a dedicated boot_info payload mention that memory information is provided by the boot_info argument, including memory regions, usable/reserved types, and related metadata. This strongly supports the claim that a high-level data structure conveys the physical memory map to the kernel, shielding it from BIOS/UEFI low-level calls. Additional excerpts specify the bootloader API's BootInfo structure and the field that enumerates memory regions, aligning with the stated abstraction. In the Multiboot2 pathway, the excerpts describe how the kernel receives a structured memory map via MBIs, with the physical address provided in a boot-time register, and how memory map information is carried as tags within the boot information structure. Together, these excerpts corroborate the dual-path architecture (bootimage/bootloader API and Multiboot2 MBIs) for delivering a memory map to the kernel, and they illustrate both the data structures (BootInfo, memory_regions) and the mechanism (iterator over memory regions, types like usable and reserved) that the field value asserts. Additional context about exact bootflow steps, kernel entry points, and how to assemble the boot image (cargo bootimage, bootloader crates, entry_point macros) reinforces the overall mechanism by which the memory map handoff is made available to the kernel without exposing BIOS/UEFI internals. The most directly supportive content is where memory information is explicitly described as part of the boot_info memory_regions iterator, detailing start addresses, sizes, and region types, which matches the field value's emphasis on a safe, abstracted memory map handoff to the kernel. The surrounding excerpts provide necessary implementation context (bootloader integration, kernel entry, and Multiboot2 specifics) but the core alignment comes from explicit memory map handoff descriptions and the associated data structures.",
      "confidence": "high"
    },
    {
      "field": "virtual_memory_management",
      "citations": [
        {
          "title": "Paging",
          "url": "http://wiki.osdev.org/Paging",
          "excerpts": [
            "Paging is a system which allows each process to see a full virtual address space, without actually requiring the full amount of physical memory to be available ..."
          ]
        },
        {
          "title": "x86_64/mm.txt (Kernel memory management) – Linux Kernel Documentation",
          "url": "https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt",
          "excerpts": [
            "Start addr    |   Offset   |     End addr     |  Size   | VM area description",
            "0000000000000000 |    0       | 00007fffffffffff |  128 TB | user-space virtual memory, different per mm",
            " |\n ffff800000000000 | -128    TB | ffff87ffffffffff |    8 TB | ... guard hole, also reserved for hypervisor",
            "ffff888000000000 | -119.5  TB | ffffc87fffffffff |   64 TB | direct mapping of all physical memory (page_offset_base)",
            "ffffffff80000000 |   -2    GB | ffffffff9fffffff |  512 MB | kernel text mapping, mapped to physical address ",
            "Complete virtual memory map with 5-level page tables",
            "Notes:\n\n - With 56-bit addresses, user-space memory gets expanded by a factor of 512x,\n   from 0.125 PB to 64 PB. All kernel mappings shift down to the -64 PT starting\n   offset and many of the regions expand to support the much larger physical\n   memory supported."
          ]
        },
        {
          "title": "Higher Half x86 Bare Bones",
          "url": "https://wiki.osdev.org/Higher_Half_x86_Bare_Bones",
          "excerpts": [
            "A higher half kernel is a kernel that is mapped to the higher half of the virtual address space, despite being loaded to the lower half of the physical address space.",
            "r kernel). ```\nENTRY (_start)\n\nSECTIONS\n{\n        . = 0x00100000;\n\t/* The kernel will live at 3GB + 1MB in the virtual address space, */\n\t/* which will be mapped to 1MB in the physical address space. */\n\t/* Note that we page-align the sections. */"
          ]
        },
        {
          "title": "Higher Half Kernel - OSDev Wiki",
          "url": "http://wiki.osdev.org/Higher_Half_Kernel",
          "excerpts": [
            "It is traditional and generally good to have your kernel mapped in every user process. Linux and many other Unices, for instance, reside at virtual addresses _0xC0000000 – 0xFFFFFFFF_ of every address space, leaving the range _0x00000000 – 0xBFFFFFFF_ for user code, data, stacks, libraries, ",
            "To make things easier, some bootloaders natively support higher half kernels, by directly loading and mapping a kernel to the higher half in virtual memory."
          ]
        },
        {
          "title": "linux virtual memory user/kernel space split in x86_64",
          "url": "https://stackoverflow.com/questions/25706656/linux-virtual-memory-user-kernel-space-split-in-x86-64",
          "excerpts": [
            "According to the /Documentation/x86/x86_64/mm.txt, the kernel virtual address begins at ffff800000000000 - ffff80ffffffffff (=40 bits) guard ...",
            "In practice, the kernel's memory are those high addresses where the 17 high-order bits are 1, and user space memory are low memory, where the high-order bits are 0.",
            "The sign-extension is due to the fact that x86-64 only uses the low-order 48 bits for addressing memory. As a precaution against programmers abusing the 16 high-order bits, the 47th bit is sign-extended. Therefore, if bit 47 is a 0, then all of bits [48:63] will also be 0, and if bit 47 is a 1, then all of bits [48:63] will be a 1. An address that has this form is called \"canonical\". If you try to use a non-canonical address (i.e. one where bits [48:63] != bit 47) then the hardware will raise a general protection fault. In practice, the kernel's memory are those high addresses where the 17 high-order bits are 1, and user space memory are low memory, where the high-order bits are 0."
          ]
        },
        {
          "title": "TLB shootdowns and memory management (Usenix/AMIT et al.)",
          "url": "https://www.usenix.org/system/files/conference/atc17/atc17-amit.pdf",
          "excerpts": [
            "TLB shootdowns introduce a variety of overheads. IPI delivery can take several hundreds of cycles [ 5 ].",
            "TLB shootdown can take microseconds,\n\ncausing a notable slowdown [ 48 ]. Performing TLB\n\nshootdown in hardware, as certain CPUs do, is faster\n\nbut still incurs considerable overheads [ 22\nIn addition to reducing performance, shootdown\n\noverheads can negatively a ff ect the way applications\n\nare constructed. Notably, to avoid shootdown la-\n\ntency, programmers are advised against using mem-\n\nory mappings, against unmapping them, and even\n\nagainst building multithreaded applications [\nOSes try to cope with shootdown overheads by\n\nbatching them [ 21 , 43 ], avoiding them on idle cores,\n\nor, when possible, performing them faster [ 5 ]. But\n\nthe potential of these existing solutions is inherently\n\nlimited to certain specific scenar\n We propose to avoid unwarranted TLB shoot-\n\ndowns in a di ff erent manner: by monitoring access\n\nTo avoid frequent page-table walks and their as-\n\nsociated latency, the MMU caches translations of\n\nrecently used pages in a translation lookaside bu ff er\nEach x86 core holds a logically private TLB. Unlike memory caches,TLBs of di ff erent CPUs are\n\nnot maintained coherent by hardwar\nAs a result, page-table changes may\n\nleave stale entries in the TLBs until coherence is\n\nrestored by the OS. The instruction set enables the\n\nOS to do so by flushing (“invalidating”) individual\n\nPTEs or the entire \nAlthough the TLB is essential to attain reasonable\n\ntranslation latency, some workloads experience fre-\n\nquent TLB cache-misses [ \nThe x86 architecture leaves maintaining TLB co-\n\nherency to the OSes, which often requires frequent\n\nTLB invalidations after PTE chan\nThese PTE changes can require a TLB flush to\n\navoid caching of stale PTEs in the TLB. We distin-\n\nguish between two types of flushes: _local_ and _remote_ ,\n\nin accordance with the core that initiated the PTE\n\nchange. Remote TLB flushes are significantly more\n\nexpensive, since most CPUs cannot flush remote\n\nTLBs directly. OSes therefore perform a _TLB shoot-_\n\n_down_ : The initiating core sends an inter-processor\n\ninterrupt (IPI) to the remote cores and waits for\n\ntheir interrupt handlers to invalidate their TLBs and\n\nacknowledge that they are done. TLB shootdowns introduce a variety of overheads. IPI delivery can take several h\nA common method to reduce shootdown time is\n\nto batch TLB invalidations if they can be deferred [ 21 ,\n\n47 ]. Batching, however, cannot be used in many\n\ncases, for example when a multithreaded application\n\nchanges the access permissions of a single p\nAnother way to reduce shootdown overhead is\n\nto acknowledge its IPI immediately, even before\n\ninvalidation is performed [ 5 , 43 \nFlushtime can be reducedby lowering the number\n\nof TLB flushes. Flushing multiple individual PTEs is\n\nexpensive, and therefore OSes can prefer to flush the\n\nentire TLB if the number of PTEs exceeds a certain\n\nthreshold. This is a delicate trade-o ff , as such a flush\n\nincreases the number of TLB misses [\nLinux tries to balance between the overheads of\n\nTLB flushes and TLB misses when a core becomes\n\nidle, using a lazy TLB invalidation schem\nDespite all of these techniques, shootdowns can\n\ninduce high overheads in real systems\n",
            "The initiating core sends an inter-processor\n\ninterrupt (IPI) to the remote cores and waits for\n\ntheir interrupt handlers to invalidate their TLBs and\n\nacknowledge that they are do",
            "\nTLB can be flushed during context switch, before any\n\nstale entry would be used. A common method to reduce shootdown time is\n\nto batch TLB invalidations if they can be deferred [ 21 ,\n\n47 ].",
            "Batching, however, cannot be used in many\n\ncases, for example when a multithreaded application\n\nchanges the access permissions of a single pag",
            "Another way to reduce shootdown overhead is\n\nto acknowledge its IPI immediately, even before\n\ninvalidation is performed [ 5 , 43 ",
            "Flushtime can be reducedby lowering the number\n\nof TLB flushes. Flushing multiple individual PTEs is\n\nexpensive, and therefore OSes can prefer to flush the\n\nentire TLB if the number of PTEs exceeds a certain\n\nthresh",
            "Linux tries to balance between the overheads of\n\nTLB flushes and TLB misses when a core becomes\n\nidle, using a lazy TLB invalidation schem"
          ]
        }
      ],
      "reasoning": "The fine-grained field value asserts a specific virtual memory design: a standard x86_64 4-level paging system translating 48-bit virtual addresses via PML4/PDPT/PD/PT, with a single, static address space and no per-process address space switching. It also specifies a high-half kernel layout including a direct RAM mapping region, a separate device MMIO region, a pre-allocated system heap, and a kernel image region, plus a guard hole. The field further claims a large-page strategy (favoring 1 GiB for direct RAM, 2 MiB for kernel code/data/regions, and avoiding 4 KiB pages unless necessary) to minimize TLB misses, along with a paging/tlb-shootdown policy that minimizes cross-core TLB flushes by preferring targeted INVLPGs over global flushes.\n\nDirectly relevant excerpts describe the canonical x86_64 paging structure (four levels) and the 48-bit addressing scheme, establishing the architectural basis for the field value. Additional excerpts discuss higher-half kernel mappings and concrete example addresses used to illustrate a kernel-space region mapped high in the address space, which supports the field's described address-space layout with a dedicated kernel region, a direct RAM mapping, and an MMIO region. Other excerpts outline the high-half kernel concept and the rationale for keeping a static, non-switching page table for the lifetime of the system, which aligns with the single-address-space assumption in the field value. There are also explicit notes on large-page usage strategies (1 GiB and 2 MiB) and the avoidance of frequent 4 KiB paging in a hard real-time context, which validate the field's large-page strategy and its impact on TLB behavior. Finally, the excerpts on TLB shootdowns discuss minimizing cross-core invalidations and using INVLPG-like operations to reduce latency—a key to achieving sub-10 microsecond P99.99 latency in a real-time unikernel. In sum, the strongest, most direct support comes from four-level paging and 48-bit canonical addressing discussions, followed by high-half/kernel layout descriptions, large-page strategies, and TLB shootdown considerations. The confidence increases when multiple sources corroborate each component of the field value, particularly the paging architecture and high-half layout, and remains medium to high for the larger portions that are supported by multiple related excerpts.",
      "confidence": "high"
    },
    {
      "field": "interrupt_and_fault_handling_framework.critical_fault_handling",
      "citations": [
        {
          "title": "os.phil-opp.com/double-fault-exceptions/",
          "url": "https://os.phil-opp.com/double-fault-exceptions/",
          "excerpts": [
            "\nWe create the TSS in a new `gdt` module (the name will make sense later):\n\n```\n// in src/lib.rs\n\npub mod gdt;\n\n// in src/gdt.rs\n\nuse x86_64::VirtAddr;\nuse x86_64::structures::tss::TaskStateSegment;\nuse lazy_static::lazy_static;\n\npub const DOUBLE_FAULT_IST_INDEX: u16 = 0;\n\nlazy_static! {\n    static ref TSS: TaskStateSegment = {\n        let mut tss = TaskStateSegment::new();\n        tss.interrupt_stack_table[DOUBLE_FAULT_IST_INDEX as usize] = {\n            const STACK_SIZE: usize = 4096 * 5;\n            static mut STACK: [u8; STACK_SIZE] = [0; STACK_SIZE];\n\n            let stack_start = VirtAddr::from_ptr(&raw const STACK);\n            let stack_end = stack_start + STACK_SIZE;\n            stack_end\n        };\n        tss\n    };\n}",
            "     unsafe {\n            idt.double_fault.set_handler_fn(double_fault_handler)\n                .set_stack_index(gdt::DOUBLE_FAULT_IST_INDEX);",
            "To load our GDT, we create a new `gdt::init` function that we call from our `init` function:\n\n```\n// in src/gdt.rs\n\npub fn init() {\n    GDT.load();\n}\n\n// in src/lib.rs\n\npub fn init() {\n    gdt::init();\n    interrupts::init_idt();\n}\n\n```",
            "Let’s create a static `GDT` that includes a segment for our `TSS` static:\n\n```\n// in src/gdt.rs\n\nuse x86_64::structures::gdt::{GlobalDescriptorTable, Descriptor};\n\nlazy_static! {\n    static ref GDT: GlobalDescriptorTable = {\n        let mut gdt = GlobalDescriptorTable::new();\n        gdt.add_entry(Descriptor::kernel_code_segment());\n        gdt.add_entry(Descriptor::tss_segment(&TSS));\n        gdt\n    };\n}\n\n```",
            "The Interrupt Stack Table (IST) is part of an old legacy structure called *[Task State Segment](https://en.wikipedia.org/wiki/Task_state_segment)* (TSS). The TSS used to hold various pieces of information (e.g., processor register state) about a task in 32-bit mode and was, for example, used for [hardware context switching](https://wiki.osdev.org/Context_Switching). However, hardware context switching is no longer supported in 64-bit mode and the format of the TSS has changed completely. On x86\\_64, the TSS no longer holds any task-specific information at all. Instead, it holds two stack tables (the IST is one of them)",
            "A double fault is a normal exception with an error code, so we can specify a handler function similar to our breakpoint handler:",
            "A double fault is a normal exception with an error code, so we can specify a handler function similar to our breakpoint handler:",
            "The IST and TSS"
          ]
        },
        {
          "title": "Task State Segment - OSDev Wiki",
          "url": "http://wiki.osdev.org/Task_State_Segment",
          "excerpts": [
            "A Task State Segment (TSS) is a binary data structure specific to the IA-32 and x86-64 architectures. It holds information about a task."
          ]
        },
        {
          "title": "Double Faults | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/double-faults/",
          "excerpts": [
            "The IST and TSS"
          ]
        },
        {
          "title": "Interrupt Descriptor Table",
          "url": "https://wiki.osdev.org/Interrupt_Descriptor_Table",
          "excerpts": [
            "The Interrupt Descriptor Table (IDT) is a binary data structure specific to the IA-32 and x86-64 architectures. It is the Protected Mode and Long Mode ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes configuring the Interrupt Stack Table (IST) to provide dedicated, known-good stacks for critical fault handlers such as Double Fault and Non-Maskable Interrupts, to prevent stack exhaustion from leading to a triple fault. The most directly relevant evidence shows the IST constant (DOUBLE_FAULT_IST_INDEX) set to 0 and the IST being populated with a dedicated stack, demonstrating the explicit use of a separate stack for critical faults. The system initializes a dedicated stack for the IST, and the constant is then used to attach that stack to theDouble Fault handler, which directly supports the goal of switching to a clean stack for reliable fault handling. Additional excerpts describe creating and loading a static TSS and a GDT that includes a TSS segment, establishing the infrastructure that supports IST usage and the correct association of the IST with exception handlers. There is also explicit discussion of the IST as part of the Task State Segment, its historical context, and its current role in providing dedicated stacks for interrupt handling in x86_64, which reinforces the architectural rationale behind using IST for critical fault handling. Supporting material further explains the interrupt descriptor table (IDT) context and references to how double-fault and related interrupt handling would be wired, illustrating the broader ecosystem in which IST-backed fault handling resides. Collectively, these excerpts corroborate the design pattern: reserve a dedicated IST index, allocate a known-good stack, and wire the critical fault handlers (double fault, NMIs) to use that stack to ensure safe, diagnosable recovery. This directly aligns with the described approach for robust, recoverable critical fault handling in a high-determinism RTOS context.",
      "confidence": "high"
    },
    {
      "field": "boot_and_initialization_specification.cpu_mode_transition",
      "citations": [
        {
          "title": "Stack Overflow: How to enter 64-bit mode on a x86-64",
          "url": "https://stackoverflow.com/questions/22962251/how-to-enter-64-bit-mode-on-a-x86-64",
          "excerpts": [
            "mov cr4, eax ; PGE- (Page Global Enable). mov eax, PML4_BASE ; Address of PML4. mov cr3, eax ; Point CR3 to PML4. mov ecx, 0xC0000080 ; EFER MSR selector. rdmsr ; Read from model specific register. or eax, EFER_LME ; Set LME (Long Mode Enable). wrmsr ; Write to model specific register.",
            "mov eax, CR4_PAE | CR4_PGE ; Set PAE- (Physical Address Extensions) and",
            "%xdefine CR0_PE 1 << 0",
            "Entering long mode can be both done from real mode and protected mode, however only protected mode is covered in the Intel and AMD64 manuals."
          ]
        },
        {
          "title": "OSDev Wiki - Setting Up Long Mode",
          "url": "https://wiki.osdev.org/Setting_Up_Long_Mode",
          "excerpts": [
            "Long mode consists of two sub modes which are: the actual 64-bit mode, and compatibility mode (a 32-bit mode, usually referred to as IA32e in the AMD64 manuals ... Setting Up Long Mode"
          ]
        }
      ],
      "reasoning": "The most directly relevant parts describe the concrete sequence of operations to enter long mode: enabling long mode through setting the LME bit in the EFER MSR and enabling paging, along with a 4-level page table setup and loading the base address into CR3. These excerpts explicitly mention enabling the EFER LME bit, configuring CR3 to point to the PML4 (4-level paging), and manipulating CR0/CR4 during the transition. They also reference the use of a far jump and GDT adjustments as part of exiting real mode and entering protected/long mode. The material that discusses 64-bit mode and the prerequisite steps (such as the initial entry into 32-bit protected mode via a GDT and PE bit in CR0) reinforces the multi-phase approach and the necessary context, even if it is less prescriptive about exact sequencing. Together, these excerpts map directly to the described multi-phase boot path: first enabling protections and switching to a compatible mode, then enabling physical addressing and paging, finally transitioning to long mode with a 64-bit code segment and a far jump. The combination of register-level toggles (CR0, CR4, EFER), paging state (CR3, 4-level page table), and control flow (far jumps, GDT updates) aligns closely with the finegrained field value that outlines this exact sequence.",
      "confidence": "medium"
    },
    {
      "field": "technology_and_crate_recommendations.reason_for_choice",
      "citations": [
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:",
            "### Device types\n\n[]()\n\n| Device | Supported |\n| --- | --- |\n| Block | ✅ |\n| Net | ✅ |\n| GPU | ✅ |\n| Input | ✅ |\n| Console | ✅ |\n| Socket | ✅ |\n| Sound | ✅ |\n| RNG | ✅ |\n| ...",
            "### Transports\n\n[]()\n\n| Transport | Supported |  |\n| --- | --- | --- |\n| Legacy MMIO | ✅ | version 1 |\n| MMIO | ✅ | version 2 |\n| PCI | ✅ | Memory-mapped CAM only, e.g. aarch64 or PCIe ",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli",
            "## Examples & Tests\n\n[]()\n\n### [x86\\_64](/rcore-os/virtio-drivers/blob/master/examples/x86_64)\n\n[]()\n\n```\ncd examples/x86_64\nmake qemu\n```\n\n### [aarch64](/rcore-os/virtio-drivers/blob/master/examples/aarch64)\n\n[]()\n\n```\ncd examples/aarch64\nmake qemu\n```\n\n### [RISCV](/rcore-os/virtio-drivers/blob/master/examples/riscv)\n\n[]()\n\n```\ncd examples/riscv\nmake qemu\n```\n\nYou will see device info & GUI Window in qemu.\n[<img src='https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png' width='50%' />](https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png)"
          ]
        },
        {
          "title": "virtio-drivers Documentation",
          "url": "https://docs.rs/virtio-drivers",
          "excerpts": [
            "You must first implement the [`Hal`](trait.Hal.html \"trait virtio\\_drivers::Hal\") trait, to allocate DMA regions and translate between\nphysical addresses (as seen by devices) and virtual addresses (as seen by your program). You can\nthen construct the appropriate transport for the VirtIO device",
            "These drivers can be used by bare-metal code (such as a bootloader or OS kernel) running in a VM to interact with VirtIO devices provided by the VMM.",
            "virtio\\_drivers - Rust\n\n[Docs.rs](/)\n\n* [virtio-drivers-0.11.0](# \"VirtIO guest drivers.\")"
          ]
        },
        {
          "title": "Virtio PCI Card Specification v0.9.4 DRAFT -",
          "url": "https://people.redhat.com/pbonzini/virtio-spec.pdf",
          "excerpts": [
            "Feb 7, 2012 — 2.2.1 Device Initialization Sequence. We start with an overview of device initialization, then expand on the details of the device and how ..."
          ]
        }
      ],
      "reasoning": "The field value centers on selecting the virtio-drivers crate as the optimal choice for implementing VirtIO in a no_std guest OS, including broad device support and the requirement to implement a Hal abstraction for DMA and address translation. The most compelling evidence comes from references that explicitly label virtio-drivers as guest drivers for no_std environments and list its broad device support (Net, Block, Console, etc.) and supported transports (MMIO, PCI). Additional excerpts emphasize the need for a Hal trait to bridge between device access and the kernel's memory/address translation, which aligns with the field's rationale for a no_std RTOS guest architecture. Other excerpts reinforce that virtio-drivers is designed for bare-metal or VM guest contexts and contrast it with host-oriented rust-vmm, which strengthens the argument for virtio-drivers in this scenario. Taken together, the excerpts strongly support the claim that virtio-drivers is the appropriate crate for this project configuration, with a no_std guest RTOS in a VM leveraging VirtIO. The reasoning connects each cited point to the field value by mapping explicit capabilities (no_std support, device types, transports, Hal integration) and architectural fit (guest-side driver needs, VM context) to the stated recommendation.",
      "confidence": "high"
    },
    {
      "field": "system_call_interface_design.abi_definition",
      "citations": [
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A",
          "url": "https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf",
          "excerpts": [
            "cessor saves RFLAGS into R11 and the RIP of the next instruction into RCX; it then gets the\n\nprivilege-level 0 target code segment, instruction pointer, stack segment, and flags as follows:",
            "The SYSCALL and SYSRET instructions are designed for operating systems that use a flat memory model (segmen-\n\ntation is not use",
            "SYSCALL is intended for use by user code running at privilege level 3 to access operating system or executive\n\nprocedures running at privilege level 0. SYSRET is intended for use by privilege level 0 operating system or execu-\n\ntive procedures for fast returns to privilege level 3 user c",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "**Target instruction pointer** — Copies the value in RCX into RI",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            "**Stack segment** — IA32\\_STAR[63:48] +",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "**Target code segment** — Reads a non-NULL selector from IA32\\_STAR[63:48] + ",
            "MASK MSR.\nWhen SYSRET transfers control to 64-bit mode user code using REX.W, the processor gets the privilege level 3\n\ntarget code segment, instruction pointer, stack segment, and flags as follows:"
          ]
        },
        {
          "title": "The AArch64 processor (aka arm64), part 20: The classic calling ...",
          "url": "https://devblogs.microsoft.com/oldnewthing/20220823-00/?p=107041",
          "excerpts": [
            "Integer and pointer parameters are passed in x0 through x7 , and floating point parameters go into v0 through v7 . ... If the return value doesn't ..."
          ]
        },
        {
          "title": "x86-64",
          "url": "https://en.wikipedia.org/wiki/X86-64",
          "excerpts": [
            "x86-64 is a 64-bit extension of the x86 instruction set. It was announced in 1999 and first available in the AMD Opteron family in 2003."
          ]
        },
        {
          "title": "AArch64 Exception Model (Arm documentation)",
          "url": "https://documentation-service.arm.com/static/63a065c41d698c4dc521cb1c",
          "excerpts": [
            "**4\\.1.3 Exception-generating instructions** There are instructions that intentionally cause an exception to be generated and taken. These\n\ninstructions are used to implement system call interfaces to allow less privileged software to\n\nrequest services from more privileged softwa",
            "Exception types** Exceptions are conditions or system events that usually require remedial action or an update of\n\nsystem status by privileged software to ensure smooth functioning of the system. An exception is\n\ntherefore any event that can cause the currently executing program to be suspended."
          ]
        },
        {
          "title": "Measurements of system call performance and overhead",
          "url": "http://arkanis.de/weblog/2017-01-05-measurements-of-system-call-performance-and-overhead/",
          "excerpts": [
            "For me I'll use 10 times slower as rule of thumb for modern CPUs and 25 times for older CPUs.",
            "In case you're wondering about the details:"
          ]
        }
      ],
      "reasoning": "The most relevant excerpts articulate the core mechanics of system call entry/exit and privilege-transition behavior. One excerpt indicates a handoff mechanism where system calls involve transferring control to privileged code using a defined target code segment and a return path that involves RCX and RIP, which is central to an ABI for syscalls and transition between user and kernel modes. Another excerpt explicitly discusses SYSRET transferring control back and mentions RCX as part of the return path, reinforcing the idea of a defined transition from a user-level to a privileged context. A separate excerpt notes that SYSCALL/SYSRET are used in the context of fast transitions between privilege levels, which is closely aligned with an ABI for syscalls. Additional excerpts describe the target instruction pointer and stack segment used during privilege transitions, which are part of the calling conventions and ABI during system calls. While some excerpts discuss high-level concepts like privilege levels and CPUID bits, they still touch on the environment in which a syscall ABI operates and the transition mechanics that an ABI would rely on. Excerpts that discuss the general calling convention and exception model provide contextual background on how system-level interfaces are designed, though they do not specify the exact x86-64 Linux-like register mapping for syscall numbers and arguments as described in the field value. The remaining excerpts touch on performance measurements related to system calls, which are tangential to the ABI definition but still relevant to understanding the broader system call interface characteristics.",
      "confidence": "medium"
    },
    {
      "field": "host_and_qemu_tuning_guide.host_cpu_isolation",
      "citations": [
        {
          "title": "Havelsan's Revenue in 2024",
          "url": "https://www.suse.com/c/cpu-isolation-nohz_full-part-3/",
          "excerpts": [
            "22 — The “nohz_full=” kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.",
            "A [cpu-list](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html) argument is passed to define the set of CPUs to isolate.",
            "Assuming you have 8 CPUs for example and you want to isolate CPUs 4, 5, 6, 7:\n\n`nohz_full=4-7`",
            "No need to pass the “rcu\\_nocbs=” kernel parameter as that is automatically taken care of while passing the “nohz\\_full=” paramete",
            "No need to pass the “rcu_nocbs=” kernel parameter as that is automatically taken care of while passing the “nohz_full=” parameter. The CPU also ...",
            "## IRQs affinity\n\nHardware IRQs (other than the timer and some other specific interrupts) may run on any CPU and disturb your isolated set. The resulting noise may not be just about interrupts stealing CPU time and trashing the CPU cache, IRQs may launch further asynchronous work on the CPU: softirq, timer, workqueue, etc… So it is usually a good idea to affine the IRQs to the CPUs outside the range of nohz\\_full.\nThis affinity can be overriden through the file:\n\n`/proc/irq/$IRQ/smp_affinity`"
          ]
        },
        {
          "title": "Tuning a real-time kernel",
          "url": "https://ubuntu.com/blog/real-time-kernel-tuning",
          "excerpts": [
            "nohz_full=<list of isolated cpus> reduces the number of scheduling-clock interrupts, improving energy efficiency and reducing OS jitter. When ..."
          ]
        }
      ],
      "reasoning": "The field value asserts a host isolation strategy driven by a set of kernel boot parameters and runtime tools. The most directly relevant pieces show that the primary interface for isolation is the nohz_full parameter, which configures full dynticks and isolates a set of CPUs from the scheduler. It also notes that a cpu-list argument defines which CPUs are isolated, which aligns with how isolcpus would be used to remove CPUs from the general scheduler when present in the boot configuration. An explicit example demonstrates the syntax nohz_full=4-7, illustrating how to specify a contiguous range of isolated CPUs. Moreover, the materials mention that rcu_nocbs offloads RCU callbacks from isolated cores, which matches the field's inclusion of RCUs offloaded from isolated CPUs. The references also note that tools like tuned with a realtime profile can automate applying these settings, and tuna can be used for manual affinity adjustments, which aligns with the runtime management aspect of the field value. Additional entries discuss that CPU isolation is described as part of the broader CPU isolation framework and that IRQ affinity considerations are commonly recommended in such contexts. Taken together, these excerpts directly support the existence and usage of isolcpus-like concepts, nohz_full for timer tick isolation, rcu_nocbs for RCU offload, cpu-list-based isolation, and runtime tooling for management, all of which corroborate the described host isolation setup.",
      "confidence": "high"
    },
    {
      "field": "security_and_isolation_strategy.intra_unikernel_isolation",
      "citations": [
        {
          "title": "[PDF] Intra-Unikernel Isolation with Intel Memory Protection Keys",
          "url": "https://www.ssrg.ece.vt.edu/papers/vee20-mpk.pdf",
          "excerpts": [
            "Intra-Unikernel Isolation with Intel Memory Protection Keys"
          ]
        },
        {
          "title": "libhermitMPK | Intra-Unikernel Isolation with Intel Memory Protection ...",
          "url": "https://ssrg-vt.github.io/libhermitMPK/",
          "excerpts": [
            "libhermitMPK is a new design bringing memory isolation inside a unikernel instance while keeping a *single address space*. By leveraging Intel Memory Protection Keys ([MPK](https://lwn.net/Articles/643797)), intra-unikernel isolation can be provided without impacting the lightweightness and performance benefits of unikernels. libhermitMPK provides isolation between trusted and untrusted components: (1) safe from unsafe [Rust](https://www.rust-lang.org) kernel code and (2) kernel from user code."
          ]
        },
        {
          "title": "Unishyper: A Rust-based unikernel enhancing reliability and efficiency of embedded systems",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S138376212400136X",
          "excerpts": [
            "To provide memory isolation support between user and kernel code as well as isolation between different user applications at the thread level, Unishyper designs the Zone mechanism based on Intel MPK [36] to isolate the code into thread gr"
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpt discusses intra-unikernel isolation specifically with Intel Memory Protection Keys, which aligns with the exact mechanism (MPK) and the intra-address-space isolation theme described in the target field value. The second excerpt also centers on MPK within the unikernel context and thus reinforces the same basic mechanism, contributing to the core claim of MPK-driven isolation inside a single address space. The third excerpt references MPK-based isolation mechanisms in related work (Unishyper) that use the same hardware feature to partition memory protections within threads, which supports the overall approach but is slightly less direct than the primary MPK-focused sources. The remaining excerpts discuss related isolation and capability-based approaches (Arm MPU, generic capabilities) that are tangential to the exact MPK-based, per-thread, multi-domain details specified in the field value and thus are less supportive.\n",
      "confidence": "medium"
    },
    {
      "field": "task_control_block_and_lifecycle.tcb_struct_definition",
      "citations": [
        {
          "title": "How long time does a context switch take in Linux (ubuntu ...",
          "url": "https://unix.stackexchange.com/questions/506564/how-long-time-does-a-context-switch-take-in-linux-ubuntu-18-04",
          "excerpts": [
            "Mar 15, 2019 — A context switch takes about 1.2 microseconds, which is about a thousand cycles, and most of that time is taken by the Linux scheduler."
          ]
        },
        {
          "title": "Threads",
          "url": "https://docs.sel4.systems/Tutorials/threads.html",
          "excerpts": [
            "Priorities. The scheduler picks the highest-priority, runnable thread. seL4 provides a priority range of 0-255, where 255 is the maximum priority (encoded in ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field value defines the Task Control Block (TCB) as a Rust struct named Task that contains fields such as state, priority, stack pointer, and a page table root, along with identifiers like id and name, and also mentions managing FPU/SIMD state during context switches. Directly supporting aspects of this field, one excerpt discusses context switching in detail, outlining the precise steps for saving and restoring CPU registers, stack pointers, and address spaces, which are the core mechanics that a TCB would coordinate during a switch between tasks. Another excerpt focuses on scheduling and priorities, noting that the scheduler selects the highest-priority runnable thread and discusses the concept of priority ranges, which aligns with the TCB's priority field and its role in scheduling decisions. Taken together, these excerpts provide evidence for the key responsibilities and data that a Task struct should encapsulate (state, priority, stack pointer, and relationship to the page table/root address space) and illustrate how the scheduler and context switch machinery interact with those fields. The explicit listing of a Task struct with fields for id, name, state, priority, cpu_affinity, kernel_stack_pointer, page_table_root, and FPU/SIMD state would be the concrete instantiation that the finegrained field value describes, while the context-switch and scheduler discussions supply the behavioral and structural context that justify those fields' existence and usage.",
      "confidence": "medium"
    },
    {
      "field": "host_and_qemu_tuning_guide.host_power_and_irq_management",
      "citations": [
        {
          "title": "Tuning a real-time kernel",
          "url": "https://ubuntu.com/blog/real-time-kernel-tuning",
          "excerpts": [
            "nohz_full=<list of isolated cpus> reduces the number of scheduling-clock interrupts, improving energy efficiency and reducing OS jitter. When ..."
          ]
        },
        {
          "title": "Havelsan's Revenue in 2024",
          "url": "https://www.suse.com/c/cpu-isolation-nohz_full-part-3/",
          "excerpts": [
            "22 — The “nohz_full=” kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.",
            "No need to pass the “rcu\\_nocbs=” kernel parameter as that is automatically taken care of while passing the “nohz\\_full=” paramete",
            "A [cpu-list](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html) argument is passed to define the set of CPUs to isolate.",
            "Assuming you have 8 CPUs for example and you want to isolate CPUs 4, 5, 6, 7:\n\n`nohz_full=4-7`",
            "## IRQs affinity\n\nHardware IRQs (other than the timer and some other specific interrupts) may run on any CPU and disturb your isolated set. The resulting noise may not be just about interrupts stealing CPU time and trashing the CPU cache, IRQs may launch further asynchronous work on the CPU: softirq, timer, workqueue, etc… So it is usually a good idea to affine the IRQs to the CPUs outside the range of nohz\\_full.\nThis affinity can be overriden through the file:\n\n`/proc/irq/$IRQ/smp_affinity`",
            "Most of RCU processing is offloaded to the CPUs outside the isolated range. The CPUs set as nohz\\_full run in [NOCB mode](https://lwn.net/Articles/522262/) , which means the RCU callbacks queued on these CPUs are executed from unbound kthreads running on non-isolated CPUs"
          ]
        },
        {
          "title": "Kernel CPU isolation parameters",
          "url": "https://www.kernel.org/doc/html/v4.20/admin-guide/kernel-parameters.html",
          "excerpts": [
            "nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified list of CPUs whose tick will be stopped\n                        whenever possible. The boot CPU will be forced outside\n                        the range to maintain the timekeeping. Any CPUs\n                        in this list will have their RCU callbacks offloaded,\n                        just as if they had also been called out in the\n                        rcu_nocbs= boot parameter.",
            "rcu_nocbs=      [KNL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y, set\n                        the specified list of CPUs to be no-callback CPUs. Invocation of these CPUs' RCU callbacks will be\n                        offloaded to \"rcuox/N\" kthreads created for that\n                        purpose, where \"x\" is \"p\" for RCU-preempt, and\n                        \"s\" for RCU-sched, and \"N\" is the CPU number.",
            "irqaffinity=    [SMP] Set the default irq affinity mask\n                        The argument is a cpu list, as described above."
          ]
        },
        {
          "title": "smp affinity setting in linux",
          "url": "https://stackoverflow.com/questions/27963354/smp-affinity-setting-in-linux",
          "excerpts": [
            "I want to load balance the interrupt (irq 75) on my virtual machine system. It has 64 bit redhat 5.8, kernel 2.6.18. There are 8 CPUs in the virtual machine."
          ]
        },
        {
          "title": "QEMU User Documentation",
          "url": "https://www.qemu.org/docs/master/system/qemu-manpage.html",
          "excerpts": [
            "The QEMU PC System emulator simulates the following peripherals: SMP is supported with a large number of virtual CPUs (upper limit is configuration dependent)."
          ]
        }
      ],
      "reasoning": "The field value centers on preventing non-deterministic behavior from CPU power management and on directing interrupts away from isolated cores to maintain predictable latency. Excerpts that describe reducing scheduling interrupts by using a nohz_full setup show a direct approach to lowering jitter on the host by offloading work from isolated CPUs and reducing tick-related interruptions. This aligns with the goal of a deterministic runtime where cores chosen for virtualization are kept free from disruptive power-management events and context switches triggered by periodic interrupts. Excerpts detailing the nohz_full kernel parameter and the associated guidance on isolating a CPU set, along with notes that IRQs can be affine'd away from the isolated set, provide concrete mechanisms for achieving lower latency variability. Additionally, real-time kernel tuning discussions explicitly state that reducing scheduling-clock interrupts via nohz_full improves responsiveness and jitter characteristics, reinforcing the same direction of host-side control over power and interrupt behavior. Other excerpts that talk about the broader CPU affinity topic, including assigning IRQs to non-isolated CPUs and the existence of a cpu-list for isolation, further support the same design principle: constrain host-managed power-saving and interrupt activity to non-critical cores to preserve determinism for the RTOS guest. The connection is that configuring these host parameters directly informs how to run an ultra-deterministic RTOS guest by minimizing host-induced latency variability due to C-states, P-states, SMT effects, and interrupt routing. The most relevant lines provide explicit parameter names and recommended practices (nohz_full, irq affinity, and related tuning) that map to the field value's guidance for disabling aggressive power-saving and controlling interrupt flow on the host. The less direct but supportive excerpts expand on the same theme by showing practical examples and broader context for CPU isolation and IRQ management in virtualized environments.",
      "confidence": "medium"
    },
    {
      "field": "host_and_qemu_tuning_guide.qemu_cli_configuration",
      "citations": [
        {
          "title": "QEMU User Documentation",
          "url": "https://www.qemu.org/docs/master/system/qemu-manpage.html",
          "excerpts": [
            "The QEMU PC System emulator simulates the following peripherals: SMP is supported with a large number of virtual CPUs (upper limit is configuration dependent)."
          ]
        },
        {
          "title": "Tuning a real-time kernel",
          "url": "https://ubuntu.com/blog/real-time-kernel-tuning",
          "excerpts": [
            "nohz_full=<list of isolated cpus> reduces the number of scheduling-clock interrupts, improving energy efficiency and reducing OS jitter. When ..."
          ]
        },
        {
          "title": "Havelsan's Revenue in 2024",
          "url": "https://www.suse.com/c/cpu-isolation-nohz_full-part-3/",
          "excerpts": [
            "A [cpu-list](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html) argument is passed to define the set of CPUs to isolate.",
            "## IRQs affinity\n\nHardware IRQs (other than the timer and some other specific interrupts) may run on any CPU and disturb your isolated set. The resulting noise may not be just about interrupts stealing CPU time and trashing the CPU cache, IRQs may launch further asynchronous work on the CPU: softirq, timer, workqueue, etc… So it is usually a good idea to affine the IRQs to the CPUs outside the range of nohz\\_full.\nThis affinity can be overriden through the file:\n\n`/proc/irq/$IRQ/smp_affinity`"
          ]
        },
        {
          "title": "Features/VirtioVhostUser",
          "url": "https://wiki.qemu.org/Features/VirtioVhostUser",
          "excerpts": [
            "The virtio-vhost-user device lets guests act as vhost device backends so that virtual network switches and storage appliance VMs can provide virtio devices to other guests.",
            "tdev:\n\n```\n$ qemu -M accel=kvm -cpu host -m 1G \\\n       -object memory-backend-file,id=mem0,mem-path=/var/tmp/foo,size=1G,share=on \\\n       -numa node,memdev=mem0 \\\n       -drive if=virtio,file=test.img,format=raw \\\n       -chardev socket,id=chardev0,path=vhost-user.sock \\\n       -netdev vhost-user,chardev=chardev0,id=netdev0 \\\n       -device virtio-net-pci,netdev=netdev0\n```\n",
            "VM2 sees a regular virtio-net device. VM2's QEMU uses the existing vhost-user feature as if it were talking to a host userspace vhost-user backend. VM1's QEMU tunnels the vhost-user protocol messages from VM1's QEMU to the new virtio-vhost-user device so that guest software in VM1 can act as the vhost-user backend."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes precise QEMU command-line configuration for a tuned host environment, including CPU pinning via taskset, NUMA alignment with -numa, memory-backend using hugepages, sharing with vhost backends, associating memory with a NUMA node, and CPU model configuration to reduce jitter. Extracted excerpts provide directly relevant guidance and concrete examples for these aspects:\n- QEMU command conventions and usage of virtio devices are discussed in the QEMU User Documentation, which establishes the context for how guests interact with virtio devices and what a typical QEMU command line looks like when configuring devices and virtualization features. This is the foundational reference for building a tuned guest environment with virtio devices.\n- Real-time kernel tuning discussions repeatedly emphasize reducing jitter via CPU isolation features such as nohz_full, which informs our approach to pinning vCPUs and minimizing scheduling interrupts in the host to create a deterministic environment for the guest.\n- Additional entries discuss IRQ affinity considerations in Linux to avoid noisy CPUs, which supports the notion of pinning and isolating certain CPUs to reduce interrupt-related latency and jitter, aligning with the goal of a stable host environment for the VM.\n- Virtio Vhost User excerpts provide concrete examples of configuring virtio networking between guests and hosts, including the use of vhost-user backends and the general pattern of virtio device usage within QEMU, which contextualizes the -device virtio-net-pci style configurations referenced by the field value.\n- The multiqueue/virtio-related notes illustrate how virtio devices interact with the guest and host, underpinning the idea that I/O virtualization must be carefully wired into the guest configuration to meet low-latency requirements, which is consistent with a tuned host environment.\n- Concrete snippets show typical command-line usage around virtio devices and related networking configurations, reinforcing practical directions for the CLI to achieve the desired host-guest performance characteristics, such as ensuring compatibility with vhost backends and efficient I/O paths.\nTaken together, these excerpts substantiate the components of the proposed QEMU CLI tuning: CPU pinning rationale and examples, NUMA considerations, memory-backend/hugepages implications, and virtio/vhost-device configurations that are all parts of a tuned host environment for a high-performance VM.",
      "confidence": "medium"
    },
    {
      "field": "task_control_block_and_lifecycle.lifecycle_management",
      "citations": [
        {
          "title": "Threads",
          "url": "https://docs.sel4.systems/Tutorials/threads.html",
          "excerpts": [
            "Priorities. The scheduler picks the highest-priority, runnable thread. seL4 provides a priority range of 0-255, where 255 is the maximum priority (encoded in ..."
          ]
        },
        {
          "title": "Task State Segment - OSDev Wiki",
          "url": "http://wiki.osdev.org/Task_State_Segment",
          "excerpts": [
            "A Task State Segment (TSS) is a binary data structure specific to the IA-32 and x86-64 architectures. It holds information about a task."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a Task lifecycle with explicit states (Initing, Runnable, Blocked, Exited) and transitions driven by both the scheduler and kernel services. One excerpt explicitly ties scheduling to a runnable state by stating that the scheduler selects the highest-priority, runnable thread, and notes that runnable tasks are the only ones considered for execution, implying a state machine where a task becomes runnable before being scheduled. This supports the concept that tasks must enter and be tracked in a Runnable state before being picked by the scheduler, which aligns with the field value's mention of moving a newly created task into Initing, then into Runnable, and later into Blocked or Exited as events occur. Another excerpt references the Task State Segment (TSS), a core OS concept that historically stores per-task state and information about a task. While not a direct contemporaneous implementation detail of the Task struct in the analyzed kernel, it reinforces the general idea that tasks have associated state and context that are managed by architectural mechanisms, matching the notion of a Task Control Block (TCB) holding state, and transitions between task states as described in the field value.",
      "confidence": "medium"
    },
    {
      "field": "host_and_qemu_tuning_guide.host_memory_configuration",
      "citations": [
        {
          "title": "How to set up KVM guests to use HugePages?",
          "url": "https://access.redhat.com/solutions/36741",
          "excerpts": [
            "Mount the HugeTLB filesystem on the host · Increase the memory lock limit on the host · Reserve HugePages and give the KVM group access to them."
          ]
        }
      ],
      "reasoning": "The field value emphasizes a memory configuration approach that minimizes latency by using hugepages, including reserving huge pages at boot, mounting a hugetlbfs filesystem for application access, and ensuring host memory lock limits are increased. The most relevant excerpt explicitly discusses enabling HugePages via a host-side HugeTLB filesystem and increasing the host's memory lock limits, as well as reserving huge pages for KVM usage. This supports the parts of the field value related to hugepages, hugetlbfs mounting, and memory lock considerations. While the field value also mentions preallocating guest memory at startup (mem-prealloc) and prelaunch memory mapping, none of the other excerpts directly address mem-prealloc, so those aspects are not evidenced here. Therefore, the excerpt provides partial support for the memory-configuration components described in the field value, particularly around hugepages and host memory-lock configuration.",
      "confidence": "medium"
    },
    {
      "field": "security_and_isolation_strategy.device_access_control",
      "citations": [
        {
          "title": "[PDF] Intra-Unikernel Isolation with Intel Memory Protection Keys",
          "url": "https://www.ssrg.ece.vt.edu/papers/vee20-mpk.pdf",
          "excerpts": [
            "Intra-Unikernel Isolation with Intel Memory Protection Keys"
          ]
        },
        {
          "title": "libhermitMPK | Intra-Unikernel Isolation with Intel Memory Protection ...",
          "url": "https://ssrg-vt.github.io/libhermitMPK/",
          "excerpts": [
            "libhermitMPK is a new design bringing memory isolation inside a unikernel instance while keeping a *single address space*. By leveraging Intel Memory Protection Keys ([MPK](https://lwn.net/Articles/643797)), intra-unikernel isolation can be provided without impacting the lightweightness and performance benefits of unikernels. libhermitMPK provides isolation between trusted and untrusted components: (1) safe from unsafe [Rust](https://www.rust-lang.org) kernel code and (2) kernel from user code."
          ]
        },
        {
          "title": "Unishyper: A Rust-based unikernel enhancing reliability and efficiency of embedded systems",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S138376212400136X",
          "excerpts": [
            "To provide memory isolation support between user and kernel code as well as isolation between different user applications at the thread level, Unishyper designs the Zone mechanism based on Intel MPK [36] to isolate the code into thread gr"
          ]
        },
        {
          "title": "VirtusCap: Capability-Based Access Control for Unikernels",
          "url": "https://www.researchgate.net/publication/312379553_VirtusCap_Capability-Based_Access_Control_for_Unikernels",
          "excerpts": [
            "VirtusCap limits privileges of unikernels using the Principle of Least Privilege to create unikernels that have only the privileges they need to accomplish their task.",
            "VirtusCap\n\nis an access-control system integrated across the virtualization\n\nlayers. At the hypervisor level, we integrate a new module\n\nfor access-control in Xen Security Modules (XSM) [25] that\n\nuses a capability-based access-control m",
            "his paper makes the following contributions:\n\n(a) We present the design of a novel architecture for\n\ncapability-based access-control with unikernels in virtu-\n\nalized hosts. (b) We design and implement a new prototype capability-\n\nbased access-control system for the XSM. (c) We design and implement a library for MirageOS that\n\nenables capability-based access-control for unik"
          ]
        },
        {
          "title": "Memory Protection Unit",
          "url": "https://developer.arm.com/documentation/107565/latest/Memory-protection/Memory-Protection-Unit",
          "excerpts": [
            "The MPU programmers' model allows privileged software to define memory regions and assign memory access permissions and memory attributes to each of them."
          ]
        },
        {
          "title": "Process isolation with Arm FuSa runtime system",
          "url": "https://community.arm.com/arm-community-blogs/b/tools-software-ides-blog/posts/process-isolation-with-fusa-rts",
          "excerpts": [
            "Feb 18, 2021 — It is used to implement spatial isolation between application processes. In FuSa RTX, an RTOS thread gets assigned to an MPU protected zone ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant material describes isolation mechanisms that align with a zero-trust, least-privilege philosophy. One excerpt details intra-unikernel isolation using Intel Memory Protection Keys to provide isolation within a single address space, which supports restricting access between trusted and untrusted components without broad privileges. This aligns with the idea of enforcing strict boundaries around device access and metadata validation in a guest-driver/context. Another excerpt explains memory protection units as a programmable boundary mechanism, which underpins the concept of enforcing access permissions and memory attributes—paralleling the need to scope DMA access and validate memory regions. A related source discusses zone-based isolation techniques for unikernels that implement least-privilege principals in virtualization layers, offering architecture for constraining capabilities and access rights across components. Additionally, a design proposal on VirtusCap outlines an architecture for capability-based access control across virtualization layers, including a library that enables capability-based restrictions within a unikernel context, directly echoing the least-privilege, verifiable-access theme for device and memory operations. Further, the Unishyper work mentions memory isolation via a zone mechanism leveraging MPK to separate user and kernel code and entreched isolation between applications, reinforcing the idea of bounded privileges and controlled interfaces. Although not all sources discuss IOMMU or virtio-iommu explicitly, the cited materials collectively provide concrete mechanisms for enforcing restricted, verifiable access boundaries and validating interactions between guest drivers and host/back-end components, which are central to a zero-trust approach in a VirtIO-based IO subsystem. The combination of lightweight, fine-grained isolation (MPK-based), capability-based access controls, and explicit isolation guarantees offers a solid foundation for reasoning about device access control in the described RTOS architecture, even though direct VirtIO/IOMMU references are not present in these excerpts.",
      "confidence": "high"
    },
    {
      "field": "project_overview",
      "citations": [
        {
          "title": "Theseus: an Experiment in Operating System Structure and ...",
          "url": "https://www.usenix.org/system/files/osdi20-boos.pdf",
          "excerpts": [
            "by K Boos · 2020 · Cited by 93 — It represents roughly four person-years of effort and comprises ~38000 lines of from-scratch Rust code, 900 lines of bootstrap assembly code ...",
            "246 crates of which 176 are first-party, and 72 unsafe code\n\nblocks or statements across 21 crates, most of which are for\n\nport I/O or special register acces",
            "However, Theseus is far less complete than commercial\n\nsystems, or experimental ones such as Singularity [ 33 ] and\n\nBarrelfish [ 8 ] that have undergone substantially more devel-\n\nopment. For example, Theseus currently lacks POSIX support\n\nand a full standard li",
            "For example, Theseus currently lacks POSIX support\n\nand a full standard library"
          ]
        },
        {
          "title": "Inline assembly - The Rust Reference",
          "url": "https://doc.rust-lang.org/reference/inline-assembly.html",
          "excerpts": [
            "With the global_asm! macro, the assembly code is emitted in a global scope, outside a function. This can be used to hand-write entire functions using assembly ..."
          ]
        },
        {
          "title": "Inline assembly - The Rust Reference",
          "url": "https://rustwiki.org/en/reference/inline-assembly.html",
          "excerpts": [
            "Inline assembly. Support for inline assembly is provided via the asm! and global_asm! macros. It can be used to embed handwritten assembly in the assembly ..."
          ]
        },
        {
          "title": "2873-inline-asm - The Rust RFC Book",
          "url": "https://rust-lang.github.io/rfcs/2873-inline-asm.html",
          "excerpts": [
            "Rust provides support for inline assembly via the asm! macro. It can be used to embed handwritten assembly in the assembly output generated by the compiler."
          ]
        },
        {
          "title": "xstate.rst",
          "url": "https://www.kernel.org/doc/Documentation/x86/xstate.rst",
          "excerpts": [
            "The x86 architecture supports floating-point extensions which are enumerated via CPUID. Applications consult CPUID and use XGETBV to evaluate which features ..."
          ]
        },
        {
          "title": "36. Using XSTATE features in user space applications",
          "url": "https://docs.kernel.org/arch/x86/xstate.html",
          "excerpts": [
            "The x86 architecture supports floating-point extensions which are enumerated via CPUID. Applications consult CPUID and use XGETBV to evaluate which features ..."
          ]
        },
        {
          "title": "RustyHermit and Unikernel Insights for MVP 1.0 Estimation",
          "url": "https://publications.rwth-aachen.de/record/976226/files/976226.pdf",
          "excerpts": [
            "RustyHermit [33][63] is a unikernel developed at RWTH Aachen University. It is\n\nwritten in Rust and is the successor of HermitCore, a C-based unikernel previously\n\ndeveloped at RWTH for high performance computing. The RustyHermit version pre-\n\nsented in the original paper [33] does not use any code written in C/C++ and only\n\ncontains 3.27 % lines of code marked **u",
            "Sung et al. proposed a way to use Intel Memory Protection\n\nKeys (MPK) for memory isolation while keeping the single address space architecture. They implemented it in a Rust based unikernel to separate safe kernel code, unsafe\n\nkernel code and application co",
            "The\n\ndesign of unikernels is a disadvantage here as it has a single address space and no further\n\npermission system within the unikernel. An attacker who compromised the application\n\nautomatically has access to all low level kernel featur",
            "The extended capabilities\n\ncan also be used to attack further systems. That said, a point has to be discussed\n\nwhich came up during RustyHermit analysis. uhyve grants RustyHermit full access to the host OS file system with the permissions\n\nof the user running uhyve. Thus, a malicious or compromised unikernel (applica-\n\ntion) could compromi"
          ]
        },
        {
          "title": "Analyzing Binary Size Bloat in Tock",
          "url": "https://www.tockos.org/blog/2024/tock-binary-size-bloat/",
          "excerpts": [
            "g) is a powerful and secure embedded operating\nsystem. While Tock was designed with resource constraints in mind,\nyears of additional features, generalizing to more platforms, and\nsecurity improvements have brought resource, and in particular, code\nsize bloat. A *lot* of code-size bloat. In 2018, when Tock was first released, a\nminimal kernel required ~7KB for code. Today, the default build for\nmost platforms is easily over 100KB.",
            "We now have a barebones version of Tock that can run processes and\nsupports a minimal set of functionality–just printing to the console.\nLet’s Pretend We Don’t Want Processes"
          ]
        },
        {
          "title": "Implementing a Lock-Free Queue in Rust",
          "url": "https://karevongeijer.com/blog/lock-free-queue-in-rust/",
          "excerpts": [
            "For a new node, we simply wrap the data in a new `MaybeUninit`, and initialize `next` as a null pointer. In the queue, similarly to the pseudocode, we create a dummy node that both `head` and `tail` point to. This dummy node is required for the algorithm (if we had let them both be `null`, we would need to update them both when enqueueing an item, rather than just `dummy.next`), and as its `data` is never read, we safely set it to an uninitialized `MaybeUninit`.",
            "The MS queue implements a linked list with lock-free enqueue and dequeue methods, using the atomic primitive compare-and-swap (CAS) to achieve lock-freedom.",
            "To actually implement the MS queue in Rust, we have to replace references with atomic pointers, integrate hazard pointers for safe memory management (using the [Haphazard](https://docs.rs/haphazard/latest/haphazard/) library), and in general ensure sound use of memory and ownership."
          ]
        },
        {
          "title": "Virtio/Block/Latency",
          "url": "https://www.linux-kvm.org/page/Virtio/Block/Latency",
          "excerpts": [
            "Delta (ns)** column is the time between two layers, e.g. **Guest benchmark** and **Guest virtio-pci** . The delta time tells us how long is being spent in a layer of the virtualization",
            "\n* virtqueue\\_pop 2.560 us is expensive, probably due to vring accesses.",
            "The virtio latency inside QEMU is the time from virtqueue notify until the interrupt is raised. This accounts for time spent in QEMU servicing I/O.",
            "The time spent in QEMU servicing a read request made up 29 us or a 23% overhead compared to a host read request.",
            "The sequential read case is optimized by the presence of a disk read cache. I think this is why the latency numbers are in the microsecond range, not the usual millisecond seek time expected from disks.",
            "The kvm latency is the time from the virtqueue notify pio exit until the interrupt is set inside the guest.",
            "The VIRTIO\\_PCI\\_QUEUE\\_NOTIFY pio has a latency of over 22 us! This is largely due to io\\_submit() taking 16",
            "tio latency inside QEMU is the time from virtqueue notify until the interrupt is raised.",
            "The paio latency is the time spent performing pread()/pwrite() syscalls.",
            "The paio latency is the time spent performing pread()/pwrite() syscalls.",
            "The standard deviation of the iothread return from select(2) is high."
          ]
        },
        {
          "title": "Best way to get high-res low-latency output of KVM? : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/k9x7uc/best_way_to_get_highres_lowlatency_output_of_kvm/",
          "excerpts": [
            "With virtio-net instead of an emulated NIC you get basically latency-free and super-high bandwidth networking between VM and host. Nvidia ...",
            "With virtio-net instead of an emulated NIC you get basically latency-free and super-high bandwidth networking between VM and host. Nvidia-> ..."
          ]
        },
        {
          "title": "62. Poll Mode Driver for Emulated Virtio NIC - Documentation",
          "url": "https://doc.dpdk.org/guides/nics/virtio.html",
          "excerpts": [
            "We provide a virtio Poll Mode Driver (PMD) as a software solution, comparing to SRIOV hardware solution, for fast guest VM to guest VM communication and guest ...",
            "In the Data Plane Development Kit (DPDK), we provide a virtio Poll Mode Driver (PMD) as a software solution, comparing to SRIOV hardware solution, for fast ..."
          ]
        },
        {
          "title": "15. Poll Mode Driver for Emulated Virtio NIC - Documentation",
          "url": "https://doc.dpdk.org/guides-16.04/nics/virtio.html",
          "excerpts": [
            "In this release, the virtio PMD driver provides the basic functionality of packet reception and transmission."
          ]
        },
        {
          "title": "Virtualization Tuning and Optimization Guide",
          "url": "https://docs.redhat.com/de/documentation/red_hat_enterprise_linux/7/html-single/virtualization_tuning_and_optimization_guide/index",
          "excerpts": [
            "Within this guide, you can find tips and suggestions for making full use of KVM performance features and options for your host systems and virtualized guests."
          ]
        },
        {
          "title": "KVM and Big VMs",
          "url": "https://www.linux-kvm.org/images/5/55/2012-forum-Andrew-Theurer-Big-SMP-VMs.pdf",
          "excerpts": [
            "by A Theurer · Cited by 11 — VM might exit when there is no lock holder preemption ... The detection itself could get too costly. – However, implementing the throttled yield_to() reduces exit ..."
          ]
        },
        {
          "title": "KVM network performance: latency vs throughput, do I ...",
          "url": "https://forum.proxmox.com/threads/kvm-network-performance-latency-vs-throughput-do-i-have-to-decide.5814/",
          "excerpts": [
            "Feb 24, 2011 — I see no difference between virtio and e1000, in both cases latency is about 0.280msec and iperf measures about 670Mbit/s."
          ]
        },
        {
          "title": "Strange (scheduling?) latency on the host when KVM guest ...",
          "url": "https://www.reddit.com/r/AsahiLinux/comments/134g00u/strange_scheduling_latency_on_the_host_when_kvm/",
          "excerpts": [
            "I've been exeperiencing perfomance issues while running tasks in qemu/kvm. The tasks inside the guest run just fine, very speedy."
          ]
        },
        {
          "title": "Cellular Mechanisms and Regulation of Quiescence - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7665062/",
          "excerpts": [
            "by O Marescal · 2020 · Cited by 269 — Quiescence is a state of reversible proliferative arrest in which cells are not actively dividing, and yet retain the capacity to reenter the cell cycle."
          ]
        },
        {
          "title": "Suspend IRQs during application busy periods - LWN.net",
          "url": "https://lwn.net/Articles/997491/",
          "excerpts": [
            "~ The short explanation (TL;DR) We propose adding a new napi config ... napi_defer_hard_irqs and gro_flush_timeout mechanism which was ..."
          ]
        },
        {
          "title": "Improve network latency for Linux based EC2 instances",
          "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ena-improve-network-latency-linux.html",
          "excerpts": [
            "Enable busy poll mode – Busy poll mode reduces latency on the network receive path. When you enable busy poll mode, the socket layer code can directly poll the ..."
          ]
        },
        {
          "title": "Save power with OVS-DPDK PMD thread load-based ...",
          "url": "https://developers.redhat.com/articles/2023/10/16/save-power-ovs-dpdk-pmd-thread-load-based-sleeping",
          "excerpts": [
            "Oct 16, 2023 — NAPI is a well-known interrupt/polling hybrid that requires all of the DPDK NICs drivers to support interrupt mode. It also requires different ..."
          ]
        },
        {
          "title": "[net] net: solve a NAPI race - Patchwork - OzLabs",
          "url": "https://patchwork.ozlabs.org/comment/1596198/",
          "excerpts": [
            "While napi_complete_done() is running, the device's IRQ is still disabled, so there cannot be an IRQ firing before napi_complete_done() is finished. Eric ..."
          ]
        },
        {
          "title": "remove redundant logic in napi callback for busy poll mode",
          "url": "https://lkml.iu.edu/hypermail/linux/kernel/1612.0/02122.html",
          "excerpts": [
            "Dec 5, 2016 — Unmask the interrupt only from irq context. > > A schenario where ... > 1)Before napi completes and call napi_complete_done > 2)After calling ..."
          ]
        },
        {
          "title": "Micro-Optimizing KVM VM-Exits",
          "url": "https://www.qemu.org/2019/11/15/micro-optimizing-kvm-vmexits/",
          "excerpts": [
            "Nov 15, 2019 — KVM (Kernel-based Virtual Machine) is the Linux kernel module that allows a host to run virtualized guests (Linux, Windows, etc)."
          ]
        },
        {
          "title": "[RFC net-next 0/5] Suspend IRQs during preferred busy poll - Patchew",
          "url": "https://patchew.org/linux/20240812125717.413108-1-jdamato@fastly.com/",
          "excerpts": [
            "- suspendX: > - set defer_hard_irqs to 100 > - set gro_flush_timeout to X,000 > - set irq_suspend_timeout to 20,000,000 > - enable busy poll via ..."
          ]
        },
        {
          "title": "Implementation and experimental performance evaluation ...",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S0140366408005252",
          "excerpts": [
            "by K Salah · 2009 · Cited by 56 — The hybrid scheme switches between interrupt disabling–enabling (DE) and polling (NAPI). In this paper, we present and discuss major changes ..."
          ]
        },
        {
          "title": "[PDF] the man-pages book - The Linux Kernel Archives",
          "url": "https://www.kernel.org/pub/linux/docs/man-pages/book/man-pages-6.9.pdf",
          "excerpts": [
            "... best effort to make this call as fast as possible. (On some ... busy_poll_budget; /* Max packets per poll */ uint8_t prefer_busy_poll ..."
          ]
        },
        {
          "title": "devices/src/virtio/net.rs - platform/external/crosvm - Git at Google",
          "url": "https://android.googlesource.com/platform/external/crosvm/+/9cfdbd9c/devices/src/virtio/net.rs",
          "excerpts": [
            "// would ensure that any changes in the creation procedure are matched in the validation. // Plus we still need to set the offload and vnet_hdr_size values ..."
          ]
        },
        {
          "title": "Frustration with VMExit and QEMU rebuilds : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/1lnqcbv/frustration_with_vmexit_and_qemu_rebuilds/",
          "excerpts": [
            "I'm using pafish to test things and I get errors with system bios reverting back to bocsh every boot and drive saying QEMU HARDDISK."
          ]
        },
        {
          "title": "virtio-fs / vm-virtio",
          "url": "https://gitlab.com/virtio-fs/vm-virtio",
          "excerpts": [
            "Nov 30, 2021 — The vm-virtio crate provides a virtio device implementation for the following: Note: We offer support only for virtio v1.0+ (the legacy interface is not ..."
          ]
        },
        {
          "title": "Virtqueue in virtio::queues - Rust",
          "url": "https://docs.tockos.org/virtio/queues/trait.virtqueue",
          "excerpts": [
            "Each queue instance provides a bidirectional communication channel to send data from the guest (VirtIO driver) to the host (VirtIO device) and back. Typically, ..."
          ]
        },
        {
          "title": "TLB Shootdown Latency in Linux",
          "url": "https://sanidhya.github.io/pubs/2018/latr.pdf",
          "excerpts": [
            "A synchronous\n\nTLB-shootdown mechanism increases the latency by several\n\nmicro-seconds for certain virtual address operations, which\n\nis known to a culprit that contributes to the tail latency of\n\nsome critical services in data centers ",
            "IPIs are the mechanism used in x86 to communicate with different cores. does not scale beyond six cores with the current TLB shoot-\n\ndown mechanism in Linux (see Figure 1",
            "a typ-\n\nical Apache workload serving small static web pages or fi",
            "takes up to 6.6 _µ_ s for 120 cores with 8 sockets\n\nand 2.7 _µ_ s for 16 cores with 2 sockets",
            "Latr on the other hand is able to\n\nefficiently use the cache coherence protocol to complete the\n\nmunmap() operation in less than 40 _µ_ s on 120 cores, reducing\n\nthe latency by 66.7% compared to Linux , as Latr ’s munmap()\n\ndoes not rely on expensive IPIs and eliminates the ACK wait\n\ntime on the initia",
            "To solve this problem, there have been two broad cate-\n\ngories of research, namely, hardware- and software-based ap-\n\nproaches.\nHardware-based approaches strive to provide TLB\n\ncache coherence in hardware (see §2.2), but require expensive\n\nhardware modifications and introduce additional verifica-\n\ntion challenges to t",
            "tes_ ( 1 ) to identify the cores\n\nincluded in a TLB shootdown. The states are made accessible to\n\nremote cores via the cache coherence protocol ( 2 ) that remote\n\ncores use to clear entries in their local TLB. the ",
            "tickless ( CONFIG\\_NO\\_HZ ) configuration disables the scheduler\n\nticks on idle cores. In such tickless kernels, the idle core’s\n\nid is not set in a Latr state’s CPU bitmask because none of\n\nthe processes are scheduled on the idle c"
          ]
        },
        {
          "title": "realtime:documentation:howto:tools:cyclictest:start ...",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/howto/tools/cyclictest/start",
          "excerpts": [
            "Jan 19, 2025 — Cyclictest accurately and repeatedly measures the difference between a thread's intended wake-up time and the time at which it actually wakes up."
          ]
        },
        {
          "title": "Demystifying the Real-Time Linux Scheduling Latency",
          "url": "https://bristot.me/demystifying-the-real-time-linux-latency/",
          "excerpts": [
            "Cyclictest is used in practice to measure the scheduling latency of each CPU of the system. Maximum observed latency values generally range from a few ..."
          ]
        },
        {
          "title": "The x86 kvm shadow mmu",
          "url": "https://docs.kernel.org/virt/kvm/x86/mmu.html",
          "excerpts": [
            "Supported page sizes include 4k, 2M, 4M, and 1G. 4M pages are treated as two separate 2M pages, on both guest and host, since the mmu always uses PAE paging.See more"
          ]
        },
        {
          "title": "(PDF) Performance Implications of Extended Page Tables ...",
          "url": "https://www.researchgate.net/publication/319660274_Performance_Implications_of_Extended_Page_Tables_on_Virtualized_x86_Processors",
          "excerpts": [
            "Aug 7, 2025 — Processing TLB misses on a virtualized x86 server requires a twodimensional page walk that can have 6x more page table lookups, hence 6x more ...See more"
          ]
        },
        {
          "title": "Efficient Memory Tiering in a Virtual Machine",
          "url": "https://arxiv.org/html/2506.06067v1",
          "excerpts": [
            "However, dTLB load misses increase by 3% mainly due to TLB shootdowns associated with consolidation and migration as we start from DRAM and ..."
          ]
        },
        {
          "title": "1641702 – check tsc scaling fea-ture of destination host on migration",
          "url": "https://bugzilla.redhat.com/show_bug.cgi?id=1641702",
          "excerpts": [
            "For migration of guests with invariant TSC to work, it is necessary that the destination host: 1) Has the same TSC frequency as the source host. or 2) Supports ..."
          ]
        },
        {
          "title": "Linux SCHED_FIFO not respecting thread priorities",
          "url": "https://stackoverflow.com/questions/63674522/linux-sched-fifo-not-respecting-thread-priorities",
          "excerpts": [
            "A SCHED_FIFO thread runs until either it is blocked by an I/O request, it is preempted by a higher priority thread, or it calls sched_yield(2)."
          ]
        },
        {
          "title": "Chapter 5. Priorities and Policies | Reference Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/reference_guide/chap-priorities_and_policies",
          "excerpts": [
            "A SCHED_FIFO thread with a priority of 1 will always be scheduled ahead of any SCHED_OTHER thread. The SCHED_RR policy is very similar to the SCHED_FIFO policy."
          ]
        },
        {
          "title": "Pin QEMU KVM cores to host CPU cores without libvirt : r/ ...",
          "url": "https://www.reddit.com/r/VFIO/comments/7avvwx/qemuaffinity_pin_qemu_kvm_cores_to_host_cpu_cores/",
          "excerpts": [
            "A python script to easily pin select QEMU threads to host CPU cores. I use this to pin the KVM and IO threads for two qemu instances to specific isolated cores."
          ]
        },
        {
          "title": "Let's talk about resources isolation | Proxmox Support Forum",
          "url": "https://forum.proxmox.com/threads/hey-proxmox-community-lets-talk-about-resources-isolation.124256/",
          "excerpts": [
            "In practice such scheduling mistakes can cause latency spikes measured in hundreds of milliseconds. Heterogenous CPUs: the simplest example ..."
          ]
        },
        {
          "title": "CPU pinning? | Page 2",
          "url": "https://forum.proxmox.com/threads/cpu-pinning.67805/page-2",
          "excerpts": [
            "I'm looking to pin the cpu's and ram in the VM to a single numa node, and let KVM handle the core scheduling and choosing."
          ]
        },
        {
          "title": "details about page table walks in multi-level paging",
          "url": "https://stackoverflow.com/questions/59641957/details-about-page-table-walks-in-multi-level-paging",
          "excerpts": [
            "Grouping allocations into the same 4M region (or on x86-64, 2M or 1G) as existing allocations avoids needing a new 4k page of PTEs, instead just ...See more"
          ]
        },
        {
          "title": "Parallel computing and the cost of TLB shoot-down",
          "url": "https://lifecs.likai.org/2010/06/parallel-computing-and-cost-of-tlb.html",
          "excerpts": [
            "Parallel computing and the cost of TLB shoot-down. Translation ... The cost of TLB shoot-down increases as the number of CPUs increase."
          ]
        },
        {
          "title": "P99 CONF Day 1: The Low-Latency Lowdown - ScyllaDB",
          "url": "https://www.scylladb.com/2022/10/19/p99-conf-day-1-the-low-latency-lowdown/",
          "excerpts": [
            "Day 1 of P99 CONF kicked off with “misery metrics” and ended with a look at P99 latency reduction as a Sisyphean task."
          ]
        },
        {
          "title": "Performance issues with win10 guest in qemu kvm",
          "url": "https://www.reddit.com/r/archlinux/comments/q3erbh/performance_issues_with_win10_guest_in_qemu_kvm/",
          "excerpts": [
            "Hello, I have a need to deploy a rather \"snappy\" win10 guest inside qemu kvm.. Reason being I hate all the microsoft trash but because of ..."
          ]
        },
        {
          "title": "Monotonic clock with KVM pv-clock",
          "url": "https://kvm.vger.kernel.narkive.com/7zaHFi1e/monotonic-clock-with-pv-clock",
          "excerpts": [
            "Hi, I'm trying to figure out how a guest OS can get a monotonic clock using KVM's paravirtual clock. At first, I thought that the clock I get using ..."
          ]
        },
        {
          "title": "VM Stopped Working, Marking Clocksource \"TSC\" as ...",
          "url": "https://forums.unraid.net/topic/58827-vm-stopped-working-marking-clocksource-tsc-as-unstable-because-skew-is-too-large/",
          "excerpts": [
            "Jul 18, 2017 — Everytime I start 2 games on my virtual machines the complete system freeze and hangs with the following error, i must restart the entire system to clear the ..."
          ]
        },
        {
          "title": "High latency in Linux KVM virtual machines",
          "url": "https://forum.proxmox.com/threads/high-latency-in-linux-kvm-virtual-machines.12685/",
          "excerpts": [
            "I am experiencing high latency and weird ping times while on the other 5 vms there is no problem. The VM's are running latest updates of debian squeeze."
          ]
        },
        {
          "title": "A new option for decreasing guest latency (cpu-pm=on)",
          "url": "https://www.reddit.com/r/VFIO/comments/cmgmt0/a_new_option_for_decreasing_guest_latency_cpupmon/",
          "excerpts": [
            "cpu-pm=on allows the guest to put the cpu to sleep without involving the host. If you use cpu isolation this is what you want. Unfortunately there is a ..."
          ]
        },
        {
          "title": "Low Latency Storage Optimizations for Proxmox, KVM, & QEMU",
          "url": "https://kb.blockbridge.com/technote/proxmox-tuning-low-latency-storage/",
          "excerpts": [
            " Mitigations",
            " Mitigations",
            " Mitigations"
          ]
        },
        {
          "title": "Cpu pinning strategy for kvm / centos7 - Server Fault",
          "url": "https://serverfault.com/questions/871053/cpu-pinning-strategy-for-kvm-centos7",
          "excerpts": [
            "Relatively simple is the isolcpus boot parameter which allows you to isolate one or more CPUs from the scheduler. This prevents the scheduler ..."
          ]
        },
        {
          "title": "CPU pinning, CPU isolation, hooks? : r/VFIO - Reddit",
          "url": "https://www.reddit.com/r/VFIO/comments/ob7m3d/cpu_pinning_cpu_isolation_hooks/",
          "excerpts": [
            "These are best achieved with kernel boot flags isolcpus=xxx rcu_nocbs=xxx and optional other flags like nohz_full=xxx to prevent those threads ..."
          ]
        },
        {
          "title": "Linux kernel 6.12 RT / Kernel & Hardware ...",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=302317",
          "excerpts": [
            "Jan 3, 2025 — The compile option PREEMPT_RT is not enabled in the standard 6.12 kernel, only PREEMP_DYNAMIC, and that changes a lot the latency, which leads ..."
          ]
        },
        {
          "title": "Torwards a more Scalable KVM Hypervisor",
          "url": "https://kvm-forum.qemu.org/2018/Torwards%20a%20more%20Scalable%20KVM%20Hypervisor.pdf",
          "excerpts": [
            "by W Li · Cited by 3 — Modern operating systems consider TLB shootdown operations to be performance critical and so optimize them to exhibit very low latency. The implementation of ..."
          ]
        },
        {
          "title": "[PDF] Agile Paging: Exceeding the Best of Nested and Shadow Paging",
          "url": "https://research.cs.wisc.edu/multifacet/papers/isca16_agile_paging.pdf",
          "excerpts": [
            "Nested paging directly performs a two-level page walk that makes TLB misses slower than unvirtualized native, but enables fast page tables ..."
          ]
        },
        {
          "title": "Transparent Huge Pages vs. Huge Pages with KVM / ovirt",
          "url": "https://www.reddit.com/r/ovirt/comments/he9bx6/transparent_huge_pages_vs_huge_pages_with_kvm/",
          "excerpts": [
            "1G HP has 2 benefits over THP: 1) no daemon running that dynamically moves memory around, 2) even fewer cycles spent looking up page tables than 2MB HPs."
          ]
        },
        {
          "title": "Insane DPC latency during disk or network IO : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/8c1xr2/insane_dpc_latency_during_disk_or_network_io/",
          "excerpts": [
            "Easily spikes between 40,000 to 70,000 microseconds, and I've recorded interrupts to process latency as high as 500,000 microseconds."
          ]
        },
        {
          "title": "[PATCH v6] virtio-net: Fix and update ... - OASIS",
          "url": "https://groups.oasis-open.org/communities/community-home/digestviewer/viewthread?GroupId=3973&MessageKey=f91de9e4-9796-4121-af9b-b9f1b41495cb&CommunityKey=2f26be99-3aa1-48f6-93a5-018dce262226",
          "excerpts": [
            "This patch makes several improvements to the notification coalescing feature, including: - Consolidating virtio_net_ctrl_coal_tx and virtio_net_ctrl_coal_rxSee more"
          ]
        },
        {
          "title": "MDPI Electronics (2024) - Sub-10µs P99.99 Latency in VirtIO-based QEMU/KVM guests: Risks and Mitigations",
          "url": "https://www.mdpi.com/2079-9292/13/9/1626",
          "excerpts": [
            "Latencies with hypervisors and AMP frameworks: On the one hand, a hypervisor increases the OS’s kernel latency—especially interrupt latency—and the network latency.",
            "Interrupt latency: After identifying latencies in OSs, MPSoCs, hypervisors, and AMP frameworks, the interrupt latency comes up as one of the main latencies in embedded systems."
          ]
        },
        {
          "title": "Exploring Rust for Unikernel Development",
          "url": "https://plos-workshop.org/2019/preprint/plos19-lankes.pdf",
          "excerpts": [
            "by S Lankes · 2019 · Cited by 43 — To enable processor features such as AVX, direct access to the control registers is required. To program these registers, RustyHermit reuses the x86 [27] ..."
          ]
        },
        {
          "title": "Assessing Unikernel Security",
          "url": "https://people.cs.pitt.edu/~babay/courses/cs3551/projects/SCADA_Unikernel/NCC_Group-Assessing_Unikernel_Security.pdf",
          "excerpts": [
            "by S Michaels · Cited by 11 — Proponents of unikernels claim that their smaller codebase and lack of excess services make them more efficient and secure than full-OS virtual ..."
          ]
        },
        {
          "title": "Unikraft: Fast, Specialized Unikernels the Easy Way",
          "url": "https://arxiv.org/pdf/2104.12721",
          "excerpts": [
            "by S Kuenzer · 2021 · Cited by 153 — Abstract. Unikernels are famous for providing excellent performance in terms of boot times, throughput and memory consump-."
          ]
        },
        {
          "title": "Introduction - The Rust Reference",
          "url": "https://doc.rust-lang.org/nightly/reference/index.html?search=used",
          "excerpts": [
            "Linkage » Linkageused (e.g. --gc-sections or -dead_strip for macOS). [link ... Note If you are using rustc to link, these rules are enforced automatically."
          ]
        },
        {
          "title": "Isolation in Rust: What is Missing?",
          "url": "https://users.cs.utah.edu/~aburtsev/doc/rust-plos21.pdf",
          "excerpts": [
            "by A Burtsev · 2021 · Cited by 7 — Hardware or language-based isolation and SFI systems require a copy of all objects passed across isolation boundary to enforce the property that ..."
          ]
        },
        {
          "title": "Isolation in Rust: What is Missing? - Vikram",
          "url": "https://arkivm.github.io/publications/2021-plos-rust-isolation.pdf",
          "excerpts": [
            "by A Burtsev · 2021 · Cited by 7 — Fault isolation and recovery in RedLeaf. In this paper, we analyze recent systems that explore software-based isolation in Rust [7, 12, 34, 39 ..."
          ]
        },
        {
          "title": "Why does Rust compile every crate that I include in my ...",
          "url": "https://www.reddit.com/r/rust/comments/1ehgi8l/why_does_rust_compile_every_crate_that_i_include/",
          "excerpts": [
            "The reason Rust primarily uses static linking is because it makes a lot of other things much easier. Deploying a Rust project means you just ..."
          ]
        },
        {
          "title": "On Memory Safety and Undefined Behavior of Unikernels in ...",
          "url": "http://publications.rwth-aachen.de/record/973239/files/973239.pdf?version=1",
          "excerpts": [
            "Nonetheless, the team has proven Rust's principle of encapsulating interior muta- bility by using unsafe internally to be sound, while finding some specific ..."
          ]
        },
        {
          "title": "Use of `panic!` in `no_std` - embedded",
          "url": "https://users.rust-lang.org/t/use-of-panic-in-no-std/115582",
          "excerpts": [
            "Aug 6, 2024 — The only macro provided for panicking is panic!() which is only taking a string payload, unlike in std which provides panic_any! capable of taking arbitrary ..."
          ]
        },
        {
          "title": "no_std issues with implementing panic_handler #3498",
          "url": "https://github.com/rust-lang/miri/issues/3498",
          "excerpts": [
            "Apr 22, 2024 — I don't quite understand why abort and halt wouldn't work for no_std environments though considering these panic handlers were made for this express purpose."
          ]
        },
        {
          "title": "Unikernels are secure. Here is why.",
          "url": "http://unikernel.org/blog/2017/unikernels-are-secure",
          "excerpts": [
            "I believe that from a security perspective unikernels can offer a level of security that is unprecedented in mainstream computing."
          ]
        },
        {
          "title": "TrustZone for Cortex-M - Arm Developer",
          "url": "https://developer.arm.com/ip-products/security-ip/trustzone/trustzone-for-cortex-m",
          "excerpts": [
            "Both Secure and Non-secure state have their own Memory Protection Unit (MPU) to provide software isolation between unprivileged software components. Banked ..."
          ]
        },
        {
          "title": "Isolation in Rust: What is Missing?",
          "url": "https://par.nsf.gov/servlets/purl/10386701",
          "excerpts": [
            "by A Burtsev · 2021 · Cited by 7 — To provide clean fault isolation semantics and subsystem termination, RedLeaf adapts the idea that isolated subsys- tems should not share state on the heap ( ..."
          ]
        },
        {
          "title": "RISC-V Memory Protection: Diving Deep into the Complexities",
          "url": "https://medium.com/@talktokarthikbk/risc-v-memory-protection-diving-deep-into-the-complexities-9d751212be6b",
          "excerpts": [
            "Missing: guest practices"
          ]
        },
        {
          "title": "Hardening virtio for emerging security use cases",
          "url": "https://www.redhat.com/en/blog/hardening-virtio-emerging-security-usecases",
          "excerpts": [
            "\n\nFrom the device implementation perspective, the security boundary is the boundary **of itself** . That is, the virtio device (green block) doesn’t trust any entity that talks to it via the transport layer (yellow blocks). This means the virtio device needs to validate any request that crosses its security boundary.",
            "** — the virtio driver won’t trust the virtio device, so it can’t assume it behaves exactly as what virtio specification required. In the case of a virtualization environment, the hypervisor/VMM is not trusted as well. This means that any metadata stored in the shared memory that can be accessed by the virtio device must not be trusted.",
            "ample, the driver should store the buffer metadata in private memory that can not be accessed by the virtio device instead of depending on the data stored in the virtqueue. The virtio driver should also perform the necessary checks on the buffer metadata produced by the virtio device and drop the illegal metadata.",
            "The virtio driver should not trust any value that is read from the virtio device. For example, malicious virtio devices may try to crash or poke the virtio driver's private memory by crafting some illegal config spaces. The virtio driver should be able to sanitize those configs or simply fail the driver probing.",
            "The virtio driver should be prepared for the unexpected notification raised by the malicious virtio device at any time.",
            "he virtio driver trusts the platform devices. For example, the virtio driver trusts the platform IOMMU device to perform the necessary checks for limiting the memory that a virtio device can acce",
            "In those cases, the guest memory is encrypted and hardware-based technologies can prevent the hypervisor from accessing arbitrary guest memory.",
            "The hypervisor itself is not trusted as well as the virtualized virtio device. A lot of hardening work on top of the virtio driver is required by the guest, so the above threat model should be used with the one defined by vendor-specific confidential computing technology (for example SEV-SNP or TDX).",
            "Most of the popular virtio backends (such as vhost and Qemu) have been hardened to prevent these kinds of possible attacks from both the guest and other virtual devices.",
            "By supporting virtio/vDPA devices in their NIC, vendors can build on this work to help protect new hardware they develop from security risks. Instead of creating vendor-specific and closed source implementations of devices and drivers that are more prone to vulnerabilities, approaches such as virtio/vDPA can save time and help vendors focus on their “secret sauce” instead.",
            "### Virtio hardening and vendor-specific hardening",
            "We believe driver hardening will become a common practice in the future as the threat models evolve. This means the hardening needs to be done not only for the virtio driver but also for other vendor-specific drivers.",
            "### Summary",
            "In this article we have reviewed two threat models virtio is required to deal with:"
          ]
        },
        {
          "title": "How to generate statically linked executables?",
          "url": "https://stackoverflow.com/questions/31770604/how-to-generate-statically-linked-executables",
          "excerpts": [
            "Rust statically links everything except glibc (and libgcc) by default.",
            "If you want a 100% statically linked binary, you can use MUSL libc, which supports both"
          ]
        },
        {
          "title": "Unikernel Security Research Overview",
          "url": "https://x41-dsec.de/news/wrapping-up-unikernel-security-research/",
          "excerpts": [
            "The single address space is also an inherent property of unikernels and should be kept in mind when discussing unikernel security.",
            "One of the main motivations for this thesis is to contribute an analysis of a unikernel written in a memory safe language. Thus, an in-depth analysis of RustyHermit was conducted, the results were presented in previous blogposts. Writing a unikernel in Rust has the benefit that memory corruption vulnerabilities can only occur in `unsafe` code parts.",
            "Another major security claim is the isolation introduced by using one unikernel for exactly one application and isolating the unikernels from the host system and each other by the hypervisor.",
            "However, this does not provide protection against initially compromising a unikernel. Thus, it can not be a replacement for classical security mitigations.",
            "The most important conclusion is that writing a secure unikernel in terms of “no vulnerabilities” is not enough."
          ]
        },
        {
          "title": "[PDF] Intra-Unikernel Isolation with Intel Memory Protection Keys",
          "url": "https://www.ssrg.ece.vt.edu/papers/vee20-mpk.pdf",
          "excerpts": [
            "\nUnikernels are minimal, single-purpose virtual machines. This new operating system model promises numerous bene-\nfits within many application domains in terms of lightweight-\nness, performance, and security. Although the isolation be-\ntween unikernels is generally recognized as strong, there\nis no isolation within a unikernel itself. This is due to the\nuse of a single, unprotected address space, a basic principle\nof unikernels that provide their lightweightness and perfor-\nmance benefits. In this paper, we propose a new design that\nbrings memory isolation inside a unikernel instance while\nkeeping a single address space. We leverage Intel’s Memory\nProtection Key to do so without impacting the lightweight-\nness and performance benefits of unikernels. We implement\nour isolation scheme within an existing unikernel written\nin Rust and use it to provide isolation between trusted and\nuntrusted components: we isolate (1) safe kernel code from\nunsafe kernel code and (2) kernel code from user code. Eval-\nuation shows that our system provides such isolation with\nvery low performance overhead."
          ]
        },
        {
          "title": "Virtio devices and drivers overview: Who is who",
          "url": "https://www.redhat.com/en/blog/virtio-devices-and-drivers-overview-headjack-and-phone",
          "excerpts": [
            "Jun 11, 2020 — This section provides a brief overview of the virtio devices, virtio drivers, examples of the different architectures you can use and the different components."
          ]
        },
        {
          "title": "How to use \"--gc-sections\" link option to reduce target binary size?",
          "url": "https://users.rust-lang.org/t/how-to-use-gc-sections-link-option-to-reduce-target-binary-size/71374",
          "excerpts": [
            "\"-Clink-arg=-Wl,--gc-sections\". in config.toml [build] rustflags ... Rust equivalent to __attribute__((section(NAME), used))?. 2, 1108 ..."
          ]
        },
        {
          "title": "Rust staticlibs and optimizing for size",
          "url": "https://internals.rust-lang.org/t/rust-staticlibs-and-optimizing-for-size/5746",
          "excerpts": [
            "Aug 10, 2017 — Gecko has opened a bug to disable LTO for builds by default but have pointed out that this comes at a great cost to binary size!"
          ]
        },
        {
          "title": "[PDF] The Case for Intra-Unikernel Isolation",
          "url": "https://www.ssrg.ece.vt.edu/papers/spma20.pdf",
          "excerpts": [
            "In the rest of this paper, we make the case for intra-unikernel isolation in Section 2. Next, we discuss the use of Intel MPK as well as hardware capabilities ..."
          ]
        },
        {
          "title": "[PDF] Software Abstraction for Intel Memory Protection Keys (Intel MPK)",
          "url": "https://www.usenix.org/system/files/atc19-park-soyeon.pdf",
          "excerpts": [
            "libmpk consists of a userspace library mainly for efficient permission change and a kernel module mainly for synchronization and metadata."
          ]
        },
        {
          "title": "libhermitMPK | Intra-Unikernel Isolation with Intel Memory Protection ...",
          "url": "https://ssrg-vt.github.io/libhermitMPK/",
          "excerpts": [
            "libhermitMPK is a new design bringing memory isolation inside a unikernel instance while keeping a *single address space*. By leveraging Intel Memory Protection Keys ([MPK](https://lwn.net/Articles/643797)), intra-unikernel isolation can be provided without impacting the lightweightness and performance benefits of unikernels. libhermitMPK provides isolation between trusted and untrusted components: (1) safe from unsafe [Rust](https://www.rust-lang.org) kernel code and (2) kernel from user code."
          ]
        },
        {
          "title": "Memory Protection Unit",
          "url": "https://developer.arm.com/documentation/107565/latest/Memory-protection/Memory-Protection-Unit",
          "excerpts": [
            "The MPU programmers' model allows privileged software to define memory regions and assign memory access permissions and memory attributes to each of them."
          ]
        },
        {
          "title": "Process isolation with Arm FuSa runtime system",
          "url": "https://community.arm.com/arm-community-blogs/b/tools-software-ides-blog/posts/process-isolation-with-fusa-rts",
          "excerpts": [
            "Feb 18, 2021 — It is used to implement spatial isolation between application processes. In FuSa RTX, an RTOS thread gets assigned to an MPU protected zone ..."
          ]
        },
        {
          "title": "VirtusCap: Capability-Based Access Control for Unikernels",
          "url": "https://www.researchgate.net/publication/312379553_VirtusCap_Capability-Based_Access_Control_for_Unikernels",
          "excerpts": [
            "VirtusCap enables multi-layer MAC with capabilities for\n\nunikernels",
            "VirtusCap limits privileges of unikernels using the Principle of Least Privilege to create unikernels that have only the privileges they need to accomplish their task.",
            "VirtusCap\n\nis an access-control system integrated across the virtualization\n\nlayers. At the hypervisor level, we integrate a new module\n\nfor access-control in Xen Security Modules (XSM) [25] that\n\nuses a capability-based access-control m",
            "our approach embodies the Principle of Least Privilege (POLP) to create unikernels that have only the privileges they need to accomplish their task.",
            "his paper makes the following contributions:\n\n(a) We present the design of a novel architecture for\n\ncapability-based access-control with unikernels in virtu-\n\nalized hosts. (b) We design and implement a new prototype capability-\n\nbased access-control system for the XSM. (c) We design and implement a library for MirageOS that\n\nenables capability-based access-control for unik"
          ]
        },
        {
          "title": "Unishyper: A Rust-based unikernel enhancing reliability and efficiency of embedded systems",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S138376212400136X",
          "excerpts": [
            "To support memory isolation between user applications, user code, and kernel code, Unishyper designs the Zone mechanism on top of Intel MPK. Unishyper ...",
            "To provide memory isolation support between user and kernel code as well as isolation between different user applications at the thread level, Unishyper designs the Zone mechanism based on Intel MPK [36] to isolate the code into thread gr",
            "Most unikernels typically adopt SAS systems, meaning all tasks, threads, and processes share the same memory addresses, which is a lack of isolation mechanisms between user code and kernel code, as well as isolation between different user applications.",
            "he Zone mechanism on top of Intel MPK to support memory isolation between user applications as well as the isolation between user code and ke"
          ]
        },
        {
          "title": "Unishyper: : A Rust-based unikernel enhancing reliability and ...",
          "url": "https://dl.acm.org/doi/10.1016/j.sysarc.2024.103199",
          "excerpts": [
            "To support memory isolation between user applications, user code, and kernel code, Unishyper designs the Zone mechanism on top of Intel MPK."
          ]
        },
        {
          "title": "[TeX] virtio-iommu.tex",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/tex/virtio-iommu.tex",
          "excerpts": [
            "The virtio-iommu device manages Direct Memory Access (DMA) from one or more endpoints. It may act both as a proxy for physical IOMMUs managing devices assigned ..."
          ]
        },
        {
          "title": "Intel® Trust Domain Extension Linux Guest Kernel Security ...",
          "url": "https://intel.github.io/ccc-linux-guest-hardening-docs/security-spec.html",
          "excerpts": [
            "May 27, 2025 — VirtIO drivers are built around the virtio ring. The ring contains descriptors, which are organized in a free list. The free list handling has ..."
          ]
        },
        {
          "title": "Memory Protection Unit (MPU)",
          "url": "https://documentation-service.arm.com/static/5ef61f08dbdee951c1ccdd48",
          "excerpts": [
            "Jul 8, 2016 — The MPU is configured by a series of memory mapped registers in the System Control Space. (SCS). This chapter lists the MPU registers, and ..."
          ]
        },
        {
          "title": "Using the RISC-V PMP with an Embedded RTOS to ...",
          "url": "https://www.youtube.com/watch?v=upkZZldpljA",
          "excerpts": [
            "Using the RISC-V PMP with an Embedded RTOS to Achieve Process Separation and Isolation. 2.9K views · 6 years ago ...more. RISC-V International."
          ]
        },
        {
          "title": "Enable Stack Protection for Shared Library",
          "url": "https://users.rust-lang.org/t/enable-stack-protection-for-shared-library/119533",
          "excerpts": [
            "I am trying to build a shared library. I want it to have a stack canary. I have installed rustc ... rustflags = [\"-Z\", \"stack-protector=strong\"]."
          ]
        },
        {
          "title": "Codegen Options - The rustc book",
          "url": "https://doc.rust-lang.org/rustc/codegen-options/index.html",
          "excerpts": [
            "When -C lto is not specified, LTO is disabled if codegen units is 1 or optimizations are disabled ( -C opt-level=0 ). That is: When -C lto is not specified:."
          ]
        },
        {
          "title": "Enforcing no-std and no-panic during build - Rust Internals",
          "url": "https://internals.rust-lang.org/t/enforcing-no-std-and-no-panic-during-build/14505",
          "excerpts": [
            "When compiling for no_std and in release mode, you can enforce that panic is never called with https://crates.io/crates/no-panics-whatsoever."
          ]
        },
        {
          "title": "StackProtector in rustc_target::spec",
          "url": "https://doc.rust-lang.org/beta/nightly-rustc/rustc_target/spec/enum.StackProtector.html",
          "excerpts": [
            "This triggers stack canary generation in functions which contain an array of a byte-sized type with more than eight elements."
          ]
        },
        {
          "title": "Follow up on stack protector research · Issue #3201 - GitHub",
          "url": "https://github.com/mobilecoinfoundation/mobilecoin/issues/3201",
          "excerpts": [
            "I'm trying to understand if there is a way to get stack protectors like this in rust. There is an old github issue that mentions this: rust-lang ..."
          ]
        },
        {
          "title": "Unishyper: A Rust-based unikernel enhancing reliability and ...",
          "url": "https://www.researchgate.net/publication/381383329_Unishyper_A_Rust-based_unikernel_enhancing_reliability_and_efficiency_of_embedded_systems",
          "excerpts": [
            "Iso-UniK leverages a recent hardware feature, named Intel Memory Protection Key (Intel MPK), to provide lightweight and efficient isolation ..."
          ]
        },
        {
          "title": "Creating a secure RTOS on Risc-V : r/embedded",
          "url": "https://www.reddit.com/r/embedded/comments/1ccy757/creating_a_secure_rtos_on_riscv/",
          "excerpts": [
            "I am building a simple secure RTOS with simple scheduler that run on Risc V , how can I make use of PMP to achieve isolation between tasks ,"
          ]
        },
        {
          "title": "Rust RFCS / unikernel security discussion",
          "url": "https://github.com/rust-lang/rfcs/issues/1005",
          "excerpts": [
            "Missing: unikernel hardening Previous thread: rust-lang/rust Now that the runtime is all but gone, no_std's purpose is a bit unclear. It's behind a feature gate, so figuring out exactly what it is and should do is part of the process to get it to be stable. Stabilize #!",
            "The Reenix paper: <http://scialex.github.io/reenix.pdf>\n\nhas some good insights, specifically on using the standard library in an osdev contex"
          ]
        },
        {
          "title": "libhermitMPK: Intra-Unikernel Isolation with Intel Memory Protection Keys",
          "url": "https://github.com/ssrg-vt/libhermitMPK",
          "excerpts": [
            "libhermitMPK: Intra-Unikernel Isolation with Intel Memory Protection Keys ... This work was implemented on the top of RustyHermit (https://github.com/hermitcore/ ...See more [rust-toolchain](/ssrg-vt/libhermitMPK/blob/master/rust-toolchain \"rust-toolchain\") | | [rust-toolchain](/ssrg-vt/libhermitMPK/blob/master/rust-toolchain \"rust-toolchain\") | [Added rust-toolchain file pointing to nightly. ](/ssrg-vt/libhermitMPK/commit/922862ea7aacdcbcf0cd43f5622ddb63faebc4ea \"Added rust-toolchain file pointing to nightly. This is useful for all of us not living on nightly as default.\") | Jul"
          ]
        },
        {
          "title": "QEMU Firmware Configuration (fw_cfg) Device",
          "url": "https://www.qemu.org/docs/master/specs/fw_cfg.html",
          "excerpts": [
            "Since v2.4, “file” fw\\_cfg items (i.e., items with selector keys above\n`FW_CFG_FILE_FIRST`, and with a corresponding entry in the fw\\_cfg file\ndirectory structure) may be inserted via the QEMU command line, using\nthe following synta",
            "\n```\n-fw_cfg [name=]<item_name>,file=<path>\n\n```",
            "Or:\n\n```\n-fw_cfg [name=]<item_name>,string=<string>\n\n```",
            "g>\n\n```\n\nSince v5.1, QEMU allows some objects to generate fw\\_cfg-specific content,\nthe content is then associated with a “file” item using the ‘gen\\_id’ option\nin the command line, using the following syntax:\n\n```\n-object <generator-type>,id=<generated_id>,[generator-specific-options] \\\n-fw_cfg [name=]<item_name>,gen_id=<generated_id>\n\n```",
            "All externally provided fw\\_cfg items are read-only to the guest",
            "Guest-side Hardware Interface",
            "Selector (Control) Register",
            "Write only",
            "Location: platform dependent (IOport or MMIO)",
            "Width: 16-bit",
            "Endianness: little-endian (if IOport), or big-endian (if MMIO)"
          ]
        },
        {
          "title": "QEMU_fw_cfg (OSDev Wiki)",
          "url": "https://wiki.osdev.org/QEMU_fw_cfg",
          "excerpts": [
            "To pass strings or files into QEMU to appear in the fw\\_cfg device, use one or more command line parameters like those shown below:",
            "```\n -fw_cfg name=opt/com.name.domain.your.example,string=1\n -fw_cfg name=opt/com.name.domain.your.example,file=example_filename.png\n -fw_cfg opt/com.name.domain.your.example,file=example_filename.png\n\n```",
            "QEMU fw cfg",
            "Names that begin with \"opt/\" are reserved for users. It is strongly recommended to use a RFQDN (Reversed Fully Qualified Domain Name) as a prefix for the resource name.",
            "In the example above, it is assumed you are the owner of the domain \"your.domain.name.com\", and your resource is named \"example\".",
            "Following this advice guarantees that there will never be a conflict, since only one entity can own a given domain name.",
            "To detect the presence of QEMU, you should read CPUID leaf 0x40000000 and check ... If you read \"QEMU\" from the data port, the fw_cfg device is present.",
            "The \"name=\" part of the argument may be omitted, as shown in the third example above.",
            "Detecting QEMU",
            "It is a very good idea for your OS to first make sure it is running under QEMU, before attempting to access the fw\\_cfg device.",
            "Accessing I/O ports for devices that don't exist on a real machine may have undefined behavior.",
            "QEMU provides three I/O ports on x86, or three MMIO addresses on other architectures",
            "*FW\\_CFG\\_PORT\\_SEL* is a 16-bit port at I/O address 0x510. You write a \"selector\" to this port to control which data will be read when reading *FW\\_CFG\\_PORT\\_DATA",
            "*FW\\_CFG\\_PORT\\_DATA* is an 8-bit port at I/O address 0x511. You read one byte at a time of the file from this port, after selecting a file by writing a value to *FW\\_CFG\\_PORT\\_SEL",
            "*FW\\_CFG\\_PORT\\_DMA* is a 32-bit port at I/O address 0x514. DMA is covered later in this article",
            "### Selectors",
            "```\n  #define FW_CFG_SIGNATURE    0x0000\n  #define FW_CFG_ID           0x0001\n  #define FW_CFG_FILE_DIR     0x0019\n\n```",
            "Listed above are a few fixed-purpose selectors for probing for the existence of the device, and for getting the number of files present in the device.",
            "### Enumerating Files",
            "The first four bytes of the *FW\\_CFG\\_FILE\\_DIR* selector will be a 32-bit big-endian number which is the number of files present in the directo",
            "Following the count, there is a sequence of entries with the following structure:",
            "th the following structure:\n\n```\n  struct FWCfgFile {\t\t/* an individual file entry, 64 bytes total */\n      uint32_t size;\t\t/* size of referenced fw_cfg item, big-endian */\n      uint16_t select;\t\t/* selector key of fw_cfg item, big-endian */\n      uint16_t reserved;\n      char name[56];\t\t/* fw_cfg item name, NUL-terminated ascii */\n  };",
            "These can be read one at a time by reading *sizeof(FWCfgFile)* bytes into a single variable of type *FWCfgFile* and examined one by one until you find a structure with the desired name.",
            "### Reading files",
            "Once you have found the *FWCfgFile* structure which has the desired name, read the *select* field from the structure. It will contain the big-endian selector for the file. After byte-swapping the selector, output it to *FW\\_CFG\\_PORT\\_SEL*, then read *size* bytes from *FW\\_CFG\\_PORT\\_DATA* and write them to the desired location in memory.",
            "An example implementation is provided in the references below."
          ]
        },
        {
          "title": "QEMU fw_cfg - Somlo CMU (Contrib.cm.u.edu)",
          "url": "http://www.contrib.andrew.cmu.edu/~somlo/QEMU_fw_cfg/",
          "excerpts": [
            "The fw\\_cfg device allows QEMU to prepare various data items (or \"blobs\") in host memory, and offers a guest-side interface allowing the guest to read (using port or memory-mapped I/O operations) the blobs after having been launched by the host"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs03/virtio-v1.0-cs03.html",
          "excerpts": [
            "Queue Size corresponds to the maximum number of buffers in the virtqueue. Queue Size value is always a power of 2. The maximum Queue Size value is 32768. This ...See more"
          ]
        },
        {
          "title": "libvirt: virtio-net multiqueue — Nova Specs documentation",
          "url": "https://specs.openstack.org/openstack/nova-specs/specs/liberty/implemented/libvirt-virtiomq.html",
          "excerpts": [
            "Sep 4, 2015 — Guest OS is limited to ~200 MSI vectors. Each NIC queue requires a MSI vector, as well as any virtio device or assigned PCI device."
          ]
        },
        {
          "title": "A journey to the vhost-users realm",
          "url": "https://www.redhat.com/en/blog/journey-vhost-users-realm",
          "excerpts": [
            "Sep 24, 2019 — This post is a technical deep dive into the vhost-user/virtio-pmd architecture targeting high performance userland networking using DPDK.",
            "Sep 24, 2019 — The main difference between the vhost-user library and the vhost-net kernel driver is the communication channel. While the vhost-net kernel ..."
          ]
        },
        {
          "title": "Vhost-user Protocol — QEMU 10.0.93 documentation - Read the Docs",
          "url": "https://qemu.readthedocs.io/en/master/interop/vhost-user.html",
          "excerpts": [
            "The front-end enables queues by sending message VHOST_USER_SET_VRING_ENABLE . vhost-user-net has historically automatically enabled the first queue pair."
          ]
        },
        {
          "title": "23. Vhost Sample Application - Documentation - DPDK",
          "url": "https://doc.dpdk.org/guides-1.8/sample_app_ug/vhost.html",
          "excerpts": [
            "The Linux* Kernel vhost-net module was developed as an offload mechanism for virtio-net. The vhost-net module enables KVM (QEMU) to offload the servicing of ..."
          ]
        },
        {
          "title": "View topic - [SOLVED] KVM / QEMU - Bridge interface",
          "url": "https://forums.gentoo.org/viewtopic-t-1144185-start-0.html",
          "excerpts": [
            "Oct 23, 2021 — With the bridge, the interface br0 must be configured with your IP address. So yes, it is to be expected that you lose the connection if you add ..."
          ]
        },
        {
          "title": "[Solved] Network Bridging Issues with Qemu/KVM VMs and ...",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=216015",
          "excerpts": [
            "Aug 17, 2016 — I am trying to bridge a Qemu/kvm VM guest so that it has an IP address and can work with the rest of my 192.168.1.x network."
          ]
        },
        {
          "title": "OASIS Virtio Specification Repository",
          "url": "https://github.com/oasis-tcs/virtio-spec",
          "excerpts": [
            "The purpose of virtio and this specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms.",
            "This repository includes the [authoritative source](https://github.com/oasis-tcs/virtio-spec/releases) of the VIRTIO (Virtual I/O) Specification document"
          ]
        },
        {
          "title": "eBPF RSS virtio-net support",
          "url": "https://www.qemu.org/docs/master/devel/ebpf_rss.html",
          "excerpts": [
            "RSS(Receive Side Scaling) is used to distribute network packets to guest virtqueues by calculating packet hash. ... tap,vhost=on & virtio-net-pci,rss=on,hash=off."
          ]
        },
        {
          "title": "Add support for vhost-user-net · Issue #8 · cloud-hypervisor ...",
          "url": "https://github.com/cloud-hypervisor/cloud-hypervisor/issues/8",
          "excerpts": [
            "May 14, 2019 — In order to let users of cloud-hypervisor plug their own network backend, the VMM should support vhost-user-net daemons through a global ..."
          ]
        },
        {
          "title": "Network",
          "url": "https://crosvm.dev/book/devices/net.html",
          "excerpts": [
            "To network a crosvm, set up a TAP interface on the host, configure routing, and start crosvm with the tap-name. Guest network is visible if VIRTIO_NET is ..."
          ]
        },
        {
          "title": "Command line options and configuration files",
          "url": "https://crosvm.dev/book/running_crosvm/options.html",
          "excerpts": [
            "Crosvm can be configured via command-line options (key-value pairs) or JSON configuration files using the --cfg argument. Command-line options take precedence."
          ]
        },
        {
          "title": "Way to pass parameters or share a directory/file to a qemu ...",
          "url": "https://stackoverflow.com/questions/56535142/way-to-pass-parameters-or-share-a-directory-file-to-a-qemu-kvm-launched-vm-on-ce",
          "excerpts": [
            "If that guest is running a Linux kernel with CONFIG_FW_CFG_SYSFS enabled, you will be able to read out the information from sysfs. An example:."
          ]
        },
        {
          "title": "Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg",
          "url": "https://git.doit.wisc.edu/SWIFT/linux-ldos/-/blob/master/Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg?ref_type=heads",
          "excerpts": [
            "A few weeks ago, QEMU switched docs/specs/fw_cfg.txt to be docs/specs/fw_cfg.rst, so update the reference in the kernel docs to reflect this."
          ]
        },
        {
          "title": "crosvm - Architecture",
          "url": "https://chromium.googlesource.com/crosvm/crosvm/+/refs/heads/factory-brya-14517.B-chromeos/ARCHITECTURE.md",
          "excerpts": [
            "A typical session of crosvm starts in main.rs where command line parsing is done to build up a Config structure. The Config is used by run_config in linux/mod."
          ]
        },
        {
          "title": "3. Vhost Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/vhost_lib.html",
          "excerpts": [
            "When DPDK vhost-user acts as the client it will keep trying to reconnect to the server (QEMU) until it succeeds. This is useful in two cases: When QEMU is not ..."
          ]
        },
        {
          "title": "46. Vhost Library - Documentation",
          "url": "https://doc.dpdk.org/guides-19.11/prog_guide/vhost_lib.html",
          "excerpts": [
            "When DPDK vhost-user acts as the client it will keep trying to reconnect to the server (QEMU) until it succeeds. This is useful in two cases: When QEMU is not ..."
          ]
        },
        {
          "title": "How to use virtio-fs — Cloud Hypervisor documentation",
          "url": "https://intelkevinputnam.github.io/cloud-hypervisor-docs-HTML/docs/fs.html",
          "excerpts": [
            "virtio-fs, also known as vhost-user-fs is a virtual device defined by the VIRTIO specification which allows any VMM to perform filesystem sharing."
          ]
        },
        {
          "title": "How to create a multiqueue TAP device in Linux",
          "url": "https://stackoverflow.com/questions/72268735/how-to-create-a-multiqueue-tap-device-in-linux",
          "excerpts": [
            "The script creates a tap device tap0 with 8 queues, adds tap0 to bridge br0, and sets tap0 up.",
            "You can use ethtool -l [ifname] in your VM. NETWORK=\" -netdev tap,ifname=${TAP:-tap0},id=n1,script=no,downscript=no,queues=4 -device virtio-net-pci,mac=AA:BB:CC:DD:CA:B1,netdev=n1,mq=on,vectors=10\"",
            "Important options are\nvirtio-net-pci and\nmq=on,vectors=10 . Information about the vectors parameter https://www.linux-kvm.org/page/Multiqueue",
            "The docs say that for\nqueues=N the value for\nvectors should be \"2N+2\". Therefore, in your example with\nqueues=4 we should use\nvectors=10 ."
          ]
        },
        {
          "title": "Harold/Harry Hodge — Getting started with Firecracker (Firecracker networking and config)",
          "url": "https://harryhodge.co.uk/posts/2024/01/getting-started-with-firecracker/",
          "excerpts": [
            "\"subnet\": \"192.168.30.0/24\"",
            "\"rangeStart\": \"192.168.30.32\"",
            "\"boot_args\": \"... ip=192.168.30.39::192.168.30.1:255.255.255.0::eth0:off\"",
            ". CNI operates with network namespaces so we create unique namespaces for each VM interface.",
            "1{",
            "\"name\": \"firecracker\",",
            "\"cniVersion\": \"1.0.0\",",
            "\"plugins\": [",
            " \"type\": \"bridge\",",
            "\"name\": \"firecracker-bridge\",",
            "\"bridge\": \"fcbr0\",",
            "\"isGateway\": true,",
            "\"ipMasq\": true,",
            "\"ipam\": {",
            "\"type\": \"host-local\",",
            "\"resolvConf\": \"/etc/resolv.conf\",",
            "\"dataDir\": \"/srv/vm/networks\",",
            "\"subnet\": \"192.168.30.0/24\",",
            "\"rangeStart\": \"192.168.30.32\",",
            "\"gateway\": \"192.168.30.1\"",
            "\"gateway\": \"192.168.30.1\"",
            "},",
            "  \"type\": \"firewall\"",
            ",",
            "  \"type\": \"tc-redirect-tap\"",
            "]",
            "}",
            "}"
          ]
        },
        {
          "title": "How do I use linked-list-allocator in Rust?",
          "url": "https://stackoverflow.com/questions/78645054/how-do-i-use-linked-list-allocator-in-rust",
          "excerpts": [
            "}"
          ]
        },
        {
          "title": "Crate x86_64 - Rust",
          "url": "https://docs.rs/x86_64",
          "excerpts": [
            "This crate provides x86_64 specific functions and data structures, and access to various system registers."
          ]
        },
        {
          "title": "Page Tables | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/page-tables/",
          "excerpts": [
            "Dec 9, 2015 — In this post we will create a paging module, which allows us to access and modify the 4-level page table. We will explore recursive page table mapping.See more"
          ]
        },
        {
          "title": "Introduction to Paging | Writing an OS in Rust",
          "url": "https://os.phil-opp.com/paging-introduction/",
          "excerpts": [
            "Jan 14, 2019 — A 4-level page table makes the translation of virtual addresses expensive because each translation requires four memory accesses. To improve ...See more"
          ]
        },
        {
          "title": "Virtio 1.2 is Coming! - Alibaba Cloud Community",
          "url": "https://www.alibabacloud.com/blog/virtio-1-2-is-coming_599615",
          "excerpts": [
            "Virto spec defines a set of virtqueue mechanisms to enable communication between guest and host. This mechanism can be applied to various ..."
          ]
        },
        {
          "title": "Virt Queue Types - NVIDIA Docs",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv190/Virt+Queue+Types",
          "excerpts": [
            "Descriptor table – occupies the descriptor area. Available ring – occupies the driver area. Used ring – occupies the device area. Each of these ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 | StandICT.eu 2026",
          "url": "https://standict.eu/standards-repository/standard/virtual-io-device-virtio-version-11",
          "excerpts": [
            "This standard describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Linux Kernel vs DPDK: HTTP Performance Showdown",
          "url": "https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/",
          "excerpts": [
            "Jul 4, 2022 — An entire network interface is dedicated to a single application, and aggressive busy polling is used to achieve high throughput and low latency ..."
          ]
        },
        {
          "title": "OSDev/Phil-opp IDT & paging",
          "url": "https://os.phil-opp.com/catching-exceptions/",
          "excerpts": [
            "Jun 25, 2016 — Thus, the hardware can automatically load the corresponding IDT entry for each exception. The Exception Table in the OSDev wiki shows the IDT ...See more For example, the caller could add a non-present entry by executing: `idt.set_handler(11, handler_fn).set_present(false)`. ",
            ")Loading the IDT\n\nNow we’re able to create new interrupt descriptor tables with registered handler functions. We just need a way to load an IDT, so that the CPU uses it. The x86 architecture uses a special register to store the active IDT and its length. In order to load a new IDT we need to update this register through the [lidt](https://www.felixcloutier.com/x86/lgdt:lidt) instruction. The `lidt` instruction expects a pointer to a special data structure, which specifies the start address of the IDT and its length:\n\n| Type | Name | Description |\n| --- | --- | --- |\n| u16 | Limit | The maximum addressable byte in the table. Equal to the table size in bytes minus 1. |\n| u64 | Offset | Virtual start address of the table. |\n\nThis structure is already contained [in the x86\\_64 crate](https://docs.rs/x86_64/0.1.0/x86_64/instructions/tables/struct.DescriptorTablePointer.html), so we don’t need to create it ourselves. The same is true for the [lidt function](https://docs.rs/x86_64/0.1.0/x86_64/instructions/tables/fn.lidt.html)."
          ]
        },
        {
          "title": "rust-osdev/x86_64 - Tables and IDT/GDT setup (excerpt from tables.rs)",
          "url": "https://github.com/rust-osdev/x86_64/blob/master/src/instructions/tables.rs",
          "excerpts": [
            "/// Load an IDT. ///",
            "pub unsafe fn lidt(idt: &DescriptorTablePointer) {",
            "/// Get the address of the current GDT. #[inline]\npub fn sgdt() -> DescriptorTablePointer {",
            "pub unsafe fn lgdt(gdt: &DescriptorTablePointer) {"
          ]
        },
        {
          "title": "Virt Queue Types - NVIDIA Docs",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2407/Virt+Queue+Types",
          "excerpts": [
            "Aug 14, 2024 — Split VQ format is the only format supported by version 1.0 of the virtio spec. In split VQ mode, each VQ is separated into three parts:."
          ]
        },
        {
          "title": "Low-Latency GPU Packet Processing",
          "url": "https://eunomia.dev/others/cuda-tutorial/13-low-latency-gpu-packet-processing/",
          "excerpts": [
            "Packet Capture: Receive packets from the network interface · Batching: Collect multiple packets to amortize transfer and launch costs · Transfer ..."
          ]
        },
        {
          "title": "Virtio, vhost, vhost-user, vsock, etc. | Lencerf's Walk",
          "url": "https://lencerf.github.io/post/2023-05-28-virtio-vhost-vsock/",
          "excerpts": [
            "May 28, 2023 — On the other hand, vhost-user off-loads data processing to another user space program, which is called a backend device. The VMM and the device ..."
          ]
        },
        {
          "title": "[PDF] What's coming in VIRTIO 1.2 - Stefan Hajnoczi",
          "url": "https://vmsplice.net/~stefan/stefanha-fosdem-2022.pdf",
          "excerpts": [
            "Sound card for multichannel PCM audio playback and capture with up to 384 kHz sample rate and. 64-bit sample depth. Reuses parts of High Definition Audio (HDA) ..."
          ]
        },
        {
          "title": "Ultra Low Latency: The Ultimate Guide to Lightning-Fast ...",
          "url": "https://www.videosdk.live/developer-hub/webtransport/ultra-low-latency",
          "excerpts": [
            "In 2025, an ultra low latency trading system aims for round-trip times well below 10 microseconds, targeting single-digit microseconds or even nanoseconds for ..."
          ]
        },
        {
          "title": "Virtio-net Feature Bits - NVIDIA Docs",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2410/Virtio-net+Feature+Bits",
          "excerpts": [
            "For more information on these bits, refer to the VIRTIO Version 1.2 Specifications. © Copyright 2024, NVIDIA. Last updated on Nov 12, 2024."
          ]
        },
        {
          "title": "8. AVP Poll Mode Driver - Documentation",
          "url": "https://doc.dpdk.org/guides/nics/avp.html",
          "excerpts": [
            "This provides DPDK applications running in virtual machines with significantly improved throughput and latency over other device types. The AVP device ..."
          ]
        },
        {
          "title": "bootloader - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/bootloader/range/%5E0.9.30",
          "excerpts": [
            "map_physical_memory : Maps the complete physical memory in the virtual address space and passes a physical_memory_offset field in the BootInfo ."
          ]
        },
        {
          "title": "OS Experiment in Rust (part 1): Creating a UEFI Loader",
          "url": "https://blog.malware.re/2023/08/20/rust-os-part1/index.html",
          "excerpts": [
            "Aug 20, 2023 — x86-64 UEFI Boot Process. Typically, the UEFI firmware in an x86-64 system will boot up, initialize the hardware, and then search the boot ..."
          ]
        },
        {
          "title": "Features/MQ virtio net - Fedora Project Wiki",
          "url": "https://fedoraproject.org/wiki/Features/MQ_virtio_net",
          "excerpts": [
            "Mar 15, 2013 — Multiqueue virtio-net provides an approach that scales the network performance as the increasing of the number of vcpus by allowing them to transfer packets ..."
          ]
        },
        {
          "title": "Building a Linux Kernel Driver using Rust",
          "url": "https://rust-exercises.ferrous-systems.com/latest/book/building-linux-kernel-driver",
          "excerpts": [
            "Building a Linux Kernel Driver using Rust. In this example we're going to build a very basic Linux Kernel driver in Rust, compile a Linux ..."
          ]
        },
        {
          "title": "Towards Practical Formal Verification for a General-Purpose ...",
          "url": "https://asterinas.github.io/2025/02/13/towards-practical-formal-verification-for-a-general-purpose-os-in-rust.html",
          "excerpts": [
            "Feb 13, 2025 — For Asterinas, the TCB is the OSTD, a (unofficial) Rust standard library for OS development. OSTD offers a set of safe abstractions enabling the ..."
          ]
        },
        {
          "title": "understanding priority arrays in linux kernel - Stack Overflow",
          "url": "https://stackoverflow.com/questions/14201296/understanding-priority-arrays-in-linux-kernel",
          "excerpts": [
            "The priority arrays also contain a priority bitmap used to efficiently discover the highest priority runnable task in the system."
          ]
        },
        {
          "title": "Implementing Real-Time Operating Systems",
          "url": "https://www.geeksforgeeks.org/operating-systems/implementing-real-time-operating-systems/",
          "excerpts": [
            "Jul 23, 2025 — A real-time operating system (RTOS) is a type of operating system that is designed to provide deterministic, timely processing of real-time events."
          ]
        },
        {
          "title": "Writing a kernel driver with Rust",
          "url": "https://not-matthias.github.io/kernel-driver-with-rust/",
          "excerpts": [
            "Writing a kernel driver with Rust",
            "#! [no_std",
            "#[panic_handler]",
            "fn panic ( _info : & PanicInfo ) -> ! {",
            "{\n    loop"
          ]
        }
      ],
      "reasoning": "Direct, explicit alignment exists between the user's field value and the excerpts that describe an effort to build a Rust-based unikernel/RTOS and to run it as a VM guest with VirtIO I/O. The field value states a project objective of creating an application-specific, unikernel-style Real-Time Operating System (RTOS) in Rust, designed to run as a guest in a hypervisor (KVM/QEMU), using VirtIO for all I/O, and aiming for sub-10 microsecond P99.99 latency for a high-frequency packet processor. The strongest support comes from excerpts describing a Rust-based unikernel project and its architectural goals (Theseus, RustyHermit/HermitCore family), which directly discuss Rust-based unikernel kernels, their architecture, and isolation strategies suitable for a no-driver-hardware approach. Specific items describe the Theseus project and related Rust-based unikernel work, including the concepts of a single address space, isolation between components, and the general approach to unikernel design in Rust. These sources map well to the field value's objective of a Rust-native unikernel RTOS with deterministic behavior in a VM, including isolation guarantees and the no-physical-driver constraint. The rationale for selecting these excerpts is their direct focus on Rust-based unikernel architectures and their architectural/confirmation of the unikernel approach, rather than general OS or VM topics.\n\nIn addition, excerpts addressing VirtIO in VM contexts and latency considerations are relevant because the field value includes a constraint to use VirtIO paravirtualization for all I/O and to target a latency bound for high-frequency packet processing. These sources discuss VirtIO networking and vhost approaches, multi-queue strategies, and interrupt/polling tradeoffs that influence latency in virtualized networking paths. They offer concrete material that can underpin latency-focused design decisions in the RTOS blueprint, particularly in how I/O is structured and how latency can be mitigated in a VM-based unikernel with VirtIO devices.\n\nTaken together, the strongest evidence comes from materials that address (a) Rust-based unikernel architectures and isolation models (Theseus, RustyHermit/HermitCore family), and (b) VirtIO networking architectures and latency considerations in virtualization contexts. The remaining latency-focused material (about sub-10µs targets) is supportive but more indirect; it helps contextualize the latency goals within VirtIO-based virtualization environments. Overall, the set of excerpts coherently supports the field value's core objective and architectural constraints, indicating a high degree of alignment between the described projects and the research question's requirements.",
      "confidence": "high"
    },
    {
      "field": "host_and_qemu_tuning_guide.network_backend_setup",
      "citations": [
        {
          "title": "Features/VirtioVhostUser",
          "url": "https://wiki.qemu.org/Features/VirtioVhostUser",
          "excerpts": [
            "The virtio-vhost-user device lets guests act as vhost device backends so that virtual network switches and storage appliance VMs can provide virtio devices to other guests.",
            "tdev:\n\n```\n$ qemu -M accel=kvm -cpu host -m 1G \\\n       -object memory-backend-file,id=mem0,mem-path=/var/tmp/foo,size=1G,share=on \\\n       -numa node,memdev=mem0 \\\n       -drive if=virtio,file=test.img,format=raw \\\n       -chardev socket,id=chardev0,path=vhost-user.sock \\\n       -netdev vhost-user,chardev=chardev0,id=netdev0 \\\n       -device virtio-net-pci,netdev=netdev0\n```\n",
            "VM2 sees a regular virtio-net device. VM2's QEMU uses the existing vhost-user feature as if it were talking to a host userspace vhost-user backend. VM1's QEMU tunnels the vhost-user protocol messages from VM1's QEMU to the new virtio-vhost-user device so that guest software in VM1 can act as the vhost-user backend."
          ]
        },
        {
          "title": "Enabling multiple queues (multiqueue) on Linux",
          "url": "https://cloud.vk.com/docs/en/computing/iaas/how-to-guides/vm-multiqueue",
          "excerpts": [
            "The virtio-net multiqueue function provides performance improvement, but it has some limitations: Guest OS is limited to ~200 MSI vectors."
          ]
        },
        {
          "title": "QEMU User Documentation",
          "url": "https://www.qemu.org/docs/master/system/qemu-manpage.html",
          "excerpts": [
            "The QEMU PC System emulator simulates the following peripherals: SMP is supported with a large number of virtual CPUs (upper limit is configuration dependent)."
          ]
        },
        {
          "title": "Tuning a real-time kernel",
          "url": "https://ubuntu.com/blog/real-time-kernel-tuning",
          "excerpts": [
            "nohz_full=<list of isolated cpus> reduces the number of scheduling-clock interrupts, improving energy efficiency and reducing OS jitter. When ..."
          ]
        }
      ],
      "reasoning": "The target field value outlines a detailed VM networking setup using VirtIO with an accelerated backend, prioritizing vhost-kernel or vhost-user. It specifies creating a host TAP device for vhost-kernel with multiqueue, enabling vhost in QEMU with -netdev vhost-user for the highest performance, and configuring a chardev socket for a vhost-user backend, along with enabling multiqueue in the guest and host and using mq and queues parameters. It also notes that vhost-user can be backed by a userspace process (e.g., a DPDK datapath) and shows that VM-to-QEMU communications for virtio rely on a Unix-domain socket path for -chardev, and a -netdev vhost-user declaration. The most directly relevant excerpts state that virtio-vhost-user enables guests to act as the vhost backend, provide concrete QEMU command-line patterns including -chardev socket and -netdev vhost-user, and describe the VM-visible behavior of virtio-net devices in this setup. Additional excerpts discuss multiqueue virtio-net usage for scaling performance, which aligns with the field's emphasis on performance and scalability, and the QEMU User Documentation context helps corroborate the system-level details of virtualization features. The real-time tuning excerpts about nohz_full offer peripheral performance context but do not define the VirtIO/VhostUser flow themselves; they are supportive but less central to the field value. Taken together, the most supportive excerpts directly confirm the presence of vhost-user workflows, chardev sockets, and netdev vhost-user usage, while other excerpts provide supplementary details about multiqueue and virtualization context.",
      "confidence": "high"
    },
    {
      "field": "concurrency_primitives_implementation.spinlock_design",
      "citations": [
        {
          "title": "Rust Atomics and Locks",
          "url": "https://marabos.nl/atomics/",
          "excerpts": [
            "An Unsafe Spin Lock"
          ]
        },
        {
          "title": "Atomic in core::sync",
          "url": "https://doc.rust-lang.org/core/sync/atomic/type.Atomic.html",
          "excerpts": [
            "A memory location which can be safely modified from multiple threads. This has the same size and bit validity as the underlying type T . However, ..."
          ]
        },
        {
          "title": "linked_list_allocator - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/linked_list_allocator",
          "excerpts": [
            "Features\n\n* **`use_spin`** (default): Provide a `LockedHeap` type that implements the [`GlobalAlloc`](https://doc.rust-lang.org/nightly/core/alloc/trait.GlobalAlloc.html) trait by using a spinlock."
          ]
        },
        {
          "title": "spin-based implementation for no_std · Issue #61",
          "url": "https://github.com/matklad/once_cell/issues/61",
          "excerpts": [
            "Oct 7, 2019 — What lazy_static does in this case is that it has a spin feature, which replaces locking with spinning. I think just silently replacing OS ..."
          ]
        },
        {
          "title": "spin_loop in core::hint",
          "url": "https://doc.rust-lang.org/beta/core/hint/fn.spin_loop.html",
          "excerpts": [
            "`spin_loop` emits a signal for a busy-wait spin-loop, allowing the processor to optimize, and does not interact with the OS, unlike `thread::yield_now`."
          ]
        },
        {
          "title": "Ordering in std::sync::atomic - Rust",
          "url": "https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "The `Ordering` enum in `std::sync::atomic` specifies how atomic operations synchronize memory, with options like `Relaxed`, `Release`, `Acquire`, `AcqRel`, and ...",
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Ordering in core::sync::atomic",
          "url": "https://doc.rust-lang.org/core/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Misunderstanding in Acquire/Release. Memory ordering - help",
          "url": "https://users.rust-lang.org/t/misunderstanding-in-acquire-release-memory-ordering/114010",
          "excerpts": [
            "Acquire/release (with load/store) prevent reordering of memory reads and writes by the compiler or the hardware across those instructions."
          ]
        }
      ],
      "reasoning": "The most relevant excerpt explicitly references an unsafe spin lock in Rust, which directly aligns with the field's focus on implementing spinlocks using atomic operations and discusses practical considerations around locking primitives in a Rust/no_std context. This excerpt provides concrete grounding for how a spin lock could be implemented, including potential use of Rust's atomic types and unsafe constructs common in low-level concurrency primitives.\n\nThe next-most-relevant excerpts discuss atomic primitives in Rust. They provide essential context for implementing spinlocks with core::sync::atomic, including how atomic operations synchronize memory and the presence of various memory orderings. This information underpins the design decisions around correctness and performance of a TTAS or similar lock in a no_std environment.\n\nA nearby reference mentions a spinlock integration in a heap allocator, which is relevant because many no_std allocators rely on a primitive spinlock to guard the global allocator. While it doesn't detail the TTAS or MCS approaches, it demonstrates practical usage of a spinlock within the constrained environment described by the field value.\n\nOther excerpts touch on spin-loop optimizations and general atomic ordering semantics. While they are not specific lock designs, they provide supporting background on busy-wait considerations and correct memory ordering, which influence the design trade-offs between fairness, latency, and throughput in high-determinism systems.\n\nTaken together, these excerpts form a coherent set of references that support, at a minimum, implementing spinlocks via atomic operations, with attention to memory ordering and practical no_std constraints; they also sketch the landscape of fairness- and scalability-related options (e.g., TTAS, Ticket/MCS) that would be relevant when expanding beyond a simple low-contention lock.",
      "confidence": "medium"
    },
    {
      "field": "physical_memory_management.frame_allocator_algorithm",
      "citations": [
        {
          "title": "Buddy memory allocation",
          "url": "https://en.wikipedia.org/wiki/Buddy_memory_allocation",
          "excerpts": [
            "The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as ...",
            "The buddy method of freeing memory is fast, with the maximal number of compactions required equal to O(highest order) = O(log2(total memory size)). Typically ..."
          ]
        },
        {
          "title": "Page Frame Allocation",
          "url": "https://wiki.osdev.org/Page_Frame_Allocation",
          "excerpts": [
            "Buddy Allocation System"
          ]
        }
      ],
      "reasoning": "The field value asserts that the Buddy Allocator is the chosen memory management algorithm for the frame allocator, with deterministic performance characterized by O(log N) time for allocations and deallocations, which helps meet sub-microsecond latency targets in an RTOS. The most supportive excerpt directly describes the buddy memory allocation technique as a memory allocation algorithm that divides memory into partitions to satisfy requests, which aligns with the stated choice and its performance characteristics. A closely related excerpt discusses the buddy method of freeing memory and notes the bounded behavior related to highest order, reinforcing the deterministic, logarithmic performance claim. Additionally, a surrounding excerpt explicitly names the Buddy Allocation System and ties it to the notion of memory management, directly matching the field value's stated algorithm. A further excerpt discusses deallocation within the buddy context (including potential worst-case considerations), which is relevant to understanding how buddy allocators handle freeing memory and maintaining fragmentation control, supporting why this approach is beneficial for large contiguous blocks. Together, these excerpts connect the described algorithm, its time complexity, and its fragmentation advantages to the fine-grained field value, validating the chosen approach for a real-time, high-determinism RTOS memory subsystem.",
      "confidence": "high"
    },
    {
      "field": "concurrency_primitives_implementation.backoff_strategies",
      "citations": [
        {
          "title": "spin-based implementation for no_std · Issue #61",
          "url": "https://github.com/matklad/once_cell/issues/61",
          "excerpts": [
            "Oct 7, 2019 — What lazy_static does in this case is that it has a spin feature, which replaces locking with spinning. I think just silently replacing OS ..."
          ]
        },
        {
          "title": "spin_loop in core::hint",
          "url": "https://doc.rust-lang.org/beta/core/hint/fn.spin_loop.html",
          "excerpts": [
            "`spin_loop` emits a signal for a busy-wait spin-loop, allowing the processor to optimize, and does not interact with the OS, unlike `thread::yield_now`."
          ]
        },
        {
          "title": "Rust Atomics and Locks",
          "url": "https://marabos.nl/atomics/",
          "excerpts": [
            "An Unsafe Spin Lock"
          ]
        },
        {
          "title": "Atomic in core::sync",
          "url": "https://doc.rust-lang.org/core/sync/atomic/type.Atomic.html",
          "excerpts": [
            "A memory location which can be safely modified from multiple threads. This has the same size and bit validity as the underlying type T . However, ..."
          ]
        },
        {
          "title": "Misunderstanding in Acquire/Release. Memory ordering - help",
          "url": "https://users.rust-lang.org/t/misunderstanding-in-acquire-release-memory-ordering/114010",
          "excerpts": [
            "Acquire/release (with load/store) prevent reordering of memory reads and writes by the compiler or the hardware across those instructions."
          ]
        },
        {
          "title": "Ordering in std::sync::atomic - Rust",
          "url": "https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "The `Ordering` enum in `std::sync::atomic` specifies how atomic operations synchronize memory, with options like `Relaxed`, `Release`, `Acquire`, `AcqRel`, and ..."
          ]
        }
      ],
      "reasoning": "The backoff strategy relies on low-level spinning and architecture-aware pause hints rather than falling back to yield-based waiting. A reference describing a spin-based implementation for no_std directly supports the approach of using very tight spin loops in a bare-metal or no_std context, which underpins the proposed backoff mechanism. The explicit mention that a spin_loop inserts a PAUSE instruction on x86 or a YIELD/WFE on ARM aligns with the described optimization of a busy-wait loop to reduce power and resource contention during spin-wait. This local, architecture-aware pause mechanism is foundational to implementing efficient backoff during lock contention. Additionally, discussions about a spin-based lock and atomic primitives provide the low-level primitives required to implement the backoff logic (e.g., atomically attempting to acquire a lock, and using spin loops around those attempts). The material on spin_loop semantics and spin-based locking demonstrates how a no_std environment can coordinate between multiple threads or cores without relying on OS facilities, which is essential for a deterministic, high-performance RTOS. Information about memory ordering and atomics further supports understanding how to order memory operations safely during repeated lock attempts, which is necessary when introducing jittered exponential backoff. Taken together, these excerpts establish the key building blocks—the architecture-specific pause hints, a spin-based locking approach in no_std, and the atomic primitives—that underpin the described backoff strategy. ",
      "confidence": "medium"
    },
    {
      "field": "physical_memory_management.dma_considerations",
      "citations": [
        {
          "title": "Buddy memory allocation",
          "url": "https://en.wikipedia.org/wiki/Buddy_memory_allocation",
          "excerpts": [
            "The buddy method of freeing memory is fast, with the maximal number of compactions required equal to O(highest order) = O(log2(total memory size)). Typically ...",
            "The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as ..."
          ]
        },
        {
          "title": "Page Frame Allocation",
          "url": "https://wiki.osdev.org/Page_Frame_Allocation",
          "excerpts": [
            "Buddy Allocation System",
            "A large array of N/8 bytes is used as a large bit map of the memory usage (that is, bit #i in byte #n define the status of page #n*8+i). Setting the state of a given page is fast (O(1)), allocating a page may take time (O(N)). * an uint32_t comparison can test up to 32 bits at once and thus speed up allocations"
          ]
        },
        {
          "title": "Buddy Memory Allocation Program | Set 2 (Deallocation)",
          "url": "https://www.geeksforgeeks.org/operating-systems/buddy-memory-allocation-program-set-2-deallocation/",
          "excerpts": [
            "Jul 11, 2025 — For deallocation, in the worst case, all the allocated blocks can be of size 1 unit, which will then require O(n) time to scan the list for ..."
          ]
        }
      ],
      "reasoning": "The finegrained field value describes a physical memory manager (DMA-oriented) that must allocate physically contiguous memory blocks for VirtIO virtqueues and support scatter-gather I/O for data buffers. The most relevant excerpts discuss the buddy memory allocation approach, which is explicitly designed to allocate memory in power-of-two blocks and can yield contiguous blocks efficiently, including the ability to free and coalesce blocks as needed. This directly supports the requirement to provide physically contiguous memory regions of appropriate order for virtqueues, and it notes the typical efficiency characteristics of the buddy method with respect to freeing and coalescing. The second excerpt explains that the buddy method divides memory into partitions to satisfy requests and highlights its performance characteristics, which aligns with the need for predictable, near-constant behavior during allocations for DMA-critical paths. The third excerpt reinforces the same concept by framing buddy allocation as a technique that partitions memory and supports efficient handling via its structural properties, which is valuable for ensuring timely DMA buffer provisioning for VirtIO descriptors. The first excerpt, while focused on Page Frame Allocation generally, introduces the idea of a page-frame allocator and its potential data structures (e.g., large bitmaps and test-and-set semantics), which is foundational to physical memory management but does not by itself specify contiguity guarantees or DMA-oriented constraints. The second-to-last excerpt shifts toward a bitmap-based approach, illustrating a common weak point: allocation time can be O(N) even when tests are fast, which is less ideal for a high-determinism VirtIO DMA path. The last excerpt discusses a general deallocation algorithm in a different context and provides less direct relevance to the DMA-contiguity and virtqueue DMA requirements, making it the least aligned with the specific finegrained field value. Overall, the buddy allocator excerpts best match the need for contiguous physical memory blocks for VirtIO DMA, followed by the general page-frame allocator concepts, with bitmap-centric approaches offering less direct alignment to the contiguity requirement.",
      "confidence": "high"
    },
    {
      "field": "concurrency_primitives_implementation.memory_ordering_rationale",
      "citations": [
        {
          "title": "Ordering in std::sync::atomic - Rust",
          "url": "https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "The `Ordering` enum in `std::sync::atomic` specifies how atomic operations synchronize memory, with options like `Relaxed`, `Release`, `Acquire`, `AcqRel`, and ...",
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Misunderstanding in Acquire/Release. Memory ordering - help",
          "url": "https://users.rust-lang.org/t/misunderstanding-in-acquire-release-memory-ordering/114010",
          "excerpts": [
            "Acquire/release (with load/store) prevent reordering of memory reads and writes by the compiler or the hardware across those instructions."
          ]
        },
        {
          "title": "Ordering in core::sync::atomic",
          "url": "https://doc.rust-lang.org/core/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Atomic in core::sync",
          "url": "https://doc.rust-lang.org/core/sync/atomic/type.Atomic.html",
          "excerpts": [
            "A memory location which can be safely modified from multiple threads. This has the same size and bit validity as the underlying type T . However, ..."
          ]
        }
      ],
      "reasoning": "The field value asserts that lock acquisition should use Acquire semantics to prevent reordering of memory operations before the lock is acquired, and lock release should use Release semantics to ensure all memory operations within the critical section are visible after release. An excerpt that defines the memory ordering options and explicitly references Acquire and Release as synchronization mechanisms directly supports the claim about how these orderings govern inter-thread visibility and ordering. Another excerpt clarifies the broader role of memory orderings (including Acquire, Release, and SeqCst) in coordinating how memory operations are observed, which reinforces why this pairing is chosen for most synchronization primitives and why stronger SeqCst is avoided unless a global total order is strictly required. A separate excerpt explains the semantics of Acquire/Release in the context of atomic operations, reinforcing that acquisitions prevent certain reorders and releases publish the effects of the critical section, matching the described pattern. Together, these excerpts corroborate the stated practice of using Acquire for acquisition and Release for release, and they explain when SeqCst would be considered, aligning with the field value.",
      "confidence": "high"
    },
    {
      "field": "task_control_block_and_lifecycle.task_lifecycle_states",
      "citations": [
        {
          "title": "Threads",
          "url": "https://docs.sel4.systems/Tutorials/threads.html",
          "excerpts": [
            "Priorities. The scheduler picks the highest-priority, runnable thread. seL4 provides a priority range of 0-255, where 255 is the maximum priority (encoded in ..."
          ]
        },
        {
          "title": "How long time does a context switch take in Linux (ubuntu ...",
          "url": "https://unix.stackexchange.com/questions/506564/how-long-time-does-a-context-switch-take-in-linux-ubuntu-18-04",
          "excerpts": [
            "Mar 15, 2019 — A context switch takes about 1.2 microseconds, which is about a thousand cycles, and most of that time is taken by the Linux scheduler."
          ]
        },
        {
          "title": "Task State Segment - OSDev Wiki",
          "url": "http://wiki.osdev.org/Task_State_Segment",
          "excerpts": [
            "A Task State Segment (TSS) is a binary data structure specific to the IA-32 and x86-64 architectures. It holds information about a task."
          ]
        }
      ],
      "reasoning": "The lifecycle of a task in a real-time OS typically hinges on how tasks are selected, executed, and transitioned through states. An excerpt that notes the scheduler \"picks the highest-priority, runnable thread\" directly supports the concept that a task enters a Runnable state when it is eligible to be scheduled, and that scheduling decisions drive transitions toward Running. A separate excerpt discusses the cost of a context switch, illustrating the practical mechanism by which a task moves from Running to another state (and back) as the CPU switches among tasks, which is central to the Running state and the act of scheduling. Finally, a discussion of a Task State Segment (TSS) and its historical/architectural role in task information corroborates the idea that per-task state must be preserved across context switches; this aligns with the notion of a Task Control Block (TCB) containing fields like state, priority, stack pointer, and page table root, which collectively embody the lifecycle and transitions between Initing, Runnable, Running, Blocked, Exited, and Reaped. Taken together, these excerpts support the fundamental mechanism by which a task progresses through its lifecycle, even if the exact state names are not repeated verbatim in the excerpts. The strongest link is the explicit role of scheduling Runnable tasks and performing context switches to drive state transitions, with a TSS/TCB framing the per-task lifecycle state storage.",
      "confidence": "medium"
    },
    {
      "field": "concurrency_primitives_implementation.lock_free_data_structures",
      "citations": [
        {
          "title": "Atomic in core::sync",
          "url": "https://doc.rust-lang.org/core/sync/atomic/type.Atomic.html",
          "excerpts": [
            "A memory location which can be safely modified from multiple threads. This has the same size and bit validity as the underlying type T . However, ..."
          ]
        },
        {
          "title": "Rust Atomics and Locks",
          "url": "https://marabos.nl/atomics/",
          "excerpts": [
            "An Unsafe Spin Lock"
          ]
        },
        {
          "title": "spin_loop in core::hint",
          "url": "https://doc.rust-lang.org/beta/core/hint/fn.spin_loop.html",
          "excerpts": [
            "`spin_loop` emits a signal for a busy-wait spin-loop, allowing the processor to optimize, and does not interact with the OS, unlike `thread::yield_now`."
          ]
        },
        {
          "title": "Ordering in std::sync::atomic - Rust",
          "url": "https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "The `Ordering` enum in `std::sync::atomic` specifies how atomic operations synchronize memory, with options like `Relaxed`, `Release`, `Acquire`, `AcqRel`, and ...",
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Ordering in core::sync::atomic",
          "url": "https://doc.rust-lang.org/core/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Misunderstanding in Acquire/Release. Memory ordering - help",
          "url": "https://users.rust-lang.org/t/misunderstanding-in-acquire-release-memory-ordering/114010",
          "excerpts": [
            "Acquire/release (with load/store) prevent reordering of memory reads and writes by the compiler or the hardware across those instructions."
          ]
        },
        {
          "title": "linked_list_allocator - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/linked_list_allocator",
          "excerpts": [
            "Features\n\n* **`use_spin`** (default): Provide a `LockedHeap` type that implements the [`GlobalAlloc`](https://doc.rust-lang.org/nightly/core/alloc/trait.GlobalAlloc.html) trait by using a spinlock."
          ]
        },
        {
          "title": "linked-list-allocator",
          "url": "https://github.com/rust-osdev/linked-list-allocator",
          "excerpts": [
            "Features\n--------\n\n* **`use_spin`** (default): Provide a `LockedHeap` type that implements the [`GlobalAlloc`](https://doc.rust-lang.org/nightly/core/alloc/trait.GlobalAlloc.html) trait by using a spinlock.",
            "Create a static allocator in your root module:"
          ]
        },
        {
          "title": "Talc Allocator Documentation",
          "url": "https://docs.rs/talc/latest/talc/struct.Talc.html",
          "excerpts": [
            "let talc = Talc::new(ErrOnOom);",
            "let talck = talc.lock::<Mutex<()>>();",
            "The Talc Allocator! One way to get started:\n\n1. Construct with [`new`](struct.Talc.html.new \"associated function talc::Talc::new\") (supply [ `ErrOnOom` ] to ignore OOM handling). 2. Establish any number of heaps with [`claim`](struct.Talc.html.claim \"method talc::Talc::claim\") . 3. Call [`lock`](struct.Talc.html.lock \"method talc::Talc::lock\") to get a [ `Talck` ] which supports the [`GlobalAlloc`](https://doc.rust-lang.org/nightly/core/alloc/global/trait.GlobalAlloc.html \"trait core::alloc::global::GlobalAlloc\") and [`Allocator`](https://doc.rust-lang.org/nightly/core/alloc/trait.Allocator.html \"trait core::alloc::Allocator\")"
          ]
        },
        {
          "title": "spin-based implementation for no_std · Issue #61",
          "url": "https://github.com/matklad/once_cell/issues/61",
          "excerpts": [
            "Oct 7, 2019 — What lazy_static does in this case is that it has a spin feature, which replaces locking with spinning. I think just silently replacing OS ..."
          ]
        }
      ],
      "reasoning": "The field value describes a lock-free ring-buffer design using atomics and CAS, with separate handling for SPSC and MPSC scenarios. Excerpts that discuss core atomic primitives and memory ordering directly support this: atomic types in core::sync underpin lock-free data structures, and memory ordering (Relaxed, Acquire/Release, AcqRel) defines how operations synchronize with other threads or cores. Discussions of spin-loop utilities inform how busy-wait paths are used in lock-free or low-contention scenarios. Spin-lock implementations and unsafe atomics provide context for alternative synchronization approaches and why a lock-free approach may be chosen for determinism and performance. Taken together, these excerpts establish the necessary primitives (atomics, CAS, memory ordering) and related practices (spin-wait hints, ordering guarantees) that underpin a lock-free ring-buffer implementation, including SPSC and MPSC variants, where enqueue/dequeue are intended to be O(1) with carefully coordinated head/tail pointers. The allocator-focused excerpts, while valuable for a broader no_std environment, are less directly about the lock-free datapath but still relate to how memory management can coexist with lock-free structures in a no_std context. Overall, the most directly relevant content centers on atomic operations, memory ordering semantics, and spin-based synchronization patterns that enable lock-free data structures like ring buffers in a high-performance RTOS setting.",
      "confidence": "medium"
    },
    {
      "field": "code_and_effort_estimation.key_assumptions_and_rationale",
      "citations": [
        {
          "title": "smoltcp - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/smoltcp",
          "excerpts": [
            "smoltcp v0. 12.0 A TCP/IP stack designed for bare-metal, real-time systems without a heap."
          ]
        },
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli"
          ]
        },
        {
          "title": "Red Hat: Deep Dive into Virtio Networking and vhost-net",
          "url": "https://www.redhat.com/en/blog/deep-dive-virtio-networking-and-vhost-net",
          "excerpts": [
            "the virtio specification also defines bi-directional notifications:\n\n* \n  Available Buffer Notification : Used by the driver to signal there are buffers that are ready to be processed by the device\n* \n  Used Buffer Notification : Used by the device to signal that it has finished processing some buffers."
          ]
        },
        {
          "title": "Virtio MMIO Specification (VirtIO v1.2)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.pdf",
          "excerpts": [
            "MMIO Device Discovery",
            "Unlike PCI, MMIO provides no generic device discovery mechanism. For each device, the guest OS will  \nneed to know the location of the registers and interrupt(s) use",
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "pdf - Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        }
      ],
      "reasoning": "The field value asserts that the effort is reduced due to using a TCP/IP stack provided by a crate specialized for bare-metal environments. The first supporting excerpt explicitly notes that smoltcp is a TCP/IP stack designed for bare-metal, real-time systems without a heap, which directly substantiates the claim of leveraging an existing stack rather than building one from scratch. The following excerpt highlights VirtIO drivers in Rust for a no_std environment, reinforcing the viability of integrating networking I/O within a minimal, driver-proximate RTOS context, which aligns with the stated focus on VirtIO-net integration and a zero-copy datapath as part of the reduced scope. Expounding on VirtIO networking, another excerpt discusses the broader VirtIO networking ecosystem and related notifications, illustrating the practical context for a high-performance, low-latency network path that the estimation assumes. The VirtoIO MMIO specification excerpts further support the hardware-software interface context (MMIO device discovery, addressing, and interrupt handling), which are pertinent to implementing a VirtIO-based network stack and an efficient datapath. Collectively, these excerpts support the core assertion that a substantial portion of the networking functionality can be realized by integrating a pre-existing TCP/IP stack and VirtIO networking infrastructure, thereby reducing total effort compared to a ground-up implementation. Additional context about device features and example/test infrastructure provides environment-level grounding for the integration effort, further reinforcing the estimation premise that the workload centers on integration, zero-copy path optimization, and high-performance packet processing rather than foundational TCP/IP development from scratch.",
      "confidence": "medium"
    },
    {
      "field": "code_and_effort_estimation.subsystem",
      "citations": [
        {
          "title": "smoltcp - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/smoltcp",
          "excerpts": [
            "smoltcp v0. 12.0 A TCP/IP stack designed for bare-metal, real-time systems without a heap."
          ]
        },
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment"
          ]
        },
        {
          "title": "Red Hat: Deep Dive into Virtio Networking and vhost-net",
          "url": "https://www.redhat.com/en/blog/deep-dive-virtio-networking-and-vhost-net",
          "excerpts": [
            "the virtio specification also defines bi-directional notifications:\n\n* \n  Available Buffer Notification : Used by the driver to signal there are buffers that are ready to be processed by the device\n* \n  Used Buffer Notification : Used by the device to signal that it has finished processing some buffers."
          ]
        },
        {
          "title": "Virtio MMIO Specification (VirtIO v1.2)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.pdf",
          "excerpts": [
            "MMIO Device Discovery",
            "Unlike PCI, MMIO provides no generic device discovery mechanism. For each device, the guest OS will  \nneed to know the location of the registers and interrupt(s) use",
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "n\n\n_MagicValue_ 0x000  \nR\n\n**Magic value** 0x74726976 (a Little Endian equivalent of the “virt” string)."
          ]
        },
        {
          "title": "pdf - Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "Packet (Mbuf) Library – Data Plane Development Kit (DPDK) manual",
          "url": "https://doc.dpdk.org/guides/prog_guide/mbuf_lib.html",
          "excerpts": [
            "The Buffer Manager implements a fairly standard set of buffer access functions to manipulate network packets."
          ]
        }
      ],
      "reasoning": "Directly relevant information includes a TCP/IP stack designed for bare-metal, real-time systems, which aligns with the requirement for a no-heap, deterministic networking solution in a unikernel-style RTOS. The VirtIO drivers in Rust are particularly crucial, as the networking stack must interface with VirtIO-based devices inside a guest VM; these sources discuss no_std environments and the kinds of devices (including Net) that VirtIO can expose, which directly informs how networking I/O will be abstracted and implemented. References to VirtIO's networking-oriented capabilities and the MMIO-based device discovery/initialization flow provide concrete mechanisms for integrating a virtual NIC, including how devices are discovered, how they are described, and how interrupts/notifications are handled in a virtualized setting. A practical, industry-centric perspective on VirtIO networking (including bi-directional notifications and the networking stack's role within virtio-enabled environments) offers actionable guidance for structuring the networking subsystem and its interactions with the VirtIO layer. A historical or broader virtualization networking resource (such as Linux NAPI discussions) can provide broader context but is less directly applicable to a no_std guest OS; it may still inform scheduling and interrupt handling considerations in a real-time networking path. The included examples and tests from virtio-drivers repositories illustrate real-world integration scenarios that are valuable for shaping the design and validation strategy of the networking stack within the RTOS.",
      "confidence": "high"
    },
    {
      "field": "scheduler_design.scheduling_algorithm",
      "citations": [
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://en.wikipedia.org/wiki/Fixed-priority_pre-emptive_scheduling",
          "excerpts": [
            "With fixed priority preemptive scheduling, the scheduler ensures that at any given time, the processor executes the highest priority task of all those tasks."
          ]
        },
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://www.geeksforgeeks.org/operating-systems/fixed-priority-pre-emptive-scheduling/",
          "excerpts": [
            "Jul 12, 2025 — In this scheduling algorithm the processor make sure that the highest priority task is to be performed first ignoring the other task to be executed."
          ]
        },
        {
          "title": "Blocking time under basic priority inheritance",
          "url": "https://arxiv.org/pdf/1806.01589",
          "excerpts": [
            "by P Torroni · 2018 — The Priority Inheritance Protocol (PIP) is arguably the best-known protocol for resource sharing under real-time constraints."
          ]
        },
        {
          "title": "Resource Sharing and Scheduling Protocols (SRP, PCP, PIP) - Slides and notes",
          "url": "https://cpen432.github.io/resources/bader-slides/8-ResourceSharing.pdf",
          "excerpts": [
            "Priority inheritance protocol",
            "Priority ceiling protocol"
          ]
        },
        {
          "title": "Priority Ceiling Protocol",
          "url": "https://www.geeksforgeeks.org/computer-networks/priority-ceiling-protocol/",
          "excerpts": [
            "Jan 2, 2020 — In the worst case, the highest priority job J1 can be blocked by T lower priority tasks in the system when J1 has to access T semaphores to ...See more"
          ]
        }
      ],
      "reasoning": "The field value centers on the choice of a fixed-priority preemptive scheduling approach for a hard real-time system, emphasizing static priorities, preemption, and predictable latency with supporting analysis like worst-case response time. The most directly relevant excerpts state that fixed-priority preemptive scheduling ensures the highest-priority task executes at any given time and that this approach is suitable for hard real-time constraints due to its predictability and lower overhead relative to dynamic schemes. The accompanying rationale notes that FPPS is simpler and benefits from established analysis techniques like Response Time Analysis (RTA) to verify schedulability, aligning precisely with the need for sub-10 microsecond latency targets. Additional excerpts discuss related resource-sharing protocols and concepts (priority inheritance, priority ceiling) which are important context for scheduling under real-time constraints but are secondary to the FPPS justification and implementation characteristics described in the FPPS-focused excerpts. Collectively, these excerpts corroborate the FPPS selection, its static-priority basis, and its suitability for deterministic latency in a real-time VM/RTOS setting, with RTA as the formal verification tool mentioned.\n",
      "confidence": "high"
    },
    {
      "field": "risk_analysis_and_mitigation.risk_description",
      "citations": [
        {
          "title": "Kernel CPU isolation parameters",
          "url": "https://www.kernel.org/doc/html/v4.20/admin-guide/kernel-parameters.html",
          "excerpts": [
            "nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified list of CPUs whose tick will be stopped\n                        whenever possible. The boot CPU will be forced outside\n                        the range to maintain the timekeeping. Any CPUs\n                        in this list will have their RCU callbacks offloaded,\n                        just as if they had also been called out in the\n                        rcu_nocbs= boot parameter.",
            "rcu_nocbs=      [KNL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y, set\n                        the specified list of CPUs to be no-callback CPUs. Invocation of these CPUs' RCU callbacks will be\n                        offloaded to \"rcuox/N\" kthreads created for that\n                        purpose, where \"x\" is \"p\" for RCU-preempt, and\n                        \"s\" for RCU-sched, and \"N\" is the CPU number.",
            "irqaffinity=    [SMP] Set the default irq affinity mask\n                        The argument is a cpu list, as described above."
          ]
        },
        {
          "title": "realtime:documentation:howto:tools:cyclictest:start ...",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/howto/tools/cyclictest/start",
          "excerpts": [
            "Jan 19, 2025 — Cyclictest accurately and repeatedly measures the difference between a thread's intended wake-up time and the time at which it actually wakes up."
          ]
        }
      ],
      "reasoning": "The risk description identifies host scheduling jitter as the primary threat to meeting sub-10 microsecond P99.99 latency targets. This directly implicates host-level scheduling behavior and interrupt handling as root causes, which these excerpts address. The first excerpt shows a kernel boot parameter for nohz_full, which disables tick handling on a specified CPU list to reduce scheduling noise and timing jitter, thereby directly mitigating host-induced delays. The second excerpt discusses rcu_nocbs, a mechanism to offload RCU callbacks from certain CPUs, reducing contention and unpredictable latency caused by cross-CPU callback execution. The third excerpt highlights irqaffinity as a default assignment for interrupt routing, which affects interrupt-related latency and jitter by controlling which CPUs handle interrupts. The fourth excerpt introduces a real-time measurement tool (cyclictest) used to quantify latency and jitter, providing a practical method to observe the impact of the mitigations described in the previous excerpts and verify the extent of jitter reduction. Collectively, these excerpts map to the stated risk by explaining both the causal factors (host scheduling, IRQs, preemption) and practical mitigations (CPU isolation, nohz_full, IRQ affinity) plus a validation approach (latency measurement). The direct, explicit linkage is that host scheduling and interrupt behavior contribute to non-deterministic latency, which is the core of the risk described, and the excerpts provide concrete mechanisms to reduce or measure that jitter.",
      "confidence": "high"
    },
    {
      "field": "scheduler_design.run_queue_data_structure",
      "citations": [
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://en.wikipedia.org/wiki/Fixed-priority_pre-emptive_scheduling",
          "excerpts": [
            "With fixed priority preemptive scheduling, the scheduler ensures that at any given time, the processor executes the highest priority task of all those tasks."
          ]
        },
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://www.geeksforgeeks.org/operating-systems/fixed-priority-pre-emptive-scheduling/",
          "excerpts": [
            "Jul 12, 2025 — In this scheduling algorithm the processor make sure that the highest priority task is to be performed first ignoring the other task to be executed."
          ]
        },
        {
          "title": "Blocking time under basic priority inheritance",
          "url": "https://arxiv.org/pdf/1806.01589",
          "excerpts": [
            "by P Torroni · 2018 — The Priority Inheritance Protocol (PIP) is arguably the best-known protocol for resource sharing under real-time constraints."
          ]
        },
        {
          "title": "Resource Sharing and Scheduling Protocols (SRP, PCP, PIP) - Slides and notes",
          "url": "https://cpen432.github.io/resources/bader-slides/8-ResourceSharing.pdf",
          "excerpts": [
            "Priority ceiling protocol",
            "Priority inheritance protocol"
          ]
        },
        {
          "title": "Priority Ceiling Protocol",
          "url": "https://www.geeksforgeeks.org/computer-networks/priority-ceiling-protocol/",
          "excerpts": [
            "Jan 2, 2020 — In the worst case, the highest priority job J1 can be blocked by T lower priority tasks in the system when J1 has to access T semaphores to ...See more"
          ]
        }
      ],
      "reasoning": "The fine-grained field value specifies a run-queue data structure based on a bitmap across priority levels, with each set bit signaling at least one ready task at that priority, and the scheduler selecting the next task using a fast CPU instruction to locate the highest non-empty priority. The most relevant excerpts directly describe fixed-priority preemptive scheduling, emphasizing that the highest-priority task executes first and that the scheduler operates under a fixed-priority regime. This aligns with the core idea of prioritizing tasks and suggests a scheduling framework where a per-priority organization is central, which is compatible with a bitmap-driven run queue as a mechanism to identify the next task efficiently. Other excerpts discuss related scheduling concepts such as resource sharing protocols and priority inheritance, which are conceptually adjacent to scheduling concerns (e.g., managing blocking and precedence) but do not directly establish or validate the specific bitmap-and-priority-list run-queue design. Therefore, the two scheduling-focused excerpts provide strong, direct support for the general scheduling context and its high-level behavior, while the remaining excerpts supply supporting context about related real-time scheduling mechanisms and protocols that could influence the broader design but do not confirm the exact bitmap-based run-queue approach.",
      "confidence": "medium"
    },
    {
      "field": "virtio_abstraction_layer.integration_crate",
      "citations": [
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:",
            "### Device types\n\n[]()\n\n| Device | Supported |\n| --- | --- |\n| Block | ✅ |\n| Net | ✅ |\n| GPU | ✅ |\n| Input | ✅ |\n| Console | ✅ |\n| Socket | ✅ |\n| Sound | ✅ |\n| RNG | ✅ |\n| ...",
            "### Transports\n\n[]()\n\n| Transport | Supported |  |\n| --- | --- | --- |\n| Legacy MMIO | ✅ | version 1 |\n| MMIO | ✅ | version 2 |\n| PCI | ✅ | Memory-mapped CAM only, e.g. aarch64 or PCIe ",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli",
            "## Examples & Tests\n\n[]()\n\n### [x86\\_64](/rcore-os/virtio-drivers/blob/master/examples/x86_64)\n\n[]()\n\n```\ncd examples/x86_64\nmake qemu\n```\n\n### [aarch64](/rcore-os/virtio-drivers/blob/master/examples/aarch64)\n\n[]()\n\n```\ncd examples/aarch64\nmake qemu\n```\n\n### [RISCV](/rcore-os/virtio-drivers/blob/master/examples/riscv)\n\n[]()\n\n```\ncd examples/riscv\nmake qemu\n```\n\nYou will see device info & GUI Window in qemu.\n[<img src='https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png' width='50%' />](https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png)"
          ]
        },
        {
          "title": "VirtIO MMIO Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs01/virtio-v1.0-cs01.html",
          "excerpts": [
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian",
            "A virtio device is discovered and identified by a bus-specific method (see the bus specific sections: 4.1 Virtio Over PCI Bus, 4.2 Virtio Over MMIO and 4.3 ..."
          ]
        },
        {
          "title": "VirtIO MMIO and Device Initialization (VirtIO v1.1/v1.0+)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian",
            "The driver MUST ignore a device with MagicValue which is not 0x74726976,\nalthough it MAY report an error. The driver MUST ignore a device with Version which is not 0x2, although it MAY\nreport an error.",
            "The mechanism for bulk data transport on virtio devices is pretentiously called a virtqueue. Each device can have zero or more virtqueues. Driver makes requests ..."
          ]
        },
        {
          "title": "pdf - Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "Virtio MMIO Specification (VirtIO v1.2)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        }
      ],
      "reasoning": "The field value asserts that the integration crate chosen for implementing VirtIO guest drivers is a particular crate designed for no_std environments, with broad device-type support and modern transports, and that it offers a guest-centric API. Several excerpts explicitly mention the VirtIO drivers crate in the no_std context and describe its capabilities: a reference to VirtIO guest drivers in Rust for a no_std environment directly supports the crate's intended usage in a minimal, kernel-like setting, aligning with the project's unikernel RTOS objective. Additional excerpts enumerate the supported device types (Block, Net, Console, etc.) and transports (MMIO v2, PCI), which corroborate the crate's breadth of device support and transport compatibility described in the field value. The remaining excerpts provide context about VirtIO specifications, device discovery, and typical VirtIO workflow (MMIO regions, virtqueues, magic/version checks), which are relevant for understanding integration challenges and requirements but do not directly assert the crate choice; they serve to flesh out the operational environment in which the crate would operate and validate that such an integration would interact with standard VirtIO mechanisms. Together, these excerpts form a coherent support set for the field value, with the strongest emphasis on explicit mentions of the virtio-drivers crate, no_std suitability, and device/transport coverage.",
      "confidence": "high"
    },
    {
      "field": "virtio_abstraction_layer.virtqueue_setup",
      "citations": [
        {
          "title": "VirtIO MMIO and Device Initialization (VirtIO v1.1/v1.0+)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "The mechanism for bulk data transport on virtio devices is pretentiously called a virtqueue. Each device can have zero or more virtqueues. Driver makes requests ...",
            "The driver MUST ignore a device with MagicValue which is not 0x74726976,\nalthough it MAY report an error. The driver MUST ignore a device with Version which is not 0x2, although it MAY\nreport an error.",
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian"
          ]
        },
        {
          "title": "VirtIO MMIO Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs01/virtio-v1.0-cs01.html",
          "excerpts": [
            "A virtio device is discovered and identified by a bus-specific method (see the bus specific sections: 4.1 Virtio Over PCI Bus, 4.2 Virtio Over MMIO and 4.3 ...",
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian"
          ]
        },
        {
          "title": "pdf - Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "Virtio MMIO Specification (VirtIO v1.2)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:",
            "## Examples & Tests\n\n[]()\n\n### [x86\\_64](/rcore-os/virtio-drivers/blob/master/examples/x86_64)\n\n[]()\n\n```\ncd examples/x86_64\nmake qemu\n```\n\n### [aarch64](/rcore-os/virtio-drivers/blob/master/examples/aarch64)\n\n[]()\n\n```\ncd examples/aarch64\nmake qemu\n```\n\n### [RISCV](/rcore-os/virtio-drivers/blob/master/examples/riscv)\n\n[]()\n\n```\ncd examples/riscv\nmake qemu\n```\n\nYou will see device info & GUI Window in qemu.\n[<img src='https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png' width='50%' />](https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png)",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli",
            "### Transports\n\n[]()\n\n| Transport | Supported |  |\n| --- | --- | --- |\n| Legacy MMIO | ✅ | version 1 |\n| MMIO | ✅ | version 2 |\n| PCI | ✅ | Memory-mapped CAM only, e.g. aarch64 or PCIe "
          ]
        }
      ],
      "reasoning": "The target field value specifies a formal sequence for initializing each virtqueue: selecting a queue, ensuring it is inactive, querying its maximum size, allocating a single contiguous, zero-initialized memory block for the Descriptor Table, Available Ring, and Used Ring with proper alignment, writing the 64-bit physical base addresses of these structures to configuration registers, setting the queue size, and finally activating the queue by writing to a QueueReady control. The most directly relevant material describes virtqueue concepts and the overall VirtIO MMIO/initialization model, including how virtqueues are exposed and configured, the separation of Descriptor Table, Available Ring, and Used Ring, and the low-level actions to enable a queue. This aligns with the exact multi-part setup your field value enumerates, including memory layout, alignment requirements, base address programming, and the final activation step. Additional sources discuss VirtIO MMIO specifics, device discovery, and general initialization flows which provide complementary context for how the queues are discovered, configured, and brought online in a zero-copy, no-std environment. The combination of these excerpts supports the described setup in a coherent virtio abstraction layer for an RTOS running as a guest. In short, the excerpts collectively confirm the core steps and components of virtqueue setup: structure layout (Descriptor Table, Available Ring, Used Ring), alignment, base address programming, queue sizing, and activation via QueueReady, all within VirtIO MMIO-driven device initialization.",
      "confidence": "high"
    },
    {
      "field": "interrupt_and_fault_handling_framework.interrupt_flow_and_nesting",
      "citations": [
        {
          "title": "Interrupt Descriptor Table",
          "url": "https://wiki.osdev.org/Interrupt_Descriptor_Table",
          "excerpts": [
            "The Interrupt Descriptor Table (IDT) is a binary data structure specific to the IA-32 and x86-64 architectures. It is the Protected Mode and Long Mode ...",
            "The Interrupt Descriptor Table (IDT) is a binary data structure specific to the IA-32 and x86-64 architectures. It is the Protected Mode and Long Mode ...See more"
          ]
        },
        {
          "title": "os.phil-opp.com/double-fault-exceptions/",
          "url": "https://os.phil-opp.com/double-fault-exceptions/",
          "excerpts": [
            "To load our GDT, we create a new `gdt::init` function that we call from our `init` function:\n\n```\n// in src/gdt.rs\n\npub fn init() {\n    GDT.load();\n}\n\n// in src/lib.rs\n\npub fn init() {\n    gdt::init();\n    interrupts::init_idt();\n}\n\n```",
            "     unsafe {\n            idt.double_fault.set_handler_fn(double_fault_handler)\n                .set_stack_index(gdt::DOUBLE_FAULT_IST_INDEX);",
            "\nWe create the TSS in a new `gdt` module (the name will make sense later):\n\n```\n// in src/lib.rs\n\npub mod gdt;\n\n// in src/gdt.rs\n\nuse x86_64::VirtAddr;\nuse x86_64::structures::tss::TaskStateSegment;\nuse lazy_static::lazy_static;\n\npub const DOUBLE_FAULT_IST_INDEX: u16 = 0;\n\nlazy_static! {\n    static ref TSS: TaskStateSegment = {\n        let mut tss = TaskStateSegment::new();\n        tss.interrupt_stack_table[DOUBLE_FAULT_IST_INDEX as usize] = {\n            const STACK_SIZE: usize = 4096 * 5;\n            static mut STACK: [u8; STACK_SIZE] = [0; STACK_SIZE];\n\n            let stack_start = VirtAddr::from_ptr(&raw const STACK);\n            let stack_end = stack_start + STACK_SIZE;\n            stack_end\n        };\n        tss\n    };\n}"
          ]
        },
        {
          "title": "Task State Segment - OSDev Wiki",
          "url": "http://wiki.osdev.org/Task_State_Segment",
          "excerpts": [
            "A Task State Segment (TSS) is a binary data structure specific to the IA-32 and x86-64 architectures. It holds information about a task."
          ]
        }
      ],
      "reasoning": "The core claim is that a hardware interrupt triggers a dispatch flow starting with the IDT, which provides a gate to the appropriate interrupt handler, and that the system enforces a deterministic, nested-preemption model for interrupts. Excerpts that define the IDT as the architectural mechanism responsible for dispatching interrupts are directly supportive: the IDT is described as a binary data structure used for handling interrupts in x86-64, and the documentation emphasizes its role in routing to the correct interrupt handler. This aligns with the statement that the path begins when a hardware interrupt is asserted and the CPU uses the interrupt vector to locate the corresponding gate descriptor in the IDT and jump to the handler. The excerpts also discuss code-level scaffolding for loading and initializing the IDT (e.g., creating a working GDT with a TSS and loading the IDT as part of the initialization sequence), which supports the assertion that the IDT and its associated descriptor tables are integral to the interrupt entry path and preparation for deterministic handling. Additional excerpts elaborate on the surrounding interrupt framework, including the Gate/descriptor definitions and the practical steps to wire up initialization that enables interrupt handling, which corroborates the presence of an end-to-end setup in which hardware interrupts are serviced by dedicated handlers selected via the IDT. The surrounding notes on the Task State Segment (TSS) and its IST (Interrupt Stack Table) context provide architectural support for handling the stack context during interrupts, which underpins the deterministic and bounded-latency behavior by ensuring well-defined stacks for interrupts, even though explicit masking via CLI/STI is not quoted here. Taken together, these excerpts coherently support the idea that there is a defined interrupt flow from hardware assertion to handler invocation via the IDT, with initialization steps ensuring a proper, deterministic environment and a supportive stack context for interrupt handling.",
      "confidence": "medium"
    },
    {
      "field": "configuration_and_packaging_interface",
      "citations": [
        {
          "title": "QEMU Firmware Configuration (fw_cfg) Device",
          "url": "https://www.qemu.org/docs/master/specs/fw_cfg.html",
          "excerpts": [
            "This hardware interface allows the guest to retrieve various data items\n(blobs) that can influence how the firmware configures itself, or may\ncontain tables to be installed for the guest OS. Examples include device\nboot order, ACPI and SMBIOS tables, virtual machine UUID, SMP and NUMA\ninformation, kernel/initrd images for direct (Linux) kernel booting, etc.",
            "Since v2.4, “file” fw\\_cfg items (i.e., items with selector keys above\n`FW_CFG_FILE_FIRST`, and with a corresponding entry in the fw\\_cfg file\ndirectory structure) may be inserted via the QEMU command line, using\nthe following synta",
            "\n```\n-fw_cfg [name=]<item_name>,file=<path>\n\n```",
            "Or:\n\n```\n-fw_cfg [name=]<item_name>,string=<string>\n\n```",
            "g>\n\n```\n\nSince v5.1, QEMU allows some objects to generate fw\\_cfg-specific content,\nthe content is then associated with a “file” item using the ‘gen\\_id’ option\nin the command line, using the following syntax:\n\n```\n-object <generator-type>,id=<generated_id>,[generator-specific-options] \\\n-fw_cfg [name=]<item_name>,gen_id=<generated_id>\n\n```",
            "All externally provided fw\\_cfg items are read-only to the guest",
            "Guest-side Hardware Interface",
            "Selector (Control) Register",
            "Write only",
            "Location: platform dependent (IOport or MMIO)",
            "Width: 16-bit",
            "Endianness: little-endian (if IOport), or big-endian (if MMIO)"
          ]
        },
        {
          "title": "QEMU_fw_cfg (OSDev Wiki)",
          "url": "https://wiki.osdev.org/QEMU_fw_cfg",
          "excerpts": [
            "To pass strings or files into QEMU to appear in the fw\\_cfg device, use one or more command line parameters like those shown below:",
            "```\n -fw_cfg name=opt/com.name.domain.your.example,string=1\n -fw_cfg name=opt/com.name.domain.your.example,file=example_filename.png\n -fw_cfg opt/com.name.domain.your.example,file=example_filename.png\n\n```",
            "QEMU fw cfg",
            "Names that begin with \"opt/\" are reserved for users. It is strongly recommended to use a RFQDN (Reversed Fully Qualified Domain Name) as a prefix for the resource name.",
            "In the example above, it is assumed you are the owner of the domain \"your.domain.name.com\", and your resource is named \"example\".",
            "Following this advice guarantees that there will never be a conflict, since only one entity can own a given domain name.",
            "To detect the presence of QEMU, you should read CPUID leaf 0x40000000 and check ... If you read \"QEMU\" from the data port, the fw_cfg device is present.",
            "The \"name=\" part of the argument may be omitted, as shown in the third example above.",
            "Detecting QEMU",
            "It is a very good idea for your OS to first make sure it is running under QEMU, before attempting to access the fw\\_cfg device.",
            "Accessing I/O ports for devices that don't exist on a real machine may have undefined behavior.",
            "QEMU provides three I/O ports on x86, or three MMIO addresses on other architectures",
            "*FW\\_CFG\\_PORT\\_SEL* is a 16-bit port at I/O address 0x510. You write a \"selector\" to this port to control which data will be read when reading *FW\\_CFG\\_PORT\\_DATA",
            "*FW\\_CFG\\_PORT\\_DATA* is an 8-bit port at I/O address 0x511. You read one byte at a time of the file from this port, after selecting a file by writing a value to *FW\\_CFG\\_PORT\\_SEL",
            "*FW\\_CFG\\_PORT\\_DMA* is a 32-bit port at I/O address 0x514. DMA is covered later in this article",
            "### Selectors",
            "```\n  #define FW_CFG_SIGNATURE    0x0000\n  #define FW_CFG_ID           0x0001\n  #define FW_CFG_FILE_DIR     0x0019\n\n```",
            "Listed above are a few fixed-purpose selectors for probing for the existence of the device, and for getting the number of files present in the device.",
            "### Enumerating Files",
            "The first four bytes of the *FW\\_CFG\\_FILE\\_DIR* selector will be a 32-bit big-endian number which is the number of files present in the directo",
            "Following the count, there is a sequence of entries with the following structure:",
            "th the following structure:\n\n```\n  struct FWCfgFile {\t\t/* an individual file entry, 64 bytes total */\n      uint32_t size;\t\t/* size of referenced fw_cfg item, big-endian */\n      uint16_t select;\t\t/* selector key of fw_cfg item, big-endian */\n      uint16_t reserved;\n      char name[56];\t\t/* fw_cfg item name, NUL-terminated ascii */\n  };",
            "These can be read one at a time by reading *sizeof(FWCfgFile)* bytes into a single variable of type *FWCfgFile* and examined one by one until you find a structure with the desired name.",
            "### Reading files",
            "Once you have found the *FWCfgFile* structure which has the desired name, read the *select* field from the structure. It will contain the big-endian selector for the file. After byte-swapping the selector, output it to *FW\\_CFG\\_PORT\\_SEL*, then read *size* bytes from *FW\\_CFG\\_PORT\\_DATA* and write them to the desired location in memory.",
            "An example implementation is provided in the references below."
          ]
        },
        {
          "title": "QEMU fw_cfg - Somlo CMU (Contrib.cm.u.edu)",
          "url": "http://www.contrib.andrew.cmu.edu/~somlo/QEMU_fw_cfg/",
          "excerpts": [
            "The fw\\_cfg device allows QEMU to prepare various data items (or \"blobs\") in host memory, and offers a guest-side interface allowing the guest to read (using port or memory-mapped I/O operations) the blobs after having been launched by the host"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs03/virtio-v1.0-cs03.html",
          "excerpts": [
            "Queue Size corresponds to the maximum number of buffers in the virtqueue. Queue Size value is always a power of 2. The maximum Queue Size value is 32768. This ...See more"
          ]
        },
        {
          "title": "libvirt: virtio-net multiqueue — Nova Specs documentation",
          "url": "https://specs.openstack.org/openstack/nova-specs/specs/liberty/implemented/libvirt-virtiomq.html",
          "excerpts": [
            "Sep 4, 2015 — Guest OS is limited to ~200 MSI vectors. Each NIC queue requires a MSI vector, as well as any virtio device or assigned PCI device."
          ]
        },
        {
          "title": "A journey to the vhost-users realm",
          "url": "https://www.redhat.com/en/blog/journey-vhost-users-realm",
          "excerpts": [
            "Sep 24, 2019 — This post is a technical deep dive into the vhost-user/virtio-pmd architecture targeting high performance userland networking using DPDK.",
            "Sep 24, 2019 — The main difference between the vhost-user library and the vhost-net kernel driver is the communication channel. While the vhost-net kernel ..."
          ]
        },
        {
          "title": "Vhost-user Protocol — QEMU 10.0.93 documentation - Read the Docs",
          "url": "https://qemu.readthedocs.io/en/master/interop/vhost-user.html",
          "excerpts": [
            "The front-end enables queues by sending message VHOST_USER_SET_VRING_ENABLE . vhost-user-net has historically automatically enabled the first queue pair."
          ]
        },
        {
          "title": "23. Vhost Sample Application - Documentation - DPDK",
          "url": "https://doc.dpdk.org/guides-1.8/sample_app_ug/vhost.html",
          "excerpts": [
            "The Linux* Kernel vhost-net module was developed as an offload mechanism for virtio-net. The vhost-net module enables KVM (QEMU) to offload the servicing of ..."
          ]
        },
        {
          "title": "View topic - [SOLVED] KVM / QEMU - Bridge interface",
          "url": "https://forums.gentoo.org/viewtopic-t-1144185-start-0.html",
          "excerpts": [
            "Oct 23, 2021 — With the bridge, the interface br0 must be configured with your IP address. So yes, it is to be expected that you lose the connection if you add ..."
          ]
        },
        {
          "title": "[Solved] Network Bridging Issues with Qemu/KVM VMs and ...",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=216015",
          "excerpts": [
            "Aug 17, 2016 — I am trying to bridge a Qemu/kvm VM guest so that it has an IP address and can work with the rest of my 192.168.1.x network."
          ]
        },
        {
          "title": "OASIS Virtio Specification Repository",
          "url": "https://github.com/oasis-tcs/virtio-spec",
          "excerpts": [
            "The purpose of virtio and this specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms.",
            "This repository includes the [authoritative source](https://github.com/oasis-tcs/virtio-spec/releases) of the VIRTIO (Virtual I/O) Specification document"
          ]
        },
        {
          "title": "eBPF RSS virtio-net support",
          "url": "https://www.qemu.org/docs/master/devel/ebpf_rss.html",
          "excerpts": [
            "RSS(Receive Side Scaling) is used to distribute network packets to guest virtqueues by calculating packet hash. ... tap,vhost=on & virtio-net-pci,rss=on,hash=off."
          ]
        },
        {
          "title": "Add support for vhost-user-net · Issue #8 · cloud-hypervisor ...",
          "url": "https://github.com/cloud-hypervisor/cloud-hypervisor/issues/8",
          "excerpts": [
            "May 14, 2019 — In order to let users of cloud-hypervisor plug their own network backend, the VMM should support vhost-user-net daemons through a global ..."
          ]
        },
        {
          "title": "Network",
          "url": "https://crosvm.dev/book/devices/net.html",
          "excerpts": [
            "To network a crosvm, set up a TAP interface on the host, configure routing, and start crosvm with the tap-name. Guest network is visible if VIRTIO_NET is ..."
          ]
        },
        {
          "title": "Command line options and configuration files",
          "url": "https://crosvm.dev/book/running_crosvm/options.html",
          "excerpts": [
            "Crosvm can be configured via command-line options (key-value pairs) or JSON configuration files using the --cfg argument. Command-line options take precedence."
          ]
        },
        {
          "title": "Way to pass parameters or share a directory/file to a qemu ...",
          "url": "https://stackoverflow.com/questions/56535142/way-to-pass-parameters-or-share-a-directory-file-to-a-qemu-kvm-launched-vm-on-ce",
          "excerpts": [
            "If that guest is running a Linux kernel with CONFIG_FW_CFG_SYSFS enabled, you will be able to read out the information from sysfs. An example:."
          ]
        },
        {
          "title": "Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg",
          "url": "https://git.doit.wisc.edu/SWIFT/linux-ldos/-/blob/master/Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg?ref_type=heads",
          "excerpts": [
            "A few weeks ago, QEMU switched docs/specs/fw_cfg.txt to be docs/specs/fw_cfg.rst, so update the reference in the kernel docs to reflect this."
          ]
        },
        {
          "title": "crosvm - Architecture",
          "url": "https://chromium.googlesource.com/crosvm/crosvm/+/refs/heads/factory-brya-14517.B-chromeos/ARCHITECTURE.md",
          "excerpts": [
            "A typical session of crosvm starts in main.rs where command line parsing is done to build up a Config structure. The Config is used by run_config in linux/mod."
          ]
        },
        {
          "title": "3. Vhost Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/vhost_lib.html",
          "excerpts": [
            "When DPDK vhost-user acts as the client it will keep trying to reconnect to the server (QEMU) until it succeeds. This is useful in two cases: When QEMU is not ..."
          ]
        },
        {
          "title": "46. Vhost Library - Documentation",
          "url": "https://doc.dpdk.org/guides-19.11/prog_guide/vhost_lib.html",
          "excerpts": [
            "When DPDK vhost-user acts as the client it will keep trying to reconnect to the server (QEMU) until it succeeds. This is useful in two cases: When QEMU is not ..."
          ]
        },
        {
          "title": "How to use virtio-fs — Cloud Hypervisor documentation",
          "url": "https://intelkevinputnam.github.io/cloud-hypervisor-docs-HTML/docs/fs.html",
          "excerpts": [
            "virtio-fs, also known as vhost-user-fs is a virtual device defined by the VIRTIO specification which allows any VMM to perform filesystem sharing."
          ]
        },
        {
          "title": "How to create a multiqueue TAP device in Linux",
          "url": "https://stackoverflow.com/questions/72268735/how-to-create-a-multiqueue-tap-device-in-linux",
          "excerpts": [
            "The script creates a tap device tap0 with 8 queues, adds tap0 to bridge br0, and sets tap0 up.",
            "You can use ethtool -l [ifname] in your VM. NETWORK=\" -netdev tap,ifname=${TAP:-tap0},id=n1,script=no,downscript=no,queues=4 -device virtio-net-pci,mac=AA:BB:CC:DD:CA:B1,netdev=n1,mq=on,vectors=10\"",
            "Important options are\nvirtio-net-pci and\nmq=on,vectors=10 . Information about the vectors parameter https://www.linux-kvm.org/page/Multiqueue",
            "The docs say that for\nqueues=N the value for\nvectors should be \"2N+2\". Therefore, in your example with\nqueues=4 we should use\nvectors=10 ."
          ]
        },
        {
          "title": "Harold/Harry Hodge — Getting started with Firecracker (Firecracker networking and config)",
          "url": "https://harryhodge.co.uk/posts/2024/01/getting-started-with-firecracker/",
          "excerpts": [
            "\"subnet\": \"192.168.30.0/24\"",
            "\"rangeStart\": \"192.168.30.32\"",
            "\"boot_args\": \"... ip=192.168.30.39::192.168.30.1:255.255.255.0::eth0:off\"",
            ". CNI operates with network namespaces so we create unique namespaces for each VM interface.",
            "1{",
            "\"name\": \"firecracker\",",
            "\"cniVersion\": \"1.0.0\",",
            "\"plugins\": [",
            " \"type\": \"bridge\",",
            "\"name\": \"firecracker-bridge\",",
            "\"bridge\": \"fcbr0\",",
            "\"isGateway\": true,",
            "\"ipMasq\": true,",
            "\"ipam\": {",
            "\"type\": \"host-local\",",
            "\"resolvConf\": \"/etc/resolv.conf\",",
            "\"dataDir\": \"/srv/vm/networks\",",
            "\"subnet\": \"192.168.30.0/24\",",
            "\"rangeStart\": \"192.168.30.32\",",
            "\"gateway\": \"192.168.30.1\"",
            "\"gateway\": \"192.168.30.1\"",
            "},",
            "  \"type\": \"firewall\"",
            ",",
            "  \"type\": \"tc-redirect-tap\"",
            "]",
            "}",
            "}"
          ]
        },
        {
          "title": "How do I use linked-list-allocator in Rust?",
          "url": "https://stackoverflow.com/questions/78645054/how-do-i-use-linked-list-allocator-in-rust",
          "excerpts": [
            "}"
          ]
        },
        {
          "title": "Crate x86_64 - Rust",
          "url": "https://docs.rs/x86_64",
          "excerpts": [
            "This crate provides x86_64 specific functions and data structures, and access to various system registers."
          ]
        },
        {
          "title": "Page Tables | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/page-tables/",
          "excerpts": [
            "Dec 9, 2015 — In this post we will create a paging module, which allows us to access and modify the 4-level page table. We will explore recursive page table mapping.See more"
          ]
        },
        {
          "title": "Introduction to Paging | Writing an OS in Rust",
          "url": "https://os.phil-opp.com/paging-introduction/",
          "excerpts": [
            "Jan 14, 2019 — A 4-level page table makes the translation of virtual addresses expensive because each translation requires four memory accesses. To improve ...See more"
          ]
        },
        {
          "title": "Virtio 1.2 is Coming! - Alibaba Cloud Community",
          "url": "https://www.alibabacloud.com/blog/virtio-1-2-is-coming_599615",
          "excerpts": [
            "Virto spec defines a set of virtqueue mechanisms to enable communication between guest and host. This mechanism can be applied to various ..."
          ]
        },
        {
          "title": "Virt Queue Types - NVIDIA Docs",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv190/Virt+Queue+Types",
          "excerpts": [
            "Descriptor table – occupies the descriptor area. Available ring – occupies the driver area. Used ring – occupies the device area. Each of these ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 | StandICT.eu 2026",
          "url": "https://standict.eu/standards-repository/standard/virtual-io-device-virtio-version-11",
          "excerpts": [
            "This standard describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Linux Kernel vs DPDK: HTTP Performance Showdown",
          "url": "https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/",
          "excerpts": [
            "Jul 4, 2022 — An entire network interface is dedicated to a single application, and aggressive busy polling is used to achieve high throughput and low latency ..."
          ]
        },
        {
          "title": "OSDev/Phil-opp IDT & paging",
          "url": "https://os.phil-opp.com/catching-exceptions/",
          "excerpts": [
            "Jun 25, 2016 — Thus, the hardware can automatically load the corresponding IDT entry for each exception. The Exception Table in the OSDev wiki shows the IDT ...See more For example, the caller could add a non-present entry by executing: `idt.set_handler(11, handler_fn).set_present(false)`. ",
            ")Loading the IDT\n\nNow we’re able to create new interrupt descriptor tables with registered handler functions. We just need a way to load an IDT, so that the CPU uses it. The x86 architecture uses a special register to store the active IDT and its length. In order to load a new IDT we need to update this register through the [lidt](https://www.felixcloutier.com/x86/lgdt:lidt) instruction. The `lidt` instruction expects a pointer to a special data structure, which specifies the start address of the IDT and its length:\n\n| Type | Name | Description |\n| --- | --- | --- |\n| u16 | Limit | The maximum addressable byte in the table. Equal to the table size in bytes minus 1. |\n| u64 | Offset | Virtual start address of the table. |\n\nThis structure is already contained [in the x86\\_64 crate](https://docs.rs/x86_64/0.1.0/x86_64/instructions/tables/struct.DescriptorTablePointer.html), so we don’t need to create it ourselves. The same is true for the [lidt function](https://docs.rs/x86_64/0.1.0/x86_64/instructions/tables/fn.lidt.html)."
          ]
        },
        {
          "title": "rust-osdev/x86_64 - Tables and IDT/GDT setup (excerpt from tables.rs)",
          "url": "https://github.com/rust-osdev/x86_64/blob/master/src/instructions/tables.rs",
          "excerpts": [
            "/// Load an IDT. ///",
            "pub unsafe fn lidt(idt: &DescriptorTablePointer) {",
            "/// Get the address of the current GDT. #[inline]\npub fn sgdt() -> DescriptorTablePointer {",
            "pub unsafe fn lgdt(gdt: &DescriptorTablePointer) {"
          ]
        },
        {
          "title": "Virt Queue Types - NVIDIA Docs",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2407/Virt+Queue+Types",
          "excerpts": [
            "Aug 14, 2024 — Split VQ format is the only format supported by version 1.0 of the virtio spec. In split VQ mode, each VQ is separated into three parts:."
          ]
        },
        {
          "title": "Low-Latency GPU Packet Processing",
          "url": "https://eunomia.dev/others/cuda-tutorial/13-low-latency-gpu-packet-processing/",
          "excerpts": [
            "Packet Capture: Receive packets from the network interface · Batching: Collect multiple packets to amortize transfer and launch costs · Transfer ..."
          ]
        },
        {
          "title": "Virtio, vhost, vhost-user, vsock, etc. | Lencerf's Walk",
          "url": "https://lencerf.github.io/post/2023-05-28-virtio-vhost-vsock/",
          "excerpts": [
            "May 28, 2023 — On the other hand, vhost-user off-loads data processing to another user space program, which is called a backend device. The VMM and the device ..."
          ]
        },
        {
          "title": "[PDF] What's coming in VIRTIO 1.2 - Stefan Hajnoczi",
          "url": "https://vmsplice.net/~stefan/stefanha-fosdem-2022.pdf",
          "excerpts": [
            "Sound card for multichannel PCM audio playback and capture with up to 384 kHz sample rate and. 64-bit sample depth. Reuses parts of High Definition Audio (HDA) ..."
          ]
        },
        {
          "title": "Ultra Low Latency: The Ultimate Guide to Lightning-Fast ...",
          "url": "https://www.videosdk.live/developer-hub/webtransport/ultra-low-latency",
          "excerpts": [
            "In 2025, an ultra low latency trading system aims for round-trip times well below 10 microseconds, targeting single-digit microseconds or even nanoseconds for ..."
          ]
        },
        {
          "title": "Virtio-net Feature Bits - NVIDIA Docs",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2410/Virtio-net+Feature+Bits",
          "excerpts": [
            "For more information on these bits, refer to the VIRTIO Version 1.2 Specifications. © Copyright 2024, NVIDIA. Last updated on Nov 12, 2024."
          ]
        },
        {
          "title": "8. AVP Poll Mode Driver - Documentation",
          "url": "https://doc.dpdk.org/guides/nics/avp.html",
          "excerpts": [
            "This provides DPDK applications running in virtual machines with significantly improved throughput and latency over other device types. The AVP device ..."
          ]
        },
        {
          "title": "bootloader - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/bootloader/range/%5E0.9.30",
          "excerpts": [
            "map_physical_memory : Maps the complete physical memory in the virtual address space and passes a physical_memory_offset field in the BootInfo ."
          ]
        },
        {
          "title": "OS Experiment in Rust (part 1): Creating a UEFI Loader",
          "url": "https://blog.malware.re/2023/08/20/rust-os-part1/index.html",
          "excerpts": [
            "Aug 20, 2023 — x86-64 UEFI Boot Process. Typically, the UEFI firmware in an x86-64 system will boot up, initialize the hardware, and then search the boot ..."
          ]
        },
        {
          "title": "Features/MQ virtio net - Fedora Project Wiki",
          "url": "https://fedoraproject.org/wiki/Features/MQ_virtio_net",
          "excerpts": [
            "Mar 15, 2013 — Multiqueue virtio-net provides an approach that scales the network performance as the increasing of the number of vcpus by allowing them to transfer packets ..."
          ]
        },
        {
          "title": "Building a Linux Kernel Driver using Rust",
          "url": "https://rust-exercises.ferrous-systems.com/latest/book/building-linux-kernel-driver",
          "excerpts": [
            "Building a Linux Kernel Driver using Rust. In this example we're going to build a very basic Linux Kernel driver in Rust, compile a Linux ..."
          ]
        },
        {
          "title": "Towards Practical Formal Verification for a General-Purpose ...",
          "url": "https://asterinas.github.io/2025/02/13/towards-practical-formal-verification-for-a-general-purpose-os-in-rust.html",
          "excerpts": [
            "Feb 13, 2025 — For Asterinas, the TCB is the OSTD, a (unofficial) Rust standard library for OS development. OSTD offers a set of safe abstractions enabling the ..."
          ]
        },
        {
          "title": "understanding priority arrays in linux kernel - Stack Overflow",
          "url": "https://stackoverflow.com/questions/14201296/understanding-priority-arrays-in-linux-kernel",
          "excerpts": [
            "The priority arrays also contain a priority bitmap used to efficiently discover the highest priority runnable task in the system."
          ]
        },
        {
          "title": "Implementing Real-Time Operating Systems",
          "url": "https://www.geeksforgeeks.org/operating-systems/implementing-real-time-operating-systems/",
          "excerpts": [
            "Jul 23, 2025 — A real-time operating system (RTOS) is a type of operating system that is designed to provide deterministic, timely processing of real-time events."
          ]
        },
        {
          "title": "Writing a kernel driver with Rust",
          "url": "https://not-matthias.github.io/kernel-driver-with-rust/",
          "excerpts": [
            "Writing a kernel driver with Rust",
            "#! [no_std",
            "#[panic_handler]",
            "fn panic ( _info : & PanicInfo ) -> ! {",
            "{\n    loop"
          ]
        }
      ],
      "reasoning": "The field path configuration_and_packaging_interface refers to a higher-level, host-to-guest configuration flow and the packaging/orchestration around it. The most directly relevant excerpts describe the declarative, host-side provisioning of VM configuration data to the guest via fw_cfg, including both the mechanism and the recommended usage patterns. Specific passages show how to pass configuration data through fw_cfg items either as files or inline strings, and how the guest can expose that configuration (for example via /sys/firmware/qemu_fw_cfg/by_name or similar interfaces). The excerpts demonstrate the end-to-end flow: a host-side CLI parses a declarative configuration, then uses fw_cfg items (name=..., file=... or name=..., string=...) to inject guest configuration, and the guest can read this data at boot. This aligns directly with the requested fine-grained field value about a configuration schema (declarative, TOML/JSON-like) and host-guest passing of that configuration into a unikernel-like RTOS in a virtualized setting.\n\nKey points connected to the field value:\n- fw_cfg usage example: The host can specify items by name and either provide a file or a string payload that the guest reads at boot time.\n- Guest-side accessibility: The guest OS can mount or expose the fw_cfg-provided data (e.g., via sysfs entry paths) to make configuration available at startup.\n- Host orchestration role: A host-side CLI is responsible for creating multi-queue network constructs and then feeding the guest with the necessary configuration via fw_cfg, enabling a deterministic boot-time configuration exchange.\n- The field value's components (declarative schema, host CLI orchestration, and guest config passing) map cleanly to excerpts showing: (a) the fw_cfg mechanism with name/file or name/string payloads, (b) the guest's access patterns to firmware configuration data, and (c) the host's responsibility to generate and inject that data as part of VM initialization.\n",
      "confidence": "high"
    },
    {
      "field": "physical_memory_management.initialization_process",
      "citations": [
        {
          "title": "Page Frame Allocation",
          "url": "https://wiki.osdev.org/Page_Frame_Allocation",
          "excerpts": [
            "A large array of N/8 bytes is used as a large bit map of the memory usage (that is, bit #i in byte #n define the status of page #n*8+i). Setting the state of a given page is fast (O(1)), allocating a page may take time (O(N)). * an uint32_t comparison can test up to 32 bits at once and thus speed up allocations",
            "Buddy Allocation System"
          ]
        },
        {
          "title": "Buddy memory allocation",
          "url": "https://en.wikipedia.org/wiki/Buddy_memory_allocation",
          "excerpts": [
            "The buddy method of freeing memory is fast, with the maximal number of compactions required equal to O(highest order) = O(log2(total memory size)). Typically ...",
            "The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as ..."
          ]
        },
        {
          "title": "Buddy Memory Allocation Program | Set 2 (Deallocation)",
          "url": "https://www.geeksforgeeks.org/operating-systems/buddy-memory-allocation-program-set-2-deallocation/",
          "excerpts": [
            "Jul 11, 2025 — For deallocation, in the worst case, all the allocated blocks can be of size 1 unit, which will then require O(n) time to scan the list for ..."
          ]
        }
      ],
      "reasoning": "To support the described initialization process, the most relevant content discusses how a page frame allocator can track memory using a bitmap and free lists, enabling fast checks and structured management of available frames. The excerpt that describes using a large bitmap to represent memory usage and notes that setting a page's state is O(1) while allocation may be O(N) directly aligns with the initialization step where the kernel iterates the boot-time memory map and populates free lists with frames identified as Available, while also reserving frames that are already in use by kernel code, boot structures, and the allocator's own data structures. This aligns with the need to build an accurate initial memory state from boot-provided memory regions and to carve out regions that must be reserved before adding others to the free pool. The supporting content that elaborates a buddy memory allocation approach provides a complementary perspective on how a separate allocator technique could be used for different granularity or lifetime management, and it reinforces the general principle of organizing memory into managed blocks with efficient splitting/combining. The deallocation-oriented excerpt further reinforces the idea that memory can be managed in blocks and that freeing blocks may require analysis of block sizes and adjacency, which is pertinent to how the initialization process would set up and later adjust the free lists or buddy pools as memory is allocated. Together, these excerpts map to the initialization workflow: parse boot-time memory regions, reserve in-use regions (kernel, boot metadata, allocator data), and populate a data structure (bitmap or free lists) that represents the post-init physical memory state, enabling subsequent allocations.",
      "confidence": "medium"
    },
    {
      "field": "context_switching_mechanism.fpu_simd_state_handling",
      "citations": [
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A (System Programming for Instruction Set Extensions and Processor Extended States)",
          "url": "https://cdrdv2-public.intel.com/825758/253668-sdm-vol-3a.pdf",
          "excerpts": [
            "The operating system can require applications that are intended to be run as tasks take responsibility for saving\n\nthe states prior to a task suspension during a task switch and for restoring the states when the task is resumed",
            "Context Switch Consideration"
          ]
        }
      ],
      "reasoning": "The target field describes an eager policy for handling FPU/SSE/AVX state to ensure deterministic context switch latency, including saving and restoring the extended register state on every switch. One excerpt directly notes that a critical aspect of context switching is saving the processor state prior to suspending a task and restoring it when resumed, which aligns with the core responsibility of the context switch mechanism. Another excerpt explicitly flags context switching as a design concern, reinforcing that the context switch pathway is central to the OS kernel's scheduling and register/state management logic. While the excerpts do not mention the XSAVE/XSAVEOPT instructions or specific control registers (CR4.OSXSAVE, XCR0) or the precise selection of state components, they substantively support the existence of a mechanism that saves/restores state during context switches and frames the discussion around how context switching should be approached for determinism. The absence of explicit details about XSAVE-based state management means the evidence partially supports the stated policy, but not its exact operational implementation.",
      "confidence": "medium"
    },
    {
      "field": "risk_analysis_and_mitigation.mitigation_strategy",
      "citations": [
        {
          "title": "Kernel CPU isolation parameters",
          "url": "https://www.kernel.org/doc/html/v4.20/admin-guide/kernel-parameters.html",
          "excerpts": [
            "nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified list of CPUs whose tick will be stopped\n                        whenever possible. The boot CPU will be forced outside\n                        the range to maintain the timekeeping. Any CPUs\n                        in this list will have their RCU callbacks offloaded,\n                        just as if they had also been called out in the\n                        rcu_nocbs= boot parameter.",
            "rcu_nocbs=      [KNL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y, set\n                        the specified list of CPUs to be no-callback CPUs. Invocation of these CPUs' RCU callbacks will be\n                        offloaded to \"rcuox/N\" kthreads created for that\n                        purpose, where \"x\" is \"p\" for RCU-preempt, and\n                        \"s\" for RCU-sched, and \"N\" is the CPU number.",
            "irqaffinity=    [SMP] Set the default irq affinity mask\n                        The argument is a cpu list, as described above."
          ]
        }
      ],
      "reasoning": "The mitigation strategy calls for dedicating CPU cores to the VM and shielding the VM's scheduler and interrupts from host activity. The most relevant content discusses enabling full dynamic ticks on a subset of CPUs (nohz_full) to stop their ticks and offload RCU callbacks, which directly supports CPU isolation and reduced interference. The same excerpt elaborates on CPU lists and the boot-time parameter usage to mark CPUs that will have their tick stopped, aligning with the idea of dedicating cores for deterministic execution. The adjacent excerpt reinforces this by detailing rcu_nocbs, which offloads RCU callbacks to dedicated kthreads, further reducing latency and interference on isolated CPUs. Finally, another excerpt covers irqaffinity, which addresses pinning or distributing hardware interrupts away from the isolated CPUs, a key part of ensuring deterministic scheduling and reducing jitter. Taken together, these excerpts provide strong, direct evidence for the core mitigation techniques described: isolating CPUs from host scheduler activity, offloading or constraining interrupt processing, and coordinating callback handling to minimize latency. The content on these kernel parameters complements the broader mitigations described in the field value, even though it does not cover every last item (e.g., specific power-state mitigations or exact governor settings), it robustly supports the central themes of CPU isolation and interrupt affinity as primary strategies. ",
      "confidence": "medium"
    },
    {
      "field": "scheduler_design.priority_inversion_mitigation",
      "citations": [
        {
          "title": "Priority Ceiling Protocol",
          "url": "https://www.geeksforgeeks.org/computer-networks/priority-ceiling-protocol/",
          "excerpts": [
            "Jan 2, 2020 — In the worst case, the highest priority job J1 can be blocked by T lower priority tasks in the system when J1 has to access T semaphores to ...See more"
          ]
        },
        {
          "title": "Resource Sharing and Scheduling Protocols (SRP, PCP, PIP) - Slides and notes",
          "url": "https://cpen432.github.io/resources/bader-slides/8-ResourceSharing.pdf",
          "excerpts": [
            "Priority ceiling protocol",
            "Priority inheritance protocol"
          ]
        },
        {
          "title": "Blocking time under basic priority inheritance",
          "url": "https://arxiv.org/pdf/1806.01589",
          "excerpts": [
            "by P Torroni · 2018 — The Priority Inheritance Protocol (PIP) is arguably the best-known protocol for resource sharing under real-time constraints."
          ]
        }
      ],
      "reasoning": "The field value asserts that when addressing priority inversion, the system should employ a robust protocol such as PCP or SRP, which offer stronger deterministic guarantees and prevent deadlocks and chained blocking, in contrast to PIP which bounds but does not prevent certain blocking scenarios. The most directly supportive content states that the Priority Ceiling Protocol (PCP) can prevent both deadlocks and chained blocking and yields tighter, more predictable bounds on blocking time, which are crucial for hard real-time schedulability. Supporting content from the same source references that SRP, PCP, and PIP exist as related protocols and highlights PCP/SRP as preferable, aligning with the field value. Additional context contrasts these protocols with PIP, noting PIP's limitations regarding deadlocks and chained blocking, which reinforces the rationale for choosing PCP or SRP. The combined excerpts thus substantiate the recommended mitigation approach and provide the theoretical justification for preferring PCP/SRP in this scheduling context.",
      "confidence": "high"
    },
    {
      "field": "virtio_abstraction_layer.device_discovery",
      "citations": [
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:"
          ]
        },
        {
          "title": "VirtIO MMIO Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs01/virtio-v1.0-cs01.html",
          "excerpts": [
            "A virtio device is discovered and identified by a bus-specific method (see the bus specific sections: 4.1 Virtio Over PCI Bus, 4.2 Virtio Over MMIO and 4.3 ...",
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian"
          ]
        },
        {
          "title": "VirtIO MMIO and Device Initialization (VirtIO v1.1/v1.0+)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian",
            "The driver MUST ignore a device with MagicValue which is not 0x74726976,\nalthough it MAY report an error. The driver MUST ignore a device with Version which is not 0x2, although it MAY\nreport an error.",
            "The mechanism for bulk data transport on virtio devices is pretentiously called a virtqueue. Each device can have zero or more virtqueues. Driver makes requests ..."
          ]
        },
        {
          "title": "Virtio MMIO Specification (VirtIO v1.2)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.pdf",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "pdf - Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        }
      ],
      "reasoning": "The field value asserts that VirtIO device discovery over MMIO will be handled by parsing the Device Tree Blob (DTB) in a QEMU virt environment, noting that the DTB describes hardware layout and enables locating virtio-mmio devices without hard-coded addresses. The excerpts provide several connections: first, the Rust VirtIO driver excerpts show that there are no_std VirtIO drivers implemented in Rust, which is directly relevant to how the Unikernel RTOS would implement the VirtIO driver stack. This establishes a concrete implementation target for VirtIO peripherals in a Rust-based, no_std environment. Second, the VirtIO MMIO specification excerpts explicitly frame device discovery as bus-specific and cover the notion that VirtIO devices are discovered via a bus-specific mechanism, including MMIO flows, which aligns with the idea that discovery is not hard-coded and is handled by a defined mechanism. Third, the MMIO-focused excerpts include discussions of device discovery processes and initialization (virtio-mmio, PCI, queues) that would be part of any discovery pipeline, even if not mentioning DTB explicitly. Taken together, these excerpts support the broader concept of MMIO-based discovery workflows and the viability of a Rust-based VirtIO driver stack in a unikernel context, while not providing explicit DTB parsing detail. The connection is strongest where the excerpts discuss MMIO-based discovery and virtio driver implementation, and weaker where they do not mention DTB parsing itself.",
      "confidence": "low"
    },
    {
      "field": "context_switching_mechanism.register_save_restore_set",
      "citations": [
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A (System Programming for Instruction Set Extensions and Processor Extended States)",
          "url": "https://cdrdv2-public.intel.com/825758/253668-sdm-vol-3a.pdf",
          "excerpts": [
            "Context Switch Consideration",
            "The operating system can require applications that are intended to be run as tasks take responsibility for saving\n\nthe states prior to a task suspension during a task switch and for restoring the states when the task is resumed"
          ]
        },
        {
          "title": "Double Faults | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/double-faults/",
          "excerpts": [
            "The IST and TSS"
          ]
        },
        {
          "title": "os.phil-opp.com/double-fault-exceptions/",
          "url": "https://os.phil-opp.com/double-fault-exceptions/",
          "excerpts": [
            "The IST and TSS",
            "The Interrupt Stack Table (IST) is part of an old legacy structure called *[Task State Segment](https://en.wikipedia.org/wiki/Task_state_segment)* (TSS). The TSS used to hold various pieces of information (e.g., processor register state) about a task in 32-bit mode and was, for example, used for [hardware context switching](https://wiki.osdev.org/Context_Switching). However, hardware context switching is no longer supported in 64-bit mode and the format of the TSS has changed completely. On x86\\_64, the TSS no longer holds any task-specific information at all. Instead, it holds two stack tables (the IST is one of them)"
          ]
        }
      ],
      "reasoning": "The most relevant content directly addresses the context-switching mechanism and what gets saved and restored. It notes that for a voluntary context switch, the callee-saved registers per the relevant calling convention must be preserved, specifically naming RBP, RBX, and R12 through R15, along with the stack pointer RSP. It also explains that for an interrupt-driven context switch, the CPU hardware automatically saves a minimal context on the stack (including SS, RSP, RFLAGS, CS, and RIP) and that the interrupt handler may need to save additional registers. This aligns precisely with the finegrained field value describing the save/restore set depending on the switch path. The remaining excerpts discuss related context-switching infrastructure (such as IST/TSS and historical descriptions) but do not enumerate the register save/restore requirements, thus providing less direct support for the specific field value beyond contextual background. The combination of explicit mention of callee-saved registers for voluntary switches and the minimal context for interrupts directly supports the field's content, while the other excerpts offer contextual support without enumerating the exact register set.",
      "confidence": "high"
    },
    {
      "field": "scheduler_design.timer_and_preemption_integration",
      "citations": [
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://en.wikipedia.org/wiki/Fixed-priority_pre-emptive_scheduling",
          "excerpts": [
            "With fixed priority preemptive scheduling, the scheduler ensures that at any given time, the processor executes the highest priority task of all those tasks."
          ]
        },
        {
          "title": "Fixed-priority pre-emptive scheduling",
          "url": "https://www.geeksforgeeks.org/operating-systems/fixed-priority-pre-emptive-scheduling/",
          "excerpts": [
            "Jul 12, 2025 — In this scheduling algorithm the processor make sure that the highest priority task is to be performed first ignoring the other task to be executed."
          ]
        },
        {
          "title": "Blocking time under basic priority inheritance",
          "url": "https://arxiv.org/pdf/1806.01589",
          "excerpts": [
            "by P Torroni · 2018 — The Priority Inheritance Protocol (PIP) is arguably the best-known protocol for resource sharing under real-time constraints."
          ]
        },
        {
          "title": "Resource Sharing and Scheduling Protocols (SRP, PCP, PIP) - Slides and notes",
          "url": "https://cpen432.github.io/resources/bader-slides/8-ResourceSharing.pdf",
          "excerpts": [
            "Priority inheritance protocol",
            "Priority ceiling protocol"
          ]
        },
        {
          "title": "Priority Ceiling Protocol",
          "url": "https://www.geeksforgeeks.org/computer-networks/priority-ceiling-protocol/",
          "excerpts": [
            "Jan 2, 2020 — In the worst case, the highest priority job J1 can be blocked by T lower priority tasks in the system when J1 has to access T semaphores to ...See more"
          ]
        }
      ],
      "reasoning": "The fine-grained field value emphasizes a high-determinism, low-jitter scheduler design with tickless or event-driven behavior, minimal timer interrupt overhead using a per-core high-resolution timer (LAPIC TSC-deadline), and explicit preemption points tied to interrupt returns and system calls. The most directly relevant material is the assertion that fixed-priority preemptive scheduling ensures the highest-priority task executes at any moment, which informs the overall imperative for deterministic task preemption behavior in the kernel. Expounding on this, understanding the guarantees of fixed-priority preemptive scheduling helps justify a design where preemption points are carefully defined (e.g., at interrupt returns and during syscall handling) to minimize jitter and ensure timely task switching.\n\nAdditional excerpts about resource sharing and scheduling protocols (PIP, PCP, SRP) offer broader context on how real-time systems manage priority and synchronization when multiple tasks contend for resources. While these protocols are not the exact mechanism used in a tickless, latency-focused RTOS, they provide insight into the trade-offs between priority handling and synchronization overhead, which is relevant when considering preemption and scheduling in a real-time environment. Understanding that priority inheritance/ceiling schemes influence blocking and latency helps in evaluating how the proposed tickless strategy interacts with contention management and ensures predictable latency for the packet-processing path.\n\nTaken together, these excerpts support the idea that a deterministic, low-jitter scheduler benefits from (a) a fixed-priority preemptive foundation to guarantee timely task execution, (b) a clear understanding of how preemption interacts with resource contention via scheduling protocols, and (c) consideration of how timer mechanisms and interrupt handling impact preemption points. While none directly mention tickless operation or LAPIC TSC-deadline mode, they establish the principles—priority-driven preemption, predictable interrupt handling, and awareness of synchronization costs—that underpin a tickless, event-driven, low-latency scheduler design.\n",
      "confidence": "medium"
    },
    {
      "field": "physical_memory_management.data_structures",
      "citations": [
        {
          "title": "Buddy memory allocation",
          "url": "https://en.wikipedia.org/wiki/Buddy_memory_allocation",
          "excerpts": [
            "The buddy method of freeing memory is fast, with the maximal number of compactions required equal to O(highest order) = O(log2(total memory size)). Typically ...",
            "The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as ..."
          ]
        },
        {
          "title": "Buddy Memory Allocation Program | Set 2 (Deallocation)",
          "url": "https://www.geeksforgeeks.org/operating-systems/buddy-memory-allocation-program-set-2-deallocation/",
          "excerpts": [
            "Jul 11, 2025 — For deallocation, in the worst case, all the allocated blocks can be of size 1 unit, which will then require O(n) time to scan the list for ..."
          ]
        },
        {
          "title": "Page Frame Allocation",
          "url": "https://wiki.osdev.org/Page_Frame_Allocation",
          "excerpts": [
            "A large array of N/8 bytes is used as a large bit map of the memory usage (that is, bit #i in byte #n define the status of page #n*8+i). Setting the state of a given page is fast (O(1)), allocating a page may take time (O(N)). * an uint32_t comparison can test up to 32 bits at once and thus speed up allocations",
            "Buddy Allocation System"
          ]
        }
      ],
      "reasoning": "The field describes a classic Buddy memory allocation design that relies on two core data structures: an array of free lists, each corresponding to a power-of-two block size (order), and a bitmap-style metadata structure that tracks the state of physical frames to enable fast buddy checks during deallocation. Excerpts that discuss the buddy allocation approach provide direct support for the concept of partitioning memory into blocks and coalescing free blocks, which aligns with the idea of an order-based free-list structure. Excerpts that discuss a bitmap or metadata used to track memory usage corroborate the need for a tracking mechanism across frames to support coalescing decisions. The specific notion of free_lists as an array keyed by order is central to the field, and while some excerpts describe the buddy concept in general terms, others explicitly reference the bitmap-based tracking and the coalescing workflow, which together reinforce the field value. The combination of buddy-allocator discussion and bitmap/metadata tracking found in the excerpts maps closely to the described two primary data structures: an ordered free-list array for block sizes and a bitmap/metadata table for frame state, which together enable efficient allocation, freeing, and merging of memory blocks. The strongest, most direct support is found in discussions of buddy allocation and its coalescing behavior, with bitmap-based tracking offering the complementary metadata support; the deallocation-focused excerpt adds relevance by highlighting coalescing-related considerations, while the page-frame/bit-map focused excerpts reinforce the role of the bitmap in memory state tracking. Overall, the evidence coherently supports the field value's two-core structures and their interaction in a Buddy Allocator design, though the explicit naming of 'free_lists' as an array is implied by the buddy discussion rather than explicitly stated in every excerpt.",
      "confidence": "medium"
    },
    {
      "field": "context_switching_mechanism.switch_paths",
      "citations": [
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A (System Programming for Instruction Set Extensions and Processor Extended States)",
          "url": "https://cdrdv2-public.intel.com/825758/253668-sdm-vol-3a.pdf",
          "excerpts": [
            "The operating system can require applications that are intended to be run as tasks take responsibility for saving\n\nthe states prior to a task suspension during a task switch and for restoring the states when the task is resumed",
            "Context Switch Consideration"
          ]
        },
        {
          "title": "Double Faults | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/double-faults/",
          "excerpts": [
            "The IST and TSS"
          ]
        },
        {
          "title": "os.phil-opp.com/double-fault-exceptions/",
          "url": "https://os.phil-opp.com/double-fault-exceptions/",
          "excerpts": [
            "The IST and TSS",
            "The Interrupt Stack Table (IST) is part of an old legacy structure called *[Task State Segment](https://en.wikipedia.org/wiki/Task_state_segment)* (TSS). The TSS used to hold various pieces of information (e.g., processor register state) about a task in 32-bit mode and was, for example, used for [hardware context switching](https://wiki.osdev.org/Context_Switching). However, hardware context switching is no longer supported in 64-bit mode and the format of the TSS has changed completely. On x86\\_64, the TSS no longer holds any task-specific information at all. Instead, it holds two stack tables (the IST is one of them)"
          ]
        }
      ],
      "reasoning": "The finegrained field value describes two distinct context-switching paths: an interrupt-driven path triggered by hardware via an interrupt and the IDT, where an interrupt frame is pushed and control transfers to an interrupt handler; and a voluntary path triggered by software (e.g., a task yield) that saves callee-saved registers, updates the current task's TCB with the stack pointer, and restores the next task's context. Direct support for the interrupt-driven path is found in the description of hardware interrupts that cause a switch via the IDT and the automatic stack/frame handling during an interrupt, which aligns with the interrupt-driven mechanism. Direct support for the voluntary path is found in statements describing a software-initiated switch that explicitly saves the required callee-saved registers, updates the task control block with the current stack pointer, and then restores the next task's context. Additionally, a broader context-switch focus is provided by discussions of the task switch process and the need to save/restore task states during switches, reinforcing the two-path model (interrupt-driven and voluntary). These elements together map directly to the requested dual-path context-switching design and the concrete steps involved in saving/restoring state, manipulating the TCB, and transitioning between tasks. The excerpt discussing the Interrupt Descriptor Table (IDT) and the hardware-triggered path, along with the explicit description of saving callee-saved registers and updating the TCB for a software yield, collectively support the identified context-switching mechanisms. The other excerpts provide peripheral context about IST/TSS historically, which is less central to the described dual-path mechanism but still related to interrupt infrastructure. ",
      "confidence": "high"
    },
    {
      "field": "virtio_abstraction_layer.device_initialization",
      "citations": [
        {
          "title": "VirtIO MMIO and Device Initialization (VirtIO v1.1/v1.0+)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "The driver MUST ignore a device with MagicValue which is not 0x74726976,\nalthough it MAY report an error. The driver MUST ignore a device with Version which is not 0x2, although it MAY\nreport an error.",
            "The mechanism for bulk data transport on virtio devices is pretentiously called a virtqueue. Each device can have zero or more virtqueues. Driver makes requests ...",
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian"
          ]
        },
        {
          "title": "VirtIO MMIO Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs01/virtio-v1.0-cs01.html",
          "excerpts": [
            "A virtio device is discovered and identified by a bus-specific method (see the bus specific sections: 4.1 Virtio Over PCI Bus, 4.2 Virtio Over MMIO and 4.3 ...",
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian"
          ]
        },
        {
          "title": "pdf - Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "Virtio MMIO Specification (VirtIO v1.2)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        }
      ],
      "reasoning": "The target initialization flow relies on a VirtIO MMIO-based handshake where a driver first verifies the device with a recognized signature (MagicValue) and a compatible version, ensuring the device is a VirtIO device before proceeding with setup. This directly supports the initial verification step that guards the entire initialization sequence against non-VirtIO devices. Beyond verification, the sequence must cover feature negotiation and marking readiness before bringing the device online, which is reflected by discussions around negotiated features, driver readiness, and the concept of requesting/handling virtio queues as part of finalizing device setup. The sources collectively describe: (a) device discovery and identification in VirtIO MMIO, (b) verification checks like the MagicValue and Version, (c) the mechanism by which a driver interacts with virtio queues to perform requests and complete setup, and (d) general VirtIO initialization flow across MMIO-based devices. Taken together, these excerpts provide direct support for the core aspects of the specified initialization sequence (reset-related checks, acknowledgment and driver states, and feature handling as part of the path to a live device) and contextual backing for the overall step-by-step process. Specific snippets emphasize: validating the device's identity (MagicValue and Version integrity), acknowledging and proceeding with driver readiness, and the driver-driven interaction with virtqueues to enable operation, which aligns with the described flow and its emphasis on a disciplined, standards-compliant startup procedure.",
      "confidence": "medium"
    },
    {
      "field": "context_switching_mechanism.implementation_details",
      "citations": [
        {
          "title": "Intel 64 and IA-32 Architectures Software Developer's Manual, Volume 3A (System Programming for Instruction Set Extensions and Processor Extended States)",
          "url": "https://cdrdv2-public.intel.com/825758/253668-sdm-vol-3a.pdf",
          "excerpts": [
            "Context Switch Consideration",
            "The operating system can require applications that are intended to be run as tasks take responsibility for saving\n\nthe states prior to a task suspension during a task switch and for restoring the states when the task is resumed"
          ]
        },
        {
          "title": "os.phil-opp.com/double-fault-exceptions/",
          "url": "https://os.phil-opp.com/double-fault-exceptions/",
          "excerpts": [
            "The Interrupt Stack Table (IST) is part of an old legacy structure called *[Task State Segment](https://en.wikipedia.org/wiki/Task_state_segment)* (TSS). The TSS used to hold various pieces of information (e.g., processor register state) about a task in 32-bit mode and was, for example, used for [hardware context switching](https://wiki.osdev.org/Context_Switching). However, hardware context switching is no longer supported in 64-bit mode and the format of the TSS has changed completely. On x86\\_64, the TSS no longer holds any task-specific information at all. Instead, it holds two stack tables (the IST is one of them)"
          ]
        }
      ],
      "reasoning": "The requested fine-grained field value describes a context-switch routine implemented in Rust with low-level control over registers and stack, using assembly intrinsics (asm! or global_asm!), and marked as #[naked] to avoid automatic function prologues/epilogues. The excerpt that explicitly frames context switch considerations acknowledges the need to address how context switches are designed, which aligns with the need to justify and implement the switching path in the kernel. This supports the notion that the routine requires precise control over register/state transitions and a careful ABI alignment. The excerpt stating that saving and restoring states during task suspension and resume directly supports the core requirement of a context switch mechanism: a task switch must preserve and restore processor state across switches. It discusses the behavior around task suspension and restoration, which is foundational to any context-switch design. Additionally, the historical note about the Interrupt Stack Table (IST) and Task State Segment (TSS) in x86-64 clarifies that hardware context switching is no longer the primary mechanism, and that TSS/IST now serve different roles (e.g., holding stack pointers) rather than task-specific CPU state. This evidences that the implementation relies on software-driven context switching, which reinforces the idea that the kernel would manage its own register/state transitions rather than relying on hardware context switches. Collectively, these excerpts corroborate that the context switch routine is designed for explicit software-driven register and stack management, potentially using naked functions and hand-tuned assembly to conform to the x86-64 System V ABI and debugging considerations. The strongest support comes from the direct statement about saving/restoring task state around switches, while the modern hardware context switching context and TSS/IST behavior provides critical constraints that push toward software-managed switching.",
      "confidence": "high"
    },
    {
      "field": "virtual_memory_management.paging_architecture",
      "citations": [
        {
          "title": "Paging",
          "url": "http://wiki.osdev.org/Paging",
          "excerpts": [
            "Paging is a system which allows each process to see a full virtual address space, without actually requiring the full amount of physical memory to be available ..."
          ]
        },
        {
          "title": "linux virtual memory user/kernel space split in x86_64",
          "url": "https://stackoverflow.com/questions/25706656/linux-virtual-memory-user-kernel-space-split-in-x86-64",
          "excerpts": [
            "According to the /Documentation/x86/x86_64/mm.txt, the kernel virtual address begins at ffff800000000000 - ffff80ffffffffff (=40 bits) guard ...",
            "The sign-extension is due to the fact that x86-64 only uses the low-order 48 bits for addressing memory. As a precaution against programmers abusing the 16 high-order bits, the 47th bit is sign-extended. Therefore, if bit 47 is a 0, then all of bits [48:63] will also be 0, and if bit 47 is a 1, then all of bits [48:63] will be a 1. An address that has this form is called \"canonical\". If you try to use a non-canonical address (i.e. one where bits [48:63] != bit 47) then the hardware will raise a general protection fault. In practice, the kernel's memory are those high addresses where the 17 high-order bits are 1, and user space memory are low memory, where the high-order bits are 0."
          ]
        }
      ],
      "reasoning": "To support the claim of a 4-level paging architecture, you need explicit references to a four-level page table hierarchy (PML4, PDPT, PD, PT) and a 48-bit virtual address space. An excerpt that introduces paging as a mechanism for giving processes virtual address spaces is relevant as foundational context for virtual memory, though it does not specify the four-level depth. It confirms the general concept of virtual memory, which is prerequisite to the detailed paging structure you describe. Content discussing canonical addressing and the kernel/user space split is relevant for supporting a 48-bit address space and the notion of separate spaces, which aligns with a modern x86-64 layout and the idea of a distinct address-space organization. Canonical addressing explanations elaborate on sign-extension behavior and the idea that valid addresses live within a canonical range, which is consistent with standard 48-bit (or effectively 47–48-bit) architectures that constrain virtual addresses to a canonical form. Together, these excerpts corroborate the presence of a structured virtual memory system with a defined address-space layout and canonical addressing constraints, which are prerequisites for describing a multi-level paging scheme. However, none of the excerpts explicitly confirm a four-level hierarchy by naming the exact four levels (PML4, PDPT, PD, PT) in direct terms, nor do they unambiguously state a single static address space with no per-process switches. One excerpt explicitly mentions a five-level page-table setup, which would contradict a strict four-level design if taken as the intended architecture, suggesting there is potential ambiguity or variation in the sources. Overall, the strongest support pertains to the existence of a structured paging system, canonical 48-bit addressing behavior, and a kernel/user space layout, but there is at least partial ambiguity about the exact level count and single-address-space constraint. Given the mixture of alignment and contradiction, the confidence is medium: the excerpts support the general architectural direction (structured paging with canonical addressing and address-space layout) but do not unambiguously confirm every specific claim (4-level depth and single static address space without switches) without further clarifying sources.",
      "confidence": "medium"
    },
    {
      "field": "virtual_memory_management.large_page_strategy",
      "citations": [
        {
          "title": "x86_64/mm.txt (Kernel memory management) – Linux Kernel Documentation",
          "url": "https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt",
          "excerpts": [
            "Complete virtual memory map with 5-level page tables"
          ]
        },
        {
          "title": "Paging",
          "url": "http://wiki.osdev.org/Paging",
          "excerpts": [
            "Paging is a system which allows each process to see a full virtual address space, without actually requiring the full amount of physical memory to be available ..."
          ]
        },
        {
          "title": "TLB shootdowns and memory management (Usenix/AMIT et al.)",
          "url": "https://www.usenix.org/system/files/conference/atc17/atc17-amit.pdf",
          "excerpts": [
            "TLB shootdowns introduce a variety of overheads. IPI delivery can take several hundreds of cycles [ 5 ].",
            "\nTLB can be flushed during context switch, before any\n\nstale entry would be used. A common method to reduce shootdown time is\n\nto batch TLB invalidations if they can be deferred [ 21 ,\n\n47 ].",
            "Linux tries to balance between the overheads of\n\nTLB flushes and TLB misses when a core becomes\n\nidle, using a lazy TLB invalidation schem",
            "TLB shootdown can take microseconds,\n\ncausing a notable slowdown [ 48 ]. Performing TLB\n\nshootdown in hardware, as certain CPUs do, is faster\n\nbut still incurs considerable overheads [ 22\nIn addition to reducing performance, shootdown\n\noverheads can negatively a ff ect the way applications\n\nare constructed. Notably, to avoid shootdown la-\n\ntency, programmers are advised against using mem-\n\nory mappings, against unmapping them, and even\n\nagainst building multithreaded applications [\nOSes try to cope with shootdown overheads by\n\nbatching them [ 21 , 43 ], avoiding them on idle cores,\n\nor, when possible, performing them faster [ 5 ]. But\n\nthe potential of these existing solutions is inherently\n\nlimited to certain specific scenar\n We propose to avoid unwarranted TLB shoot-\n\ndowns in a di ff erent manner: by monitoring access\n\nTo avoid frequent page-table walks and their as-\n\nsociated latency, the MMU caches translations of\n\nrecently used pages in a translation lookaside bu ff er\nEach x86 core holds a logically private TLB. Unlike memory caches,TLBs of di ff erent CPUs are\n\nnot maintained coherent by hardwar\nAs a result, page-table changes may\n\nleave stale entries in the TLBs until coherence is\n\nrestored by the OS. The instruction set enables the\n\nOS to do so by flushing (“invalidating”) individual\n\nPTEs or the entire \nAlthough the TLB is essential to attain reasonable\n\ntranslation latency, some workloads experience fre-\n\nquent TLB cache-misses [ \nThe x86 architecture leaves maintaining TLB co-\n\nherency to the OSes, which often requires frequent\n\nTLB invalidations after PTE chan\nThese PTE changes can require a TLB flush to\n\navoid caching of stale PTEs in the TLB. We distin-\n\nguish between two types of flushes: _local_ and _remote_ ,\n\nin accordance with the core that initiated the PTE\n\nchange. Remote TLB flushes are significantly more\n\nexpensive, since most CPUs cannot flush remote\n\nTLBs directly. OSes therefore perform a _TLB shoot-_\n\n_down_ : The initiating core sends an inter-processor\n\ninterrupt (IPI) to the remote cores and waits for\n\ntheir interrupt handlers to invalidate their TLBs and\n\nacknowledge that they are done. TLB shootdowns introduce a variety of overheads. IPI delivery can take several h\nA common method to reduce shootdown time is\n\nto batch TLB invalidations if they can be deferred [ 21 ,\n\n47 ]. Batching, however, cannot be used in many\n\ncases, for example when a multithreaded application\n\nchanges the access permissions of a single p\nAnother way to reduce shootdown overhead is\n\nto acknowledge its IPI immediately, even before\n\ninvalidation is performed [ 5 , 43 \nFlushtime can be reducedby lowering the number\n\nof TLB flushes. Flushing multiple individual PTEs is\n\nexpensive, and therefore OSes can prefer to flush the\n\nentire TLB if the number of PTEs exceeds a certain\n\nthreshold. This is a delicate trade-o ff , as such a flush\n\nincreases the number of TLB misses [\nLinux tries to balance between the overheads of\n\nTLB flushes and TLB misses when a core becomes\n\nidle, using a lazy TLB invalidation schem\nDespite all of these techniques, shootdowns can\n\ninduce high overheads in real systems\n"
          ]
        },
        {
          "title": "Higher Half x86 Bare Bones",
          "url": "https://wiki.osdev.org/Higher_Half_x86_Bare_Bones",
          "excerpts": [
            "r kernel). ```\nENTRY (_start)\n\nSECTIONS\n{\n        . = 0x00100000;\n\t/* The kernel will live at 3GB + 1MB in the virtual address space, */\n\t/* which will be mapped to 1MB in the physical address space. */\n\t/* Note that we page-align the sections. */",
            "A higher half kernel is a kernel that is mapped to the higher half of the virtual address space, despite being loaded to the lower half of the physical address space."
          ]
        },
        {
          "title": "Higher Half Kernel - OSDev Wiki",
          "url": "http://wiki.osdev.org/Higher_Half_Kernel",
          "excerpts": [
            "It is traditional and generally good to have your kernel mapped in every user process. Linux and many other Unices, for instance, reside at virtual addresses _0xC0000000 – 0xFFFFFFFF_ of every address space, leaving the range _0x00000000 – 0xBFFFFFFF_ for user code, data, stacks, libraries, "
          ]
        },
        {
          "title": "linux virtual memory user/kernel space split in x86_64",
          "url": "https://stackoverflow.com/questions/25706656/linux-virtual-memory-user-kernel-space-split-in-x86-64",
          "excerpts": [
            "According to the /Documentation/x86/x86_64/mm.txt, the kernel virtual address begins at ffff800000000000 - ffff80ffffffffff (=40 bits) guard ..."
          ]
        }
      ],
      "reasoning": "The proposed field value centers on using large pages to dramatically increase TLB coverage and reduce page-walk overhead in a high-determinism RTOS. Direct paging references show that modern systems design relies on hierarchical page tables to support large virtual address spaces, with the complete virtual memory map and multi-level paging enabling flexible mappings. This provides a foundation for deploying 1GiB and 2MiB pages to cover large contiguous memory regions such as the direct physical memory area, kernel code/data, heap, and device MMIO regions. Context about higher-half kernel mappings shows a model where the kernel resides in a distinct virtual address region, which is compatible with large-page strategy since large mappings can be placed in the kernel's address space to maximize TLB efficiency. Additionally, materials on TLB shootdowns and memory management describe the overheads associated with page-table changes and TLB invalidations, and they argue that batching and smarter invalidation strategies are ways to mitigate these costs. This supports the rationale that using larger page sizes reduces the frequency of TLB misses and page walks, thereby improving latency characteristics. The combination of a 5-level page-table layout (which enables very large address spaces) and guidance on TLB overheads provides concrete backing for selecting large-page strategies to maximize TLB hit rate and minimize costly page walks. The discussion of higher-half kernel mappings and canonical addressing further reinforces how large contiguous mappings can be placed in the virtual address space without conflicting with user-space mappings, aligning with the intended separation and performance goals of a unikernel-like RTOS. Overall, the excerpts align with the idea of aggressively using large pages to cover substantial memory regions, while reserving small pages only for truly fine-grained mappings where necessary.",
      "confidence": "high"
    },
    {
      "field": "virtual_memory_management.page_fault_handling",
      "citations": [
        {
          "title": "Paging",
          "url": "http://wiki.osdev.org/Paging",
          "excerpts": [
            "Paging is a system which allows each process to see a full virtual address space, without actually requiring the full amount of physical memory to be available ..."
          ]
        },
        {
          "title": "x86_64/mm.txt (Kernel memory management) – Linux Kernel Documentation",
          "url": "https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt",
          "excerpts": [
            "Complete virtual memory map with 5-level page tables"
          ]
        },
        {
          "title": "Higher Half x86 Bare Bones",
          "url": "https://wiki.osdev.org/Higher_Half_x86_Bare_Bones",
          "excerpts": [
            "A higher half kernel is a kernel that is mapped to the higher half of the virtual address space, despite being loaded to the lower half of the physical address space.",
            "r kernel). ```\nENTRY (_start)\n\nSECTIONS\n{\n        . = 0x00100000;\n\t/* The kernel will live at 3GB + 1MB in the virtual address space, */\n\t/* which will be mapped to 1MB in the physical address space. */\n\t/* Note that we page-align the sections. */"
          ]
        },
        {
          "title": "Higher Half Kernel - OSDev Wiki",
          "url": "http://wiki.osdev.org/Higher_Half_Kernel",
          "excerpts": [
            "It is traditional and generally good to have your kernel mapped in every user process. Linux and many other Unices, for instance, reside at virtual addresses _0xC0000000 – 0xFFFFFFFF_ of every address space, leaving the range _0x00000000 – 0xBFFFFFFF_ for user code, data, stacks, libraries, ",
            "To make things easier, some bootloaders natively support higher half kernels, by directly loading and mapping a kernel to the higher half in virtual memory."
          ]
        },
        {
          "title": "linux virtual memory user/kernel space split in x86_64",
          "url": "https://stackoverflow.com/questions/25706656/linux-virtual-memory-user-kernel-space-split-in-x86-64",
          "excerpts": [
            "According to the /Documentation/x86/x86_64/mm.txt, the kernel virtual address begins at ffff800000000000 - ffff80ffffffffff (=40 bits) guard ...",
            "In practice, the kernel's memory are those high addresses where the 17 high-order bits are 1, and user space memory are low memory, where the high-order bits are 0.",
            "The sign-extension is due to the fact that x86-64 only uses the low-order 48 bits for addressing memory. As a precaution against programmers abusing the 16 high-order bits, the 47th bit is sign-extended. Therefore, if bit 47 is a 0, then all of bits [48:63] will also be 0, and if bit 47 is a 1, then all of bits [48:63] will be a 1. An address that has this form is called \"canonical\". If you try to use a non-canonical address (i.e. one where bits [48:63] != bit 47) then the hardware will raise a general protection fault. In practice, the kernel's memory are those high addresses where the 17 high-order bits are 1, and user space memory are low memory, where the high-order bits are 0."
          ]
        },
        {
          "title": "TLB shootdowns and memory management (Usenix/AMIT et al.)",
          "url": "https://www.usenix.org/system/files/conference/atc17/atc17-amit.pdf",
          "excerpts": [
            "TLB shootdowns introduce a variety of overheads. IPI delivery can take several hundreds of cycles [ 5 ].",
            "TLB shootdown can take microseconds,\n\ncausing a notable slowdown [ 48 ]. Performing TLB\n\nshootdown in hardware, as certain CPUs do, is faster\n\nbut still incurs considerable overheads [ 22\nIn addition to reducing performance, shootdown\n\noverheads can negatively a ff ect the way applications\n\nare constructed. Notably, to avoid shootdown la-\n\ntency, programmers are advised against using mem-\n\nory mappings, against unmapping them, and even\n\nagainst building multithreaded applications [\nOSes try to cope with shootdown overheads by\n\nbatching them [ 21 , 43 ], avoiding them on idle cores,\n\nor, when possible, performing them faster [ 5 ]. But\n\nthe potential of these existing solutions is inherently\n\nlimited to certain specific scenar\n We propose to avoid unwarranted TLB shoot-\n\ndowns in a di ff erent manner: by monitoring access\n\nTo avoid frequent page-table walks and their as-\n\nsociated latency, the MMU caches translations of\n\nrecently used pages in a translation lookaside bu ff er\nEach x86 core holds a logically private TLB. Unlike memory caches,TLBs of di ff erent CPUs are\n\nnot maintained coherent by hardwar\nAs a result, page-table changes may\n\nleave stale entries in the TLBs until coherence is\n\nrestored by the OS. The instruction set enables the\n\nOS to do so by flushing (“invalidating”) individual\n\nPTEs or the entire \nAlthough the TLB is essential to attain reasonable\n\ntranslation latency, some workloads experience fre-\n\nquent TLB cache-misses [ \nThe x86 architecture leaves maintaining TLB co-\n\nherency to the OSes, which often requires frequent\n\nTLB invalidations after PTE chan\nThese PTE changes can require a TLB flush to\n\navoid caching of stale PTEs in the TLB. We distin-\n\nguish between two types of flushes: _local_ and _remote_ ,\n\nin accordance with the core that initiated the PTE\n\nchange. Remote TLB flushes are significantly more\n\nexpensive, since most CPUs cannot flush remote\n\nTLBs directly. OSes therefore perform a _TLB shoot-_\n\n_down_ : The initiating core sends an inter-processor\n\ninterrupt (IPI) to the remote cores and waits for\n\ntheir interrupt handlers to invalidate their TLBs and\n\nacknowledge that they are done. TLB shootdowns introduce a variety of overheads. IPI delivery can take several h\nA common method to reduce shootdown time is\n\nto batch TLB invalidations if they can be deferred [ 21 ,\n\n47 ]. Batching, however, cannot be used in many\n\ncases, for example when a multithreaded application\n\nchanges the access permissions of a single p\nAnother way to reduce shootdown overhead is\n\nto acknowledge its IPI immediately, even before\n\ninvalidation is performed [ 5 , 43 \nFlushtime can be reducedby lowering the number\n\nof TLB flushes. Flushing multiple individual PTEs is\n\nexpensive, and therefore OSes can prefer to flush the\n\nentire TLB if the number of PTEs exceeds a certain\n\nthreshold. This is a delicate trade-o ff , as such a flush\n\nincreases the number of TLB misses [\nLinux tries to balance between the overheads of\n\nTLB flushes and TLB misses when a core becomes\n\nidle, using a lazy TLB invalidation schem\nDespite all of these techniques, shootdowns can\n\ninduce high overheads in real systems\n",
            "The initiating core sends an inter-processor\n\ninterrupt (IPI) to the remote cores and waits for\n\ntheir interrupt handlers to invalidate their TLBs and\n\nacknowledge that they are do",
            "\nTLB can be flushed during context switch, before any\n\nstale entry would be used. A common method to reduce shootdown time is\n\nto batch TLB invalidations if they can be deferred [ 21 ,\n\n47 ].",
            "Batching, however, cannot be used in many\n\ncases, for example when a multithreaded application\n\nchanges the access permissions of a single pag",
            "Another way to reduce shootdown overhead is\n\nto acknowledge its IPI immediately, even before\n\ninvalidation is performed [ 5 , 43 ",
            "Flushtime can be reducedby lowering the number\n\nof TLB flushes. Flushing multiple individual PTEs is\n\nexpensive, and therefore OSes can prefer to flush the\n\nentire TLB if the number of PTEs exceeds a certain\n\nthresh",
            "Linux tries to balance between the overheads of\n\nTLB flushes and TLB misses when a core becomes\n\nidle, using a lazy TLB invalidation schem"
          ]
        }
      ],
      "reasoning": "The field describes a page fault handling policy in a virtual memory system designed for determinism with pre-mapped memory and no recovery. The most directly relevant content is about paging concepts and how virtual memory is organized in a multi-level page-table system, including where and how the kernel is mapped and how address spaces are laid out. For instance:\n- An explanation of paging as a mechanism to give processes a virtual address space while limiting physical memory requirements directly informs how page faults could arise if virtual addresses are not backed by physical frames, which is central to a policy that treats any fault as a critical error.\n- Descriptions of the complete virtual memory map with multi-level page tables provide the architectural scaffolding for where faults could occur and how address translation is structured, which is essential for deciding that a fault would be unrecoverable and require a halt or reboot in a deterministic system.\n- Discussions on higher-half/kernel mappings relay where the kernel resides in the virtual address space, affecting fault handling semantics since kernel vs. user space faults must be reasoned about in a deterministic OS.\n- References to advanced paging details (e.g., 5-level page tables, canonical addresses, and kernel/user space split) illustrate how the system would need to enforce strict mappings so that faults from invalid or unmapped addresses can be avoided or immediately fatal if they occur.\n- While none of the excerpts explicitly state a panic-on-fault policy, the combination of pre-mapped memory, the emphasis on strict memory-layout design, and the general paging architecture strongly supports a model where page faults are treated as unrecoverable events that justify halting or rebooting to preserve determinism. The absence of demand paging or recovery semantics in these sources aligns with the proposed approach.\n",
      "confidence": "high"
    },
    {
      "field": "configuration_and_packaging_interface.host_cli_orchestration",
      "citations": [
        {
          "title": "How to create a multiqueue TAP device in Linux",
          "url": "https://stackoverflow.com/questions/72268735/how-to-create-a-multiqueue-tap-device-in-linux",
          "excerpts": [
            "You can use ethtool -l [ifname] in your VM. NETWORK=\" -netdev tap,ifname=${TAP:-tap0},id=n1,script=no,downscript=no,queues=4 -device virtio-net-pci,mac=AA:BB:CC:DD:CA:B1,netdev=n1,mq=on,vectors=10\"",
            "The docs say that for\nqueues=N the value for\nvectors should be \"2N+2\". Therefore, in your example with\nqueues=4 we should use\nvectors=10 .",
            "Important options are\nvirtio-net-pci and\nmq=on,vectors=10 . Information about the vectors parameter https://www.linux-kvm.org/page/Multiqueue"
          ]
        },
        {
          "title": "Features/MQ virtio net - Fedora Project Wiki",
          "url": "https://fedoraproject.org/wiki/Features/MQ_virtio_net",
          "excerpts": [
            "Mar 15, 2013 — Multiqueue virtio-net provides an approach that scales the network performance as the increasing of the number of vcpus by allowing them to transfer packets ..."
          ]
        },
        {
          "title": "A journey to the vhost-users realm",
          "url": "https://www.redhat.com/en/blog/journey-vhost-users-realm",
          "excerpts": [
            "Sep 24, 2019 — This post is a technical deep dive into the vhost-user/virtio-pmd architecture targeting high performance userland networking using DPDK."
          ]
        },
        {
          "title": "OASIS Virtio Specification Repository",
          "url": "https://github.com/oasis-tcs/virtio-spec",
          "excerpts": [
            "The purpose of virtio and this specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs03/virtio-v1.0-cs03.html",
          "excerpts": [
            "Queue Size corresponds to the maximum number of buffers in the virtqueue. Queue Size value is always a power of 2. The maximum Queue Size value is 32768. This ...See more"
          ]
        },
        {
          "title": "Vhost-user Protocol — QEMU 10.0.93 documentation - Read the Docs",
          "url": "https://qemu.readthedocs.io/en/master/interop/vhost-user.html",
          "excerpts": [
            "The front-end enables queues by sending message VHOST_USER_SET_VRING_ENABLE . vhost-user-net has historically automatically enabled the first queue pair."
          ]
        },
        {
          "title": "Virtio, vhost, vhost-user, vsock, etc. | Lencerf's Walk",
          "url": "https://lencerf.github.io/post/2023-05-28-virtio-vhost-vsock/",
          "excerpts": [
            "May 28, 2023 — On the other hand, vhost-user off-loads data processing to another user space program, which is called a backend device. The VMM and the device ..."
          ]
        },
        {
          "title": "Ultra Low Latency: The Ultimate Guide to Lightning-Fast ...",
          "url": "https://www.videosdk.live/developer-hub/webtransport/ultra-low-latency",
          "excerpts": [
            "In 2025, an ultra low latency trading system aims for round-trip times well below 10 microseconds, targeting single-digit microseconds or even nanoseconds for ..."
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a host-side Rust CLI that orchestrates QEMU command-line generation for a high-performance VM networking setup, including multi-queue TAP devices, network bridges, and virtio-net/VirtIO backends and vhost configurations. Excerpts that discuss multiqueue TAP devices, Linux networking, and virtio/vhost concepts provide direct support or context for these host-side orchestration tasks. Specifically:\n- A discussion of multiqueue TAP device setup in Linux demonstrates the exact kind of host-networking primitive the CLI would need to script and manage, including how to configure queues and the associated tooling. This is a core capability the CLI would invoke when constructing the QEMU command line and associated host resources.\n- Explanations of multi-queue virtio-net configurations and the related parameter vectors (queues, vectors, mq) illustrate the exact command-line flags and resource planning that the orchestrator should generate and manage, aligning with the CLI's responsibility to assemble a correct and optimized QEMU invocation.\n- References to vhost-user and vhost-backends describe the higher-level backend options the host tooling may select or coordinate with, ensuring the CLI can wire up either kernel or user-space backends as part of the VM networking stack.\n- Documents on VirtIO network standard features and the relationship between virtio-net queues and host/guest interactions provide the architectural grounding for how the CLI should model and generate the necessary host-side networking configuration.\n- Additional material touching on related infrastructure (e.g., general virtio/vhost concepts) reinforces the broader ecosystem the CLI must integrate with (e.g., kernel vhost paths vs. user-space vhost-user paths) when emitting the final QEMU command line.\nTaken together, these excerpts map tightly to the task of implementing a dedicated host-side Rust CLI that parses a declarative configuration and emits QEMU flags for multi-queue virtio-net networking, bridges, and vhost backends, providing concrete guidance and constraints for the CLI's responsibilities and the syntax of the generated command line.",
      "confidence": "high"
    },
    {
      "field": "virtual_memory_management.tlb_management_policy",
      "citations": [
        {
          "title": "TLB shootdowns and memory management (Usenix/AMIT et al.)",
          "url": "https://www.usenix.org/system/files/conference/atc17/atc17-amit.pdf",
          "excerpts": [
            "Linux tries to balance between the overheads of\n\nTLB flushes and TLB misses when a core becomes\n\nidle, using a lazy TLB invalidation schem",
            "\nTLB can be flushed during context switch, before any\n\nstale entry would be used. A common method to reduce shootdown time is\n\nto batch TLB invalidations if they can be deferred [ 21 ,\n\n47 ].",
            "TLB shootdown can take microseconds,\n\ncausing a notable slowdown [ 48 ]. Performing TLB\n\nshootdown in hardware, as certain CPUs do, is faster\n\nbut still incurs considerable overheads [ 22\nIn addition to reducing performance, shootdown\n\noverheads can negatively a ff ect the way applications\n\nare constructed. Notably, to avoid shootdown la-\n\ntency, programmers are advised against using mem-\n\nory mappings, against unmapping them, and even\n\nagainst building multithreaded applications [\nOSes try to cope with shootdown overheads by\n\nbatching them [ 21 , 43 ], avoiding them on idle cores,\n\nor, when possible, performing them faster [ 5 ]. But\n\nthe potential of these existing solutions is inherently\n\nlimited to certain specific scenar\n We propose to avoid unwarranted TLB shoot-\n\ndowns in a di ff erent manner: by monitoring access\n\nTo avoid frequent page-table walks and their as-\n\nsociated latency, the MMU caches translations of\n\nrecently used pages in a translation lookaside bu ff er\nEach x86 core holds a logically private TLB. Unlike memory caches,TLBs of di ff erent CPUs are\n\nnot maintained coherent by hardwar\nAs a result, page-table changes may\n\nleave stale entries in the TLBs until coherence is\n\nrestored by the OS. The instruction set enables the\n\nOS to do so by flushing (“invalidating”) individual\n\nPTEs or the entire \nAlthough the TLB is essential to attain reasonable\n\ntranslation latency, some workloads experience fre-\n\nquent TLB cache-misses [ \nThe x86 architecture leaves maintaining TLB co-\n\nherency to the OSes, which often requires frequent\n\nTLB invalidations after PTE chan\nThese PTE changes can require a TLB flush to\n\navoid caching of stale PTEs in the TLB. We distin-\n\nguish between two types of flushes: _local_ and _remote_ ,\n\nin accordance with the core that initiated the PTE\n\nchange. Remote TLB flushes are significantly more\n\nexpensive, since most CPUs cannot flush remote\n\nTLBs directly. OSes therefore perform a _TLB shoot-_\n\n_down_ : The initiating core sends an inter-processor\n\ninterrupt (IPI) to the remote cores and waits for\n\ntheir interrupt handlers to invalidate their TLBs and\n\nacknowledge that they are done. TLB shootdowns introduce a variety of overheads. IPI delivery can take several h\nA common method to reduce shootdown time is\n\nto batch TLB invalidations if they can be deferred [ 21 ,\n\n47 ]. Batching, however, cannot be used in many\n\ncases, for example when a multithreaded application\n\nchanges the access permissions of a single p\nAnother way to reduce shootdown overhead is\n\nto acknowledge its IPI immediately, even before\n\ninvalidation is performed [ 5 , 43 \nFlushtime can be reducedby lowering the number\n\nof TLB flushes. Flushing multiple individual PTEs is\n\nexpensive, and therefore OSes can prefer to flush the\n\nentire TLB if the number of PTEs exceeds a certain\n\nthreshold. This is a delicate trade-o ff , as such a flush\n\nincreases the number of TLB misses [\nLinux tries to balance between the overheads of\n\nTLB flushes and TLB misses when a core becomes\n\nidle, using a lazy TLB invalidation schem\nDespite all of these techniques, shootdowns can\n\ninduce high overheads in real systems\n",
            "TLB shootdowns introduce a variety of overheads. IPI delivery can take several hundreds of cycles [ 5 ].",
            "Another way to reduce shootdown overhead is\n\nto acknowledge its IPI immediately, even before\n\ninvalidation is performed [ 5 , 43 ",
            "Flushtime can be reducedby lowering the number\n\nof TLB flushes. Flushing multiple individual PTEs is\n\nexpensive, and therefore OSes can prefer to flush the\n\nentire TLB if the number of PTEs exceeds a certain\n\nthresh",
            "Batching, however, cannot be used in many\n\ncases, for example when a multithreaded application\n\nchanges the access permissions of a single pag",
            "The initiating core sends an inter-processor\n\ninterrupt (IPI) to the remote cores and waits for\n\ntheir interrupt handlers to invalidate their TLBs and\n\nacknowledge that they are do"
          ]
        }
      ],
      "reasoning": "The proposed policy emphasizes avoiding invalidations at runtime and minimizing cross-core TLB coordination. The excerpts collectively describe practical approaches to reducing TLB invalidations and their latency: one excerpt discusses lazy TLB invalidation as a mechanism to avoid frequent invalidations, which aligns with a design intent to minimize runtime TLB pressure. Another excerpt explicitly notes that TLB shootdowns introduce IPI-related overhead and are a key performance concern, which motivates strategies to reduce such events. Additional excerpts describe batching TLB invalidations to amortize their cost and to defer or minimize invalidations when possible, reinforcing the idea of a low-invalidation, high-performance policy. There is also discussion of using IPI coordination and thresholds to decide when to flush or invalidate, which supports the concept of avoiding unnecessary cross-core TLB traffic and aiming for fewer global flushes. Taken together, these excerpts support a policy oriented toward minimizing runtime TLB invalidations via lazy invalidation, batching, and coordinated handling, while acknowledging that some degree of invalidation may still occur in edge cases (e.g., device hot-plug) and that isolated repairs via targeted instructions (INVLPG) may be used instead of full TLB flushes. The exact claim that page tables are created once at boot and CR3 never reloads is not directly evidenced in these excerpts; the strongest support is for minimizing and localizing invalidations and using lazy/update-batching strategies to reduce global TLB churn.",
      "confidence": "medium"
    },
    {
      "field": "security_and_isolation_strategy.compile_time_hardening",
      "citations": [
        {
          "title": "[PDF] Intra-Unikernel Isolation with Intel Memory Protection Keys",
          "url": "https://www.ssrg.ece.vt.edu/papers/vee20-mpk.pdf",
          "excerpts": [
            "Intra-Unikernel Isolation with Intel Memory Protection Keys"
          ]
        },
        {
          "title": "libhermitMPK | Intra-Unikernel Isolation with Intel Memory Protection ...",
          "url": "https://ssrg-vt.github.io/libhermitMPK/",
          "excerpts": [
            "libhermitMPK is a new design bringing memory isolation inside a unikernel instance while keeping a *single address space*. By leveraging Intel Memory Protection Keys ([MPK](https://lwn.net/Articles/643797)), intra-unikernel isolation can be provided without impacting the lightweightness and performance benefits of unikernels. libhermitMPK provides isolation between trusted and untrusted components: (1) safe from unsafe [Rust](https://www.rust-lang.org) kernel code and (2) kernel from user code."
          ]
        },
        {
          "title": "Unishyper: A Rust-based unikernel enhancing reliability and efficiency of embedded systems",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S138376212400136X",
          "excerpts": [
            "To provide memory isolation support between user and kernel code as well as isolation between different user applications at the thread level, Unishyper designs the Zone mechanism based on Intel MPK [36] to isolate the code into thread gr"
          ]
        },
        {
          "title": "VirtusCap: Capability-Based Access Control for Unikernels",
          "url": "https://www.researchgate.net/publication/312379553_VirtusCap_Capability-Based_Access_Control_for_Unikernels",
          "excerpts": [
            "VirtusCap limits privileges of unikernels using the Principle of Least Privilege to create unikernels that have only the privileges they need to accomplish their task.",
            "VirtusCap\n\nis an access-control system integrated across the virtualization\n\nlayers. At the hypervisor level, we integrate a new module\n\nfor access-control in Xen Security Modules (XSM) [25] that\n\nuses a capability-based access-control m",
            "his paper makes the following contributions:\n\n(a) We present the design of a novel architecture for\n\ncapability-based access-control with unikernels in virtu-\n\nalized hosts. (b) We design and implement a new prototype capability-\n\nbased access-control system for the XSM. (c) We design and implement a library for MirageOS that\n\nenables capability-based access-control for unik"
          ]
        },
        {
          "title": "Memory Protection Unit",
          "url": "https://developer.arm.com/documentation/107565/latest/Memory-protection/Memory-Protection-Unit",
          "excerpts": [
            "The MPU programmers' model allows privileged software to define memory regions and assign memory access permissions and memory attributes to each of them."
          ]
        },
        {
          "title": "Process isolation with Arm FuSa runtime system",
          "url": "https://community.arm.com/arm-community-blogs/b/tools-software-ides-blog/posts/process-isolation-with-fusa-rts",
          "excerpts": [
            "Feb 18, 2021 — It is used to implement spatial isolation between application processes. In FuSa RTX, an RTOS thread gets assigned to an MPU protected zone ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant excerpts describe mechanisms for isolating components within a unikernel or privileged domain. In particular, a document focused on intra-unikernel isolation with Intel Memory Protection Keys describes fine-grained memory protection to separate trusted and untrusted kernel components, which aligns with a philosophy of reducing risk surface and hardening the runtime environment. Another excerpt discusses a dedicated memory protection unit and its usage to enforce spatial isolation, which supports the broader goal of a hardened, compartmentalized runtime. Additional excerpts describe capability-based access control schemes across virtualized layers and unikernels, which complements hardening by enforcing least privilege and isolation boundaries at the software architecture level. These sources collectively reinforce the core idea of isolating and constraining code execution paths within a unikernel, which underpins compile-time hardening strategies aimed at shrinking the attack surface, even if they do not spell out the exact compiler flags or panic strategies named in the fine-grained field value. The remaining excerpts discuss broader isolation mechanisms and security-oriented architectures that further support a hardened, defense-in-depth approach suitable for a deterministically secure unikernel RTOS, even though they do not provide direct statements about the specific compile-time options listed in the field value.",
      "confidence": "medium"
    },
    {
      "field": "configuration_and_packaging_interface.guest_configuration_passing",
      "citations": [
        {
          "title": "QEMU Firmware Configuration (fw_cfg) Device",
          "url": "https://www.qemu.org/docs/master/specs/fw_cfg.html",
          "excerpts": [
            "This hardware interface allows the guest to retrieve various data items\n(blobs) that can influence how the firmware configures itself, or may\ncontain tables to be installed for the guest OS. Examples include device\nboot order, ACPI and SMBIOS tables, virtual machine UUID, SMP and NUMA\ninformation, kernel/initrd images for direct (Linux) kernel booting, etc.",
            "Since v2.4, “file” fw\\_cfg items (i.e., items with selector keys above\n`FW_CFG_FILE_FIRST`, and with a corresponding entry in the fw\\_cfg file\ndirectory structure) may be inserted via the QEMU command line, using\nthe following synta",
            "\n```\n-fw_cfg [name=]<item_name>,file=<path>\n\n```",
            "Or:\n\n```\n-fw_cfg [name=]<item_name>,string=<string>\n\n```"
          ]
        },
        {
          "title": "QEMU fw_cfg - Somlo CMU (Contrib.cm.u.edu)",
          "url": "http://www.contrib.andrew.cmu.edu/~somlo/QEMU_fw_cfg/",
          "excerpts": [
            "The fw\\_cfg device allows QEMU to prepare various data items (or \"blobs\") in host memory, and offers a guest-side interface allowing the guest to read (using port or memory-mapped I/O operations) the blobs after having been launched by the host"
          ]
        },
        {
          "title": "QEMU_fw_cfg (OSDev Wiki)",
          "url": "https://wiki.osdev.org/QEMU_fw_cfg",
          "excerpts": [
            "To pass strings or files into QEMU to appear in the fw\\_cfg device, use one or more command line parameters like those shown below:",
            "```\n -fw_cfg name=opt/com.name.domain.your.example,string=1\n -fw_cfg name=opt/com.name.domain.your.example,file=example_filename.png\n -fw_cfg opt/com.name.domain.your.example,file=example_filename.png\n\n```",
            "```\n  #define FW_CFG_SIGNATURE    0x0000\n  #define FW_CFG_ID           0x0001\n  #define FW_CFG_FILE_DIR     0x0019\n\n```",
            "Listed above are a few fixed-purpose selectors for probing for the existence of the device, and for getting the number of files present in the device.",
            "Following the count, there is a sequence of entries with the following structure:",
            "th the following structure:\n\n```\n  struct FWCfgFile {\t\t/* an individual file entry, 64 bytes total */\n      uint32_t size;\t\t/* size of referenced fw_cfg item, big-endian */\n      uint16_t select;\t\t/* selector key of fw_cfg item, big-endian */\n      uint16_t reserved;\n      char name[56];\t\t/* fw_cfg item name, NUL-terminated ascii */\n  };",
            "These can be read one at a time by reading *sizeof(FWCfgFile)* bytes into a single variable of type *FWCfgFile* and examined one by one until you find a structure with the desired name.",
            "### Reading files",
            "Once you have found the *FWCfgFile* structure which has the desired name, read the *select* field from the structure. It will contain the big-endian selector for the file. After byte-swapping the selector, output it to *FW\\_CFG\\_PORT\\_SEL*, then read *size* bytes from *FW\\_CFG\\_PORT\\_DATA* and write them to the desired location in memory."
          ]
        }
      ],
      "reasoning": "The field value asserts a deterministic configuration-passing mechanism from host to guest using the QEMU fw_cfg device. First, the fw_cfg device is described as a mechanism for the guest to access data blobs prepared by the host, which aligns with the idea of a dedicated host–guest interface for boot-time configuration. This establishes the fw_cfg device as the foundational channel for passing configuration data into the VM. Next, the host-side usage of -fw_cfg on the command line to specify items by name and source (either a file or a string) directly supports the described boot-time configuration flow: the host injects configuration payloads into the fw_cfg namespace that the guest can later discover. The guest-side accessibility is reinforced by references to items becoming accessible under /sys/firmware/qemu_fw_cfg/by_name/ when CONFIG_FW_CFG_SYSFS is enabled, which matches the assertion that the guest can read structured configuration data at boot time via a well-known filesystem path. This combination – fw_cfg as the conduit, host-provided items via -fw_cfg, and guest-access via a sysfs hierarchy – directly substantiates the proposed mechanism for deterministic boot-time configuration transfer. Additional excerpts detailing how fw_cfg items can be provided as files or as strings and enumerated/read (e.g., file directories, selectors, and the read flow) flesh out the end-to-end process: (a) the host can pass items using name and either file or string payloads, (b) the guest enumerates those items through the fw_cfg interface, and (c) the guest reads the actual content from the configured items, with the OSDev guidance elaborating the structure and reading sequence. Taken together, these sources coherently support a deterministic, out-of-band configuration path from host to guest at boot via fw_cfg, including a usable guest-access path and host invocation syntax. Together, they justify the field value's emphasis on using fw_cfg as the recommended mechanism and on exposing configuration to the guest in a structured, accessible way that does not rely on traditional kernel command lines alone.",
      "confidence": "high"
    },
    {
      "field": "configuration_and_packaging_interface.idempotency_and_failure_modes",
      "citations": [
        {
          "title": "How to create a multiqueue TAP device in Linux",
          "url": "https://stackoverflow.com/questions/72268735/how-to-create-a-multiqueue-tap-device-in-linux",
          "excerpts": [
            "You can use ethtool -l [ifname] in your VM. NETWORK=\" -netdev tap,ifname=${TAP:-tap0},id=n1,script=no,downscript=no,queues=4 -device virtio-net-pci,mac=AA:BB:CC:DD:CA:B1,netdev=n1,mq=on,vectors=10\"",
            "Important options are\nvirtio-net-pci and\nmq=on,vectors=10 . Information about the vectors parameter https://www.linux-kvm.org/page/Multiqueue",
            "The docs say that for\nqueues=N the value for\nvectors should be \"2N+2\". Therefore, in your example with\nqueues=4 we should use\nvectors=10 ."
          ]
        },
        {
          "title": "Harold/Harry Hodge — Getting started with Firecracker (Firecracker networking and config)",
          "url": "https://harryhodge.co.uk/posts/2024/01/getting-started-with-firecracker/",
          "excerpts": [
            "\"subnet\": \"192.168.30.0/24\"",
            "\"rangeStart\": \"192.168.30.32\"",
            "\"boot_args\": \"... ip=192.168.30.39::192.168.30.1:255.255.255.0::eth0:off\"",
            "\"gateway\": \"192.168.30.1\"",
            "\"gateway\": \"192.168.30.1\"",
            ". CNI operates with network namespaces so we create unique namespaces for each VM interface."
          ]
        }
      ],
      "reasoning": "The field value requires the provisioning process to be idempotent and robust, with the host-side CLI taking ownership of resource lifecycle rather than letting the VM or hypervisor automatically manage TAP interfaces. The most relevant content explicitly discusses host-managed TAP device provisioning and configuration in a multi-queue context, including creating a TAP device (tap0), attaching it to a bridge, and configuring multiple queues for high-performance networking. This aligns with an idempotent provisioning approach where the CLI can detect existing TAP devices and bridges, reuse them, or recreate as needed, rather than relying on QEMU to create or manage these resources. Additional excerpts describe multiqueue TAP device setups, the use of virtio-net in conjunction with host-side networking, and host networking configurations such as subnet and bridge associations, which provide concrete context for how host-side resources are provisioned, configured, and potentially cleaned up. Together, these excerpts support the notion that host-controlled resource lifecycle (via pre-flight checks and cleanup on failure or exit) is the practical approach described in the fine-grained field value, particularly in the areas of TAP device creation, bridging, and host networking configuration. They do not, however, provide explicit commands in full for the cleanup routine, but the provided host-centric networking material strongly supports the intended behavior of idempotent provisioning and host-managed lifecycle. Specifically, references to creating a multiqueue TAP device, associating it with a bridge, and configuring relevant networking parameters are the strongest signals that host-controlled resource provisioning is feasible and aligns with the described approach.",
      "confidence": "medium"
    },
    {
      "field": "configuration_and_packaging_interface.configuration_schema",
      "citations": [
        {
          "title": "QEMU fw_cfg - Somlo CMU (Contrib.cm.u.edu)",
          "url": "http://www.contrib.andrew.cmu.edu/~somlo/QEMU_fw_cfg/",
          "excerpts": [
            "The fw\\_cfg device allows QEMU to prepare various data items (or \"blobs\") in host memory, and offers a guest-side interface allowing the guest to read (using port or memory-mapped I/O operations) the blobs after having been launched by the host"
          ]
        },
        {
          "title": "QEMU_fw_cfg (OSDev Wiki)",
          "url": "https://wiki.osdev.org/QEMU_fw_cfg",
          "excerpts": [
            "To pass strings or files into QEMU to appear in the fw\\_cfg device, use one or more command line parameters like those shown below:",
            "Listed above are a few fixed-purpose selectors for probing for the existence of the device, and for getting the number of files present in the device.",
            "### Enumerating Files",
            "The first four bytes of the *FW\\_CFG\\_FILE\\_DIR* selector will be a 32-bit big-endian number which is the number of files present in the directo",
            "Following the count, there is a sequence of entries with the following structure:",
            "th the following structure:\n\n```\n  struct FWCfgFile {\t\t/* an individual file entry, 64 bytes total */\n      uint32_t size;\t\t/* size of referenced fw_cfg item, big-endian */\n      uint16_t select;\t\t/* selector key of fw_cfg item, big-endian */\n      uint16_t reserved;\n      char name[56];\t\t/* fw_cfg item name, NUL-terminated ascii */\n  };",
            "These can be read one at a time by reading *sizeof(FWCfgFile)* bytes into a single variable of type *FWCfgFile* and examined one by one until you find a structure with the desired name.",
            "### Reading files",
            "Once you have found the *FWCfgFile* structure which has the desired name, read the *select* field from the structure. It will contain the big-endian selector for the file. After byte-swapping the selector, output it to *FW\\_CFG\\_PORT\\_SEL*, then read *size* bytes from *FW\\_CFG\\_PORT\\_DATA* and write them to the desired location in memory."
          ]
        },
        {
          "title": "Way to pass parameters or share a directory/file to a qemu ...",
          "url": "https://stackoverflow.com/questions/56535142/way-to-pass-parameters-or-share-a-directory-file-to-a-qemu-kvm-launched-vm-on-ce",
          "excerpts": [
            "If that guest is running a Linux kernel with CONFIG_FW_CFG_SYSFS enabled, you will be able to read out the information from sysfs. An example:."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs03/virtio-v1.0-cs03.html",
          "excerpts": [
            "Queue Size corresponds to the maximum number of buffers in the virtqueue. Queue Size value is always a power of 2. The maximum Queue Size value is 32768. This ...See more"
          ]
        }
      ],
      "reasoning": "The fine-grained field value describes a declarative, file-based configuration schema for a virtual machine that defines resources, devices, and guest-specific boot-time settings to be passed into a unikernel at boot. The most compelling support comes from passages that describe a fw_cfg (firmware configuration) mechanism in QEMU that allows the host to populate configuration blobs or files and for the guest to read them via a standardized interface. This directly demonstrates a path for host-to-guest configuration transfer and discovery, which underpins a file-based schema that the guest can consume at boot. Additional passages explain how fw_cfg items can be supplied from the command line or host-generated content, and how the guest reads file entries from the fw_cfg filesystem to obtain the desired data. Together, these excerpts illustrate a concrete mechanism for passing a structured, version-controlled configuration state from host to guest, consistent with the idea of a declarative configuration schema in a portable format like TOML or JSON, and for exposing device-related and guest-configuration parameters at boot time. Some excerpts provide broader context about fw_cfg item enumeration and the protocol for reading data, which supports the notion of a defined schema surface the guest can rely on at startup. Finally, related material on VirtIO and VIRTIO queues provides architectural context for how I/O devices might be represented in the VM configuration, though it is somewhat less directly tied to the file-based host-to-guest configuration transfer than fw_cfg-focused excerpts.",
      "confidence": "high"
    },
    {
      "field": "virtual_memory_management.address_space_layout",
      "citations": [
        {
          "title": "x86_64/mm.txt (Kernel memory management) – Linux Kernel Documentation",
          "url": "https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt",
          "excerpts": [
            "ffff888000000000 | -119.5  TB | ffffc87fffffffff |   64 TB | direct mapping of all physical memory (page_offset_base)",
            "Complete virtual memory map with 5-level page tables"
          ]
        },
        {
          "title": "linux virtual memory user/kernel space split in x86_64",
          "url": "https://stackoverflow.com/questions/25706656/linux-virtual-memory-user-kernel-space-split-in-x86-64",
          "excerpts": [
            "According to the /Documentation/x86/x86_64/mm.txt, the kernel virtual address begins at ffff800000000000 - ffff80ffffffffff (=40 bits) guard ..."
          ]
        },
        {
          "title": "Higher Half Kernel - OSDev Wiki",
          "url": "http://wiki.osdev.org/Higher_Half_Kernel",
          "excerpts": [
            "It is traditional and generally good to have your kernel mapped in every user process. Linux and many other Unices, for instance, reside at virtual addresses _0xC0000000 – 0xFFFFFFFF_ of every address space, leaving the range _0x00000000 – 0xBFFFFFFF_ for user code, data, stacks, libraries, "
          ]
        },
        {
          "title": "Higher Half x86 Bare Bones",
          "url": "https://wiki.osdev.org/Higher_Half_x86_Bare_Bones",
          "excerpts": [
            "A higher half kernel is a kernel that is mapped to the higher half of the virtual address space, despite being loaded to the lower half of the physical address space."
          ]
        }
      ],
      "reasoning": "The most directly supportive information specifies a direct mapping of all physical memory at a high virtual address, described as a direct mapping region starting at a high half address. This matches the requested element of a 1:1 mapping of physical RAM within the higher-half layout. The excerpt explicitly labels a direct mapping region with a start address that corresponds to the high-half space and a page-offset base, aligning with the requested layout.",
      "confidence": "high"
    },
    {
      "field": "virtio_network_datapath_model",
      "citations": [
        {
          "title": "rcore-os/virtio-drivers GitHub Repository",
          "url": "https://github.com/rcore-os/virtio-drivers",
          "excerpts": [
            "VirtIO guest drivers in Rust. For **no\\_std** environment",
            "### Device types\n\n[]()\n\n| Device | Supported |\n| --- | --- |\n| Block | ✅ |\n| Net | ✅ |\n| GPU | ✅ |\n| Input | ✅ |\n| Console | ✅ |\n| Socket | ✅ |\n| Sound | ✅ |\n| RNG | ✅ |\n| ...",
            "### Transports\n\n[]()\n\n| Transport | Supported |  |\n| --- | --- | --- |\n| Legacy MMIO | ✅ | version 1 |\n| MMIO | ✅ | version 2 |\n| PCI | ✅ | Memory-mapped CAM only, e.g. aarch64 or PCIe ",
            "### Device-independent features\n\n[]()\n\n| Feature flag | Supported |  |\n| --- | --- | --- |\n| `VIRTIO_F_INDIRECT_DESC` | ✅ | Indirect descriptors |\n| `VIRTIO_F_EVENT_IDX` | ✅ | `avail_event` and `used_event` fields |\n| `VIRTIO_F_VERSION_1` | TODO | VirtIO version 1 compli",
            "## Examples & Tests\n\n[]()\n\n### [x86\\_64](/rcore-os/virtio-drivers/blob/master/examples/x86_64)\n\n[]()\n\n```\ncd examples/x86_64\nmake qemu\n```\n\n### [aarch64](/rcore-os/virtio-drivers/blob/master/examples/aarch64)\n\n[]()\n\n```\ncd examples/aarch64\nmake qemu\n```\n\n### [RISCV](/rcore-os/virtio-drivers/blob/master/examples/riscv)\n\n[]()\n\n```\ncd examples/riscv\nmake qemu\n```\n\nYou will see device info & GUI Window in qemu.\n[<img src='https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png' width='50%' />](https://github.com/rcore-os/virtio-drivers/raw/master/examples/riscv/virtio-test-gpu.png)",
            "VirtIO guest drivers in Rust. For no_std environment. Support status: Device types: Transports: Device-independent features: Examples & Tests:"
          ]
        },
        {
          "title": "Virtio PCI Card Specification v0.9.4 DRAFT -",
          "url": "https://people.redhat.com/pbonzini/virtio-spec.pdf",
          "excerpts": [
            "Feb 7, 2012 — 2.2.1 Device Initialization Sequence. We start with an overview of device initialization, then expand on the details of the device and how ...",
            "Often a driver will suppress transmission interrupts using the VRING_AVAIL_F_NO_INTERRUPT flag (see 2.4.2) and check for used packets in the ..."
          ]
        },
        {
          "title": "memory-barriers.txt",
          "url": "https://www.kernel.org/doc/Documentation/memory-barriers.txt",
          "excerpts": [
            "... MMIO effects on accesses through relaxed memory I/O windows. These barriers are required even on non-SMP systems as they affect the order in which memory ..."
          ]
        },
        {
          "title": "Introduction to Virtio Protocol | openEuler",
          "url": "https://www.openeuler.org/en/blog/yorifang/virtio-spec-overview.html",
          "excerpts": [
            "QEMU sets a specific MMIO address space, which is accessed by the front-end driver to trigger VMExit. After exiting to the KVM, the front ...",
            "QEMU/KVM has a complete interrupt simulation implementation framework. For the virtio-pci device, MSIx can be presented to the VM through ...",
            "Nov 3, 2020 — The virtio protocol is offloaded to hardware (for example, virtio-net NICs and virtio-scsi) to accelerate physical machines and virtual machines (VMs).",
            "Nov 3, 2020 — A virtio device consists of four parts: device status field, feature bits, device configuration space, and one or more virtqueues. The device ...See more"
          ]
        },
        {
          "title": "address order of virtio-mmio devices - Patchwork - OzLabs",
          "url": "https://patchwork.ozlabs.org/patch/434610/",
          "excerpts": [
            "Jan 29, 2015 — Hi, I find that virtio-mmio devices are assigned highest-to-lowest addresses as they are found on the command line."
          ]
        },
        {
          "title": "Re: [qemu-system-aarch64] dtb flag for aarch64 Linux kernel",
          "url": "https://lists.nongnu.org/archive/html/qemu-discuss/2019-10/msg00014.html",
          "excerpts": [
            "> After that I decided to dump dtb that is generated inside Qemu: > qemu-system-aarch64 -M virt,dumpdtb=virt.dtb -cpu cortex-a57 -nographic ..."
          ]
        },
        {
          "title": "'virt' Generic Virtual Platform (virt) - QEMU - Read the Docs",
          "url": "https://qemu-stsquad.readthedocs.io/en/docs-review/system/riscv/virt.html",
          "excerpts": [
            "The virt machine automatically generates a device tree blob (“dtb”) which it passes to the guest, if there is no -dtb option. This provides information ..."
          ]
        },
        {
          "title": "VirtioNetMMIO",
          "url": "https://developer.arm.com/documentation/100964/latest/Fast-Models-components/Peripheral-components/VirtioNetMMIO",
          "excerpts": [
            "This is a model of a virtual Ethernet virtio device over MMIO transport, supporting both the legacy and v1.0 OASIS virtio specifications."
          ]
        },
        {
          "title": "Virtio Specification v1.1",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio Specification v1.0",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ...",
            "Each virtqueue consists of three parts: Descriptor Table; Available Ring; Used Ring. where each part is physically-contiguous in guest memory, and has different ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/virtio-v1.2.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1",
          "url": "https://www.kraxel.org/virtio/virtio-v1.1-cs01-video-v3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01-diff.html",
          "excerpts": [
            "\nThere are two parts to device  virtqueue  operation: supplying new available  buffers to\nthe device, and processing used buffers from the device. Note: As an example, the simplest virtio network device has two virtqueues: the\ntransmit virtqueue and the receive virtqueue. The driver adds outgoing\n(device-readable) packets to the transmit virtqueue, and then frees them\nafter they are used. Similarly, incoming (device-writable) buffers are\nadded to the receive virtqueue, and processed after they are used.",
            "\nThe driver offers buffers to one of the device’s virtqueues as follows:\n\n1. The driver places the buffer into free descriptor(s) in the descriptor table,\n   chaining as necessary (see [2.6.5]() [The Virtqueue Descriptor Table]()). 2. The driver places the index of the head of the descriptor chain into the\n   next ring entry of the available ring. 3. Steps [1]() and [2]() [1]() and [2]() MAY be performed repeatedly if batching is possible. 4. The driver performs suitable a a suitable memory barrier to ensure the device\n   sees the updated descriptor table and available ring before the next step.",
            "the simplest virtio network device has two virtqueues: the\ntransmit virtqueue and the receive virtqueue.",
            "The driver would send\n   the VIRTIO\\_NET\\_CTRL\\_MQ\\_VQ\\_PAIRS\\_SET command specifying the\n   number of the transmit and receive queues t",
            "An indirect descriptor without a valid next (with flags&VIRTQ_DESC_F_NEXT off) signals the end of the descriptor. A single indirect descriptor table can include ...",
            "A driver MUST NOT decrement the available idx on a virtqueue (ie. there is no way to “unexpose” buffers). 2.6.7 Used Buffer Notification Suppression.",
            "VIRTIO_F_EVENT_IDX would interrupt send a used buffer notification to the driver after the first buffer is used (and again after the 65536th buffer, etc). 2.6.8 ...",
            "Each virtqueue consists of three parts: Descriptor Table - occupies the Descriptor Area; Available Ring - occupies the Driver Area; Used Ring ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio on Linux",
          "url": "https://docs.kernel.org/driver-api/virtio/virtio.html",
          "excerpts": [
            "This document will focus on the common case of a Linux kernel running in a virtual machine and using paravirtualized devices provided by the hypervisor.",
            "The virtio spec defines these transport methods in detail, including device discovery, capabilities and interrupt handling. The communication between the driver ...",
            "Device discovery and probing¶. In the kernel, the virtio core contains the virtio bus driver and transport-specific drivers like virtio-pci and virtio-mmio ."
          ]
        },
        {
          "title": "Virtio",
          "url": "http://wiki.osdev.org/Virtio",
          "excerpts": [
            "VirtIO is a standardized interface which allows virtual machines access to simplified \"virtual\" devices, such as block devices, network adapters and consoles."
          ]
        },
        {
          "title": "Introduction to VirtIO",
          "url": "https://blogs.oracle.com/linux/post/introduction-to-VirtIO",
          "excerpts": [
            "It's an interface that allows a virtual machine to use its host's devices via minimized virtual devices called VirtIO devices.",
            "May 24, 2022 — The resulting used ring is shown below in figure 12: Descriptor Table + Avail Ring + Used Ring 2. Figure 12. Here we can see how a used ring ...",
            "May 24, 2022 — VirtIO in Qemu. In this section we're going to take a look at an example VirtIO device in Qemu and see how one of its VirtQueues works. The ..."
          ]
        },
        {
          "title": "VirtIO MMIO Specification",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs01/virtio-v1.0-cs01.html",
          "excerpts": [
            "\nThe driver will typically initialize the virtual queue in the following way:\n\n1. Select the queue writing its index (first queue is 0) to QueueSel . 2. Check if the queue is not already in use: read QueueReady , and expect a\n    returned value of zero (0x0). 3. Read maximum queue size (number of elements) from QueueNumMax . If\n    the returned value is zero (0x0) the queue is not available. 4. Allocate and zero the queue pages, making sure the memory is physically\n    contiguous. It is recommended to align the Used Ring to an optimal\n    boundary (usually the page size). 5. Notify the device about the queue size by writing the size to QueueNum . 6. Write physical addresses of the queue’s Descriptor Table, Available Ring\n    and Used Ring to (respectively) the QueueDescLow / QueueDescHigh , QueueAvailLow / QueueAvailHigh and QueueUsedLow / QueueUsedHigh register pairs. 7. Write 0x1 to QueueReady .",
            "Descriptor Table | 16 | 16 ∗ (Queue Size)",
            "Available Ring | 2 | 6 \\+ 2 ∗ (Queue Size",
            " |\n|  |  |\n|  |  |\n|  |  |\n|  |  |\n|",
            "A virtio device is discovered and identified by a bus-specific method (see the bus specific sections: 4.1 Virtio Over PCI Bus, 4.2 Virtio Over MMIO and 4.3 ...",
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "VirtIO MMIO and Device Initialization (VirtIO v1.1/v1.0+)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html",
          "excerpts": [
            "MMIO virtio devices provide a set of memory mapped control registers followed by a\ndevice-specific configuration space, described in the table [4\\.1]() . All register values are organized as Little Endian",
            "  \nOffset from\n base  \nDirection | Function  \nDescription |",
            " |\n|  |  |\n|  |  |\n| MagicValue  \n0x000  \nR | Magic value  \n0x74726976 (a Little Endian equivalent",
            " * * * | * * * |\n| Version  \n0x004  \nR | Device version number  \n0x2. Note: Legacy devices (see",
            "DeviceID  \n0x008  \nR | Virtio Subsystem Device ID",
            "VendorID  \n0x00c  \nR | Virtio Subsystem Vendor ID",
            "Unlike PCI, MMIO provides no generic device discovery mechanism. For each device,\nthe guest OS will need to know the location of the registers and interrupt(s) used. The suggested binding for systems using flattened device trees is shown in this\nexample:",
            "HostFeaturesSel  \n0x014  \nW | Device (host) features word selection",
            "// EXAMPLE : virtio\\_block device taking 512 bytes at 0 x1e000 , interrupt 42\\. virtio\\_block@1e000 {  \ncompatible = \" virtio , mmio \";  \nreg = <0 x1e000 0 x200 >;  \ninterrupts = <42>",
            "The driver MUST ignore a device with MagicValue which is not 0x74726976,\nalthough it MAY report an error. The driver MUST ignore a device with Version which is not 0x2, although it MAY\nreport an error.",
            "The device MUST handle the case of zero or more normal chained descriptors followed by a single descriptor with flags&VIRTQ_DESC_F_INDIRECT. Note: While unusual ... Latest version:  \n  <https://docs.oasis-open.org/virtio/virtio/v1.0/virtio-v1.0.html> <http://ozlabs.org/~rusty/virtio-spec/virtio-0.9.5.pdf>\n*",
            " Descriptor Area - used for describing buffers",
            "Available Ring - for the Driver Area",
            "Used Ring - for the Device Area",
            "The mechanism for bulk data transport on virtio devices is pretentiously called a virtqueue. Each device can have zero or more virtqueues. Driver makes requests ...",
            "\n   The virtio console device is a simple device for data input and output. A\n   device MAY have one or more ports. Each port has a pair of input and\n   output virtqueues. Moreover, a device has a pair of control IO virtqueues.",
            "es.\nThe\n   control virtqueues are used to communicate information between the device\n   and the driver about ports being opened and closed on either side of the\n   connection, indication from the device about whether a particular port is a console\n   port, adding new ports, port hot-plug/unplug, etc., and indication from\n   the driver about whether a port or a device was successfully added, port\n   open/close, etc.",
            "tc. For data IO, one or more empty buffers are placed in the receive\n   queue for incoming data and outgoing characters are placed in the transmit\n   queue.",
            "ed. If VIRTIO\\_CONSOLE\\_F\\_EMERG\\_WRITE is set then the driver can use emergency\n   write to output a single character without initializing virtio queues, or even\n   acknowledging the feature.",
            "Device has support for multiple\n       ports; max\\_nr\\_ports is valid and control virtqueues will b",
            "The port 0 receive and transmit queues always exist: other queues only exist if\n   VIRTIO\\_CONSOLE\\_F\\_MULTIPORT is set.",
            "... feature negotiation facilities built into virtio, several devices are defined. The following device IDs are used to identify different types of virtio devices.",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtio MMIO Specification (VirtIO v1.2)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.pdf",
          "excerpts": [
            "MMIO virtio devices provide a set of memory mapped control registers followed by a device-specific config-  \nuration space, described in the table [4\\.1](-.html) . All register values are organized as Little En",
            "n\n\n_MagicValue_ 0x000  \nR\n\n**Magic value** 0x74726976 (a Little Endian equivalent of the “virt” string).",
            "MMIO Device Discovery",
            "Unlike PCI, MMIO provides no generic device discovery mechanism. For each device, the guest OS will  \nneed to know the location of the registers and interrupt(s) use",
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ...",
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "pdf - Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf",
          "excerpts": [
            "by MS Tsirkin · 2022 · Cited by 23 — This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design ..."
          ]
        },
        {
          "title": "[PDF] Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.pdf",
          "excerpts": [
            "VIRTIO_NET_F_VQ_NOTF_COAL(52) Device supports virtqueue notification coalescing. VIRTIO_NET_F_NOTF_COAL(53) Device supports notifications ...",
            "Abstract: This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, ..."
          ]
        },
        {
          "title": "virtio-queue - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/virtio-queue",
          "excerpts": [
            "The virtio-queue crate provides a virtio device implementation for a virtio queue, a virtio descriptor and a chain of such descriptors.",
            "Two formats of virtio queues are defined in the specification: split virtqueues\nand packed virtqueues.",
            "The `virtio-queue` crate offers support only for the [split virtqueues](https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01.html) forma",
            "The main abstraction is the `Queue` .",
            "Jun 26, 2025 — The virtio-queue crate provides a virtio device implementation for a virtio queue, a virtio descriptor and a chain of such descriptors.",
            "The device should validate before reading/writing to a buffer that it is\n  device-readable/device-writabl"
          ]
        },
        {
          "title": "Implementing a virtio-blk driver in my own operating system",
          "url": "https://brennan.io/2020/03/22/sos-block-device/",
          "excerpts": [
            "The “mmio” portion stands for Memory Mapped I/O, which means that all of the registers for these devices appear simply as memory addresses.",
            "Mar 22, 2020 — In this blog post, I'm going to describe how I wrote my first block device driver in my operating system, SOS."
          ]
        },
        {
          "title": "virtio-v1.3-csd01 (OASIS) - MMIO device handling and MMIO device discovery (Section 4.x)",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/csd01/virtio-v1.3-csd01-diff-from-v1.2-cs01.pdf",
          "excerpts": [
            "Unlike PCI, MMIO provides no generic device discovery mechanism. For each device, the guest OS will\n\nneed to know the location of the registers and interrupt(s) used",
            "IO Device Register Layout**\n\nMMIO virtio devices provide a set of memory mapped control registers followed by a device-specific config-\n\nuration space, described in the table ",
            "The driver MUST start the device initialization by reading and checking values from _MagicValue_ and _Versi"
          ]
        },
        {
          "title": "NVIDIA DOCA Virtio-net Service Guide",
          "url": "https://docs.nvidia.com/doca/archive/2-9-0/nvidia+doca+virtio-net+service+guide/index.html",
          "excerpts": [
            "... Virtio-net Deployment\" page for more information. Queue Depth. By default, queue depth is set to 256. It is common to use a larger queue depth (e.g., 1024)."
          ]
        },
        {
          "title": "[PDF] Configuring and Benchmarking Open vSwitch, DPDK and vhost-user",
          "url": "http://events17.linuxfoundation.org/sites/events/files/slides/Configuring%20and%20Benchmarking%20%20Open%20vSwitch%2C%20DPDK%20and%20vhost-user_Pei%20Zhang.pdf",
          "excerpts": [
            "vhost-user protocol allows qemu shares virtqueues with a user space ... Real-Time always keeps low latency, it's used for latency-sensitive.",
            "by P Zhang · 2017 · Cited by 2 — Real-Time always keeps low latency, it's used for latency-sensitive workloads. Real-Time KVM(RT-KVM) is the extension of KVM, it allows the ..."
          ]
        },
        {
          "title": "Introduction to virtio-networking and vhost-net",
          "url": "https://www.redhat.com/en/blog/introduction-virtio-networking-and-vhost-net",
          "excerpts": [
            "Sep 9, 2019 — In this post we will provide a high level solution overview of the virtio-networking architecture, based on establishing an interface between the host kernel ...",
            "Sep 9, 2019 — We will present the basic building blocks including KVM, qemu and libvirt. We will look at the virtio spec and vhost protocol, and Open ...",
            "Sep 9, 2019 — Virtio-net is a virtual ethernet card and is the most complex device supported so far by virtio. In this post we will provide a high level ...",
            "Sep 9, 2019 — As explained the control plane simply implements the virtio spec for the vhost-net kernel module and the qemu process to communicate which then ...",
            "Sep 9, 2019 — vhost-net is the backend running in the host kernel space. virtio-net is the frontend running in the guest kernel space. The following diagram ..."
          ]
        },
        {
          "title": "Virtualization Tuning and Optimization Guide",
          "url": "https://docs.redhat.com/es/documentation/red_hat_enterprise_linux/7/epub/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-blockio-multi-queue_virtio-scsi",
          "excerpts": [
            "VirtIO devices have low overhead thanks to the fact that they are designed specifically for use in Virtualization environments. However not all guest operating ...",
            "The Red Hat Enterprise Linux Virtualization Tuning and Optimization Guide covers KVM and virtualization performance. Within this guide, you can find tips ..."
          ]
        },
        {
          "title": "rust-vmm/vhost-device - GitHub",
          "url": "https://github.com/rust-vmm/vhost-device",
          "excerpts": [
            "This repository hosts various 'vhost-user' device backends in their own crates. See their individual README.md files for specific information about those crates ..."
          ]
        },
        {
          "title": "Virtualization Tuning and Optimization Guide (Red Hat Enterprise Linux 7)",
          "url": "https://docs.redhat.com/pt-br/documentation/red_hat_enterprise_linux/7/html-single/virtualization_tuning_and_optimization_guide/index",
          "excerpts": [
            "Multi-queue virtio-net provides an approach that scales the network performance as the number of vCPUs increases, by allowing them to transfer packets through more than one virtqueue pair at a time.",
            "vhost\\_net moves part of the Virtio driver from the user space into the kernel. This reduces copy operations, lowers latency and CPU usage",
            "To use multi-queue virtio-net, enable support in the guest by adding the following to the guest XML configuration (where the value of *N* is from 1 to 256, as the kernel supports up to 256 queues for a multi-queue tap device",
            "When running a virtual machine with *N* virtio-net queues in the guest, enable the multi-queue support with the following command (where the value of *M* is from 1 to",
            "eled packet processing. Multi-queue virtio-net provides the greatest performance benefit when:\n\n* Traffic packets are relatively large. * The guest is active on many connections at the same time, with traffic running between guests, guest to host, or guest to an external system. * The number of queues is equal to the number of vCPUs.",
            "It typically reduces the host CPU overhead by up to 15% when transmitting large packets between a guest network and an external network, without affecting throughput.",
            "An additional data copy is normally created during transmit as a threat mitigation technique against denial of service and information leak attacks. Enabling zero copy transmit disables this threat mitigation technique."
          ]
        },
        {
          "title": "Multiqueue virtio-net",
          "url": "https://www.linux-kvm.org/page/Multiqueue",
          "excerpts": [
            "Multiqueue virtio-net",
            "Overview",
            "This page provides information about the design of multi-queue virtio-net, an approach enables packet sending/receiving processing to scale with the number of available vcpus of guest. This page provides an overview of multiqueue virtio-net and discusses the design of various parts involved. The page also contains some basic performance test result.",
            "Today's high-end server have more processors, guests running on them tend have an increasing number of vcpus. The scale of the protocol stack in guest in restricted because of the single queue virtio-net:",
            "* The network performance does not scale as the number of vcpus increasing: Guest can not transmit or retrieve packets in parallel as virtio-net have only one TX and RX, virtio-net drivers must be synchronized before sending and receiving packets.",
            "Multiqueue nic were more common used and is well supported by linux kernel, but current virtual nic can not utilize the multi queue support: the tap and virtio-net backend must serialize the co-current transmission/receiving request comes from different cpus.",
            "In order the remove those bottlenecks, we must allow the paralleled packet processing by introducing multi queue support for both back-end and guest drivers.",
            "Ideally, we may let the packet handing be done by processors in parallel without interleaving and scale the network performance as the number of vcpus increasing.",
            "Rationale",
            "Rationale",
            "To make sure the whole stack could be worked in parallel, the parallelism of not only the front-end (guest driver) but also the back-end (vhost and tap/macvtap) must be explored. This is done by:",
            "* Allowing multiple sockets to be attached to tap/macvtap",
            "* Using multiple threaded vhost to serve as the backend of a multiqueue capable virtio-net adapter",
            "* Use a multi-queue awared virtio-net driver to send and receive packets to/from each queue",
            "High level Design",
            "The main goals of multiqueue is to explore the parallelism of each module who is involved in the packet transmission and reception:",
            "* macvtap/tap: For single queue virtio-net, one socket of macvtap/tap was abstracted as a queue for both tx and rx. We can reuse and extend this abstraction to allow macvtap/tap can dequeue and enqueue packets from multiple sockets. Then each socket can be treated as a tx and rx, and macvtap/tap is fact a multi-queue device in the host. The host network codes can then transmit and receive packets in parallel.",
            "* vhost: The parallelism could be done through using multiple vhost threads to handle multiple sockets. Currently, there's two choices in design. + 1:1 mapping between vhost threads and sockets. This method does not need vhost changes and just launch the the same number of vhost threads as queues. Each vhost thread is just used to handle one tx ring and rx ring just as they are used for single queue virtio-net. + M:N mapping between vhost threads and sockets. This methods allow a single vhost thread to poll more than one tx/rx rings and sockests and use separated threads to handle tx and rx request",
            "* qemu: qemu is in charge of the fllowing things\n  + allow multiple tap file descriptors to be used for a single emulated nic\n  + userspace multiqueue virtio-net implementation which is used to maintaining compatibility, doing management and migration\n  + control the vhost based on the userspace multiqueue virti",
            "* guest driver\n  + Allocate multiple rx/tx queues\n  + Assign each queue a MSI-X vector in order to parallize the packet processing in guest s",
            "Choices and considerations",
            "* 1:1 or M:N, 1:1 is much more simpler than M:N for both coding and queue/vhost management in qemu. And in theory, it could provides better parallelism than M:N. Performance test is needed to be done by both of the implementation.",
            "* Whether use a per-cpu queue: Morden 10gb card ( and M$ RSS) suggest to use the abstract of per-cpu queue that tries to allocate as many tx/rx queues as cpu numbers and does a 1:1 mapping between them. This can provides better parallelism and ",
            "The big picture looks like:",
            "1.9.4 vhost",
            "vhost: The parallelism could be done through using multiple vhost threads to handle multiple sockets. Currently, there's two choices in design.",
            "+ 1:1 mapping between vhost threads and sockets.",
            "+ M:N mapping between vhost threads and sockets.",
            "1.9.5 qemu changes",
            "1.9.6 guest driver",
            "The big picture is shown as.",
            "This page provides information about the design of multi-queue virtio-net, an approach enables packet sending/receiving processing to scale with the number of ...",
            "This page provides information about the design of multi-queue virtio-net, an approach enables packet sending/receiving processing to scale with the number of available vcpus of guest. This page provides an overview of multiqueue virtio-net and discusses the design of various parts involved. The page also contains some basic performance test result. This work is in progress and the design may changes. Contact\n-------\n"
          ]
        },
        {
          "title": "Intel KVM Tuning Guide: virtio-net & vhost-net",
          "url": "https://www.intel.com/content/www/us/en/developer/articles/guide/kvm-tuning-guide-on-xeon-based-systems.html",
          "excerpts": [
            "Virtio-net is the main virtual ethernet network interface for virtual machine in KVM/Qemu environment. Its functionality is described next. A frontend driver sits in the virtual machine kernel. Then guest workload issues network packets into the frontend device/driver via the virtqueues, which is shared ring buffer between frontend and backend. Then a backend driver is implemented as a user land driver in Qemu. Finally, it receives network packets from the virtqueues and transfers them to the physical network.",
            "Enable vhost-net",
            "\n\nUser land Qemu virtio-net driver has some disadvantages.\nContext switch between kernel and Qemu is needed to handle packets. This is an expensive operation, causes a lot of processing time and packet process latency. Vhost-net is a kernel acceleration module for virtio-net, which offloads the virtio-net data plane handling from user land to kernel. It leverages Linux ioeventfd mechanism, any guest access to the virtqueue memory mapped notification address will directly invoke vhost-net worker thread to handle the packets. Meanwhile, Linux kernel irqfd mechanism is also leveraged so that IRQs can be injected into guest by any thread on host. These asynchronous mechanisms improve virtio-net performance a lot. Use ",
            "Enable Multiqueue",
            "As the number of vCPUs increases, it is desirable to enable multi-queue virtio-net to boost performance of network packet handling. One vCPU can be dedicated to one virtqueue so that the overall parallelism and throughput can be improved significantly.",
            "Enable packed virtqueues",
            "Traditionally Virtio shares split queue between host and virtual machine. Packed virtqueues is an alternative compact virtqueue layout that is both read and written by both host and guest, which is more efficient regarding to performance.",
            "Enable Zero Copy Transmission",
            "To enable transmission zero copy in Linux kernel vhost-net driver, use the following commands:\n\n```\n# modprobe -r vhost_net\n\n# modprobe vhost_net experimental_zcopytx=1\n```",
            "Enable Queue Polling",
            "Vhost-net is also possible to enable Queue Polling for better performance. The basic idea is to busy wait for a while after all packets has been handled to see if there is any new packet arriving.",
            "\nUser land Qemu virtio-net driver has some disadvantages.\nContext switch between kernel and Qemu is needed to handle packets. This is an expensive operation, causes a lot of processing time and packet process latency. Vhost-net is a kernel acceleration module for virtio-net, which offloads the virtio-net data plane handling from user land to kernel. It leverages Linux ioeventfd mechanism, any guest access to the virtqueue memory mapped notification address will directly invoke vhost-net worker thread to handle the packets. Meanwhile, Linux kernel irqfd mechanism is also leveraged so that IRQs can be injected into guest by any thread on host. These asynchronous mechanisms improve virtio-net performance a lot. Use “vhost=on” in the “-netdev tap” Qemu command line option to enable vhost-net, like the following example\n\n```\n-netdev tap,id=nic0,vhost=on,script=$IFUP_SCRIPT,downscript=no \\\n\n-device virtio-net-pci,mq=on,netdev=nic0,mac=$MAC\n```",
            "Enable Multiqueue**\n\nAs the number of vCPUs increases, it is desirable to enable multi-queue virtio-net to boost performance of network packet handling. One vCPU can be dedicated to one virtqueue so that the overall parallelism and throughput can be improved significantly.\nUse “queues=<num>” in the “-netdev tap” Qemu command line option to specify the number of queues, and use “mq=on” in the “-device virtio-net-pci” command line option to expose multi-queue to guest, like the following example:\n\n```\n-netdev tap,id=nic0,vhost=on,queues=4,script=$IFUP_SCRIPT,downscript=no \\\n\n-device virtio-net-pci,mq=on,packed=on,netdev=nic0,mac=$MAC\n``",
            "Enable packed virtqueues**\n\nTraditionally Virtio shares split queue between host and virtual machine. Packed virtqueues is an alternative compact virtqueue layout that is both read and written by both host and guest, which is more efficient regarding to performance. Use “packed=on” in “-device virtio-net-pci” Qemu command line to enable packed virtqueues. ```\n-netdev tap,id=nic0,vhost=on,queues=4,script=$IFUP_SCRIPT,downscript=no \\\n\n-device virtio-net-pci,mq=on,packed=on,netdev=nic0,mac=$MAC\n``",
            "Enable Zero Copy Transmission**\n\nTo enable transmission zero copy in Linux kernel vhost-net driver, use the following commands:\n\n```\n# modprobe -r vhost_net\n\n# modprobe vhost_net experimental_zcopytx=1\n```\n\nAnd the following to verify it has been set successfully:\n\n```\n# cat /sys/modules/vhost_net/parameters/experimental_zcopytx\n```\n\n**Enable Queue Polling**\n\nVhost-net is also possible to enable Queue Polling for better performance. The basic idea is to busy wait for a while after all packets has been handled to see if there is any new packet arrivin",
            "The basic idea is to busy wait for a while after all packets has been handled to see if there is any new packet arriving.",
            "This guide aims to help the reader to get the best virtualization performance on 3 rd Generation Intel Xeon Scalable processor based platforms.",
            "Aug 12, 2021 — This causes a lot performance jitter for KVM/Qemu virtualization. With recent Linux kernel, it is possible to offload the timer injection to ..."
          ]
        },
        {
          "title": "Red Hat/DPDK vhost-user and latency considerations",
          "url": "https://access.redhat.com/solutions/3394851",
          "excerpts": [
            "the RX and TX buffers of qemu’s virtio queue are limited to a default of 256 and a maximum of 1024 entries.",
            "vhost user has since 2 sides:",
            "*Slave** - Open vSwitch or any other software switc",
            "vhost user can run in 2 modes:",
            "*vhostuser-client** - qemu is the server, the software switch is the clien",
            "*vhostuser** - the software switch is the server, qemu is the clien",
            "vhost user is based on the vhost architecture and implements all features in user space.",
            "When the instance is polling, CPU caches are hot, low extra latency or delays, and the right processes in the host and in the VM are running, further reducing latency.",
            "The guest can receive an interrupt if flag `VRING_AVAIL_F_NO_INTERRUPT` is not se",
            "The advantage of running DPDK within the instance as part of the NFV application lies in how PMDs handle traffic: because PMDs are constantly polling, they can switch off the interrupt constantly and hence OVS DPDK does not have to kick the VM any more.",
            "This saves OVS DPDK from having to write to the socket and hence spares it from having to execute a write system call to the kernel.",
            "The guest OS can hence enable or disable interrupts.",
            "he message.\nThe protocol defines 2 sides of the communication, master and slave. Master is\nthe application that shares its virtqueues, in our case QEMU. Slave is the\nconsumer of the virtqueues",
            "Vhost as a userspace interface",
            "One surprising aspect of the vhost architecture is that it is not tied to KVM in any way. Vhost is a userspace interface and has no dependency on the KVM kernel module.",
            "When a guest kicks the host because it has placed buffers onto a virtqueue, there needs to be a way to signal the vhost worker thread that there is work to do.",
            "Instead vhost instances are set up with an eventfd file descriptor which the vhost worker thread watches for activity.",
            "Jun 14, 2024 — While Linux kernel networking ( NAPI ) uses a mix of interrupt and poll mode processing, it is still exposed to a high number of interrupts. OVS ...",
            "Jun 14, 2024 — A detailed view of the vhost user protocol and its implementation in OVS DPDK, qemu and virtio-net. Solution Verified - Updated June 14 2024 ..."
          ]
        },
        {
          "title": "vhost-user-backend (Rust) README",
          "url": "https://crates.io/crates/vhost-user-backend",
          "excerpts": [
            "\n\nThe main thread and virtio queue working threads will concurrently access the underlying virtio\nqueues, so all virtio queue in multi-threading model. But the main thread only accesses virtio\nqueues for configuration, so client could adopt locking policies to optimize for the virtio queue\nworking threads.",
            "Example code to handle virtio messages from a virtio queue:",
            "o client could adopt locking policies to optimize for the virtio queue\nworking threads. ## []() Example\n\nExample code to handle virtio messages from a virtio queue:\n\n```rust\nimpl VhostUserBackendMut for VhostUserService {\n    fn process_queue (& mut self , vring: &VringMutex) -> Result < bool > {\n        let mut used_any = false ;\n        let mem = match & self .mem {\n            Some (m) => m. memory (),\n            None => return Err (Error::NoMemoryConfigured),\n        };\n\n        let mut vring_state = vring. get_mut ();\n\n        while let Some (avail_desc) = vring_state\n            . get_queue_mut ()\n            . iter ()\n            . map_err (|_| Error::IterateQueue)? . next ()\n        {\n            // Process the request...\n\n            if self .event_idx {\n                if vring_state. add_used (head_index, 0 ). is_err () {\n                    warn! ( \"Couldn't return used descriptors to the ring\" );\n                }\n\n                match vring_state. needs_notification () {\n                    Err (_)"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - GitHub Pages",
          "url": "https://stefanha.github.io/virtio/vhost-user-slave.html",
          "excerpts": [
            "Each virtqueue consists of three parts: Descriptor Table; Available Ring; Used Ring. where each part is physically-contiguous in guest memory, and has different ...",
            "The driver MAY use used_event to advise the device that interrupts are unnecessary until the device writes entry with an index specified by used_event into the ..."
          ]
        },
        {
          "title": "Packed virtqueue: How to reduce overhead with virtio - Red Hat",
          "url": "https://www.redhat.com/en/blog/packed-virtqueue-how-reduce-overhead-virtio",
          "excerpts": [
            "We start by covering the simpler and less optimized split virtqueue layout. This layout is relatively easy to implement and to debug thus it's ...",
            "Jul 15, 2020 — Packed virtqueue amends it by merging the three rings in just one location in virtual environment guest memory."
          ]
        },
        {
          "title": "VirtIO-net Emulated Devices",
          "url": "https://docs.nvidia.com/networking/display/bluefielddpuosv398/VirtIO-net+Emulated+Devices",
          "excerpts": [
            "... virtio queues with a depth of 1024 entries. This device is uniquely identified by its index. This index is used to query and update device ...See more"
          ]
        },
        {
          "title": "Performance Tuning",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2407/Performance+Tuning",
          "excerpts": [
            "By default, queue depth is set to 256. It is common to use a larger queue depth (e.g., 1024). This cannot be requested from the driver side but ...See more"
          ]
        },
        {
          "title": "Chapter 14. Optimizing virtual machine performance | 8",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/optimizing-virtual-machine-performance-in-rhel_monitoring-and-managing-system-status-and-performance",
          "excerpts": [
            "To set up the multi-queue virtio-net feature for a VM, use the virsh edit command to edit to the XML configuration of the VM. In the XML, add the following to ..."
          ]
        },
        {
          "title": "Busy Poll - Documentation - NXP Semiconductors",
          "url": "https://docs.nxp.com/bundle/GUID-39A0A446-70E5-4ED7-A580-E7508B61A5F1/page/GUID-16CBBE22-9F68-4C17-853F-AC89E3FB4F43.html",
          "excerpts": [
            "Mar 31, 2017 — Busy polling is a mechanism to reduce the latency on the network receive path. When enabled, it allows the socket layer code to directly poll the receive queue."
          ]
        },
        {
          "title": "Guía de ajuste y optimización de la virtualización",
          "url": "https://translate.google.com/translate?u=https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html-single/virtualization_tuning_and_optimization_guide/index&hl=es&sl=en&tl=es&client=srp",
          "excerpts": [
            "Las CPU virtuales (vCPU) se implementan como subprocesos normales, manejados por el programador de Linux. Los invitados heredan características como NUMA y ...Large performance improvements can be obtained by adhering to the system's specific NUMA topology. Use this option to automatically generate a pinning ..."
          ]
        },
        {
          "title": "[net-next,v3,4/4] net: vhost: add rx busy polling in tx path",
          "url": "https://patchwork.ozlabs.org/patch/937322/",
          "excerpts": [
            "[net-next,v3,4/4] net: vhost: add rx busy polling in tx path. Message ID ... latency. When running the tests, the vhost-net kthread of that VM, is alway ..."
          ]
        },
        {
          "title": "Chapter 15. Optimizing virtual machine performance | 9",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/monitoring_and_managing_system_status_and_performance/optimizing-virtual-machine-performance-in-rhel_monitoring-and-managing-system-status-and-performance",
          "excerpts": [
            "Virtual CPUs (vCPUs) are implemented as threads on the host, handled by the Linux scheduler. VMs do not automatically inherit optimization features, such as ..."
          ]
        },
        {
          "title": "Virtqueues and virtio ring: How the data travels",
          "url": "https://www.redhat.com/en/blog/virtqueues-and-virtio-ring-how-data-travels",
          "excerpts": [
            "This way, every time the driver wants to make available a buffer it needs to check the avail\\_event on the used ring: If driver’s idx field was equal to avail\\_event, it’s time to send a notification, ignoring the lower bit of used ring flags member (`VIRTQ_USED_F_NO_NOTIFY`). Similarly, if `VIRTIO_F_EVENT_IDX` has been negotiated, the device will check used\\_event to know if it needs to send a notification or not. This can be very effective for maintaining a virtqueue of buffers for the device to write, like in the virtio-net device receive queue.",
            "Jul 8, 2020 — The split virtqueue format separates the virtqueue into three areas, where each area is writable by either the driver or the device, but not .",
            "— The split virtqueue format separates the virtqueue into three areas, where each area is writable by either the driver or the device, but not ... * For other emulated devices like vhost-net or vhost-user, a memory mapping needs to be done, like POSIX shared memory. A file descriptor to that memory is shared through vhost protocol.",
            "**Listing: Split Virtqueue descriptor layout**",
            "The descriptor area (or descriptor ring) is the first one that needs to be understood. It contains an array of a number of guest addressed buffers and its length. Each descriptor also contains a set of flags indicating more information about it. For example, the buffer continues in another descriptor buffer if the 0x1 bit is set, and the buffer is write-only for the device if the bit 0x2 is set, and is read-only if it is clear.",
            "### Avail ring: Supplying data to the device",
            "The next interesting structure is the driver area, or avail ring. Is the room where the driver places the descriptor (indexes) the device is going to consume.",
            "A more effective way of notifications toggle is enabled if the `VIRTIO_F_EVENT_IDX` feature bit is negotiated by device and driver: Instead of disable them in a binary fashion, driver and device can specify how far the other can progress before a notification is required using an specific descriptor id.",
            "This id is advertised using a extra le16 member at the end of the structure, so they grow like this:",
            "The struct layout is:",
            "```\nstruct virtq_avail {              struct virtq_used {\n  le16 flags;                       le16 flags;\n  le16 idx;                         le16 idx;\n  le16 ring[ /* Queue Size */ ];    struct virtq_used_elem ring[Q. size];\n  le16 used_event;                  le16 avail_event;\n};                                };\n\n```",
            "Jul 8, 2020 — Indirect descriptors are a way to dispatch a larger number of descriptors in a batch, increasing the ring capacity. The driver stores a table of ...",
            "Jul 8, 2020 — A virtqueue is just a queue of guest's buffers that the host consumes, either reading them or writing to them.",
            "The idx field indicates where the driver would put the next descriptor entry in the avail ring (modulo the queue size). On the other hand, the ...",
            "Jul 8, 2020 — A virtqueue is just a queue of guest's buffers that the host consumes, either reading them or writing to them.See more",
            "Jul 8, 2020 — Split virtqueue: the beauty of simplicity. The split virtqueue format separates the virtqueue into three areas, where each area is writable by ..."
          ]
        },
        {
          "title": "Achieving the Ultimate Performance with KVM",
          "url": "https://www.usenix.org/sites/default/files/conference/protected-files/srecon20americas_slides_krosnov.pdf",
          "excerpts": [
            "by B Krosnov — Next three are statistics describing scheduling latency: 7) sum of ... regular virtio vs vhost-net vs vhost-user. Linux Bridge vs OVS in-kernel vs OVS-DPDK."
          ]
        },
        {
          "title": "Using Open vSwitch* with DPDK for Inter-VM NFV ...",
          "url": "https://www.intel.com/content/www/us/en/develop/articles/using-open-vswitch-with-dpdk-for-inter-vm-nfv-applications.html",
          "excerpts": [
            "Feb 14, 2018 — This article shows step-by-step how to configure OvS-DPDK for inter-VM application use cases. Specifically, we create an OvS vSwitch bridge with two DPDK vhost ..."
          ]
        },
        {
          "title": "Hands on vhost-net: Do. Or do not. There is no try",
          "url": "https://www.redhat.com/en/blog/hands-vhost-net-do-or-do-not-there-no-try",
          "excerpts": [
            "Sep 18, 2019 — If we compare TCP and UDP latency of both architectures, we see that the use of vhost-net provides a consistent improvement with respect to QEMU ...",
            "Sep 18, 2019 — Vhost-net has silently become the default traffic offloading mechanism for qemu-kvm based virtual environments leveraging the standard virtio ...",
            "Vhost-net has silently become the default traffic offloading mechanism for qemu-kvm based virtual environments leveraging the standard virtio networking interface. This mechanism allows the network processing to be performed in a kernel module freeing the qemu process and improving the overall network performance."
          ]
        },
        {
          "title": "6.3. Enabling vhost-net zero-copy",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-network_configuration-enabling_vhost_net_zero_copy",
          "excerpts": [
            "In Red Hat Enterprise Linux 7, vhost-net zero-copy is disabled by default. To enable this action on a permanent basis, add a new file vhost-net.conf to /etc/ ..."
          ]
        },
        {
          "title": "5.4. Network Tuning Techniques",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/sect-virtualization_tuning_optimization_guide-networking-techniques",
          "excerpts": [
            "To enable zero copy transmit mode, set the experimental_zcopytx kernel module parameter for the vhost_net module to 1. For detailed instructions, see the ...",
            "Multi-queue virtio-net provides an approach that scales the network performance as the number of vCPUs increases, by allowing them to transfer packets through ..."
          ]
        },
        {
          "title": "(PDF) Performance of OVS-DPDK in Container Networks",
          "url": "https://www.researchgate.net/publication/347649234_Performance_of_OVS-DPDK_in_Container_Networks",
          "excerpts": [
            "Dec 23, 2020 — In this paper, we design and integrate OVS-DPDK in container networking in two cases. We deployed the two containers in a host and used OVS-DPDK ..."
          ]
        },
        {
          "title": "KVM vhost-net vs userspace networking",
          "url": "https://gist.github.com/vancluever/afb682641fd3cfc1fb4cbeda0bfc44fc",
          "excerpts": [
            "The test is simple: We set up iperf3 on the guest, and watch vmstat on the host while we run the test on the guest. We repeat the test after loading the ...",
            "We get a 20% savings worth of CPU performance. This can be discerned by just looking at the idle time (37% using vhost_net versus 17% without it)."
          ]
        },
        {
          "title": "Dealing with high DPC latency in QEMU guests : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/sbairq/dealing_with_high_dpc_latency_in_qemu_guests/",
          "excerpts": [
            "I installed LatencyMon and am seeing extremely high DPC and ISR routine execution times. Just in the time I wrote this post, dxgkrnl.sys had a 13797.655 μs DPC ..."
          ]
        },
        {
          "title": "Network Latency with Time Sensitive Networking on Virtual ...",
          "url": "https://www.linaro.org/blog/network-latency-with-time-sensitive-networking-on-virtual-machine/",
          "excerpts": [
            "May 15, 2023 — In this article, we look at how to address Network Latency through Time Sensitive Networking on a Virtual Machine. Read more here!",
            "In this article, we look at how to address Network Latency through Time Sensitive Networking on a Virtual Machine. Read more here!"
          ]
        },
        {
          "title": "Provide a zero-copy method on KVM virtio-net.",
          "url": "https://lwn.net/Articles/413239/",
          "excerpts": [
            "Nov 4, 2010 — Article. We provide an zero-copy method which driver side may get external buffers to DMA. ... data transfer thru guest virtio-net frontend."
          ]
        },
        {
          "title": "rte_mempool Struct Reference - Documentation - DPDK",
          "url": "https://doc.dpdk.org/api/structrte__mempool.html",
          "excerpts": [
            "The `rte_mempool` structure is the RTE mempool structure, which is a ring or pool to store objects.",
            "|\n\nDetailed Description\n--------------------\n\nThe RTE mempool structure."
          ]
        },
        {
          "title": "5. Mempool Library - Documentation",
          "url": "https://doc.dpdk.org/guides-16.04/prog_guide/mempool_lib.html",
          "excerpts": [
            "In the DPDK, it is identified by name and uses a ring to store free objects. It provides some other optional services such as a per-core object cache and an ..."
          ]
        },
        {
          "title": "lib/mempool/rte_mempool.h · main · DPDK / dts-future",
          "url": "https://oss.iol.unh.edu/dpdk/future-dts/-/blob/main/lib/mempool/rte_mempool.h",
          "excerpts": [
            "* RTE Mempool. *. * A memory pool is an allocator of fixed-size object. It is.See more"
          ]
        },
        {
          "title": "[RFC,0/4] Virtio uses DMA API for all devices - Patchwork",
          "url": "https://patchwork.ozlabs.org/cover/946708/",
          "excerpts": [
            "IBMs power plaforms are always cache coherent. There are some powerpc platforms have not cache coherent DMA, but I guess this scheme isn't intended for them."
          ]
        },
        {
          "title": "Paravirtualized Devices in crosvm - a Performance Panacea for ...",
          "url": "https://prilik.com/blog/post/crosvm-paravirt/",
          "excerpts": [
            "The virtio spec describes the setup and management of virtqueues - lockless data structures that provide asynchronous communication between ..."
          ]
        },
        {
          "title": "smoltcp::wire - Rust - Docs.rs",
          "url": "https://docs.rs/smoltcp/latest/smoltcp/wire/index.html",
          "excerpts": [
            "Low-level packet access and construction. The wire module deals with the packet representation. It provides two levels of functionality."
          ]
        },
        {
          "title": "RTE Mempool API (DPDK)",
          "url": "https://doc.dpdk.org/api/rte__mempool_8h.html",
          "excerpts": [
            "RTE Mempool. A memory pool is an allocator of fixed-size object. It is identified by its name, and uses a ring to store free objects. It ...See more",
            "It provides some other optional services, like a per-core object cache, and an alignment helper to ensure that objects are padded to spread them equally on all RAM channels, ranks, and so on.",
            "Note: the mempool implementation is not preemptible. An lcore must not be interrupted by another task that uses the same mempool (because it uses a ring which is not preemptible).",
            "Internal: no object from the pool can be used for device IO (DMA).",
            "alignment helper to ensure that objects are padded to spread them equally on all RAM channels, ranks, and so on."
          ]
        },
        {
          "title": "Packet (Mbuf) Library – Data Plane Development Kit (DPDK) manual",
          "url": "https://doc.dpdk.org/guides/prog_guide/mbuf_lib.html",
          "excerpts": [
            "The rte\\_mbuf header structure is kept as small as possible and currently uses\njust two cache lines, with the most frequently used fields being on the first\nof the two cache lines",
            "Message buffers that are used to carry network packets can handle buffer chaining\nwhere multiple buffers are required to hold the complete packet.",
            "The Buffer Manager implements a fairly standard set of buffer access functions to manipulate network packets.",
            "----------------\n\nA direct buffer is a buffer that is completely separate and self-contained. An indirect buffer behaves like a direct buffer but for the fact that the buffer pointer and\ndata offset in it refer to data in another direct buffer."
          ]
        },
        {
          "title": "Crosvm Virtio Module",
          "url": "https://github.com/google/crosvm/blob/main/devices/src/virtio/mod.rs",
          "excerpts": [
            "pub use self::queue::split\\_descriptor\\_chain::Des",
            "pub use self::queue::split\\_descriptor\\_chain::SplitDescriptorChai",
            "The Chrome OS Virtual Machine Monitor - Mirror of https://chromium.googlesource.com/crosvm/crosvm/ - crosvm/devices/src/virtio/mod.rs at main ...",
            "The Chrome OS Virtual Machine Monitor - Mirror of https://chromium.googlesource.com/crosvm/crosvm/ - crosvm/devices/src/virtio/mod.rs at main ..."
          ]
        },
        {
          "title": "2. Memory Pool Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/mempool_lib.html",
          "excerpts": [
            "The default mempool handler is ring based. It provides some other optional services such as a per-core object cache and an alignment helper to ensure that ...",
            "A memory pool is an allocator of a fixed-sized object. In the DPDK, it is identified by name and uses a mempool handler to store free objects.See more"
          ]
        },
        {
          "title": "lib/mbuf/rte_mbuf.h File Reference - Documentation - DPDK",
          "url": "https://doc.dpdk.org/api/rte__mbuf_8h.html",
          "excerpts": [
            "The first 64 bytes of the mbuf corresponds to fields that are used early in the receive path. If the cache line of the architecture is higher than 64B, the ..."
          ]
        },
        {
          "title": "virtio_queue - Rust",
          "url": "https://docs.rs/virtio-queue",
          "excerpts": [
            "Provides high-level interface over the sequence of memory regions defined by readable descriptors in the descriptor chain.",
            "Struct to maintain information and manipulate state of a virtio queue for multi-threaded context. Reader: Provides high-level interface over the sequence of ..."
          ]
        },
        {
          "title": "devices::virtio::wl - Rust",
          "url": "https://crosvm.dev/doc/devices/virtio/wl/index.html",
          "excerpts": [
            "Each WlVfd represents one virtual file descriptor created by either the guest or the host. Virtual file descriptors contain actual file descriptors, either a ..."
          ]
        },
        {
          "title": "smoltcp::phy - Rust - Docs.rs",
          "url": "https://docs.rs/smoltcp/latest/smoltcp/phy/index.html",
          "excerpts": [
            "The phy module deals with the network devices. It provides a trait for transmitting and receiving frames, Device and implementations of it."
          ]
        },
        {
          "title": "FDIOS-VPP Mechanical Sympathy (Codi Lime blog)",
          "url": "https://codilime.com/blog/fdios-vpp-mechanical-sympathy/",
          "excerpts": [
            "The truth is that VPP was designed for zero-copy interoperability with DPDK from the start.",
            "The memory is split into buffers, laid out contiguously.",
            "The memory pools are, in principle, threadsafe (they're protected by a spinlock), but as an optimization, each thread has its own cache within a pool with up to 512 entries per thread, so in the usual case, no thread synchronization is needed.",
            "There are no linked lists involved here. Free buffers are stored as indices (more on indices themselves later) in an array.",
            " Buffers that would cross page boundaries are not used, for such a buffer, we cannot use DMA as we don’t have a guarantee its backing physical memory would be contiguous.",
            "Obviously, the biggest optimization that gives VPP its performance is processing packets in bunches (called vectors), which greatly improves the performance of the instruction cache.",
            "Buffers throughout VPP are referred to by a 4-byte u32 index rather than a pointer.",
            "he DPDK plugin is the only user of VLIB\\_BUFFER\\_SET\\_EXT\\_HDR\\_SIZE. The truth is that VPP was designed for zero-copy interoperability with DPDK from the st",
            "To summarize and visualize this “double-header” design, here’s a VPP buffer in full glory.",
            "Aside from registering the aforementioned enqueue and dequeue callbacks in the rte\\_mempool) it needs to",
            "* register every page of all the buffer pools as DMA eligible for all devices on initialization,",
            "* fill out DPDK headers (rte\\_mempool\\_objhdr and rte\\_mbuf) during buffer pool initializati",
            "* translate packet lengths and offsets (and metadata for segmented packets) from rte\\_mbuf to vlib\\_buffer\\_t headers on ingre",
            "* translate the rte\\_mbuf addresses to buffer addresses (or, to be precise, indices), an",
            "* perform the opposite translations on egress."
          ]
        },
        {
          "title": "Memif PMD Documentation",
          "url": "https://doc.dpdk.org/guides/nics/memif.html",
          "excerpts": [
            "Shared memory packet interface (memif) PMD allows for DPDK and any other client using memif (DPDK, VPP, libmemif) to communicate using shared memory.",
            "Zero-copy client can be enabled with memif configuration option ‘zero-copy=yes’. This option\nis only relevant to client and requires eal argument ‘–single-file-segments’.",
            "Region 0 is created by memif driver and contains rings.",
            "Buffers are dequeued and enqueued as needed.",
            "region n:\n\n| Buffers |\n| memseg |"
          ]
        },
        {
          "title": "Crosvm Documentation",
          "url": "https://crosvm.dev/doc/crosvm/",
          "excerpts": [
            "wl-dmabuf — Enables the non-upstream virtio wayland protocol. This can be used in conjuction with the gpu feature to enable a zero-copy display pipeline.",
            "wl-dmabuf — Enables the non-upstream virtio wayland protocol. This can be used in conjuction with the gpu feature to enable a zero-copy display pipeline. x ... [modify\\_snd](fn.modify_snd.html \"fn crosvm::modify_snd\") 🔒\n* [modify\\_usb"
          ]
        },
        {
          "title": "DescriptorChain in devices::virtio - Rust",
          "url": "https://crosvm.dev/doc/devices/virtio/struct.DescriptorChain.html",
          "excerpts": [
            "A virtio descriptor chain. This is a low-level representation of the memory regions in a descriptor chain. Most code should use virtio::Reader and ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.3 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/csd01/virtio-v1.2-csd01.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/csprd05/virtio-v1.0-csprd05.html",
          "excerpts": [
            "This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "6. Ring Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/ring_lib.html",
          "excerpts": [
            "Ring Library. The ring allows the management of queues. Instead of having a linked list of infinite size, the rte_ring has the following properties:."
          ]
        },
        {
          "title": "devices::virtio - Rust",
          "url": "https://crosvm.dev/doc/devices/virtio/index.html",
          "excerpts": [
            "Provides high-level interface over the sequence of memory regions defined by readable descriptors in the descriptor chain. Rng. Virtio device for exposing ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.2",
          "url": "https://www.oasis-open.org/standard/virtual-i-o-device-virtio-version-1-2/",
          "excerpts": [
            "Describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like physical devices."
          ]
        },
        {
          "title": "VIRTIO_F_VERSION_1 identifier - Linux source code v4.6 - Bootlin ...",
          "url": "https://elixir.bootlin.com/linux/v4.6/ident/VIRTIO_F_VERSION_1",
          "excerpts": [
            "Elixir Cross Referencer - VIRTIO_F_VERSION_1 identifier references search for Linux v4.6. Defined as a macro in include/uapi/linux/virtio_config.h."
          ]
        },
        {
          "title": "FD.io VPP: src/vlib/buffer.h File Reference",
          "url": "https://docs.fd.io/vpp/18.11/d0/d83/vlib_2buffer_8h.html",
          "excerpts": [
            "Detailed Description. vlib buffer structure definition and a few select access methods. This structure and the buffer allocation mechanism should perhaps live ..."
          ]
        },
        {
          "title": "Stop Copying, Start Teleporting: How Zero-Copy Rust ...",
          "url": "https://medium.com/@FAANG/stop-copying-start-teleporting-how-zero-copy-rust-slashes-latency-by-60-in-real-world-pipelines-e76318ca2113",
          "excerpts": [
            "Jun 14, 2025 — Zero-copy is a design contract: ownership of memory moves without duplicating payload. Rust's borrow checker is a contractual lawyer that ..."
          ]
        },
        {
          "title": "GuestMemoryMmap in vm_memory::mmap - Rust - Docs.rs",
          "url": "https://docs.rs/vm-memory/latest/vm_memory/mmap/struct.GuestMemoryMmap.html",
          "excerpts": [
            "Represents the entire physical memory of the guest by tracking all its memory regions. Each region is an instance of GuestRegionMmap , being backed by a mapping ..."
          ]
        },
        {
          "title": "[dpdk-stable] [PATCH v3 1/2] ring: synchronize the load ...",
          "url": "https://mails.dpdk.org/archives/stable/2018-October/010314.html",
          "excerpts": [
            "Oct 31, 2018 — ... SPSC or MPMC. For 1 lcore, it also improves a little, about 3 ~ 4%. It is a big improvement, in case of MPMC, with two lcores and ring size ..."
          ]
        },
        {
          "title": "Buffer Metadata — The Vector Packet Processor 20.01 ...",
          "url": "https://fd.io/docs/vpp/v2101/gettingstarted/developers/metadata",
          "excerpts": [
            "Each vlib_buffer_t (packet buffer) carries buffer metadata which describes the current packet-processing state. The underlying techniques have been used for ..."
          ]
        },
        {
          "title": "DMA based API proposal · Issue #14 · rust-embedded ...",
          "url": "https://github.com/rust-embedded/embedded-hal/issues/14",
          "excerpts": [
            "Jun 14, 2017 — In this issue I'm going to present the DMA API I have implemented for the blue-pill and that I have used in my recent robotic application."
          ]
        },
        {
          "title": "[C++][DPDK] Create a rte_mempool with the correct ...",
          "url": "https://stackoverflow.com/questions/47902359/cdpdk-create-a-rte-mempool-with-the-correct-private-size-byte-alignment",
          "excerpts": [
            "I'm trying to modify the length of a received packet by copying it in a mempool, then creating a rte_mbuf with said mempool, which i can modify."
          ]
        },
        {
          "title": "7. Mbuf Library — DPDK documentation",
          "url": "https://doc.dpdk.org/guides-1.8/prog_guide/mbuf_lib.html",
          "excerpts": [
            "The mbuf library provides the ability to allocate and free buffers (mbufs) that may be used by the DPDK application to store message buffers.See more"
          ]
        },
        {
          "title": "Stop Copying, Start Teleporting Bytes: Rust's Zero- ...",
          "url": "https://medium.com/@FAANG/stop-copying-start-teleporting-bytes-rusts-zero-copy-super-power-for-2025-scale-throughput-a6a88b44d7f4",
          "excerpts": [
            "Zero-copy is the only structural fix: not “fewer” copies—zero in the hot path. Yet most Rust teams stall after hitting the borrow-checker wall, ..."
          ]
        },
        {
          "title": "vm-memory - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/vm-memory",
          "excerpts": [
            "Creating a VM physical memory object in hypervisor specific ways using the GuestMemoryMmap implementation of the GuestMemory trait: fn ..."
          ]
        },
        {
          "title": "NAPI (New API) in Linux Kernel Networking",
          "url": "https://www.linkedin.com/pulse/napi-new-api-linux-kernel-networking-david-zhu-wcz6c",
          "excerpts": [
            "Originally designed to reduce interrupt overhead during high network traffic, NAPI implements a hybrid approach that combines interrupts and ...",
            "The budget parameter is crucial to NAPI's operation: It limits the number of packets processed in a single poll cycle; Prevents a single network ..."
          ]
        },
        {
          "title": "networking:napi [Wiki]",
          "url": "https://wiki.linuxfoundation.org/networking/napi",
          "excerpts": [
            "Oct 16, 2022 — NAPI processes packet events in what is known as napi→poll() method. Typically, only packet receive events are processed in napi→poll(). The ...",
            "Oct 16, 2022 — NAPI (“New API ”) is an extension to the device driver packet processing framework, which is designed to improve the performance of high-speed networking."
          ]
        },
        {
          "title": "Polling vs Interrupts: Exploring their Differences and ...",
          "url": "https://www.totalphase.com/blog/2023/10/polling-interrupts-exploring-differences-applications/?srsltid=AfmBOopH04zCUB8IjvbxHxTbedApkCt6-wIxrMQsyH0NvpXw8HXOkHG_",
          "excerpts": [
            "Oct 18, 2023 — Polling involves continually checking the status of a device, while interrupts allow devices to signal when an event needs immediate attention."
          ]
        },
        {
          "title": "busy_poll | sysctl-explorer.net",
          "url": "https://sysctl-explorer.net/net/core/busy_poll/",
          "excerpts": [
            "Low latency busy poll timeout for poll and select. (needs CONFIG_NET_RX_BUSY_POLL) Approximate time in us to busy loop waiting for events."
          ]
        },
        {
          "title": "9.3. Configuration Tools | Red Hat Enterprise Linux | 7",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-networking-configuration_tools",
          "excerpts": [
            "To enable busy polling on specific sockets, do the following. Set sysctl.net.core.busy_poll to a value other than 0 . This parameter controls the number of ...",
            "To enable busy polling on specific sockets, do the following. Set sysctl.net.core.busy_poll to a value other than 0 . This parameter controls the number of ..."
          ]
        },
        {
          "title": "Documentation for /proc/sys/net - The Linux Kernel documentation",
          "url": "https://docs.kernel.org/admin-guide/sysctl/net.html",
          "excerpts": [
            "busy_poll¶. Low latency busy poll timeout for poll and select. (needs CONFIG_NET_RX_BUSY_POLL) Approximate time in us to busy loop waiting for events."
          ]
        },
        {
          "title": "[PATCH] Introduction of Virtio Network device interrupt ...",
          "url": "https://groups.oasis-open.org/communities/community-home/digestviewer/viewthread?GroupId=3973&MessageKey=37921b29-730e-4ecb-9166-281cf1ce94cb&CommunityKey=2f26be99-3aa1-48f6-93a5-018dce262226",
          "excerpts": [
            "Control a network device interrupt coalescing parameters using the control virtqueue. A new control class was added: VIRTIO_NET_CTRL_INTR_COAL."
          ]
        },
        {
          "title": "Interrupt coalescing - Documentation - NXP Semiconductors",
          "url": "https://docs.nxp.com/bundle/UG10143/page/topics/interrupt_coalescing_001.html",
          "excerpts": [
            "With the interrupt coalescing feature, packets are collected and one single interrupt is generated for multiple packets to avoid flooding the system with ..."
          ]
        },
        {
          "title": "Interrupt Coalescing under virtio-net - L",
          "url": "http://liujunming.github.io/2024/05/26/Interrupt-Coalescing-under-virtio-net/",
          "excerpts": [
            "May 26, 2024 — Net DIM算法通过统计当前网络中单个队列的流量信息和中断次数，自适应计算中断调整方向和步长，并将结果配置下发到设备，以达到提升网络吞吐量的目的。",
            "virtio-net already supports per-queue moderation parameter setting. Based on this, we use the [netdim](https://github.com/torvalds/linux/blob/master/lib/dim/net_dim.c) library of linux to support dynamic coalescing moderation for virtio-net.",
            "When the traffic between virtqueues is unbalanced, for example, one virtqueue is busy and another virtqueue is idle, then it will be very useful to control coalescing parameters at the virtqueue granularity.",
            "Dynamic Interrupt Moderation (DIM) (in networking) refers to changing the interrupt moderation configuration of a channel in order to optimize packet processing**. *The mechanism includes an algorithm which decides if and how to change moderation parameters for a channel, usually by performing an analysis on runtime data sampled from the system*. Net DIM is such a mechanism. In each iteration of the algorithm, it analyses a given sample of the data, compares it to the previous sample and if required, it can decide to change some of the interrupt moderation configuration fields. The data sample is composed of data bandwidth, the number of packets and the number of events. The time between samples is also measu",
            "Net DIM compares the current and the previous data and returns an adjusted interrupt moderation configuration object**. In some cases, the algorithm might decide not to change anything. **The configuration fields are the minimum duration (microseconds) allowed between events and the maximum number of wanted packets per event**(笔者注：在virtio-net中，configuration fields就是max\\_usecs和max\\_packets，感觉是maximum duration allowed between events才对). The Net DIM algorithm ascribes importance to increase bandwidth over reducing interru",
            "spec中 VIRTIO_NET_F_VQ_NOTF_COAL 和 VIRTIO_NET_F_NOTF_COAL 这两个feature bit是Interrupt Coalescing相关特性。 VIRTIO_NET_F_VQ_NOTF_COAL(52) ...See more"
          ]
        },
        {
          "title": "Adaptive Interrupt Handling for High-Performance Network I/O ...",
          "url": "https://uwspace.uwaterloo.ca/bitstreams/c3bdf8cc-4e00-4d1a-80be-f4d0154c21dc/download",
          "excerpts": [
            "by M Shafiei · 2025 — This thesis investigates the design and evaluation of a hybrid interrupt handling mech- anism, IRQ Suspend, within the Linux network stack. The ...",
            "A key advantage of busy polling over interrupt-driven methods is that the application determines when polling should begin. ... employs an ..."
          ]
        },
        {
          "title": "The Definitive KVM (Kernel-based Virtual Machine) API ...",
          "url": "https://www.kernel.org/doc/html/v6.1/virt/kvm/api.html",
          "excerpts": [
            "The kvm API is a set of ioctls that are issued to control various aspects of a virtual machine. The ioctls belong to the following classes:"
          ]
        },
        {
          "title": "UsingVhost",
          "url": "https://www.linux-kvm.org/page/UsingVhost",
          "excerpts": [
            "VhostNet provides better latency ( 10% less than e1000 on my system) and greater throughput (8x the normal virtio, around 7~8 Gigabits/sec here) for network."
          ]
        },
        {
          "title": "VM storage latancy | Proxmox Support Forum",
          "url": "https://forum.proxmox.com/threads/vm-storage-latancy.159735/",
          "excerpts": [
            "You are correct that virtio-blk can exhibit lower latency. In our internal data, I can see a 1-2 microsecond advantage for virtio-blk on Linux."
          ]
        },
        {
          "title": "NetworkingTodo",
          "url": "https://linux-kvm.org/page/NetworkingTodo",
          "excerpts": [
            "interrupt coalescing. Reduce the number of interrupt Rx interrupt coalescing should be good for rx stream throughput. Tx interrupt coalescing will help the ..."
          ]
        },
        {
          "title": "Red Hat: Deep Dive into Virtio Networking and vhost-net",
          "url": "https://www.redhat.com/en/blog/deep-dive-virtio-networking-and-vhost-net",
          "excerpts": [
            "the virtio specification also defines bi-directional notifications:\n\n* \n  Available Buffer Notification : Used by the driver to signal there are buffers that are ready to be processed by the device\n* \n  Used Buffer Notification : Used by the device to signal that it has finished processing some buffers.",
            "The virtio specification also allows the notifications to be enabled or disabled dynamically. That way, devices and drivers can batch buffer notifications or even actively poll for new buffers in virtqueues (busy polling). This approach is better suited for high traffic rates.",
            "The vhost-net is a kernel driver that implements the handler side of the vhost protocol to implement an efficient data plane, i.e., packet forwarding.",
            "During the initialization the vhost-net kernel driver creates a kernel thread called vhost-$pid, where $pid is the hypervisor process pid. This ...",
            "Sep 12, 2019 — In this post we will explain the vhost-net architecture described in the introduction, to make it clear how everything works together from a technical point of ...",
            "Sep 12, 2019 — We'll start by describing how the different virtio spec standard components and shared memory regions are arranged in the hypervisor, how QEMU ..."
          ]
        },
        {
          "title": "Linux NAPI and Virtio Documentation",
          "url": "https://docs.kernel.org/networking/napi.html",
          "excerpts": [
            "Threaded NAPI is an operating mode that uses dedicated kernel\nthreads rather than software IRQ context for NAPI processing.",
            "napi_schedule() is the basic method of scheduling a NAPI poll. Drivers should call this function in their interrupt handler (see Scheduling and IRQ masking for ...",
            "o enable busy polling, there are two choices:\n\n1. `/proc/sys/net/core/busy_poll` can be set with a time in useconds to busy\n   loop waiting for",
            "poll. 2. Applications using recent kernels can issue an ioctl on the epoll context\n   file descriptor to set ( `EPIOCSPARAMS` ) or get ( `EPIOCGPARAMS` ) `struct\n   epoll_params`",
            "The NAPI budget for busy polling is lower than the default (which makes\nsense given the low latency intention of normal busy polling).",
            "IRQ should only be unmasked after a successful call to napi_complete_done() : if (budget && napi_complete_done(&v->napi, work_done)) { mydrv_unmask_rxtx_irq ..."
          ]
        },
        {
          "title": "Linux NAPI and related concepts",
          "url": "https://www.kernel.org/doc/html/v6.5/networking/napi.html",
          "excerpts": [
            " an interrupt. The host then schedules a NAPI instance to process the events. The device may ... The host then schedules a NAPI instance to process the events. The device may also be polled for events via NAPI without receiving\ninterrupts first ( [busy",
            "The two most important elements of NAPI are the struct napi\\_struct\nand the associated poll method",
            "NAPI instances most often correspond 1:1:1 to interrupts and queue pairs",
            "Drivers should keep the interrupts masked after scheduling\nthe NAPI instance - until NAPI polling finishes any further\ninterrupts are unnecessary.",
            "NAPI does not perform any explicit event coalescing by default. In most scenarios batching happens due to IRQ coalescing which is done\nby the device.",
            "There are cases where software coalescing is helpful. NAPI can be configured to arm a repoll timer instead of unmasking\nthe hardware interrupts as soon as all packets are processed.",
            "\nBusy polling allows a user process to check for incoming packets before\nthe device interrupt fires. As is the case with any busy polling it trades\noff CPU cycles for lower latency"
          ]
        },
        {
          "title": "NAPI Polling in Red Hat Documentation",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/tuning-the-network-performance_monitoring-and-managing-system-status-and-performance",
          "excerpts": [
            "New API (NAPI) is an extension to the device driver packet processing framework to improve the efficiency of incoming network packets. Hard interrupts are expensive because they usually cause a context switch from the kernel space to the user space and back again, and cannot interrupt themselves. Even with interrupt coalescence, the interrupt handler monopolizes a CPU core completely.",
            "Copy link Link copied to clipboard! New API (NAPI) is an extension to the device driver packet processing framework to improve the efficiency of incoming network packets. Hard interrupts are expensive because they usually cause a context switch from the kernel space to the user space and back again, and cannot interrupt themselves.",
            "Under normal operation, the kernel issues an initial hard interrupt, followed by a soft interrupt request (SoftIRQ) handler that polls the network card using NAPI routines. To prevent SoftIRQs from monopolizing a CPU core, the polling routine has a budget that determines the CPU time the SoftIRQ can consume.",
            "On Red Hat Enterprise Linux, processes named `ksoftirqd/ cpu-number` run these routines and call driver-specific code functions. To monitor the SoftIRQ counters for each CPU core, enter:\n\n```\nwatch -n1 'grep -E \"CPU|NET_RX|NET_TX\" /proc/softirqs'\n```\n\n```plaintext\n# watch -n1 'grep -E \"CPU|NET_RX|NET_TX\" /proc/softirqs'\n                    CPU0       CPU1\t  CPU2       CPU3\tCPU4\t   CPU5       CPU6\t CPU7\n      NET_TX:\t   49672      52610\t 28175      97288      12633\t  19843      18746     220689\n      NET_RX:         96       1615        789         46         31\t   1735       1315     470798\n```",
            "Copy to Clipboard Copied! Toggle word wrap Toggle overflow\n\nThe command dynamically updates the output. Press `Ctrl` \\+ `C` to interrupt the output. ### [34\\.2.3. NAPI Polling]()\n\nCopy link Link copied to clipboard! New API (NAPI) is an extension to the device driver packet processing framework to improve the efficiency of incoming network packets. Hard interrupts are expensive because they usually cause a context switch from the kernel space to the user space and back again, and cannot interrupt themselves. Even with interrupt coalescence, the interrupt handler monopolizes a CPU core completely. With NAPI, the driver can use a polling mode instead of being hard-interrupted by the kernel for every packet that is receive",
            "### [34\\.2.3. NAPI Polling]()",
            " ### [34\\.2.3. NAPI Polling]()",
            "the polling routine has a budget that determines the CPU time the SoftIRQ can consume. On completion of the SoftIRQ poll routine, the kernel exits the routine and schedules it to run again at a later time to repeat the process of receiving packets from the network card.",
            " ### [34\\.2.4. The irqbalance service]()",
            "Copy link Link copied to clipboard! On systems both with and without Non-Uniform Memory Access (NUMA) architecture, the `irqbalance` service balances interrupts effectively across CPU cores, based on system conditions. The `irqbalance` service runs in the background and monitors the CPU load every 10 seconds. The service moves interrupts to other CPU cores when a CPU’s load is too high. As a result, the system performs well and handles load more efficiently. If `irqbalance` is not running, usually the CPU core 0 handles most of the interrupts.",
            "Disabling `irqbalance` can negatively impact the network throughput."
          ]
        },
        {
          "title": "Linux Network Performance Ultimate Guide | kiennt26's home",
          "url": "https://ntk148v.github.io/posts/linux-network-performance-ultimate-guide/",
          "excerpts": [
            "Feb 24, 2023 — The design of NAPI allows the driver to go into a polling mode instead of being HardIRQ for every required packet receive. Step 1->9 in brief:.",
            "Tuning a NIC for optimum throughput and latency is a complex process with many factors to consider. There is no generic configuration that can ..."
          ]
        },
        {
          "title": "virtio_net.c source code [linux/drivers/net ...",
          "url": "https://codebrowser.dev/linux/linux/drivers/net/virtio_net.c.html",
          "excerpts": [
            "if (napi_complete_done(n: napi, work_done: processed)) {. 738, if (unlikely(virtqueue_poll(vq, opaque))). 739, virtqueue_napi_schedule(napi, vq);. 740, else."
          ]
        },
        {
          "title": "Virtio networking: A case study of I/O paravirtualization",
          "url": "https://lettieri.iet.unipi.it/virtualization/2015/io-paravirtualization-tour.pdf",
          "excerpts": [
            "This strategy (similar to Linux NAPI) allows both producer and consumer to temporarily switch from interrupt mode to polled mode. In this way, notifications ..."
          ]
        },
        {
          "title": "Virtio networking: A case study of I/O paravirtualization",
          "url": "https://lettieri.iet.unipi.it/hacking/2018/20161124-io-paravirtualization-tour.pdf",
          "excerpts": [
            "Dec 1, 2016 — This strategy (similar to Linux NAPI) allows both producer and consumer to temporarily switch from interrupt mode to polled mode. In this ...See more"
          ]
        },
        {
          "title": "[PDF] Virtio PCI Card Specification v0.9.4 DRAFT - - OzLabs",
          "url": "https://ozlabs.org/~rusty/virtio-spec/old/virtio-0.9.4.pdf",
          "excerpts": [
            "Once the idx field of the virtqueue is updated, the device will be able to access the descriptor entries we've created and the memory they refer ..."
          ]
        },
        {
          "title": "[PATCH v2] virtio-net: Fix and update ... - OASIS",
          "url": "https://groups.oasis-open.org/communities/community-home/digestviewer/viewthread?GroupId=3973&MessageKey=3e4330c0-7046-424f-b535-4d0115c37363&CommunityKey=2f26be99-3aa1-48f6-93a5-018dce262226",
          "excerpts": [
            "If the VIRTIO_NET_F_NOTF_COAL feature has not been negotiated, the driver MUST NOT issue VIRTIO_NET_CTRL_NOTF_COAL commands. -- 2.34.1."
          ]
        },
        {
          "title": "virtio-net: Fix and update VIRTIO_NET_F_NOTF_COAL ...",
          "url": "https://github.com/oasis-tcs/virtio-spec/issues/159",
          "excerpts": [
            "Make several improvements to the notification coalescing feature, including: Consolidating virtio_net_ctrl_coal_tx and ..."
          ]
        },
        {
          "title": "Virtio/Block/Latency",
          "url": "https://www.linux-kvm.org/page/Virtio/Block/Latency",
          "excerpts": [
            "The kvm latency is the time from the virtqueue notify pio exit until the interrupt is set inside the guest. This number does not include vmexit/entry time.",
            "The virtio latency inside QEMU is the time from virtqueue notify until the interrupt is raised. This accounts for time spent in QEMU servicing I/O.",
            "The time spent in QEMU servicing a read request made up 29 us or a 23% overhead compared to a host read request.",
            "The sequential read case is optimized by the presence of a disk read cache. I think this is why the latency numbers are in the microsecond range, not the usual millisecond seek time expected from disks.",
            "Delta (ns)** column is the time between two layers, e.g. **Guest benchmark** and **Guest virtio-pci** . The delta time tells us how long is being spent in a layer of the virtualization",
            "\n* virtqueue\\_pop 2.560 us is expensive, probably due to vring accesses.",
            "The kvm latency is the time from the virtqueue notify pio exit until the interrupt is set inside the guest.",
            "The VIRTIO\\_PCI\\_QUEUE\\_NOTIFY pio has a latency of over 22 us! This is largely due to io\\_submit() taking 16",
            "tio latency inside QEMU is the time from virtqueue notify until the interrupt is raised.",
            "The paio latency is the time spent performing pread()/pwrite() syscalls.",
            "The paio latency is the time spent performing pread()/pwrite() syscalls.",
            "The standard deviation of the iothread return from select(2) is high."
          ]
        },
        {
          "title": "Best way to get high-res low-latency output of KVM? : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/k9x7uc/best_way_to_get_highres_lowlatency_output_of_kvm/",
          "excerpts": [
            "With virtio-net instead of an emulated NIC you get basically latency-free and super-high bandwidth networking between VM and host. Nvidia ...",
            "With virtio-net instead of an emulated NIC you get basically latency-free and super-high bandwidth networking between VM and host. Nvidia-> ..."
          ]
        },
        {
          "title": "62. Poll Mode Driver for Emulated Virtio NIC - Documentation",
          "url": "https://doc.dpdk.org/guides/nics/virtio.html",
          "excerpts": [
            "We provide a virtio Poll Mode Driver (PMD) as a software solution, comparing to SRIOV hardware solution, for fast guest VM to guest VM communication and guest ...",
            "In the Data Plane Development Kit (DPDK), we provide a virtio Poll Mode Driver (PMD) as a software solution, comparing to SRIOV hardware solution, for fast ..."
          ]
        },
        {
          "title": "15. Poll Mode Driver for Emulated Virtio NIC - Documentation",
          "url": "https://doc.dpdk.org/guides-16.04/nics/virtio.html",
          "excerpts": [
            "In this release, the virtio PMD driver provides the basic functionality of packet reception and transmission."
          ]
        },
        {
          "title": "Virtualization Tuning and Optimization Guide",
          "url": "https://docs.redhat.com/de/documentation/red_hat_enterprise_linux/7/html-single/virtualization_tuning_and_optimization_guide/index",
          "excerpts": [
            "Within this guide, you can find tips and suggestions for making full use of KVM performance features and options for your host systems and virtualized guests."
          ]
        },
        {
          "title": "KVM and Big VMs",
          "url": "https://www.linux-kvm.org/images/5/55/2012-forum-Andrew-Theurer-Big-SMP-VMs.pdf",
          "excerpts": [
            "by A Theurer · Cited by 11 — VM might exit when there is no lock holder preemption ... The detection itself could get too costly. – However, implementing the throttled yield_to() reduces exit ..."
          ]
        },
        {
          "title": "KVM network performance: latency vs throughput, do I ...",
          "url": "https://forum.proxmox.com/threads/kvm-network-performance-latency-vs-throughput-do-i-have-to-decide.5814/",
          "excerpts": [
            "Feb 24, 2011 — I see no difference between virtio and e1000, in both cases latency is about 0.280msec and iperf measures about 670Mbit/s."
          ]
        },
        {
          "title": "Strange (scheduling?) latency on the host when KVM guest ...",
          "url": "https://www.reddit.com/r/AsahiLinux/comments/134g00u/strange_scheduling_latency_on_the_host_when_kvm/",
          "excerpts": [
            "I've been exeperiencing perfomance issues while running tasks in qemu/kvm. The tasks inside the guest run just fine, very speedy."
          ]
        },
        {
          "title": "Cellular Mechanisms and Regulation of Quiescence - PMC",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7665062/",
          "excerpts": [
            "by O Marescal · 2020 · Cited by 269 — Quiescence is a state of reversible proliferative arrest in which cells are not actively dividing, and yet retain the capacity to reenter the cell cycle."
          ]
        },
        {
          "title": "Suspend IRQs during application busy periods - LWN.net",
          "url": "https://lwn.net/Articles/997491/",
          "excerpts": [
            "~ The short explanation (TL;DR) We propose adding a new napi config ... napi_defer_hard_irqs and gro_flush_timeout mechanism which was ..."
          ]
        },
        {
          "title": "Improve network latency for Linux based EC2 instances",
          "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ena-improve-network-latency-linux.html",
          "excerpts": [
            "Enable busy poll mode – Busy poll mode reduces latency on the network receive path. When you enable busy poll mode, the socket layer code can directly poll the ..."
          ]
        },
        {
          "title": "Save power with OVS-DPDK PMD thread load-based ...",
          "url": "https://developers.redhat.com/articles/2023/10/16/save-power-ovs-dpdk-pmd-thread-load-based-sleeping",
          "excerpts": [
            "Oct 16, 2023 — NAPI is a well-known interrupt/polling hybrid that requires all of the DPDK NICs drivers to support interrupt mode. It also requires different ..."
          ]
        },
        {
          "title": "[net] net: solve a NAPI race - Patchwork - OzLabs",
          "url": "https://patchwork.ozlabs.org/comment/1596198/",
          "excerpts": [
            "While napi_complete_done() is running, the device's IRQ is still disabled, so there cannot be an IRQ firing before napi_complete_done() is finished. Eric ..."
          ]
        },
        {
          "title": "remove redundant logic in napi callback for busy poll mode",
          "url": "https://lkml.iu.edu/hypermail/linux/kernel/1612.0/02122.html",
          "excerpts": [
            "Dec 5, 2016 — Unmask the interrupt only from irq context. > > A schenario where ... > 1)Before napi completes and call napi_complete_done > 2)After calling ..."
          ]
        },
        {
          "title": "Micro-Optimizing KVM VM-Exits",
          "url": "https://www.qemu.org/2019/11/15/micro-optimizing-kvm-vmexits/",
          "excerpts": [
            "Nov 15, 2019 — KVM (Kernel-based Virtual Machine) is the Linux kernel module that allows a host to run virtualized guests (Linux, Windows, etc)."
          ]
        },
        {
          "title": "[RFC net-next 0/5] Suspend IRQs during preferred busy poll - Patchew",
          "url": "https://patchew.org/linux/20240812125717.413108-1-jdamato@fastly.com/",
          "excerpts": [
            "- suspendX: > - set defer_hard_irqs to 100 > - set gro_flush_timeout to X,000 > - set irq_suspend_timeout to 20,000,000 > - enable busy poll via ..."
          ]
        },
        {
          "title": "Implementation and experimental performance evaluation ...",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S0140366408005252",
          "excerpts": [
            "by K Salah · 2009 · Cited by 56 — The hybrid scheme switches between interrupt disabling–enabling (DE) and polling (NAPI). In this paper, we present and discuss major changes ..."
          ]
        },
        {
          "title": "[PDF] the man-pages book - The Linux Kernel Archives",
          "url": "https://www.kernel.org/pub/linux/docs/man-pages/book/man-pages-6.9.pdf",
          "excerpts": [
            "... best effort to make this call as fast as possible. (On some ... busy_poll_budget; /* Max packets per poll */ uint8_t prefer_busy_poll ..."
          ]
        },
        {
          "title": "devices/src/virtio/net.rs - platform/external/crosvm - Git at Google",
          "url": "https://android.googlesource.com/platform/external/crosvm/+/9cfdbd9c/devices/src/virtio/net.rs",
          "excerpts": [
            "// would ensure that any changes in the creation procedure are matched in the validation. // Plus we still need to set the offload and vnet_hdr_size values ..."
          ]
        },
        {
          "title": "Frustration with VMExit and QEMU rebuilds : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/1lnqcbv/frustration_with_vmexit_and_qemu_rebuilds/",
          "excerpts": [
            "I'm using pafish to test things and I get errors with system bios reverting back to bocsh every boot and drive saying QEMU HARDDISK."
          ]
        },
        {
          "title": "virtio-fs / vm-virtio",
          "url": "https://gitlab.com/virtio-fs/vm-virtio",
          "excerpts": [
            "Nov 30, 2021 — The vm-virtio crate provides a virtio device implementation for the following: Note: We offer support only for virtio v1.0+ (the legacy interface is not ..."
          ]
        },
        {
          "title": "Virtqueue in virtio::queues - Rust",
          "url": "https://docs.tockos.org/virtio/queues/trait.virtqueue",
          "excerpts": [
            "Each queue instance provides a bidirectional communication channel to send data from the guest (VirtIO driver) to the host (VirtIO device) and back. Typically, ..."
          ]
        },
        {
          "title": "Implementing a Lock-Free Queue in Rust",
          "url": "https://karevongeijer.com/blog/lock-free-queue-in-rust/",
          "excerpts": [
            "The MS queue implements a linked list with lock-free enqueue and dequeue methods, using the atomic primitive compare-and-swap (CAS) to achieve lock-freedom.",
            "To actually implement the MS queue in Rust, we have to replace references with atomic pointers, integrate hazard pointers for safe memory management (using the [Haphazard](https://docs.rs/haphazard/latest/haphazard/) library), and in general ensure sound use of memory and ownership.",
            "For a new node, we simply wrap the data in a new `MaybeUninit`, and initialize `next` as a null pointer. In the queue, similarly to the pseudocode, we create a dummy node that both `head` and `tail` point to. This dummy node is required for the algorithm (if we had let them both be `null`, we would need to update them both when enqueueing an item, rather than just `dummy.next`), and as its `data` is never read, we safely set it to an uninitialized `MaybeUninit`.",
            "The logic of the `enqueue` is quite similar to the pseudocode, and except being a bit more verbose and taking a `HazardPointer` as input, you can see the exact same flow.",
            "This is the core of the algorithm, which at its core is rather simple. The part which is often hard to grasp is that all operations have to help partial enqueues at different points, to guarantee lock-freedom."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/next/admin-guide/kernel-parameters.html",
          "excerpts": [
            "\n                        [KNL] The optional argument is a cpu list,\n                        as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y,\n                        enable the no-callback CPU mode, which prevents\n                        such CPUs' callbacks from being invoked in\n                        softirq context. Invocation of such CPUs' RCU\n                        callbacks will instead be offloaded to \"rcuox/N\"\n                        kthreads created for that purpose, where \"x\" is\n                        \"p\" for RCU-preempt, \"s\" for RCU-sched, and \"g\"\n                        for the kthreads that mediate grace periods;",
            "isolcpus=       [KNL,SMP,ISOL] Isolate a given set of CPUs from disturbance. [Deprecated - use cpusets instead]",
            "A residual 1Hz tick is offloaded to workqueues, which you\n                          need to affine to housekeeping through the global\n                          workqueue's affinity configured via the\n                          /sys/devices/virtual/workqueue/cpumask sysfs file, or\n                          by using the 'domain' flag described below.",
            "Note that performing domain isolation this way\n                          is irreversible: it's not possible to bring back a CPU to\n                          the domains once isolated through isolcpus. It's strongly\n                          advised to use cpusets instead to disable scheduler load\n                          balancing through the \"cpuset.sched_load_balance\" file.",
            "Format: [flag-list,]<cpu-list>",
            "Specify one or more CPUs to isolate from disturbances",
            "nohz",
            "domain\n                          Isolate from the general SMP balancing and scheduling\n                          algorithms.",
            "managed_irq\n\n                          Isolate from being targeted by managed interrupts\n                          which have an interrupt mask containing isolated\n                          CPUs. The affinity of managed interrupts is\n                          handled by the kernel and cannot be changed via\n                          the /proc/irq/* interfaces",
            "cpu lists¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html",
          "excerpts": [
            "cpu lists:¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ...",
            "cpu lists:¶. Some kernel parameters take a list of CPUs as a value, e.g. isolcpus, nohz_full, irqaffinity, rcu_nocbs. The format of this list is: <cpu number> ..."
          ]
        },
        {
          "title": "The kernel's command-line parameters",
          "url": "https://www.kernel.org/doc/html/v4.19/admin-guide/kernel-parameters.html",
          "excerpts": [
            "The following is a consolidated list of the kernel parameters as implemented by the __setup(), core_param() and module_param() macros and sorted into English ..."
          ]
        },
        {
          "title": "Chapter 14. Minimizing system latency by isolating ...",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/8/html/optimizing_rhel_8_for_real_time_for_low_latency_operation/assembly_binding-interrupts-and-processes_optimizing-rhel8-for-real-time-for-low-latency-operation",
          "excerpts": [
            "Isolating interrupts (IRQs) from user processes on different dedicated CPUs can minimize or eliminate latency in real-time environments."
          ]
        },
        {
          "title": "Performance Tuning Guide | Red Hat Enterprise Linux | 7",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html-single/performance_tuning_guide/index",
          "excerpts": [
            "Once a CPU is isolated, you must manually assign processes to the isolated CPU, either with the CPU affinity system calls or the numactl command. To isolate ..."
          ]
        },
        {
          "title": "How do I get tickless kernel to work? nohz_full, rcu_nocbs ...",
          "url": "https://stackoverflow.com/questions/20133523/how-do-i-get-tickless-kernel-to-work-nohz-full-rcu-nocbs-isolcpus-what-else",
          "excerpts": [
            "I just installed Ubuntu 13.10 with new 3.11 kernel. And in 3.10, it has the tickless feature where I can run a process without being interrupted by local timer."
          ]
        },
        {
          "title": "4.3. Interrupts and IRQ Tuning | Performance Tuning Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/performance_tuning_guide/s-cpu-irq",
          "excerpts": [
            "The default value for smp_affinity is f , meaning that the IRQ can be serviced on any of the CPUs in the system. Setting this value to 1 , as follows, means ...",
            "IRQs have an associated \"affinity\" property, smp_affinity , which defines the CPU cores that are allowed to execute the ISR for that IRQ. This property can ..."
          ]
        },
        {
          "title": "SMP IRQ affinity",
          "url": "https://docs.kernel.org/core-api/irq/irq-affinity.html",
          "excerpts": [
            "It's not allowed to turn off all CPUs, and if an IRQ controller does not support IRQ affinity then the value will not change from the default of all cpus."
          ]
        },
        {
          "title": "KVM/Qemu Virtualization Tuning Guide on 3rd Generation ...",
          "url": "https://cdrdv2-public.intel.com/686407/kvm-tuning-guide-icx.pdf",
          "excerpts": [
            "To achieve best performance, it is critical to run virtual machine with the correct CPU affinity, memory affinity and IRQ affinity, especially ..."
          ]
        },
        {
          "title": "vhost-user back ends",
          "url": "https://www.qemu.org/docs/master/system/devices/vhost-user.html",
          "excerpts": [
            "vhost-user back ends are way to service the request of VirtIO devices outside of QEMU itself. To do this there are a number of things required.",
            "The user needs to specify\nall the required parameters including:",
            "Device `virtio-id`",
            "The `num_vqs` it needs and their `vq_size`",
            "The `config_size` if needed",
            "g. ## vhost-user daemon []( \"Permalink to this heading\")",
            "\nThis is a separate process that is connected to by QEMU via a socket\nfollowing the [Vhost-user Protocol](../../interop/vhost-user.html)",
            "There are a number of daemons\nthat can be built when enabled by the project although any daemon that\nmeets the specification for a given device can be used.",
            "d. ## Shared memory object []( \"Permalink to this heading\")",
            "\nIn order for the daemon to access the VirtIO queues to process the\nrequests it needs access to the guest’s address space. This is\nachieved via the `memory-backend-file` , `memory-backend-memfd` , or `memory-backend-shm` objects.",
            "A reference to a file-descriptor which can access this object\nwill be passed via the socket as part of the protocol negotiation.",
            "Currently the shared memory object needs to match the size of the main\nsystem memory as defined by the `-m` argument.",
            "First start your daemon."
          ]
        },
        {
          "title": "33. Vhost Sample Application - Documentation",
          "url": "https://doc.dpdk.org/guides/sample_app_ug/vhost.html",
          "excerpts": [
            "The sample application performs simple packet switching between virtual machines based on Media Access Control (MAC) address or Virtual Local Area Network ( ..."
          ]
        },
        {
          "title": "Vhost-user Protocol — QEMU documentation",
          "url": "https://qemu-project.gitlab.io/qemu/interop/vhost-user.html",
          "excerpts": [
            "The front-end and back-end can be either a client (i.e. connecting) or server (listening) in the socket communication. Support for platforms other than Linux  .",
            " Vhost-user Protocol — QEMU documentation",
            "This protocol is aiming to complement the `ioctl` interface used to\ncontrol the vhost implementation in the Linux kernel.",
            "It implements the control plane needed to establish virtqueue sharing with a user space process on the same host.",
            "It implements the control plane needed to establish virtqueue sharing with a user space process on the same host.",
            "The _front-end_ is the application that shares its virtqueues, in\nour case QEMU.",
            "The _back-end_ is the consumer of the virtqueues.",
            "In the current implementation QEMU is the _front-end_ , and the _back-end_ is the external process consuming the virtio queues, for example a\nsoftware Ethernet switch running in user space, such as Snabbswitch,\nor a block device back-end processing read & write to a virtual\ndisk.",
            "It uses communication over a Unix\ndomain socket to share file descriptors in the ancillary data of the\nmessage.",
            "A way for requesting shared memory represented by a file descriptor so it can be passed over a UNIX domain socket and then mapped by the other process.",
            "The vhost-user protocol complements the ioctl interface, establishing virtqueue sharing between a user space process and the front-end (like QEMU) and back-end ..."
          ]
        },
        {
          "title": "3.13. Isolating CPUs Using tuned-profiles-realtime",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime",
          "excerpts": [
            "The recommended way to do this for Red Hat Enterprise Linux for Real Time is to use the tuned daemon and its tuned-profiles-realtime package."
          ]
        },
        {
          "title": "Chapter 20. Isolating CPUs using tuned-profiles-real-time",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/8/html/optimizing_rhel_8_for_real_time_for_low_latency_operation/assembly_isolating-cpus-using-tuned-profiles-realtime_optimizing-rhel8-for-real-time-for-low-latency-operation",
          "excerpts": [
            "The recommended way to do this for RHEL for Real Time is to use the TuneD daemon and its tuned-profiles-realtime package. Note."
          ]
        },
        {
          "title": "Low Latency Tuning Guide",
          "url": "https://rigtorp.se/low-latency-guide/",
          "excerpts": [
            "This guide describes how to tune your AMD64/x86_64 hardware and Linux system for running real-time or low latency workloads."
          ]
        },
        {
          "title": "Welcome to QEMU's documentation!",
          "url": "https://www.qemu.org/docs/master/",
          "excerpts": [
            "Welcome to QEMU's documentation!   · Introduction · Invocation · Device Emulation · Keys in the graphical frontends · Keys in the character backend multiplexer · QEMU ...",
            "QEMU Firmware Configuration (fw_cfg) Device · IBM's Flexible Service Interface ... This documentation is for QEMU version 10.1.50. QEMU and this manual ..."
          ]
        },
        {
          "title": "31. Vhost Sample Application - Documentation",
          "url": "https://doc.dpdk.org/guides-16.04/sample_app_ug/vhost.html",
          "excerpts": [
            "The vhost sample application demonstrates integration of the Data Plane Development Kit (DPDK) with the Linux* KVM hypervisor by implementing the vhost-net ..."
          ]
        },
        {
          "title": "Network emulation — QEMU documentation",
          "url": "https://www.qemu.org/docs/master/system/devices/net.html",
          "excerpts": [
            "QEMU can simulate several network cards (eg PCI or ISA cards on the PC target) and can connect them to a network backend on the host or an emulated hub."
          ]
        },
        {
          "title": "36. Vdpa Sample Application - Documentation",
          "url": "https://doc.dpdk.org/guides/sample_app_ug/vdpa.html",
          "excerpts": [
            "The vdpa sample application creates vhost-user sockets by using the vDPA backend. vDPA stands for vhost Data Path Acceleration which utilizes virtio ring ..."
          ]
        },
        {
          "title": "QEMU vhost-user-input - Input emulation",
          "url": "https://www.qemu.org/docs/master/system/devices/vhost-user-input.html",
          "excerpts": [
            "This document describes the setup and usage of the Virtio input device. The Virtio input device is a paravirtualized device for input events."
          ]
        },
        {
          "title": "Documentation/Networking",
          "url": "https://wiki.qemu.org/Documentation/Networking",
          "excerpts": [
            "Dec 18, 2023 — User Networking is implemented using \"slirp\", which provides a full TCP/IP stack within QEMU and uses that stack to implement a virtual NAT'd network."
          ]
        },
        {
          "title": "Writing VirtIO backends for QEMU",
          "url": "https://www.qemu.org/docs/master/devel/virtio-backends.html",
          "excerpts": [
            "For example -device virtio-foo represents the foo device using mmio and -device virtio-foo-pci is the same class of device using the PCI transport.",
            "This document attempts to outline the information a developer needs to\nknow to write device emulations in QEMU. It is specifically focused on\nimplementing VirtIO devices. For VirtIO the frontend is the driver\nrunning on the guest. The backend is the everything that QEMU needs to\ndo to handle the emulation of the VirtIO device. This can be done\nentirely in QEMU, divided between QEMU and the kernel (vhost) or\nhandled by a separate process which is configured by QEMU\n(vhost-user).",
            "_ops vs TYPE\\_VHOST\\_USER\\_BACKEND []( \"Permalink to this heading\")\n\nThere are two choices to how to implement vhost code"
          ]
        },
        {
          "title": "Kernel CPU isolation parameters",
          "url": "https://www.kernel.org/doc/html/v4.20/admin-guide/kernel-parameters.html",
          "excerpts": [
            "isolcpus=       [KNL,SMP,ISOL] Isolate a given set of CPUs from disturbance.\n[Deprecated - use cpusets instead]\n                        Format: [flag-list,]<cpu-list>\n\n                        Specify one or more CPUs to isolate from disturbances\n                        specified in the flag list (default: domain):\n\n                        nohz\n                          Disable the tick when a single task runs. A residual 1Hz tick is offloaded to workqueues, which you\n                          need to affine to housekeeping through the global\n                          workqueue's affinity configured via the\n                          /sys/devices/virtual/workqueue/cpumask sysfs file, or\n                          by using the 'domain' flag described below. NOTE: by default the global workqueue runs on all CPUs,\n                          so to protect individual CPUs the 'cpumask' file has to\n                          be configured manually after bootup. domain\n                          Isolate from the general SMP balancing and scheduling\n                          algorithms. Note that performing domain isolation this way\n                          is irreversible: it's not possible to bring back a CPU to\n                          the domains once isolated through isolcpus. It's strongly\n                          advised to use cpusets instead to disable scheduler load\n                          balancing through the \"cpuset.sched_load_balance\" file.\n ... \nOn the positive side, it reduces\n                        interrupt wake-up latency, which may improve performance\n                        in certain environments such as networked servers or\n                        real-time systems. nohibernate     [HIBERNATION] Disable hibernation and resume. nohz=           [KNL] Boottime enable/disable dynamic tick",
            "nohz_full=      [KNL,BOOT,SMP,ISOL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_NO_HZ_FULL=y, set\n                        the specified list of CPUs whose tick will be stopped\n                        whenever possible. The boot CPU will be forced outside\n                        the range to maintain the timekeeping. Any CPUs\n                        in this list will have their RCU callbacks offloaded,\n                        just as if they had also been called out in the\n                        rcu_nocbs= boot parameter.",
            "rcu_nocbs=      [KNL]\n                        The argument is a cpu list, as described above. In kernels built with CONFIG_RCU_NOCB_CPU=y, set\n                        the specified list of CPUs to be no-callback CPUs. Invocation of these CPUs' RCU callbacks will be\n                        offloaded to \"rcuox/N\" kthreads created for that\n                        purpose, where \"x\" is \"p\" for RCU-preempt, and\n                        \"s\" for RCU-sched, and \"N\" is the CPU number.",
            "irqaffinity=    [SMP] Set the default irq affinity mask\n                        The argument is a cpu list, as described above."
          ]
        },
        {
          "title": "Havelsan's Revenue in 2024",
          "url": "https://www.suse.com/c/cpu-isolation-nohz_full-part-3/",
          "excerpts": [
            "22 — The “nohz_full=” kernel boot parameter is the current main interface to configure full dynticks along with CPU Isolation.",
            "A [cpu-list](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html) argument is passed to define the set of CPUs to isolate.",
            "Assuming you have 8 CPUs for example and you want to isolate CPUs 4, 5, 6, 7:\n\n`nohz_full=4-7`",
            "Most of RCU processing is offloaded to the CPUs outside the isolated range. The CPUs set as nohz\\_full run in [NOCB mode](https://lwn.net/Articles/522262/) , which means the RCU callbacks queued on these CPUs are executed from unbound kthreads running on non-isolated CPUs",
            "No need to pass the “rcu\\_nocbs=” kernel parameter as that is automatically taken care of while passing the “nohz\\_full=” paramete",
            "## IRQs affinity\n\nHardware IRQs (other than the timer and some other specific interrupts) may run on any CPU and disturb your isolated set. The resulting noise may not be just about interrupts stealing CPU time and trashing the CPU cache, IRQs may launch further asynchronous work on the CPU: softirq, timer, workqueue, etc… So it is usually a good idea to affine the IRQs to the CPUs outside the range of nohz\\_full.\nThis affinity can be overriden through the file:\n\n`/proc/irq/$IRQ/smp_affinity`",
            "No need to pass the “rcu_nocbs=” kernel parameter as that is automatically taken care of while passing the “nohz_full=” parameter. The CPU also ..."
          ]
        },
        {
          "title": "Features/VirtioVhostUser",
          "url": "https://wiki.qemu.org/Features/VirtioVhostUser",
          "excerpts": [
            "The virtio-vhost-user device lets guests act as vhost device backends so that virtual network switches and storage appliance VMs can provide virtio devices to other guests.",
            "tdev:\n\n```\n$ qemu -M accel=kvm -cpu host -m 1G \\\n       -object memory-backend-file,id=mem0,mem-path=/var/tmp/foo,size=1G,share=on \\\n       -numa node,memdev=mem0 \\\n       -drive if=virtio,file=test.img,format=raw \\\n       -chardev socket,id=chardev0,path=vhost-user.sock \\\n       -netdev vhost-user,chardev=chardev0,id=netdev0 \\\n       -device virtio-net-pci,netdev=netdev0\n```\n",
            "VM2 sees a regular virtio-net device. VM2's QEMU uses the existing vhost-user feature as if it were talking to a host userspace vhost-user backend. VM1's QEMU tunnels the vhost-user protocol messages from VM1's QEMU to the new virtio-vhost-user device so that guest software in VM1 can act as the vhost-user backend."
          ]
        },
        {
          "title": "Tuning a real-time kernel",
          "url": "https://ubuntu.com/blog/real-time-kernel-tuning",
          "excerpts": [
            "nohz_full=<list of isolated cpus> reduces the number of scheduling-clock interrupts, improving energy efficiency and reducing OS jitter. When ..."
          ]
        },
        {
          "title": "Low Latency Performance Tuning for Red Hat Enterprise ...",
          "url": "https://access.redhat.com/sites/default/files/attachments/201501-perf-brief-low-latency-tuning-rhel7-v2.1.pdf",
          "excerpts": [
            "After implementing low latency tuning guidelines from the server vendor, verify CPU frequencies (p- · states) and idle states (c-states) using the turbostat ..."
          ]
        },
        {
          "title": "How to set up KVM guests to use HugePages?",
          "url": "https://access.redhat.com/solutions/36741",
          "excerpts": [
            "Mount the HugeTLB filesystem on the host · Increase the memory lock limit on the host · Reserve HugePages and give the KVM group access to them."
          ]
        },
        {
          "title": "smp affinity setting in linux",
          "url": "https://stackoverflow.com/questions/27963354/smp-affinity-setting-in-linux",
          "excerpts": [
            "I want to load balance the interrupt (irq 75) on my virtual machine system. It has 64 bit redhat 5.8, kernel 2.6.18. There are 8 CPUs in the virtual machine."
          ]
        },
        {
          "title": "Enabling multiple queues (multiqueue) on Linux",
          "url": "https://cloud.vk.com/docs/en/computing/iaas/how-to-guides/vm-multiqueue",
          "excerpts": [
            "The virtio-net multiqueue function provides performance improvement, but it has some limitations: Guest OS is limited to ~200 MSI vectors."
          ]
        },
        {
          "title": "QEMU User Documentation",
          "url": "https://www.qemu.org/docs/master/system/qemu-manpage.html",
          "excerpts": [
            "The QEMU PC System emulator simulates the following peripherals: SMP is supported with a large number of virtual CPUs (upper limit is configuration dependent).",
            "-fw_cfg [name=]name,file=file",
            "Add named fw\\_cfg entry with contents from file file. If the filename contains comma, you must double it (for instance,"
          ]
        },
        {
          "title": "qemu-qmp-ref(7) — qemu-system-common — Debian testing",
          "url": "https://manpages.debian.org/testing/qemu-system-common/qemu-qmp-ref.7.en.html",
          "excerpts": [
            "This document describes all commands currently supported by QMP. Most of the time their usage is exactly the same as in the user Monitor, this means that any ..."
          ]
        },
        {
          "title": "Linux Scheduler Tuning for Low-Latency",
          "url": "https://medium.com/@eren.c.uysal/linux-scheduler-tuning-for-low-latency-ff301da47e3e",
          "excerpts": [
            "You will learn how kernel parameters, CPU isolation, real-time policies, and cgroup configuration can reduce scheduling jitter and improve ..."
          ]
        },
        {
          "title": "27. Vhost Sample Application — DPDK 2.0.0 documentation",
          "url": "https://doc.dpdk.org/guides-2.0/sample_app_ug/vhost.html",
          "excerpts": [
            "The DPDK vhost-net sample code is based on vhost library. Vhost library is developed for user space ethernet switch to easily integrate with vhost functionality ...",
            "The DPDK vhost-net sample code demonstrates KVM (QEMU) offloading the servicing of a Virtual Machine's (VM's) virtio-net devices to a DPDK-based application in ...",
            "The vhost-net module enables KVM (QEMU) to offload the servicing of virtio-net devices to the vhost-net kernel module, reducing the context switching and packet ..."
          ]
        },
        {
          "title": "35. Vhost Sample Application - Documentation",
          "url": "https://doc.dpdk.org/guides-18.02/sample_app_ug/vhost.html",
          "excerpts": [
            "The vhost sample application demonstrates integration of the Data Plane Development Kit (DPDK) ... For basic vhost-user support, QEMU 2.2 (or above) is required."
          ]
        },
        {
          "title": "DPDK vHost User Ports",
          "url": "https://docs.openvswitch.org/en/latest/topics/dpdk/vhost-user/",
          "excerpts": [
            "Quick Example¶. This example demonstrates how to add two dpdkvhostuserclient ports to an existing bridge called br0 :."
          ]
        },
        {
          "title": "QEMU Documentation on Multi-Process and vhost/ virtio-mmio",
          "url": "https://www.qemu.org/docs/master/devel/multi-process.html",
          "excerpts": [
            "Virtio guest device drivers can be connected to vhost user applications\nin order to perform their IO operations. This model uses special virtio\ndevice drivers in the guest and vhost user device objects in QEMU, but\nonce the QEMU vhost user code has configured the vhost user application,\nmission-mode IO is performed by the application.",
            ". ##### vhost socket []( \"Permalink to this heading\")",
            "As mentioned above, one of the tasks of the vhost device object within\nQEMU is to contact the vhost application and send it configuration\ninformation about this device instance.",
            "##### vhost MMIO store acceleration",
            "VMs are often run using HW virtualization features via the KVM kernel\ndriver. This driver allows QEMU to accelerate the emulation of guest CPU\ninstructions by running the guest in a virtual HW mode. When the guest\nexecutes instructions that cannot be executed by virtual HW mode,\nexecution returns to the KVM driver so it can inform QEMU to emulate the\ninstructions in SW.",
            "One of the events that can cause a return to QEMU is when a guest device\ndriver accesses an IO location.",
            "This path is accelerated by the QEMU\nvirtio code by setting up an eventfd file descriptor that the vhost\napplication can directly receive MMIO store notifications from the KVM\ndriver, instead of needing them to be sent to the QEMU process first.",
            "##### vhost interrupt acceleration",
            "Another optimization used by the vhost application is the ability to\ndirectly inject interrupts into the VM via the KVM driver, again,\nbypassing the need to send the interrupt back to the QEMU process first.",
            "The QEMU virtio setup code configures the KVM driver with an eventfd\nthat triggers the device interrupt in the guest when the eventfd is\nwritten. This irqfd file descriptor is then passed to the vhost user\napplication program.",
            "##### vhost access to guest memory",
            "The vhost application is also allowed to directly access guest memory,\ninstead of needing to send the data as messages to QEMU. This is also\ndone with file descriptors sent to the vhost user application by QEMU. These descriptors can be passed to `mmap()` by the vhost application\nto map the guest address space into the vhost application.",
            "Much of the vhost model can be re-used by separated device emulation. In particular, the ideas of using a socket between QEMU and the device emulation ..."
          ]
        },
        {
          "title": "Optimizing Red Hat Enterprise Linux for Real Time - Real Time Tuning Guide (Low Latency Operation)",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/8/html-single/optimizing_rhel_8_for_real_time_for_low_latency_operation/index",
          "excerpts": [
            "The initial mechanism for isolating CPUs is specifying the boot parameter `isolcpus=cpulist` on the kernel boot command line.",
            "The `/etc/tuned/realtime-variables.conf` configuration file includes the default variable content as `isolated_cores=${f:calc_isolated_cores:2}` .",
            "In `tuned-profiles-realtime` version 2.19 and later, the built-in function `calc_isolated_cores` applies the initial CPU setup automatically.",
            "The recommended way to do this for RHEL for Real Time is to use the `TuneD` daemon and its `tuned-profiles-realtime` package."
          ]
        },
        {
          "title": "22.18. Configuring Clock Sources | Deployment Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/deployment_guide/s1-configuring_clock_sources",
          "excerpts": [
            "In the above example, the kernel is using kvm-clock. This was selected at boot time as this is a virtual machine. To override the default clock source, add a ..."
          ]
        },
        {
          "title": "KVM pvclock | Richard WM Jones",
          "url": "https://rwmj.wordpress.com/2010/10/15/kvm-pvclock/",
          "excerpts": [
            "KVM pvclock lets guests read the host's wall clock time. It's really very simple: the guest sets aside a page of its RAM and asks the host to write time into ..."
          ]
        },
        {
          "title": "SLES 15 SP5 | Virtualization Guide | VM Guest clock settings",
          "url": "https://documentation.suse.com/sles/15-SP5/html/SLES-all/sec-kvm-managing-clock.html",
          "excerpts": [
            "KVM provides a paravirtualized clock which is supported via the kvm_clock driver. It is strongly recommended to use kvm_clock . Use the following command inside ..."
          ]
        },
        {
          "title": "Setting Clock Source for VMs on Linux x86-64",
          "url": "https://docs.oracle.com/en/database/oracle/oracle-database/23/cwlin/setting-clock-source-for-virtual-machine-on-linux.html",
          "excerpts": [
            "As the root user, check if the tsc clock source is available on your system. · If the tsc clock source is available, then set tsc as the current clock source."
          ]
        },
        {
          "title": "How to detect if RDTSC returns a constant rate counter ...",
          "url": "https://stackoverflow.com/questions/4509727/how-to-detect-if-rdtsc-returns-a-constant-rate-counter-value",
          "excerpts": [
            "The TSC rate is ensured to be invariant across all P-States, C-States, and stop grant transitions (such as STPCLK Throttling); therefore the TSC ..."
          ]
        },
        {
          "title": "Why isn't RDTSC a serializing instruction?",
          "url": "https://stackoverflow.com/questions/12065721/why-isnt-rdtsc-a-serializing-instruction",
          "excerpts": [
            "Although it is commonly recognized that rdtsc is not a serializing instruction, they serialize themselves. For back-to-back rdtsc s, the latter never carries ..."
          ]
        },
        {
          "title": "Using RDTSCP instructions",
          "url": "https://community.intel.com/t5/Intel-ISA-Extensions/Disabling-HW-prefetcher/m-p/1176703",
          "excerpts": [
            "The RDTSC instruction cannot be ordered by anything short of a serializing instruction, and there are not many of those available in user mode. CPUID is the preferred serializing instruction in user mode, but it has a very high latency.",
            "RDTSCP will not execute until all prior stores have executed, but if you want to defer execution until it is guaranteed that the results of the store have become visible, additional serialization is needed. Are you sure about RDTSC?",
            ".. \nThe LFENCE instruction is ordered with respect to load instructions, other LFENCE instructions, any MFENCE instructions, and any serializing instructions (such as the CPUID instruction). It is not ordered with respect to store instructions or the SFENCE instruction."
          ]
        },
        {
          "title": "Time Stamp Counter (TSC)",
          "url": "https://aakinshin.net/vignettes/tsc/",
          "excerpts": [
            "It tells us that both RDTSC and RDTSCP are supported and the invariant TSC is available. You can do the same thing on Linux with the following command ..."
          ]
        },
        {
          "title": "Pitfalls of TSC usage - Oliver Yang",
          "url": "http://oliveryang.net/2015/09/pitfalls-of-TSC-usage/",
          "excerpts": [
            "Hypervisor will emulate TSC, then rdtsc is not directly executed on physical CPUs. This mode causes performance degrade for rdtsc instruction, ...",
            "For example, VM live migration may cause TSC sync problems if source and target hosts are different from hardware and software levels,."
          ]
        },
        {
          "title": "Virtualize Anything but Time",
          "url": "https://medium.com/@lattehuimin/virtualize-anything-but-time-3d16376faa2",
          "excerpts": [
            "ACPI and HPET are system-wide counters, which have lower resolution, and usually have slower access time, but are unaffected by processor speed ..."
          ]
        },
        {
          "title": "kvm-clock clocksource efficiency versus tsc...",
          "url": "https://kvm.vger.kernel.narkive.com/JvKu5TVI/clock-clocksource-efficiency-versus-tsc",
          "excerpts": [
            "When running inside a KVM VM the default clock source is kvm-clock. This clock takes about 160ns per call and also does not synchronise the application."
          ]
        },
        {
          "title": "solution to rdtsc out of order execution? - c++ - Stack Overflow",
          "url": "https://stackoverflow.com/questions/54690703/solution-to-rdtsc-out-of-order-execution",
          "excerpts": [
            "I am trying to replace clock_gettime(CLOCK_REALTIME, &ts) with rdtsc to benchmark code execution time in terms of cpu cycles rather than server time."
          ]
        },
        {
          "title": "Virtualize Everything but Time",
          "url": "https://www.usenix.org/event/osdi10/tech/slides/broomhead.pdf",
          "excerpts": [
            "by T Broomhead · Cited by 59 — Latency to read counter higher on DomU. ▫ Support Needed. ๏. Expose HPET to Clocksource in guest OSs. ๏. Add hypercall to access platform timer (HPET here)."
          ]
        },
        {
          "title": "use RDTSCP for instruction stream serialized read of TSC",
          "url": "https://groups.google.com/g/golang-codereviews/c/beOsvmuc-nw",
          "excerpts": [
            "The serialization semantics of LFENCE on AMD + // are dependent on MSR C001_1029 and CPU generation. + // LFENCE on Intel does wait for all ..."
          ]
        },
        {
          "title": "KVMClock",
          "url": "https://www.linux-kvm.org/page/KVMClock",
          "excerpts": [
            "You can verify kvm-clock is enable by checking /sys/devices/system/clocksource/clocksource0/current_clocksource. Output should say \"kvm-clock\"."
          ]
        },
        {
          "title": "8.10. Virtual Machine Timer Management with libvirt",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-virtualization-tips_and_tricks-libvirt_managed_timers",
          "excerpts": [
            "KVM pvclock, or kvm-clock lets guest virtual machines read the host physical machine's wall clock time. Previous ..."
          ]
        },
        {
          "title": "devices::virtio::pvclock - Rust",
          "url": "https://crosvm.dev/doc/devices/virtio/pvclock/index.html",
          "excerpts": [
            "Virtio-pvclock emulates clock behavior during VM suspend/resume, adjusting hardware clock offsets and providing the guest kernel with suspension duration."
          ]
        },
        {
          "title": "The Speed of Time (Brendan Gregg)",
          "url": "https://www.brendangregg.com/blog/2021-09-26/the-speed-of-time.html",
          "excerpts": [
            "\"For EC2 instances launched on the AWS Xen Hypervisor, it's a best practice to use the tsc clock source. Other EC2 instance types, such as C5 or M5, use the AWS Nitro Hypervisor. The recommended clock source for the AWS Nitro Hypervisor is kvm-clock.\"",
            "In 2019 myself and others tested kvm-clock and found it was only about 20% slower than tsc.",
            "The recommended clock source for the AWS Nitro Hypervisor is kvm-clock.",
            "ter (thanks!). In 2019 myself and others tested kvm-clock and found it was only about 20% slower than tsc. That's much better than the xen clocksource, but still slow enough to resist switching over absent a reason (such as an reemergence of tsc clock drift issues)."
          ]
        },
        {
          "title": "Stack Overflow: LFENCE/RDTSCP/RDTSC Discussion",
          "url": "https://stackoverflow.com/questions/37452772/x86-64-usage-of-lfence",
          "excerpts": [
            "In the above case, LFENCE ensures all earlier loads it complete before it (Since SDM says: LFENCE instructions cannot pass earlier reads.)",
            "why do we need two separate instructions LFENCE and MFENCE? is answered in the Intel SDM in section \"5.6.4 - SSE2 Cacheability Control and ..."
          ]
        },
        {
          "title": "RDTSCP and Time-Stamp Counter - Felix Cloutier",
          "url": "https://www.felixcloutier.com/x86/rdtscp",
          "excerpts": [
            "RDTSCP",
            "Reads the current value of the processor's time-stamp counter (a 64-bit MSR) into the EDX:EAX registers and also reads the value of the IA32_TSC_AUX MSR.See more",
            "The EDX register is loaded with the high-order 32 bits of the IA32\\_TSC MSR; the EAX register is loaded with the low-order 32 bits of the IA32\\_TSC MSR; and the ECX register is loaded with the low-order 32-bits of IA32\\_TSC\\_AUX ",
            "The processor monotonically increments the time-stamp counter MSR every clock cycle and resets it to 0 whenever the processor is reset.",
            "The RDTSCP instruction is not a serializing instruction, but it does wait until all previous instructions have executed and all previous loads are globally visible.",
            "If software requires RDTSCP to be executed only after all previous stores are globally visible, it can execute MFENCE immediately before RDTSCP. If software ..."
          ]
        },
        {
          "title": "RDTSC is not a serializing instruction",
          "url": "https://edk2.groups.io/g/devel/message/105297",
          "excerpts": [
            "May 24, 2023 — The RDTSC instruction is not a serializing instruction. It does not necessarily wait until all previous instructions have been executed ..."
          ]
        },
        {
          "title": "Time Stamp Counter - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Time_Stamp_Counter",
          "excerpts": [
            "The RDTSCP instruction is a variant of RDTSC that features partial serialization of the instruction stream, but should not be considered as serializing."
          ]
        },
        {
          "title": "Solved: Measure the execution time using RDTSC - Intel Community",
          "url": "https://community.intel.com/t5/Intel-ISA-Extensions/Measure-the-execution-time-using-RDTSC/td-p/1365538",
          "excerpts": [
            "I understand the CPUID instructions ensure full serialization to prevent the execution of other instructions crossing these instructions."
          ]
        },
        {
          "title": "[JDK-8355287] Update Invariant TSC cpu frequency ...",
          "url": "https://bugs.openjdk.org/browse/JDK-8355287",
          "excerpts": [
            "Apr 22, 2025 — Newer hardware has incorporated the Invariant TSC information as a CPUID leaf, no longer providing it in, and recommending reading it from, the ...See more"
          ]
        },
        {
          "title": "Chapter 15. Timestamping | Reference Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/reference_guide/chap-timestamping",
          "excerpts": [
            "The preferred clock source is the Time Stamp Counter (TSC), but if it is not available the High Precision Event Timer (HPET) is the second best option."
          ]
        },
        {
          "title": "Timers and time management in the Linux kernel. Part 6. - 0xax",
          "url": "https://0xax.gitbooks.io/linux-insides/content/Timers/linux-timers-6.html",
          "excerpts": [
            "The /sys/devices/system/clocksource/clocksourceN provides two special files to achieve this: available_clocksource - provides information about available clock ..."
          ]
        },
        {
          "title": "include/linux/timecounter.h - kernel/msm - Git at Google",
          "url": "https://android.googlesource.com/kernel/msm/+/refs/heads/android-msm-crosshatch-4.9-s-preview-1/include/linux/timecounter.h",
          "excerpts": [
            "* Provides completely state-free accessors to the underlying hardware. * Depending on which hardware it reads, the cycle counter may wrap. * around quickly."
          ]
        },
        {
          "title": "How to convert Cycles to nanoseconds - linux kernel",
          "url": "https://stackoverflow.com/questions/33151479/how-to-convert-cycles-to-nanoseconds",
          "excerpts": [
            "I wanted to convert Cycles based on CPU counter register values to nano seconds in Linux. I can see Linux provides cyc_to_ns()/clocksource_cyc2 ..."
          ]
        },
        {
          "title": "Timers and time management in the Linux kernel. Part 2. - xinqiu",
          "url": "https://xinqiu.gitbooks.io/linux-insides-cn/content/Timers/linux-timers-2.html",
          "excerpts": [
            "The main formula by which we convert cycles to the nanoseconds: ((u64) cycles * mult) >> shift;. is not 100% accurate. Instead the number is taken as close ..."
          ]
        },
        {
          "title": "Linux — How the Kernel Counts Time - Level Up Coding",
          "url": "https://levelup.gitconnected.com/linux-how-the-kernel-counts-time-626f2cdf83f4",
          "excerpts": [
            "Jan 25, 2025 — clocks_calc_mult_shift(&cs->mult, &cs->shift, freq, NSEC_PER_SEC / scale, sec * scale); }. This code calculates the scaling factors required ...See more"
          ]
        },
        {
          "title": "timecounter Struct Reference - Linux Kernel - Huihoo",
          "url": "https://docs.huihoo.com/doxygen/linux/kernel/3.7/structtimecounter.html",
          "excerpts": [
            "The nanosecond counter will only wrap around after ~585 years. : the cycle counter used by this instance : most recent cycle counter value seen by ..."
          ]
        },
        {
          "title": "RDTSC and serialization (forum discussion)",
          "url": "https://hero.handmade.network/forums/code-discussion/t/827-rdtsc_can_be_executed_out_of_order",
          "excerpts": [
            "CPUID can be executed at any  \n> privilege level to serialize instruction execution with no effect on program flow,  \n> except that the EAX, EBX, ECX and EDX registers are m",
            " RDTSC instruction can be executed out-of-order. This means that CPU can execute some of instructions that are after RDTSC instruction before where it is placed ... So measurements can be inaccurate.",
            "The solution is to call a serializing instruction before calling the RDTSC one. A serializing instruction is an instruction that forces the CPU to complete every  \n> preceding instruction of the C code before continuing the program execution. By  \n> doing so we guarantee that only the code that is under measurement will be  \n> executed in between the RDTSC calls and that no part of that code will be  \n> executed outside th",
            " Intel recommends to execute CPUID instruction before rdtsc: [https://www-ssl.intel.com/content...enchmark-code-execution-paper.pdf]",
            "gly, the natural choice to avoid out of order execution would be to call CPUID just before  \n> both RTDSC calls"
          ]
        },
        {
          "title": "pvclock(4) - FreeBSD Manual Pages",
          "url": "https://man.freebsd.org/cgi/man.cgi?query=pvclock&sektion=4&manpath=OpenBSD+6.9",
          "excerpts": [
            "DESCRIPTION The pvclock driver supports the paravirtual clock that is available in KVM and other hypervisors. pvclock uses a shared page between the host ..."
          ]
        },
        {
          "title": "PTP_KVM support for arm/arm64",
          "url": "https://docs.kernel.org/6.0/virt/kvm/arm/ptp_kvm.html",
          "excerpts": [
            "PTP_KVM is used for high precision time sync between host and guests. It relies on transferring the wall clock and counter value from the host to the guest ...See more"
          ]
        },
        {
          "title": "Virtualization Deployment and Administration Guide | Red Hat ...",
          "url": "https://docs.redhat.com/en/documentation/Red_Hat_Enterprise_Linux/7/html-single/virtualization_deployment_and_administration_guide/index",
          "excerpts": [
            "Red Hat Enterprise Linux 7.5 and later provide a virtual PTP hardware clock (PHC), which enables the guests to synchronize to the host with a sub ..."
          ]
        },
        {
          "title": "tsc_frequency_cpuid in devices::tsc::cpuid - Rust",
          "url": "https://crosvm.dev/doc/devices/tsc/cpuid/fn.tsc_frequency_cpuid.html",
          "excerpts": [
            "Gets the TSC frequency for cpuid leaf 0x15 from the existing leaves 0x15 and 0x16. §Arguments. cpuid_count : function that returns the CPUID information for ..."
          ]
        },
        {
          "title": "Is mfence for rdtsc necessary on x86_64 platform?",
          "url": "https://stackoverflow.com/questions/41786929/is-mfence-for-rdtsc-necessary-on-x86-64-platform",
          "excerpts": [
            "mfence is there to force serialization in CPU before rdtsc. Usually you will find cpuid there (which is also serializing instruction).See more"
          ]
        },
        {
          "title": "Time synchronization - SEAPATH - Atlassian",
          "url": "https://lf-energy.atlassian.net/wiki/display/SEAP/Time+synchronization",
          "excerpts": [
            "Feb 21, 2025 — In this example, our ptp_kvm device is ptp0. Synchronize the system clock with the PTP clock. This can be done with the tool phc2sys from the ..."
          ]
        },
        {
          "title": "Documentation/virt/kvm/arm/ptp_kvm.rst",
          "url": "https://cm-gitlab.stanford.edu/ntonnatt/linux/-/blob/688ddb30065dce1e2e4584e6eb2b1bef580c8069/Documentation/virt/kvm/arm/ptp_kvm.rst",
          "excerpts": [
            "PTP_KVM is used for high precision time sync between host and guests. It relies on transferring the wall clock and counter value from the host to the guest ..."
          ]
        },
        {
          "title": "Telco Cloud Infra Timing Application - ptp4l, phc2sys and ...",
          "url": "http://www.techplayon.com/telco-cloud-infra-timing-application-ptp4l-phc2sys-and-pmc/",
          "excerpts": [
            "Mar 28, 2024 — KVM virtual ptp driver can helps all the VMs in the same compute node to achieve equal time accuracy by using one ptp capable NIC as time source ..."
          ]
        },
        {
          "title": "1379822 – Feature to update System Clock from kvm- ...",
          "url": "https://bugzilla.redhat.com/show_bug.cgi?id=1379822",
          "excerpts": [
            "PTP device added to improve time synchronization of KVM guests The PTP device has been added for KVM guest virtual machines. It enhances the *kvmclocks* ..."
          ]
        },
        {
          "title": "Getting TSC rate from x86 kernel",
          "url": "https://stackoverflow.com/questions/35123379/getting-tsc-rate-from-x86-kernel",
          "excerpts": [
            "But it does provide one for getting the mult and shift values that can be used to convert TSC counts to nanoseconds. Those values are derived ..."
          ]
        },
        {
          "title": "Which inline assembly code is correct for rdtscp?",
          "url": "https://stackoverflow.com/questions/14783782/which-inline-assembly-code-is-correct-for-rdtscp",
          "excerpts": [
            "It is better to do the shift and add to merge both 32-bit halves in C++ statement rather than inline, this allows the compiler to schedule those ..."
          ]
        },
        {
          "title": "Optimization question: which is faster? : r/asm",
          "url": "https://www.reddit.com/r/asm/comments/1913l2m/optimization_question_which_is_faster/",
          "excerpts": [
            "Intel's current architecture manual says to use mfence before and lfence after in order to prevent reordering around rdtscp , not cpuid . As for ...See more"
          ]
        },
        {
          "title": "Clock sources, Clock events, sched_clock() and delay timers",
          "url": "https://docs.kernel.org/timers/timekeeping.html",
          "excerpts": [
            "You will notice that the clock event device code is based on the same basic idea about translating counters to nanoseconds using mult and shift arithmetic, and ..."
          ]
        },
        {
          "title": "Inline assembly - The Rust Reference",
          "url": "https://docs.adacore.com/live/wave/rust/html/rust_ug/_static/reference/inline-assembly.html",
          "excerpts": [
            "Support for inline assembly is stable on the following architectures: x86 and x86-64; ARM; AArch64 and Arm64EC; RISC-V; LoongArch; s390x."
          ]
        },
        {
          "title": "PTP_KVM support for arm/arm64",
          "url": "https://docs.kernel.org/virt/kvm/arm/ptp_kvm.html",
          "excerpts": [
            "PTP\\_KVM is used for high precision time sync between host and guests. It relies on transferring the wall clock and counter value from the\nhost to the guest using a KVM-specific hypercall",
            "PTP\\_KVM support for arm/arm6"
          ]
        },
        {
          "title": "Virtual PTP hardware clock on KVM guests",
          "url": "https://kimmo.suominen.com/blog/2022/09/virtual-ptp-hardware-clock-on-kvm-guests/",
          "excerpts": [
            "Using a virtual PTP hardware clock with the `ptp_kvm` driver and [chrony](https://chrony.tuxfamily.org) is an easy solution for keeping time on guest machines synchronized to their host.",
            "Load the driver on the running system to check that it exists:\n\n> ```\n> modprobe ptp_kvm\n>\n> ```",
            "Configure `chrony` to use the virtual clock by adding the following line to `/etc/chrony/chrony.conf`:\n\n> ```\n> refclock PHC /dev/ptp0 poll 2 stratum 14\n>\n> `",
            "Setting the stratum of the clock to 14 makes it very unlikely that any network clients would select this system as their time source over the network."
          ]
        },
        {
          "title": "RDTSC with Rust: Inline assembly and timing basics",
          "url": "https://lukas-prokop.at/articles/2021-11-10-rdtsc-with-rust-asm",
          "excerpts": [
            "Example data",
            "data\n\n```\n[rdtsc.rs:47] manufacturer_id = \"GenuineIntel\"\n[rdtsc.rs:129] &measurements = [\n    5530,\n    4828,\n    4694,\n    4688,\n    4878,\n    4694,\n    …\n    4706,\n    511258,\n    4724,\n    …\n    4672,\n    20628,\n    6672,\n    …\n    4658,\n    4800,\n    4670,\n    4668,\n    4808,\n    …\n    4686,\n    4686,\n    4856,\n]\nmin = 4654\nmean = 7358.24\nmedian = 4690\nmax = 511258\n```\n\nI regularly get one (or two) outliers (here: {511258, 20628}). Thus, you should take a look at the median and 95% percentile, not mean and maximum.",
            "Conclusion",
            "on\n\nSubjectively, I prefer the new `asm! (…)` macro syntax in contrast to the `llvm_asm! (…)` syntax. Objectively, rust provides a mature inline assembly interface. RDTSC allows you to evaluate benchmarks in clock cycles. This is preferred over time because we want to independent on the frequency. In practice, you want to use a proper benchmarking library like the [criterion](https://bheisler.github.io/criterion.rs/book/analysis.html) crate which takes care of warmup and accurate measurements. In a F.A.Q., they address associated issues. And in cryptography, one benchmarking library is [SUPERCOP](http://bench.cr.yp.to/supercop.html) which can give some inspiration."
          ]
        },
        {
          "title": "Low latency storage optimizations for Proxmox, KVM & ...",
          "url": "https://www.reddit.com/r/Proxmox/comments/1133u7k/low_latency_storage_optimizations_for_proxmox_kvm/",
          "excerpts": [
            "Here's what we found: It is possible to achieve QD1 guest latencies within roughly 10 microseconds of bare metal. For network-attached storage, ..."
          ]
        },
        {
          "title": "qemu/hw/i386/kvm/clock.c at master",
          "url": "https://github.com/qemu/qemu/blob/master/hw/i386/kvm/clock.c",
          "excerpts": [
            "/* * QEMU KVM support, paravirtual clock device * * Copyright (C) 2011 Siemens ... struct pvclock_vcpu_time_info time;. uint64_t delta;. uint64_t nsec_lo ..."
          ]
        },
        {
          "title": "Timekeeping in virtual machines",
          "url": "https://wiki.osdev.org/Timekeeping_in_virtual_machines",
          "excerpts": [
            "This hypercall is used to get the parameters to calculate a host's clock (KVM_CLOCK_PAIRING_WALLCLOCK for CLOCK_REALTIME). The host copies the following ..."
          ]
        },
        {
          "title": "Linux Kernel: include/linux/clocksource.h Source File - Huihoo",
          "url": "https://docs.huihoo.com/doxygen/linux/kernel/3.7/include_2linux_2clocksource_8h_source.html",
          "excerpts": [
            "3 * This file contains the structure definitions for clocksources. 4 *. 5 * If you are not a clocksource, or timekeeping code, ..."
          ]
        },
        {
          "title": "KVM impact on long tail latency",
          "url": "https://groups.google.com/g/mechanical-sympathy/c/PDEFu3Y3Tcg/m/Ydhtq4b0ChUJ",
          "excerpts": [
            "I am working with a requirement to keep latency at P99.999 under 30 milliseconds over several minute time scales under RHEL 6.3 and KVM."
          ]
        },
        {
          "title": "kernel/time/clocksource.c - linux-anshuman",
          "url": "https://git.gitlab.arm.com/linux-arm/linux-anshuman/-/blob/98962465ed9e6ea99c38e0af63fe1dcb5a79dc25/kernel/time/clocksource.c",
          "excerpts": [
            "The dynamic tick allows the kernel to sleep for periods longer than a single tick, but it does not limit the sleep time currently."
          ]
        },
        {
          "title": "LMAX Disruptor User Guide",
          "url": "https://lmax-exchange.github.io/disruptor/user-guide/index.html",
          "excerpts": [
            "The Disruptor is a library that provides a concurrent ring buffer data structure. It is designed to provide a low-latency, high-throughput work queue in ..."
          ]
        },
        {
          "title": "10. DPDK Telemetry API User Guide",
          "url": "https://doc.dpdk.org/guides-19.08/howto/telemetry.html",
          "excerpts": [
            "This document describes how the Data Plane Development Kit(DPDK) Telemetry API is used for querying port statistics from incoming traffic."
          ]
        },
        {
          "title": "168. DPDK Telemetry API Tests - Documentation",
          "url": "https://doc.dpdk.org/dts/test_plans/telemetry_test_plan.html",
          "excerpts": [
            "This test for Telemetry API for Service Assurance can be run on linux userspace. The application which initializes packet forwarding will act as the server."
          ]
        },
        {
          "title": "Rate limiter system design - Dilip Kumar",
          "url": "https://dilipkumar.medium.com/rate-limiter-system-design-c67f74e78771",
          "excerpts": [
            "Rolling Sliding window counter or Bucket token design are best suited to implement Rate limiter as a Server. Local vs global rate limiter."
          ]
        },
        {
          "title": "Design A Rate Limiter",
          "url": "https://bytebytego.com/courses/system-design-interview/design-a-rate-limiter",
          "excerpts": [
            "A token bucket is a container that has pre-defined capacity. Tokens are put in the bucket at preset rates periodically. Once the bucket is full, no more tokens ..."
          ]
        },
        {
          "title": "Rate Limiting Algorithms Explained with Code",
          "url": "https://blog.algomaster.io/p/rate-limiting-algorithms-explained-with-code",
          "excerpts": [
            "The Token Bucket algorithm is one of the most popular and widely used rate limiting approaches due to its simplicity and effectiveness. How It ..."
          ]
        },
        {
          "title": "Ordering between crossbeam channels - help",
          "url": "https://users.rust-lang.org/t/ordering-between-crossbeam-channels/55780",
          "excerpts": [
            "Feb 18, 2021 — Basically, it's a question of whether the channel behaves like a Relaxed memory ordering, or like a Release / Acquire ordering, where these ..."
          ]
        },
        {
          "title": "A Lock-Free Ring Buffer in Rust: Design and Implementation",
          "url": "https://medium.com/@olcay.d.cabbas/a-lock-free-ring-buffer-in-rust-design-and-implementation-1114307d2d5f",
          "excerpts": [
            "In this article, I'll walk you through my lock-free ring buffer in Rust, explain how it works, and show why it's a neat tool for building fast, ..."
          ]
        },
        {
          "title": "crossbeam - Rust - Docs.rs",
          "url": "https://docs.rs/crossbeam/",
          "excerpts": [
            "ArrayQueue , a bounded MPMC queue that allocates a fixed-capacity buffer on construction. SegQueue , an unbounded MPMC queue that allocates small buffers, ..."
          ]
        },
        {
          "title": "Building a lock free continuous ring buffer in Rust",
          "url": "https://news.ycombinator.com/item?id=20096946",
          "excerpts": [
            "Jun 4, 2019 — The article is discussing an exclusive-writer, exclusive-reader ring buffer. Interleaved writers is intentionally not considered.",
            "There are plenty of hard lock free algorithms, but an SPSC ring buffer is not exactly rocket science and it is a good introduction on the ..."
          ]
        },
        {
          "title": "opentelemetry - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/opentelemetry",
          "excerpts": [
            "May 23, 2025 — OpenTelemetry is an Observability framework and toolkit designed to create and manage telemetry data such as traces, metrics, and logs."
          ]
        },
        {
          "title": "foundations::telemetry::metrics - Rust",
          "url": "https://docs.rs/foundations/latest/foundations/telemetry/metrics/index.html",
          "excerpts": [
            "Metrics-related functionality. Foundations provides simple and ergonomic interface to Prometheus metrics: Use metrics macro to define regular metrics."
          ]
        },
        {
          "title": "hdrhistogram - Rust",
          "url": "https://docs.rs/hdrhistogram",
          "excerpts": [
            "HDR Histogram is designed for recording histograms of value measurements in latency and performance sensitive applications. Measurements show value recording ..."
          ]
        },
        {
          "title": "What is the purpose of crossbeam_queue? : r/rust - Reddit",
          "url": "https://www.reddit.com/r/rust/comments/10yzynu/what_is_the_purpose_of_crossbeam_queue/",
          "excerpts": [
            "Crossbeam_queue crate meant to be used for sharing assynchronous data between threads without Arc, Mutex or RwLock?"
          ]
        },
        {
          "title": "Tracing and Logging High-Level Design",
          "url": "https://projectacrn.github.io/latest/developer-guides/hld/hld-trace-log.html",
          "excerpts": [
            "Nov 5, 2024 — There are two use scenarios of sbuf: sbuf can serve as a lockless ring buffer to share data from ACRN HV to Service VM in non-overwritten mode."
          ]
        },
        {
          "title": "RingBuffer: The Secret Weapon for High-Performance Java ...",
          "url": "https://medium.com/@amit.agarwal0422/ringbuffer-the-secret-weapon-for-high-performance-java-applications-ebabdb64ce58",
          "excerpts": [
            "Designed to pass data between threads with minimal contention. Lock-free (or almost lock-free), using clever low-level algorithms (like CAS — ..."
          ]
        },
        {
          "title": "Keeping printk() under control",
          "url": "https://lwn.net/Articles/66091/",
          "excerpts": [
            "By default, the code limits messages to one every five seconds. It will, however, allow ten messages through in a short period before the rate ...See more"
          ]
        },
        {
          "title": "How to get rid of log buffer overloading? | by Jaimin Parmar",
          "url": "https://medium.com/@parmarjaimin.19/how-to-get-rid-of-log-buffer-overloading-426313c830cd",
          "excerpts": [
            "In the Linux kernel, there is one function printk_ratelimit() using this function we will achieve a limited message log. Function Definition:See more"
          ]
        },
        {
          "title": "Rate limiting the printk instances",
          "url": "https://subscription.packtpub.com/book/cloud-and-networking/9781789953435/5/ch05lvl1sec31/rate-limiting-the-printk-instances",
          "excerpts": [
            "When we emit printk instances from a code path that is executed very often, the sheer amount of printk instances might quickly overflow the kernel log buffer ( ...See more"
          ]
        },
        {
          "title": "The perils of printk()",
          "url": "https://lwn.net/Articles/705938/",
          "excerpts": [
            "The kernel already has a printk_deferred() function that goes out of its way to avoid taking external locks, once again deferring output to a ..."
          ]
        },
        {
          "title": "0013-printk-remove-deferred-printing.patch",
          "url": "https://www.osadl.org/monitoring/patches/rbs8s/0013-printk-remove-deferred-printing.patch.html",
          "excerpts": [
            "Since printing occurs either atomically or from the printing kthread, there is no need for any deferring or tracking possible recursion paths."
          ]
        },
        {
          "title": "What is the exactly Perf buffer size?",
          "url": "https://stackoverflow.com/questions/44584439/what-is-the-exactly-perf-buffer-size",
          "excerpts": [
            "The perf buffer size is defined in terms of pages. It is defined to be 1 + 2^n pages, where one page is required to store metadata about the ring buffer that ..."
          ]
        },
        {
          "title": "Perf ring buffer",
          "url": "https://docs.kernel.org/userspace-api/perf_ring_buffer.html",
          "excerpts": [
            "When using the perf record tool, we can specify the ring buffer size with option -m or --mmap-pages= , the given size will be rounded up to a power of two that ..."
          ]
        },
        {
          "title": "portable_atomic - Rust - Docs.rs",
          "url": "https://docs.rs/portable-atomic",
          "excerpts": [
            "If your crate supports no-std environment and requires atomic CAS, enabling the require-cas feature will allow the portable-atomic to display a helpful error ..."
          ]
        },
        {
          "title": "portable-atomic - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/portable-atomic",
          "excerpts": [
            "Jun 6, 2025 — If your crate supports no-std environment and requires atomic CAS, enabling the require-cas feature will allow the portable-atomic to display a ..."
          ]
        },
        {
          "title": "latency_trace - Rust",
          "url": "https://docs.rs/latency_trace",
          "excerpts": [
            "This framework uses hdrhistogram :: Histogram to collect latency information as it provides an efficient data structure for high-fidelity data collection across ...",
            "The `latency_trace` crate measures latency for sync/async functions using Rust tracing and `hdrhistogram` for both sync and async code."
          ]
        },
        {
          "title": "Why is `ringbuf` crate so fast? : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/1h3bqv0/why_is_ringbuf_crate_so_fast/",
          "excerpts": [
            "If I change the ordering of load() from Acquire to Relaxed (which I think is OK), my ring buffer becomes much faster. If I change the ordering ..."
          ]
        },
        {
          "title": "High performance async binary logging?",
          "url": "https://groups.google.com/g/mechanical-sympathy/c/7HdRCacJCDk",
          "excerpts": [
            "I've developed one that is lock-free, garage-free, and copy-free in the main logging path. It is also specifically designed to deal with burst traffic by ..."
          ]
        },
        {
          "title": "Lock-free Progress Guarantees in a circular buffer queue",
          "url": "https://stackoverflow.com/questions/45907210/lock-free-progress-guarantees-in-a-circular-buffer-queue",
          "excerpts": [
            "A lock-free algorithm guarantees that at least one thread is able to make forward progress regardless of what any other threads are doing."
          ]
        },
        {
          "title": "Rust Crate Crossbeam's ArrayQueue Can Deadlock - Zhiyao Ma",
          "url": "https://zyma.me/post/crossbeam-arrayqueue-deadlock/",
          "excerpts": [
            "Aug 6, 2023 — Although `ArrayQueue` contains no explicit spin lock or mutex, its code structure forms a big spin lock and thus deadlock is possible."
          ]
        },
        {
          "title": "Crossbeam ArrayQueue - Rust",
          "url": "https://docs.rs/crossbeam/latest/crossbeam/queue/struct.ArrayQueue.html",
          "excerpts": [
            "A bounded multi-producer multi-consumer queue. This queue allocates a fixed-capacity buffer on construction, which is used to store pushed\nelements.",
            "The queue cannot hold more elements than the buffer allows.",
            "Alternatively, [`force_push`](struct.ArrayQueue.html.force_push \"method crossbeam::queue::ArrayQueue::force_push\") makes it possible for\nthis queue to be used as a ring-buffer."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/csd01/virtio-v1.0-csd01.html",
          "excerpts": [
            "This document describes the speciﬁcations of the “virtio” family of devices. These devices are found in virtual environments, yet by design they are not all ..."
          ]
        },
        {
          "title": "[PDF] Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/csd02/virtio-v1.0-csd02.pdf",
          "excerpts": [
            "Otherwise, if the VIRTIO_F_EVENT_IDX feature bit is negotiated: • The driver MUST set flags to 0. virtio-v1.0-csd02. Standards Track Work ..."
          ]
        },
        {
          "title": "ringbuf - Rust - Docs.rs",
          "url": "https://docs.rs/ringbuf",
          "excerpts": [
            "Lock-free SPSC FIFO ring buffer with direct access to inner data. Usage At first you need to create the ring buffer itself. HeapRb is recommended but you may ..."
          ]
        },
        {
          "title": "Building a fast SPSC queue: atomics, memory ordering ...",
          "url": "https://www.reddit.com/r/cpp/comments/1ivlv7e/building_a_fast_spsc_queue_atomics_memory/",
          "excerpts": [
            "I wrote some articles on building a fast SPSC bounded queue (aka circular buffer). They introduce lock-free techniques and explore optimizations like memory ..."
          ]
        },
        {
          "title": "agerasev/ringbuf: Lock-free SPSC FIFO ring buffer with ... - GitHub",
          "url": "https://github.com/agerasev/ringbuf",
          "excerpts": [
            "Features · Lock-free operations - they succeed or fail immediately without blocking or waiting. · Arbitrary item type (not only Copy ). · Items can be inserted and ..."
          ]
        },
        {
          "title": "ringbuf - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/ringbuf",
          "excerpts": [
            "Mar 24, 2025 — Lock-free SPSC FIFO ring buffer with direct access to inner data."
          ]
        },
        {
          "title": "How does a crossbeam channel work?",
          "url": "https://users.rust-lang.org/t/how-does-a-crossbeam-channel-work/46698",
          "excerpts": [
            "Aug 1, 2020 — If you write to an atomic integer with atomic ordering Release , and then later read from that atomic integer with the ordering Acquire ..."
          ]
        },
        {
          "title": "Stack Overflow: how to avoid printk log dropping in linux kernel",
          "url": "https://stackoverflow.com/questions/26413750/how-to-avoid-printk-log-dropping-in-linux-kernel",
          "excerpts": [
            "Easy, just add \"printk.synchronous=1\" to kernel cmdline. The printk() has became completely async now. You can add printk.synchronous=1 to let printk() be sync.",
            "The printk() has became completely async now."
          ]
        },
        {
          "title": "Virtio-Console — Project ACRN™ 3.4-unstable documentation",
          "url": "https://projectacrn.github.io/latest/developer-guides/hld/virtio-console.html",
          "excerpts": [
            "Nov 5, 2024 — The Virtio-console is a simple device for data input and output. The console's virtio device ID is 3 and can have from 1 to 16 ports."
          ]
        },
        {
          "title": "[PDF] Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/cs01/virtio-v1.1-cs01.pdf",
          "excerpts": [
            "Abstract: This document describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, ..."
          ]
        },
        {
          "title": "hdrhistogram - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/hdrhistogram",
          "excerpts": [
            "Nov 18, 2023 — The resulting \"HDR\" histogram allows for fast and accurate analysis of the extreme ranges of data with non-normal distributions, like latency."
          ]
        },
        {
          "title": "Ordering in std::sync::atomic - Rust",
          "url": "https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "The `Ordering` enum in `std::sync::atomic` specifies how atomic operations synchronize memory, with options like `Relaxed`, `Release`, `Acquire`, `AcqRel`, and ...",
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "Misunderstanding in Acquire/Release. Memory ordering - help",
          "url": "https://users.rust-lang.org/t/misunderstanding-in-acquire-release-memory-ordering/114010",
          "excerpts": [
            "Acquire/release (with load/store) prevent reordering of memory reads and writes by the compiler or the hardware across those instructions."
          ]
        },
        {
          "title": "Rust Atomics and Locks",
          "url": "https://marabos.nl/atomics/",
          "excerpts": [
            "An Unsafe Spin Lock",
            "A Safe Interface Using a Lock Guard",
            "The Memory Model",
            "Happens-Before Relationship",
            "Relaxed Ordering",
            "Release and Acquire Ordering",
            "Sequentially Consistent Ordering",
            "Fences",
            "Common Misconceptions",
            "A Minimal Implementation",
            "Locking: Mutexes and RwLocks",
            "mutex",
            "Chapter 7.\nUnderstanding the Processor",
            "Chapter 3.\nMemory Ordering",
            "Chapter 4. Building Our Own Spin Lock",
            "Chapter 9.\nBuilding Our Own Locks",
            "Building Our Own Channels"
          ]
        },
        {
          "title": "spin_loop in core::hint",
          "url": "https://doc.rust-lang.org/beta/core/hint/fn.spin_loop.html",
          "excerpts": [
            "`spin_loop` emits a signal for a busy-wait spin-loop, allowing the processor to optimize, and does not interact with the OS, unlike `thread::yield_now`."
          ]
        },
        {
          "title": "Ordering in core::sync::atomic",
          "url": "https://doc.rust-lang.org/core/sync/atomic/enum.Ordering.html",
          "excerpts": [
            "Memory orderings specify the way atomic operations synchronize memory. In its weakest Ordering::Relaxed, only the memory directly touched by the operation is ..."
          ]
        },
        {
          "title": "core::hint",
          "url": "https://doc.rust-lang.org/core/hint/index.html",
          "excerpts": [
            "spin_loop: Emits a machine instruction to signal the processor that it is running in a busy-wait spin-loop (“spin lock”). unreachable_unchecked ⚠: Informs ..."
          ]
        },
        {
          "title": "What's a good Mutex to use for #![no_std]? : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/dfmv25/whats_a_good_mutex_to_use_for_no_std/",
          "excerpts": [
            "We're working on defining a generic no_std Mutex trait in the Embedded Working Group, you might be interested in checking it out!",
            "The 2 main options I know of are spin and lock_api . spin is seen a bit more commonly among no_std projects. lock_api isn't seen as often in ..."
          ]
        },
        {
          "title": "Spinlocks Considered Harmful - matklad",
          "url": "https://matklad.github.io/2020/01/02/spinlocks-considered-harmful.html",
          "excerpts": [
            "Jan 2, 2020 — A spinlock is implemented entirely in user space: from OS point of view, a “spinning” thread looks exactly like a thread that does a heavy computation.",
            "A crate uses Mutex or other synchronization mechanism from std; Someone asks for #[no_std] support; Mutex is swapped for some variation of ..."
          ]
        },
        {
          "title": "No_std, no allocator/heap, no void* ... how?",
          "url": "https://users.rust-lang.org/t/no-std-no-allocator-heap-no-void-how/115809?page=2",
          "excerpts": [
            "Aug 12, 2024 — Hardware itself has logic to help the software with concurrency problems (ex: spinlocks); we'd better leverage those! ... primitives to be then ..."
          ]
        },
        {
          "title": "spin-based implementation for no_std · Issue #61",
          "url": "https://github.com/matklad/once_cell/issues/61",
          "excerpts": [
            "Oct 7, 2019 — What lazy_static does in this case is that it has a spin feature, which replaces locking with spinning. I think just silently replacing OS ..."
          ]
        },
        {
          "title": "#![no_std] Is there a lightweight way to share data between ...",
          "url": "https://www.reddit.com/r/rust/comments/1ewg5g2/no_std_is_there_a_lightweight_way_to_share_data/",
          "excerpts": [
            "Even though it'd be possible to guard the shared resource with a Mutex or other synchronization primitives provided by the RTOS it is simply ..."
          ]
        },
        {
          "title": "Atomic in core::sync",
          "url": "https://doc.rust-lang.org/core/sync/atomic/type.Atomic.html",
          "excerpts": [
            "A memory location which can be safely modified from multiple threads. This has the same size and bit validity as the underlying type T . However, ..."
          ]
        },
        {
          "title": "Rust Atomics and Locks: Low-Level Concurrency in Practice",
          "url": "https://www.amazon.com/Rust-Atomics-Locks-Low-Level-Concurrency/dp/1098119444",
          "excerpts": [
            "In this practical book, Mara Bos, leader of the Rust library team, helps Rust programmers of all levels gain a clear understanding of low-level concurrency."
          ]
        },
        {
          "title": "Algorithms for scalable synchronization on shared-memory ...",
          "url": "https://www.cs.rochester.edu/u/scott/papers/1991_TOCS_synch.pdf",
          "excerpts": [
            "por JM MELLOR-CRUMMEY · 1991 · Mencionado por 2006 — The ticket lock, the array-based queuing locks, and the MCS lock all guarantee that processors attempting to acquire a lock will succeed in FIFO order. This ..."
          ]
        },
        {
          "title": "Safety and Liveness of MCS Lock—Layer by Layer",
          "url": "https://flint.cs.yale.edu/flint/publications/mcslock-tr.pdf",
          "excerpts": [
            "por J Kim · Mencionado por 37 — The MCS lock provides inter-CPU locks with FIFO ordering, ensuring mutual exclusion (safety) and bounded waiting time (liveness). It is a list-based queuing ..."
          ]
        },
        {
          "title": "lock-free-multi-producer-single-consumer-ring-buffer",
          "url": "https://crates.io/crates/lock-free-multi-producer-single-consumer-ring-buffer",
          "excerpts": [
            "Jan 18, 2019 — A lock-free, multi-producer, single-consumer (MPSC) ring buffer. Optimized for sending and receiving 'bursts' of messages."
          ]
        },
        {
          "title": "thingbuf - Rust",
          "url": "https://docs.rs/thingbuf",
          "excerpts": [
            "thingbuf is a lock-free, array-based concurrent ring buffer, used for high-throughput bounded MPSC channels, and is a good choice for general use."
          ]
        },
        {
          "title": "mpmc_bounded_queue.h",
          "url": "https://github.com/couchbase/phosphor/blob/master/thirdparty/dvyukov/include/dvyukov/mpmc_bounded_queue.h",
          "excerpts": [
            "\\file * This file contains an implementation of a MPMC queue by Dmitry Vyukov. It is * largely the same as the original source except for: minor formatting ..."
          ]
        },
        {
          "title": "Concurrent Lock Free SPSC Queue - by Youwei Zhuo",
          "url": "https://medium.com/@youweizhuo/concurrent-lock-free-queue-c9abd8bf700d",
          "excerpts": [
            "A lock-free, cache-efficient multi-core synchronization mechanism for line-rate network traffic monitoring."
          ]
        },
        {
          "title": "Spin lock with exponential backoff · Issue #3349 · redisson/ ...",
          "url": "https://github.com/redisson/redisson/issues/3349",
          "excerpts": [
            "Jan 17, 2021 — It makes this lock inefficient in clusters with more than 5 nodes. It would be great to have a spin lock with exponential backoff, because it ..."
          ]
        },
        {
          "title": "Use of WFE and SEV instructions by spin-locks - Arm Developer",
          "url": "https://developer.arm.com/documentation/ddi0406/c/Application-Level-Architecture/Application-Level-Memory-Model/Synchronization-and-semaphores/Use-of-WFE-and-SEV-instructions-by-spin-locks",
          "excerpts": [
            "ARMv7 and ARMv6K provide Wait For Event and Send Event instructions, WFE and SEV , that can assist with reducing power consumption and bus contention."
          ]
        },
        {
          "title": "Are cache-line-ping-pong and false sharing the same?",
          "url": "https://stackoverflow.com/questions/30684974/are-cache-line-ping-pong-and-false-sharing-the-same",
          "excerpts": [
            "False sharing and cache-line ping-ponging are related but not the same thing. False sharing can cause cache-line ping-ponging, but it is not the only possible ..."
          ]
        },
        {
          "title": "Mutex Trait for std and no_std environments - Rust Users Forum",
          "url": "https://users.rust-lang.org/t/mutex-trait-for-std-and-no-std-environments/67727",
          "excerpts": [
            "I'm trying to write code that requires some basic locking that compiles with no_std and std. For that I'm looking for a Mutex trait implementation."
          ]
        },
        {
          "title": "Rust bounded concurrency without semaphores",
          "url": "https://medium.com/@jaderd/rust-bounded-concurrency-without-semaphores-368ff683ebdc",
          "excerpts": [
            "... primitives like Go channels and Rust's multi-producer, single-consumer… ... concurrency, and OS-level tuning for extreme backend performance."
          ]
        },
        {
          "title": "Exponential Back off with Jitter",
          "url": "https://medium.com/@amittikoo84/exponential-back-off-with-jitter-29cc8f2ad887",
          "excerpts": [
            "In this article, we'll explore what decorrelated jitter is, how it can benefit API calls, and how to implement it in your API testing. What is ..."
          ]
        },
        {
          "title": "Use of Wait For Event (WFE) and Send Event (SEV) with Locks",
          "url": "https://developer.arm.com/documentation/ddi0406/c/Appendices/Barrier-Litmus-Tests/Exclusive-accesses-and-barriers/Use-of-Wait-For-Event--WFE--and-Send-Event--SEV--with-Locks?lang=en",
          "excerpts": [
            "The ARMv7 architecture includes Wait For Event and Send Event instructions, that can be executed to reduce the required number of iterations of a ..."
          ]
        },
        {
          "title": "Cache Ping Pong: Crack Down on Unsanctioned Games",
          "url": "https://www.jabperf.com/cracking-down-on-unsanctioned-cache-ping-pong-games/",
          "excerpts": [
            "Likewise, cache ping-pong is a dangerous game for multithreaded applications. Decades of research detail its harmful effect on application ..."
          ]
        },
        {
          "title": "core::sync::atomic - Rust Documentation",
          "url": "https://doc.rust-lang.org/core/sync/atomic/",
          "excerpts": [
            "All atomic types in this module are guaranteed to be [lock-free](https://en.wikipedia.org/wiki/Non-blocking_algorithm) if they’re\navailable. This means they don’t internally acquire a global mutex.",
            "Atomic operations may be implemented at the instruction layer with\nlarger-size atomics. For example some platforms use 4-byte atomic\ninstructions to implement `AtomicI8`.",
            "Each atomic access takes an [`Ordering`](enum.Ordering.html \"enum core::sync::atomic::Ordering\") which defines how the operation interacts with the\nhappens-before order.",
            "ample is not tested on wasm\")\n\n```\nuse std::sync::Arc;\nuse std::sync::atomic::{AtomicUsize, Ordering};\nuse std::{hint, thread};\n\nfn main() {\n    let spinlock = Arc::new(AtomicUsize::new(1));\n\n    let spinlock_clone = Arc::clone(&spinlock);\n\n    let thread = thread::spawn(move || {\n        spinlock_clone.store(0, Ordering::Release);\n    });\n\n    // Wait for the other thread to release the lock\n    while spinlock.load(Ordering::Acquire) != 0 {\n        hint::spin_loop();\n    }\n\n    if let Err(panic) = thread.join() {\n        println! (\"Thread had an error: {panic:?\n ... \n"
          ]
        },
        {
          "title": "Rust Atomics and Locks",
          "url": "https://www.oreilly.com/library/view/rust-atomics-and/9781098119430/",
          "excerpts": [
            "The Rust programming language is extremely well suited for concurrency, and its ecosystem has many libraries that include lots of concurrent data structures, locks, and more.",
            "Basics of Rust Concurrency",
            "\n  \n  Threads in Rust Scoped Threads Shared Ownership and Reference Counting Statics Leaking Reference Counting Borrowing and Data Races Interior Mutability Cell RefCell Mutex and RwLock Atomics UnsafeCell Thread Safety: Send and Sync Locking: Mutexes and RwLocks Rust’s Mutex Lock Poisoning Reader-Writer Lock Waiting: Parking and Condition Variables Thr",
            "Atomics",
            "Atomic Load and Store Operations",
            "Example: Stop Flag",
            "Compare-and-Exchange Operations",
            "Memory Ordering",
            "Reordering and Optimizations The Memory Model Happens-Before Relationship Spawning and Joining Relaxed Ordering Release and Acquire Ordering Example: Locking Example: Lazy Initialization with Indirection Consume Ordering Sequentially Consistent Ordering Fences Common Misconceptions Summary",
            "Building Our Own Spin Lock",
            "A Minimal Implementation An Unsafe Spin Lock A Safe Interface Using a Lock Guard Summary",
            "Building Our Own Channels",
            "A Simple Mutex-Based Channel An Unsafe One-Shot Channel Safety Through Runtime Checks Safety Through Types Borrowing to Avoid Allocation Blocking Summary",
            "How to build your own locking and synchronization primitives correctly"
          ]
        },
        {
          "title": "Bounded MPMC queue",
          "url": "https://www.1024cores.net/home/lock-free-algorithms/queues/bounded-mpmc-queue",
          "excerpts": [
            "The cost of enqueue/dequeue is 1 CAS per operation.",
            "No amortization, just 1 CAS. No dynamic memory allocation/management during operation.",
            "On my dual-core laptop enqueue/dequeue takes 75 cycles on average in a synthetic multi-threaded benchmark."
          ]
        },
        {
          "title": "Lib .rs Concurrency Catalog",
          "url": "https://lib.rs/concurrency",
          "excerpts": [
            "High-performance concurrency, synchronization, scheduling, and logging primitives in Rust",
            "Async synchronization primitives for #[no_std] rust. v1.1.2",
            "Concurrency — list of Rust libraries/crates // Lib.rs",
            "* [#### spin\n  \n  \n  \n  Spin-based synchronization primitives\n  \n  \n  \n  v 0\\.10.0 11\\.4 **M** no-std # rw-lock # spin-lock # mutex # mutex-lock](/crates/spi",
            "* [#### lf-shardedringbuf\n  \n  \n  \n  An async, lock-free, sharded, cache-aware SPSC/MPSC/MPMC ring buffer in Rust"
          ]
        },
        {
          "title": "Design and Structure of Theseus - The Theseus OS Book",
          "url": "https://www.theseus-os.com/Theseus/book/design/design.html",
          "excerpts": [
            "Theseus's Design and Structure. Theseus is a safe-language OS, in which everything runs in a single address space (SAS) and single privilege level (SPL)."
          ]
        },
        {
          "title": "Theseus: an Experiment in Operating System Structure and State ...",
          "url": "https://www.usenix.org/conference/osdi20/presentation/boos",
          "excerpts": [
            "This paper describes an operating system (OS) called Theseus. Theseus is the result of multi-year experimentation to redesign and improve OS modularity."
          ]
        },
        {
          "title": "Theseus OS",
          "url": "https://github.com/theseus-os",
          "excerpts": [
            "Theseus is a modern OS written from scratch in Rust that explores intralingual design: closing the semantic gap between compiler and hardware.See more"
          ]
        },
        {
          "title": "rusty-hermit 0.3.32",
          "url": "https://docs.rs/crate/rusty-hermit/0.3.32",
          "excerpts": [
            "RustyHermit is a rewrite of HermitCore written in Rust. The ownership model of Rust guarantees memory/thread-safety and enables us to eliminate many classes of ..."
          ]
        },
        {
          "title": "rusty-hermit 0.3.10 - Docs.rs",
          "url": "https://docs.rs/rusty-hermit/0.3.10/",
          "excerpts": [
            "RustyHermit - A Rust-based, lightweight unikernel. Build Status ... Virtio support (comming soon); Network suppot (comming soon). Credits.",
            "HermitCore is a unikernel targeting a scalable and predictable runtime for high-performance and cloud computing. Unikernel means, you bundle your application ..."
          ]
        },
        {
          "title": "The Hermit Operating System - Rust OSDev",
          "url": "https://rust-osdev.com/showcase/hermit/",
          "excerpts": [
            "Hermit uses smoltcp as the network stack, which is completely written in Rust. ... For instance, the support of virtiofs is at an early stage.",
            "Hermit is a unikernel project, that is completely written in Rust. Unikernels are application images that directly contain the kernel as a library."
          ]
        },
        {
          "title": "Tock Embedded OS - GitHub",
          "url": "https://github.com/tock",
          "excerpts": [
            "Tock is an embedded operating system designed for running multiple concurrent, mutually distrustful applications on low-memory and low-power microcontrollers."
          ]
        },
        {
          "title": "kernel - Rust",
          "url": "https://docs.tockos.org/",
          "excerpts": [
            "Core Tock Kernel. The kernel crate implements the core features of Tock as well as shared code that many chips, capsules, and boards use."
          ]
        },
        {
          "title": "tock/doc/Getting_Started.md at master - GitHub",
          "url": "https://github.com/tock/tock/blob/master/doc/Getting_Started.md",
          "excerpts": [
            "Tock Getting Started Guide. This covers how to install the toolchain on your platform to start using and developing Tock."
          ]
        },
        {
          "title": "Tock Getting Started Guide",
          "url": "https://opensecura.googlesource.com/3p/tock/tock/+/HEAD/doc/Getting_Started.md",
          "excerpts": [
            "Tock builds a unique kernel for every board it supports. Boards include details like pulling together the correct chips and pin assignments. To build a kernel, ..."
          ]
        },
        {
          "title": "Tock binary size - Blog",
          "url": "https://tweedegolf.nl/en/blog/126/tock-binary-size",
          "excerpts": [
            "Jun 9, 2024 — A lot of code size bloat. In 2018, when Tock was first released, a minimal kernel required ~7KB for code. Today, the default build for most ..."
          ]
        },
        {
          "title": "Building Your First Tock App - Hackster.io",
          "url": "https://www.hackster.io/alevy/building-your-first-tock-app-540695",
          "excerpts": [
            "In this project we will compile the Tock kernel and a simple blink app, and install both onto the Hail development board."
          ]
        },
        {
          "title": "Introducing tock: a digital clock for the terminal : r/rust",
          "url": "https://www.reddit.com/r/rust/comments/c1qpdl/introducing_tock_a_digital_clock_for_the_terminal/",
          "excerpts": [
            "Introducing tock: a digital clock for the terminal. r/rust ... r/rust - Tock OS: embedded operating system for Cortex-M microcontrollers."
          ]
        },
        {
          "title": "RustyHermit and Unikernel Insights for MVP 1.0 Estimation",
          "url": "https://publications.rwth-aachen.de/record/976226/files/976226.pdf",
          "excerpts": [
            "s a TCP/IP stack,\n\nRustyHermit uses smoltcp <sup>1</sup> which is also completely written in Rust.",
            "RustyHermit [33][63] is a unikernel developed at RWTH Aachen University. It is\n\nwritten in Rust and is the successor of HermitCore, a C-based unikernel previously\n\ndeveloped at RWTH for high performance computing. The RustyHermit version pre-\n\nsented in the original paper [33] does not use any code written in C/C++ and only\n\ncontains 3.27 % lines of code marked **u",
            "Sung et al. proposed a way to use Intel Memory Protection\n\nKeys (MPK) for memory isolation while keeping the single address space architecture. They implemented it in a Rust based unikernel to separate safe kernel code, unsafe\n\nkernel code and application co",
            "The\n\ndesign of unikernels is a disadvantage here as it has a single address space and no further\n\npermission system within the unikernel. An attacker who compromised the application\n\nautomatically has access to all low level kernel featur",
            "The extended capabilities\n\ncan also be used to attack further systems. That said, a point has to be discussed\n\nwhich came up during RustyHermit analysis. uhyve grants RustyHermit full access to the host OS file system with the permissions\n\nof the user running uhyve. Thus, a malicious or compromised unikernel (applica-\n\ntion) could compromi"
          ]
        },
        {
          "title": "Analyzing Binary Size Bloat in Tock",
          "url": "https://www.tockos.org/blog/2024/tock-binary-size-bloat/",
          "excerpts": [
            "g) is a powerful and secure embedded operating\nsystem. While Tock was designed with resource constraints in mind,\nyears of additional features, generalizing to more platforms, and\nsecurity improvements have brought resource, and in particular, code\nsize bloat. A *lot* of code-size bloat. In 2018, when Tock was first released, a\nminimal kernel required ~7KB for code. Today, the default build for\nmost platforms is easily over 100KB.",
            "We now have a barebones version of Tock that can run processes and\nsupports a minimal set of functionality–just printing to the console.\nLet’s Pretend We Don’t Want Processes"
          ]
        },
        {
          "title": "Theseus: an Experiment in Operating System Structure and ...",
          "url": "https://www.academia.edu/51083894/Theseus_an_Experiment_in_Operating_System_Structure_and_State_Management",
          "excerpts": [
            "... four person-years of effort and comprises ~38000 lines of lifetime of the owned value. The syntax in L11 gives the name from-scratch Rust code, 900 lines of ..."
          ]
        },
        {
          "title": "rust-vmm/vm-memory: Virtual machine's guest memory crate",
          "url": "https://github.com/rust-vmm/vm-memory",
          "excerpts": [
            "The vm-memory crate provides a set of traits to decouple VM memory consumers from VM memory providers. Based on these traits, VM memory consumers can access ..."
          ]
        },
        {
          "title": "vm_memory - Rust",
          "url": "https://docs.rs/vm-memory/",
          "excerpts": [
            "The `vm_memory` crate provides traits for allocating, handling, and interacting with VM's physical memory, decoupling consumers from providers."
          ]
        },
        {
          "title": "smoltcp - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/smoltcp",
          "excerpts": [
            "smoltcp v0. 12.0 A TCP/IP stack designed for bare-metal, real-time systems without a heap."
          ]
        },
        {
          "title": "at master · rcore-os/virtio-drivers",
          "url": "https://github.com/rcore-os/virtio-drivers?search=1",
          "excerpts": [
            "VirtIO guest drivers in Rust. Contribute to rcore-os/virtio-drivers development by creating an account on GitHub."
          ]
        },
        {
          "title": "virtio-drivers/src/queue.rs at master",
          "url": "https://github.com/rcore-os/virtio-drivers/blob/master/src/queue.rs",
          "excerpts": [
            "VirtIO guest drivers in Rust. Contribute to rcore-os/virtio-drivers development by creating an account on GitHub."
          ]
        },
        {
          "title": "rust-vmm",
          "url": "https://github.com/rust-vmm",
          "excerpts": [
            "rust-vmm-container Public Container with all dependencies required for running rust-vmm crates integration tests."
          ]
        },
        {
          "title": "Another Rust-y OS: Theseus joins Redox in pursuit of safer, more ...",
          "url": "https://www.theregister.com/2021/01/14/rust_os_theseus/",
          "excerpts": [
            "Currently Theseus is on GitHub and comprises 38,000 lines of Rust code and 900 lines of assembly . The operating system can be built on Linux, Windows using WSL (Windows Subsystem for Linux), macOS, or in a Docker container. It can be run in the QEMU emulator."
          ]
        },
        {
          "title": "smoltcp-rs/smoltcp: a smol tcp/ip stack",
          "url": "https://github.com/smoltcp-rs/smoltcp",
          "excerpts": [
            "smoltcp is a standalone, event-driven TCP/IP stack that is designed for bare-metal, real-time systems. Its design goals are simplicity and robustness."
          ]
        },
        {
          "title": "smoltcp - GitHub",
          "url": "https://github.com/smoltcp-rs",
          "excerpts": [
            "TCP/IP Stack for Embedded Rust. smoltcp has 3 repositories available. Follow their code on GitHub."
          ]
        },
        {
          "title": "Tock (operating system) - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/Tock_(operating_system)",
          "excerpts": [
            "Missing: minimal 7KB"
          ]
        },
        {
          "title": "quartiq/smoltcp-nal: An embedded-nal implementation for ... - GitHub",
          "url": "https://github.com/quartiq/smoltcp-nal",
          "excerpts": [
            "This repository provides an implementation of a TCP- and UDP-capable network stack that can be used for any library that leverages the embedded-nal."
          ]
        },
        {
          "title": "aspenmesh/tock: a mock clock library for golang",
          "url": "https://github.com/aspenmesh/tock",
          "excerpts": [
            "Instead you can use a mock clock library which lets you implement your own fake wall clock that you can advance under test conditions. You get fast unit tests, ..."
          ]
        },
        {
          "title": "amit levy @amit",
          "url": "https://discuss.systems/@amit",
          "excerpts": [
            "Join us for the TockWorld conference this September, focusing on Tock, operating systems and embedded systems security, and Rust."
          ]
        },
        {
          "title": "RustyHermit and HermitCore in the arXiv paper",
          "url": "https://arxiv.org/pdf/2407.18431",
          "excerpts": [
            "The new Unikernel named RustyHermit consists of only\n3.27% unsafe Rust, with the rest of the code base consisting of\nsafe Rust, dramatically decreasing the surface area where memory\nerrors could originate.",
            "They were able to get a 7.5:1 proof-to-code\nratio, which is higher than other formerly verified micro-kernels"
          ]
        },
        {
          "title": "RustyHermit and Related Unikernel Research",
          "url": "https://dl.acm.org/doi/fullHtml/10.1145/3674805.3690756",
          "excerpts": [
            "The new Unikernel named RustyHermit consists of only 3.27% unsafe Rust, with the rest of the code base consisting of safe Rust, dramatically decreasing the surface area where memory errors could originate.",
            "The authors proposed a novel architecture for an OS by embedding a WebAssembly (WASM) runtime into the kernel that can run programs written in any language that can compile down to WASM.",
            "Unikernels are designed to do one thing and one thing only, thus limiting the complexity that a general-purpose operating system has to deal with [ [20]() ]."
          ]
        },
        {
          "title": "pjungkamp/rusty-hermit: HermitCore bindings for Rust",
          "url": "https://github.com/PJungkamp/rusty-hermit",
          "excerpts": [
            "RustyHermit is a unikernel targeting a scalable and predictable runtime for high-performance and cloud computing. Unikernel means, you bundle your ..."
          ]
        },
        {
          "title": "virtio-drivers - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/virtio-drivers",
          "excerpts": [
            "May 12, 2025 — virtio-drivers = \"0.11.0\". Documentation. docs.rs/virtio-drivers/0.11.0. Repository. github.com/rcore-os/virtio-drivers. Owners. Runji Wang ..."
          ]
        },
        {
          "title": "boyter/scc: Sloc, Cloc and Code",
          "url": "https://github.com/boyter/scc",
          "excerpts": [
            "A tool similar to cloc, sloccount and tokei. For counting the lines of code, blank lines, comment lines, and physical lines of source code in many programming ..."
          ]
        },
        {
          "title": "rcore-os/rCore-Tutorial-v3",
          "url": "https://github.com/rcore-os/rCore-Tutorial-v3",
          "excerpts": [
            "Features",
            "    * Platform supported:",
            "qemu-system-riscv64 simulator or dev boards based on Kendryte K210 SoC such as Maix Dock",
            "    * OS",
            "       * concurrency of multiple processes each of which contains mutiple native threads",
            "       * preemptive scheduling(Round-Robin algorithm)",
            "       * dynamic memory management in kernel",
            "       * virtual memory",
            "       * a simple file system with a block cache",
            "       * an interactive shell in the userspace",
            "    * only 4K+ LoC",
            "    * A detailed documentation in Chinese in spite of the lack of comments in the code(English version is not available at present)",
            "Prerequisites",
            "Install Rust",
            "See official guide . Install some tools:",
            "$ rustup target add riscv64gc-unknown-none-elf",
            "$ cargo install cargo-binutils --vers =0.3.3",
            "$ rustup component add llvm-tools-preview",
            "$ rustup component add rust-src",
            "Install Qemu",
            "Here we manually compile and install Qemu 7.0.0."
          ]
        },
        {
          "title": "[PDF] Real-time KVM from the ground up - Wiki",
          "url": "https://wiki.linuxfoundation.org/_media/realtime/events/rt-summit2016/kvm_rik-van-riel.pdf",
          "excerpts": [
            "Measured scheduling latencies inside KVM guest. ○ Minimum: 5us ... RHEL7.x Scheduler Latency (cyclictest). Intel Ivy Bridge 2.4 Ghz, 128 GB mem ..."
          ]
        },
        {
          "title": "Realtime KVM - LWN.net",
          "url": "https://lwn.net/Articles/656807/",
          "excerpts": [
            "Realtime is about determinism, not speed. Realtime workloads are those\nwhere missing deadlines is bad: it results in voice breaking up in telecommunications\nequipment, missed opportunities in stock trading, and exploding rockets in\nvehicle control and avionics.",
            " A PREEMPT\\_RT kernel tries hard to make every part of the Linux\nkernel preemptible, except for short sections of code.",
            "To achieve these numbers, of course, the system needs to be\ncarefully set up to\navoid all kinds of high-latency system operations: no CPU frequency changes,\nno CPU hotplug, no loading or unloading of kernel modules, and no swapping.",
            "Bare-metal latencies\nare less than 2 microseconds, but KVM's measurement of 6-microsecond\nlatencies is also a very \ngood result"
          ]
        },
        {
          "title": "Chapter 36. Configuring huge pages",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/configuring-huge-pages_monitoring-and-managing-system-status-and-performance",
          "excerpts": [
            "With Red Hat Enterprise Linux 8, you can use huge pages for applications that work with big data sets, and improve the performance of such applications."
          ]
        },
        {
          "title": "Interrupt moderation approaches for virtio. ...",
          "url": "https://netdevconf.org/0x18/docs/netdev-0x18-paper37-talk-slides/Interrupt%20moderation%20approaches%20for%20virtio.pdf",
          "excerpts": [
            "How to reduce interrupts for the vdpa device? 2. How to reduce interrupt for the virtio PCI device? 3. How to moderate them adaptatively?"
          ]
        },
        {
          "title": "Network latency problems on Win10 VM : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/gcpagr/network_latency_problems_on_win10_vm/",
          "excerpts": [
            "Try switching scream to network mode, that will reduce your interrupts. Have you enabled msi interrupts?"
          ]
        },
        {
          "title": "Is there any benefit to running a realtime Linux kernel for ...",
          "url": "https://www.reddit.com/r/homelab/comments/t5gbzd/is_there_any_benefit_to_running_a_realtime_linux/",
          "excerpts": [
            "It would probably hurt performance significantly, Unless you're tailoring the system specifically to a single workload."
          ]
        },
        {
          "title": "A Comparison of Scheduling Latency in Linux, PREEMPT RT ...",
          "url": "https://people.mpi-sws.org/~bbb/papers/pdf/ospert13.pdf",
          "excerpts": [
            "por FCBB Brandenburg · Mencionado por 92 — Long-term monitoring of apparent latency in PREEMPT RT. Linux real-time systems. ... Performance tuning towards a KVM-based low latency virtualization system. In ...",
            "by FCBB Brandenburg · Cited by 92 — Scheduling latency under Linux and its principal real-time variant, the PREEMPT RT patch, are typically measured us- ing cyclictest, a tracing tool that treats ..."
          ]
        },
        {
          "title": "Real-Time VMs using KVM — ECI documentation",
          "url": "https://eci.intel.com/docs/3.3/development/tutorials/enable-rt-vms.html",
          "excerpts": [
            "Simultaneously, a cyclictest benchmark will be executed on the real-time VM. Stressor VM Image Creation¶. Follow the same steps from Real-Time VM Image Creation ..."
          ]
        },
        {
          "title": "[PDF] Translation Pass-Through for Near-Native Paging Performance in VMs",
          "url": "https://www.usenix.org/system/files/atc23-bergman.pdf",
          "excerpts": [
            "the nested page walk cost. However, replacing the existing radix page tables with hashed page tables requires significant changes to existing software and ..."
          ]
        },
        {
          "title": "[PDF] Accelerating Two-Dimensional Page Walks for Virtualized Systems",
          "url": "https://pages.cs.wisc.edu/~remzi/Classes/838/Spring2013/Papers/p26-bhargava.pdf",
          "excerpts": [
            "This section discusses the performance cost of page walks and shows that guest and nested page entries exhibit both a high degree of reuse and a ..."
          ]
        },
        {
          "title": "Optimizing the TLB Shootdown Algorithm with Page ...",
          "url": "https://www.usenix.org/sites/default/files/conference/protected-files/atc17_slides_amit.pdf",
          "excerpts": [
            "The system optimizes TLB shootdowns by using PTE access-bit tracking to prevent unnecessary flushes, based on the PTE access-bit."
          ]
        },
        {
          "title": "Struggling to get rid of stuttering in Windows VM : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/14oupr2/struggling_to_get_rid_of_stuttering_in_windows_vm/",
          "excerpts": [
            "I'm having a very similar problem and I think I might have found a solution. I use Arch Linux as host with libvirt + qemu + virt-manager."
          ]
        },
        {
          "title": "[PDF] Making Page Walks Single-Access Cache Hits - uu .diva",
          "url": "https://uu.diva-portal.org/smash/get/diva2:1633977/FULLTEXT02.pdf",
          "excerpts": [
            "This approach has the benefit of not requiring any OS changes, but comes at the cost of the complexity of scanning the structure at address ..."
          ]
        },
        {
          "title": "Add support to do threaded napi busy poll",
          "url": "https://lwn.net/Articles/1034942/",
          "excerpts": [
            "5 days ago — ... busy poll interfaces for a low latency usecase with stable P99. Please note that the throughput and cpu efficiency is a non-goal. For ..."
          ]
        },
        {
          "title": "[Qemu-devel] [PATCH V2] tap: vhost busy polling support",
          "url": "https://lists.gnu.org/archive/html/qemu-devel/2016-04/msg00931.html",
          "excerpts": [
            "This patch add the capability of basic vhost net busy polling which is supported by recent kernel. User could configure the maximum number ..."
          ]
        },
        {
          "title": "27. Vhost Sample Application — DPDK 2.1.0 documentation",
          "url": "https://doc.dpdk.org/guides-2.1/sample_app_ug/vhost.html",
          "excerpts": [
            "Missing: busy_poll sk_busy_poll"
          ]
        },
        {
          "title": "Real world tips, tricks, and notes of using epoll-based busy ...",
          "url": "https://netdevconf.info/0x18/docs/netdev-0x18-paper10-talk-slides/Real%20world%20tips,%20tricks,%20and%20notes%20of%20using%20epoll-based%20busy%20polling%20v2.pdf",
          "excerpts": [
            "Busy poll helped us reduce latency in prod. - Needed some tweaks to get us there. - System-wide config is too coarse-grained. - More work to be done."
          ]
        },
        {
          "title": "What's a ballpark figure for PCIe interrupt-to-userspace latency?",
          "url": "https://community.osr.com/t/whats-a-ballpark-figure-for-pcie-interrupt-to-userspace-latency/41717",
          "excerpts": [
            "An expected latency for a typical PCIe card to push some useful info from the card to userspace would be .05-.1us + .3-1us + .5-8us + 1-10us + 1us (x2) ="
          ]
        },
        {
          "title": "Managing Interrupt in Virtio-PCI - Michael Zhao - Medium",
          "url": "https://michael2012z.medium.com/managing-interrupt-in-virtio-pci-bfe585117b49",
          "excerpts": [
            "This article introduces 2 ways of sending interrupts in virtio devices with PCI transport layer. This is not a formal technique manual."
          ]
        },
        {
          "title": "TLB Shootdown Latency in Linux",
          "url": "https://sanidhya.github.io/pubs/2018/latr.pdf",
          "excerpts": [
            "A synchronous TLB-shootdown mechanism increases the latency by several micro-seconds for certain virtual address operations, which is known to a culprit that ... This\n\nis mainly because most existing systems use expensive inter-\n\nprocessor interrupts (IPIs) <sup>1</sup> to deliver a TLB shootdown:\n\ne.g., an IPI takes up to 6.6 _µ_ s for 120 cores with 8 sockets\n\nand 2.7 _µ_ s for 16 cores with 2 sockets. Even worse, current\n\nTLB shootdown mechanisms handle invalidation in a _syn-_\n\n_chronous_ manner. That is, a core initiating a TLB shootdown\n\nfirst sends IPIs to all remote cores and then waits for their\n\nacknowledgments, while the corresponding IPI interrupt\n\nhandlers on the remote cores complete the invalidation of a\n\nTLB entry (see §2.1 for details). Such an expensive TLB shootdown severely affects the\n\noverall performance of applications that frequently trigger\n\nmemory management operations that change the page ta-\n\nble [ 2 , 62 ], such as web servers like Apache [ 4 ] and data\n\nanalytic engines like MapReduce [ 25 , 52 ]. For example, a typ-\n\nical Apache workload serving small static web pages or files\n\n1 IPIs are the mechanism used in x86 to communicate with different cores. does not scale beyond six cores with the current TLB shoot-\n\ndown mechanism in Linux (see Figure 1). Once alleviated by\n\nour new mechanism, Apache can handle 46.3% more TLB\n\nshootdowns and thus improve its throughput by 59.9%. More\n\nimportantly, it is all possible without any application-level\n\nmodifications. To solve this problem, there have been two broad cate-\n\ngories of research, namely, hardware- and software-based ap-\n\nproaches.\n ... \nThe TLB shootdown mechanism outlined above shows\n\nthree types of performance overheads: sending IPIs to remote\n\ncores, which has an increased overhead on large NUMA\n\nmachines; handling interrupts on remote cores, which might\n\nbe delayed due to temporarily disabled interrupts; and the\n\nwait time for ACK s on the initiating core. **AutoNUMA page migration. ** AutoNUMA page migration\n\nis a feature provided by Linux to migrate pages to the NUMA\n\nnode where they are being frequently accessed from, to avoid\n\ncostly cross-NUMA-domain accesses. To identify pages that are predominantly used on a remote\n\nNUMA node, the AutoNUMA background task periodically\n\nscans a process’ address space and changes page table entries\n\nwhich triggers frequent TLB shootdowns. After the TLB\n\nshootdown for a specific virtual address, any subsequent\n\nmemory access to this virtual address triggers a page fault. If,\n\nduring this page fault, a page is accessed twice from a NUMA\n\nnode different from the current node the page resides on, the\n\npage will be migrated to the node accessing the page, pending\n\nother factors such as enough free memory on the target node. Figure 3a gives a high-level overview of this process in Linux\n\nP A\n\nClear PTE TLB inv Send IPI\n\nProcess A (P A )\n\nProcess A\n\nTLB inv\n\nTLB inv\n\nIPI\n\nACK\n\nWait for ACKs Unmap\n\nProcess A\n\nProcess A\n\nProcess A\n\nTime\n\nACK\n\nIPI\n\nCore 2\n\nCore 1\n\nCore 3\n\n6µs\n\n**(a)** Page munmap des",
            "A synchronous\n\nTLB-shootdown mechanism increases the latency by several\n\nmicro-seconds for certain virtual address operations, which\n\nis known to a culprit that contributes to the tail latency of\n\nsome critical services in data centers ",
            "IPIs are the mechanism used in x86 to communicate with different cores. does not scale beyond six cores with the current TLB shoot-\n\ndown mechanism in Linux (see Figure 1",
            "a typ-\n\nical Apache workload serving small static web pages or fi",
            "takes up to 6.6 _µ_ s for 120 cores with 8 sockets\n\nand 2.7 _µ_ s for 16 cores with 2 sockets",
            "Latr on the other hand is able to\n\nefficiently use the cache coherence protocol to complete the\n\nmunmap() operation in less than 40 _µ_ s on 120 cores, reducing\n\nthe latency by 66.7% compared to Linux , as Latr ’s munmap()\n\ndoes not rely on expensive IPIs and eliminates the ACK wait\n\ntime on the initia",
            "To solve this problem, there have been two broad cate-\n\ngories of research, namely, hardware- and software-based ap-\n\nproaches.\nHardware-based approaches strive to provide TLB\n\ncache coherence in hardware (see §2.2), but require expensive\n\nhardware modifications and introduce additional verifica-\n\ntion challenges to t",
            "tes_ ( 1 ) to identify the cores\n\nincluded in a TLB shootdown. The states are made accessible to\n\nremote cores via the cache coherence protocol ( 2 ) that remote\n\ncores use to clear entries in their local TLB. the ",
            "tickless ( CONFIG\\_NO\\_HZ ) configuration disables the scheduler\n\nticks on idle cores. In such tickless kernels, the idle core’s\n\nid is not set in a Latr state’s CPU bitmask because none of\n\nthe processes are scheduled on the idle c"
          ]
        },
        {
          "title": "realtime:documentation:howto:tools:cyclictest:start ...",
          "url": "https://wiki.linuxfoundation.org/realtime/documentation/howto/tools/cyclictest/start",
          "excerpts": [
            "Jan 19, 2025 — Cyclictest accurately and repeatedly measures the difference between a thread's intended wake-up time and the time at which it actually wakes up."
          ]
        },
        {
          "title": "Demystifying the Real-Time Linux Scheduling Latency",
          "url": "https://bristot.me/demystifying-the-real-time-linux-latency/",
          "excerpts": [
            "Cyclictest is used in practice to measure the scheduling latency of each CPU of the system. Maximum observed latency values generally range from a few ..."
          ]
        },
        {
          "title": "The x86 kvm shadow mmu",
          "url": "https://docs.kernel.org/virt/kvm/x86/mmu.html",
          "excerpts": [
            "Supported page sizes include 4k, 2M, 4M, and 1G. 4M pages are treated as two separate 2M pages, on both guest and host, since the mmu always uses PAE paging.See more"
          ]
        },
        {
          "title": "(PDF) Performance Implications of Extended Page Tables ...",
          "url": "https://www.researchgate.net/publication/319660274_Performance_Implications_of_Extended_Page_Tables_on_Virtualized_x86_Processors",
          "excerpts": [
            "Aug 7, 2025 — Processing TLB misses on a virtualized x86 server requires a twodimensional page walk that can have 6x more page table lookups, hence 6x more ...See more"
          ]
        },
        {
          "title": "Efficient Memory Tiering in a Virtual Machine",
          "url": "https://arxiv.org/html/2506.06067v1",
          "excerpts": [
            "However, dTLB load misses increase by 3% mainly due to TLB shootdowns associated with consolidation and migration as we start from DRAM and ..."
          ]
        },
        {
          "title": "1641702 – check tsc scaling fea-ture of destination host on migration",
          "url": "https://bugzilla.redhat.com/show_bug.cgi?id=1641702",
          "excerpts": [
            "For migration of guests with invariant TSC to work, it is necessary that the destination host: 1) Has the same TSC frequency as the source host. or 2) Supports ..."
          ]
        },
        {
          "title": "Linux SCHED_FIFO not respecting thread priorities",
          "url": "https://stackoverflow.com/questions/63674522/linux-sched-fifo-not-respecting-thread-priorities",
          "excerpts": [
            "A SCHED_FIFO thread runs until either it is blocked by an I/O request, it is preempted by a higher priority thread, or it calls sched_yield(2)."
          ]
        },
        {
          "title": "Chapter 5. Priorities and Policies | Reference Guide",
          "url": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/7/html/reference_guide/chap-priorities_and_policies",
          "excerpts": [
            "A SCHED_FIFO thread with a priority of 1 will always be scheduled ahead of any SCHED_OTHER thread. The SCHED_RR policy is very similar to the SCHED_FIFO policy."
          ]
        },
        {
          "title": "Pin QEMU KVM cores to host CPU cores without libvirt : r/ ...",
          "url": "https://www.reddit.com/r/VFIO/comments/7avvwx/qemuaffinity_pin_qemu_kvm_cores_to_host_cpu_cores/",
          "excerpts": [
            "A python script to easily pin select QEMU threads to host CPU cores. I use this to pin the KVM and IO threads for two qemu instances to specific isolated cores."
          ]
        },
        {
          "title": "Let's talk about resources isolation | Proxmox Support Forum",
          "url": "https://forum.proxmox.com/threads/hey-proxmox-community-lets-talk-about-resources-isolation.124256/",
          "excerpts": [
            "In practice such scheduling mistakes can cause latency spikes measured in hundreds of milliseconds. Heterogenous CPUs: the simplest example ..."
          ]
        },
        {
          "title": "CPU pinning? | Page 2",
          "url": "https://forum.proxmox.com/threads/cpu-pinning.67805/page-2",
          "excerpts": [
            "I'm looking to pin the cpu's and ram in the VM to a single numa node, and let KVM handle the core scheduling and choosing."
          ]
        },
        {
          "title": "details about page table walks in multi-level paging",
          "url": "https://stackoverflow.com/questions/59641957/details-about-page-table-walks-in-multi-level-paging",
          "excerpts": [
            "Grouping allocations into the same 4M region (or on x86-64, 2M or 1G) as existing allocations avoids needing a new 4k page of PTEs, instead just ...See more"
          ]
        },
        {
          "title": "Parallel computing and the cost of TLB shoot-down",
          "url": "https://lifecs.likai.org/2010/06/parallel-computing-and-cost-of-tlb.html",
          "excerpts": [
            "Parallel computing and the cost of TLB shoot-down. Translation ... The cost of TLB shoot-down increases as the number of CPUs increase."
          ]
        },
        {
          "title": "P99 CONF Day 1: The Low-Latency Lowdown - ScyllaDB",
          "url": "https://www.scylladb.com/2022/10/19/p99-conf-day-1-the-low-latency-lowdown/",
          "excerpts": [
            "Day 1 of P99 CONF kicked off with “misery metrics” and ended with a look at P99 latency reduction as a Sisyphean task."
          ]
        },
        {
          "title": "Performance issues with win10 guest in qemu kvm",
          "url": "https://www.reddit.com/r/archlinux/comments/q3erbh/performance_issues_with_win10_guest_in_qemu_kvm/",
          "excerpts": [
            "Hello, I have a need to deploy a rather \"snappy\" win10 guest inside qemu kvm.. Reason being I hate all the microsoft trash but because of ..."
          ]
        },
        {
          "title": "Monotonic clock with KVM pv-clock",
          "url": "https://kvm.vger.kernel.narkive.com/7zaHFi1e/monotonic-clock-with-pv-clock",
          "excerpts": [
            "Hi, I'm trying to figure out how a guest OS can get a monotonic clock using KVM's paravirtual clock. At first, I thought that the clock I get using ..."
          ]
        },
        {
          "title": "VM Stopped Working, Marking Clocksource \"TSC\" as ...",
          "url": "https://forums.unraid.net/topic/58827-vm-stopped-working-marking-clocksource-tsc-as-unstable-because-skew-is-too-large/",
          "excerpts": [
            "Jul 18, 2017 — Everytime I start 2 games on my virtual machines the complete system freeze and hangs with the following error, i must restart the entire system to clear the ..."
          ]
        },
        {
          "title": "High latency in Linux KVM virtual machines",
          "url": "https://forum.proxmox.com/threads/high-latency-in-linux-kvm-virtual-machines.12685/",
          "excerpts": [
            "I am experiencing high latency and weird ping times while on the other 5 vms there is no problem. The VM's are running latest updates of debian squeeze."
          ]
        },
        {
          "title": "A new option for decreasing guest latency (cpu-pm=on)",
          "url": "https://www.reddit.com/r/VFIO/comments/cmgmt0/a_new_option_for_decreasing_guest_latency_cpupmon/",
          "excerpts": [
            "cpu-pm=on allows the guest to put the cpu to sleep without involving the host. If you use cpu isolation this is what you want. Unfortunately there is a ..."
          ]
        },
        {
          "title": "Low Latency Storage Optimizations for Proxmox, KVM, & QEMU",
          "url": "https://kb.blockbridge.com/technote/proxmox-tuning-low-latency-storage/",
          "excerpts": [
            " Mitigations",
            " Mitigations",
            " Mitigations"
          ]
        },
        {
          "title": "Cpu pinning strategy for kvm / centos7 - Server Fault",
          "url": "https://serverfault.com/questions/871053/cpu-pinning-strategy-for-kvm-centos7",
          "excerpts": [
            "Relatively simple is the isolcpus boot parameter which allows you to isolate one or more CPUs from the scheduler. This prevents the scheduler ..."
          ]
        },
        {
          "title": "CPU pinning, CPU isolation, hooks? : r/VFIO - Reddit",
          "url": "https://www.reddit.com/r/VFIO/comments/ob7m3d/cpu_pinning_cpu_isolation_hooks/",
          "excerpts": [
            "These are best achieved with kernel boot flags isolcpus=xxx rcu_nocbs=xxx and optional other flags like nohz_full=xxx to prevent those threads ..."
          ]
        },
        {
          "title": "Linux kernel 6.12 RT / Kernel & Hardware ...",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=302317",
          "excerpts": [
            "Jan 3, 2025 — The compile option PREEMPT_RT is not enabled in the standard 6.12 kernel, only PREEMP_DYNAMIC, and that changes a lot the latency, which leads ..."
          ]
        },
        {
          "title": "Torwards a more Scalable KVM Hypervisor",
          "url": "https://kvm-forum.qemu.org/2018/Torwards%20a%20more%20Scalable%20KVM%20Hypervisor.pdf",
          "excerpts": [
            "by W Li · Cited by 3 — Modern operating systems consider TLB shootdown operations to be performance critical and so optimize them to exhibit very low latency. The implementation of ..."
          ]
        },
        {
          "title": "[PDF] Agile Paging: Exceeding the Best of Nested and Shadow Paging",
          "url": "https://research.cs.wisc.edu/multifacet/papers/isca16_agile_paging.pdf",
          "excerpts": [
            "Nested paging directly performs a two-level page walk that makes TLB misses slower than unvirtualized native, but enables fast page tables ..."
          ]
        },
        {
          "title": "Transparent Huge Pages vs. Huge Pages with KVM / ovirt",
          "url": "https://www.reddit.com/r/ovirt/comments/he9bx6/transparent_huge_pages_vs_huge_pages_with_kvm/",
          "excerpts": [
            "1G HP has 2 benefits over THP: 1) no daemon running that dynamically moves memory around, 2) even fewer cycles spent looking up page tables than 2MB HPs."
          ]
        },
        {
          "title": "Insane DPC latency during disk or network IO : r/VFIO",
          "url": "https://www.reddit.com/r/VFIO/comments/8c1xr2/insane_dpc_latency_during_disk_or_network_io/",
          "excerpts": [
            "Easily spikes between 40,000 to 70,000 microseconds, and I've recorded interrupts to process latency as high as 500,000 microseconds."
          ]
        },
        {
          "title": "[PATCH v6] virtio-net: Fix and update ... - OASIS",
          "url": "https://groups.oasis-open.org/communities/community-home/digestviewer/viewthread?GroupId=3973&MessageKey=f91de9e4-9796-4121-af9b-b9f1b41495cb&CommunityKey=2f26be99-3aa1-48f6-93a5-018dce262226",
          "excerpts": [
            "This patch makes several improvements to the notification coalescing feature, including: - Consolidating virtio_net_ctrl_coal_tx and virtio_net_ctrl_coal_rxSee more"
          ]
        },
        {
          "title": "MDPI Electronics (2024) - Sub-10µs P99.99 Latency in VirtIO-based QEMU/KVM guests: Risks and Mitigations",
          "url": "https://www.mdpi.com/2079-9292/13/9/1626",
          "excerpts": [
            "Latencies with hypervisors and AMP frameworks: On the one hand, a hypervisor increases the OS’s kernel latency—especially interrupt latency—and the network latency.",
            "Interrupt latency: After identifying latencies in OSs, MPSoCs, hypervisors, and AMP frameworks, the interrupt latency comes up as one of the main latencies in embedded systems."
          ]
        },
        {
          "title": "Exploring Rust for Unikernel Development",
          "url": "https://plos-workshop.org/2019/preprint/plos19-lankes.pdf",
          "excerpts": [
            "by S Lankes · 2019 · Cited by 43 — To enable processor features such as AVX, direct access to the control registers is required. To program these registers, RustyHermit reuses the x86 [27] ..."
          ]
        },
        {
          "title": "Assessing Unikernel Security",
          "url": "https://people.cs.pitt.edu/~babay/courses/cs3551/projects/SCADA_Unikernel/NCC_Group-Assessing_Unikernel_Security.pdf",
          "excerpts": [
            "by S Michaels · Cited by 11 — Proponents of unikernels claim that their smaller codebase and lack of excess services make them more efficient and secure than full-OS virtual ..."
          ]
        },
        {
          "title": "Unikraft: Fast, Specialized Unikernels the Easy Way",
          "url": "https://arxiv.org/pdf/2104.12721",
          "excerpts": [
            "by S Kuenzer · 2021 · Cited by 153 — Abstract. Unikernels are famous for providing excellent performance in terms of boot times, throughput and memory consump-."
          ]
        },
        {
          "title": "Introduction - The Rust Reference",
          "url": "https://doc.rust-lang.org/nightly/reference/index.html?search=used",
          "excerpts": [
            "Linkage » Linkageused (e.g. --gc-sections or -dead_strip for macOS). [link ... Note If you are using rustc to link, these rules are enforced automatically."
          ]
        },
        {
          "title": "Isolation in Rust: What is Missing?",
          "url": "https://users.cs.utah.edu/~aburtsev/doc/rust-plos21.pdf",
          "excerpts": [
            "by A Burtsev · 2021 · Cited by 7 — Hardware or language-based isolation and SFI systems require a copy of all objects passed across isolation boundary to enforce the property that ..."
          ]
        },
        {
          "title": "Isolation in Rust: What is Missing? - Vikram",
          "url": "https://arkivm.github.io/publications/2021-plos-rust-isolation.pdf",
          "excerpts": [
            "by A Burtsev · 2021 · Cited by 7 — Fault isolation and recovery in RedLeaf. In this paper, we analyze recent systems that explore software-based isolation in Rust [7, 12, 34, 39 ..."
          ]
        },
        {
          "title": "Why does Rust compile every crate that I include in my ...",
          "url": "https://www.reddit.com/r/rust/comments/1ehgi8l/why_does_rust_compile_every_crate_that_i_include/",
          "excerpts": [
            "The reason Rust primarily uses static linking is because it makes a lot of other things much easier. Deploying a Rust project means you just ..."
          ]
        },
        {
          "title": "On Memory Safety and Undefined Behavior of Unikernels in ...",
          "url": "http://publications.rwth-aachen.de/record/973239/files/973239.pdf?version=1",
          "excerpts": [
            "Nonetheless, the team has proven Rust's principle of encapsulating interior muta- bility by using unsafe internally to be sound, while finding some specific ..."
          ]
        },
        {
          "title": "Use of `panic!` in `no_std` - embedded",
          "url": "https://users.rust-lang.org/t/use-of-panic-in-no-std/115582",
          "excerpts": [
            "Aug 6, 2024 — The only macro provided for panicking is panic!() which is only taking a string payload, unlike in std which provides panic_any! capable of taking arbitrary ..."
          ]
        },
        {
          "title": "no_std issues with implementing panic_handler #3498",
          "url": "https://github.com/rust-lang/miri/issues/3498",
          "excerpts": [
            "Apr 22, 2024 — I don't quite understand why abort and halt wouldn't work for no_std environments though considering these panic handlers were made for this express purpose."
          ]
        },
        {
          "title": "Unikernels are secure. Here is why.",
          "url": "http://unikernel.org/blog/2017/unikernels-are-secure",
          "excerpts": [
            "I believe that from a security perspective unikernels can offer a level of security that is unprecedented in mainstream computing."
          ]
        },
        {
          "title": "TrustZone for Cortex-M - Arm Developer",
          "url": "https://developer.arm.com/ip-products/security-ip/trustzone/trustzone-for-cortex-m",
          "excerpts": [
            "Both Secure and Non-secure state have their own Memory Protection Unit (MPU) to provide software isolation between unprivileged software components. Banked ..."
          ]
        },
        {
          "title": "Isolation in Rust: What is Missing?",
          "url": "https://par.nsf.gov/servlets/purl/10386701",
          "excerpts": [
            "by A Burtsev · 2021 · Cited by 7 — To provide clean fault isolation semantics and subsystem termination, RedLeaf adapts the idea that isolated subsys- tems should not share state on the heap ( ..."
          ]
        },
        {
          "title": "RISC-V Memory Protection: Diving Deep into the Complexities",
          "url": "https://medium.com/@talktokarthikbk/risc-v-memory-protection-diving-deep-into-the-complexities-9d751212be6b",
          "excerpts": [
            "Missing: guest practices"
          ]
        },
        {
          "title": "Hardening virtio for emerging security use cases",
          "url": "https://www.redhat.com/en/blog/hardening-virtio-emerging-security-usecases",
          "excerpts": [
            "\n\nFrom the device implementation perspective, the security boundary is the boundary **of itself** . That is, the virtio device (green block) doesn’t trust any entity that talks to it via the transport layer (yellow blocks). This means the virtio device needs to validate any request that crosses its security boundary.",
            "** — the virtio driver won’t trust the virtio device, so it can’t assume it behaves exactly as what virtio specification required. In the case of a virtualization environment, the hypervisor/VMM is not trusted as well. This means that any metadata stored in the shared memory that can be accessed by the virtio device must not be trusted.",
            "ample, the driver should store the buffer metadata in private memory that can not be accessed by the virtio device instead of depending on the data stored in the virtqueue. The virtio driver should also perform the necessary checks on the buffer metadata produced by the virtio device and drop the illegal metadata.",
            "The virtio driver should not trust any value that is read from the virtio device. For example, malicious virtio devices may try to crash or poke the virtio driver's private memory by crafting some illegal config spaces. The virtio driver should be able to sanitize those configs or simply fail the driver probing.",
            "The virtio driver should be prepared for the unexpected notification raised by the malicious virtio device at any time.",
            "he virtio driver trusts the platform devices. For example, the virtio driver trusts the platform IOMMU device to perform the necessary checks for limiting the memory that a virtio device can acce",
            "In those cases, the guest memory is encrypted and hardware-based technologies can prevent the hypervisor from accessing arbitrary guest memory.",
            "The hypervisor itself is not trusted as well as the virtualized virtio device. A lot of hardening work on top of the virtio driver is required by the guest, so the above threat model should be used with the one defined by vendor-specific confidential computing technology (for example SEV-SNP or TDX).",
            "Most of the popular virtio backends (such as vhost and Qemu) have been hardened to prevent these kinds of possible attacks from both the guest and other virtual devices.",
            "By supporting virtio/vDPA devices in their NIC, vendors can build on this work to help protect new hardware they develop from security risks. Instead of creating vendor-specific and closed source implementations of devices and drivers that are more prone to vulnerabilities, approaches such as virtio/vDPA can save time and help vendors focus on their “secret sauce” instead.",
            "### Virtio hardening and vendor-specific hardening",
            "We believe driver hardening will become a common practice in the future as the threat models evolve. This means the hardening needs to be done not only for the virtio driver but also for other vendor-specific drivers.",
            "### Summary",
            "In this article we have reviewed two threat models virtio is required to deal with:"
          ]
        },
        {
          "title": "How to generate statically linked executables?",
          "url": "https://stackoverflow.com/questions/31770604/how-to-generate-statically-linked-executables",
          "excerpts": [
            "Rust statically links everything except glibc (and libgcc) by default.",
            "If you want a 100% statically linked binary, you can use MUSL libc, which supports both"
          ]
        },
        {
          "title": "Unikernel Security Research Overview",
          "url": "https://x41-dsec.de/news/wrapping-up-unikernel-security-research/",
          "excerpts": [
            "The single address space is also an inherent property of unikernels and should be kept in mind when discussing unikernel security.",
            "One of the main motivations for this thesis is to contribute an analysis of a unikernel written in a memory safe language. Thus, an in-depth analysis of RustyHermit was conducted, the results were presented in previous blogposts. Writing a unikernel in Rust has the benefit that memory corruption vulnerabilities can only occur in `unsafe` code parts.",
            "Another major security claim is the isolation introduced by using one unikernel for exactly one application and isolating the unikernels from the host system and each other by the hypervisor.",
            "However, this does not provide protection against initially compromising a unikernel. Thus, it can not be a replacement for classical security mitigations.",
            "The most important conclusion is that writing a secure unikernel in terms of “no vulnerabilities” is not enough."
          ]
        },
        {
          "title": "[PDF] Intra-Unikernel Isolation with Intel Memory Protection Keys",
          "url": "https://www.ssrg.ece.vt.edu/papers/vee20-mpk.pdf",
          "excerpts": [
            "\nUnikernels are minimal, single-purpose virtual machines. This new operating system model promises numerous bene-\nfits within many application domains in terms of lightweight-\nness, performance, and security. Although the isolation be-\ntween unikernels is generally recognized as strong, there\nis no isolation within a unikernel itself. This is due to the\nuse of a single, unprotected address space, a basic principle\nof unikernels that provide their lightweightness and perfor-\nmance benefits. In this paper, we propose a new design that\nbrings memory isolation inside a unikernel instance while\nkeeping a single address space. We leverage Intel’s Memory\nProtection Key to do so without impacting the lightweight-\nness and performance benefits of unikernels. We implement\nour isolation scheme within an existing unikernel written\nin Rust and use it to provide isolation between trusted and\nuntrusted components: we isolate (1) safe kernel code from\nunsafe kernel code and (2) kernel code from user code. Eval-\nuation shows that our system provides such isolation with\nvery low performance overhead.",
            "Intra-Unikernel Isolation with Intel Memory Protection Keys",
            "2.4\nIntel Memory Protection Keys (MPK)\nIntel Memory Protection Keys is a new hardware feature pro-\nviding per-thread permission control over groups of pages",
            "We implement a prototype on top of RustyHermit to demon-\nstrate our techniques. We can leverage Rust’s features such\nas Rust Macros [38] to provide developers with a convenient\nway to use our isolation mechanism on the existing kernel\nsource code.",
            "Protection Keys and MPK Permission",
            "ory requires two\nMPK protection keys. The protection key of 1 is used for\nthe safe kernel memory region permission, while 2 is used\nfor the unsafe kernel memory regions.",
            "A thread running with a PKRU value of\n0x00 is the most trusted entity at that point.",
            "However, when\nthe thread executes an unsafe kernel code block, its PKRU\nis set to contain 0xC (0b0000_1100). This PKRU value pro-\nhibits the thread from accessing the group of pages of pkey 1,\nwhich corresponds to the safe memory regions.",
            "In the same\nway, 0x3C (0b0011_1100) in the PKRU register prevents the\nthread from accessing both safe (pkey 1) and unsafe (pkey\n2) kernel memory regions, providing the isolation of kernel\nfrom user memory.",
            "The lack of intra-unikernel isolation is a serious security\nconcern. We designed an isolation scheme for components\nwithin a unikernel instance. Relying on the Intel MPK tech-\nnology allows us to keep the single address space feature\nof unikernels and thus maintain their performance benefit"
          ]
        },
        {
          "title": "[PDF] Memory Protection Keys: Facts, Key Extension Perspectives, and ...",
          "url": "https://taesoo.kim/pubs/2023/park:mpkfacts.pdf",
          "excerpts": [
            "MPK's domain-wise memory protection naturally enables in-process memory isolation by separating an application's code and/or data into multiple domains and ..."
          ]
        },
        {
          "title": "Virtio devices and drivers overview: Who is who",
          "url": "https://www.redhat.com/en/blog/virtio-devices-and-drivers-overview-headjack-and-phone",
          "excerpts": [
            "Jun 11, 2020 — This section provides a brief overview of the virtio devices, virtio drivers, examples of the different architectures you can use and the different components."
          ]
        },
        {
          "title": "How to use \"--gc-sections\" link option to reduce target binary size?",
          "url": "https://users.rust-lang.org/t/how-to-use-gc-sections-link-option-to-reduce-target-binary-size/71374",
          "excerpts": [
            "\"-Clink-arg=-Wl,--gc-sections\". in config.toml [build] rustflags ... Rust equivalent to __attribute__((section(NAME), used))?. 2, 1108 ..."
          ]
        },
        {
          "title": "Rust staticlibs and optimizing for size",
          "url": "https://internals.rust-lang.org/t/rust-staticlibs-and-optimizing-for-size/5746",
          "excerpts": [
            "Aug 10, 2017 — Gecko has opened a bug to disable LTO for builds by default but have pointed out that this comes at a great cost to binary size!"
          ]
        },
        {
          "title": "[PDF] The Case for Intra-Unikernel Isolation",
          "url": "https://www.ssrg.ece.vt.edu/papers/spma20.pdf",
          "excerpts": [
            "In the rest of this paper, we make the case for intra-unikernel isolation in Section 2. Next, we discuss the use of Intel MPK as well as hardware capabilities ..."
          ]
        },
        {
          "title": "[PDF] Software Abstraction for Intel Memory Protection Keys (Intel MPK)",
          "url": "https://www.usenix.org/system/files/atc19-park-soyeon.pdf",
          "excerpts": [
            "libmpk consists of a userspace library mainly for efficient permission change and a kernel module mainly for synchronization and metadata."
          ]
        },
        {
          "title": "libhermitMPK | Intra-Unikernel Isolation with Intel Memory Protection ...",
          "url": "https://ssrg-vt.github.io/libhermitMPK/",
          "excerpts": [
            "libhermitMPK is a new design bringing memory isolation inside a unikernel instance while keeping a *single address space*. By leveraging Intel Memory Protection Keys ([MPK](https://lwn.net/Articles/643797)), intra-unikernel isolation can be provided without impacting the lightweightness and performance benefits of unikernels. libhermitMPK provides isolation between trusted and untrusted components: (1) safe from unsafe [Rust](https://www.rust-lang.org) kernel code and (2) kernel from user code."
          ]
        },
        {
          "title": "Memory Protection Unit",
          "url": "https://developer.arm.com/documentation/107565/latest/Memory-protection/Memory-Protection-Unit",
          "excerpts": [
            "The MPU programmers' model allows privileged software to define memory regions and assign memory access permissions and memory attributes to each of them."
          ]
        },
        {
          "title": "Process isolation with Arm FuSa runtime system",
          "url": "https://community.arm.com/arm-community-blogs/b/tools-software-ides-blog/posts/process-isolation-with-fusa-rts",
          "excerpts": [
            "Feb 18, 2021 — It is used to implement spatial isolation between application processes. In FuSa RTX, an RTOS thread gets assigned to an MPU protected zone ..."
          ]
        },
        {
          "title": "VirtusCap: Capability-Based Access Control for Unikernels",
          "url": "https://www.researchgate.net/publication/312379553_VirtusCap_Capability-Based_Access_Control_for_Unikernels",
          "excerpts": [
            "VirtusCap enables multi-layer MAC with capabilities for\n\nunikernels",
            "VirtusCap limits privileges of unikernels using the Principle of Least Privilege to create unikernels that have only the privileges they need to accomplish their task.",
            "VirtusCap\n\nis an access-control system integrated across the virtualization\n\nlayers. At the hypervisor level, we integrate a new module\n\nfor access-control in Xen Security Modules (XSM) [25] that\n\nuses a capability-based access-control m",
            "our approach embodies the Principle of Least Privilege (POLP) to create unikernels that have only the privileges they need to accomplish their task.",
            "his paper makes the following contributions:\n\n(a) We present the design of a novel architecture for\n\ncapability-based access-control with unikernels in virtu-\n\nalized hosts. (b) We design and implement a new prototype capability-\n\nbased access-control system for the XSM. (c) We design and implement a library for MirageOS that\n\nenables capability-based access-control for unik"
          ]
        },
        {
          "title": "Unishyper: A Rust-based unikernel enhancing reliability and efficiency of embedded systems",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S138376212400136X",
          "excerpts": [
            "To support memory isolation between user applications, user code, and kernel code, Unishyper designs the Zone mechanism on top of Intel MPK. Unishyper ...",
            "To provide memory isolation support between user and kernel code as well as isolation between different user applications at the thread level, Unishyper designs the Zone mechanism based on Intel MPK [36] to isolate the code into thread gr",
            "Most unikernels typically adopt SAS systems, meaning all tasks, threads, and processes share the same memory addresses, which is a lack of isolation mechanisms between user code and kernel code, as well as isolation between different user applications.",
            "he Zone mechanism on top of Intel MPK to support memory isolation between user applications as well as the isolation between user code and ke"
          ]
        },
        {
          "title": "Unishyper: : A Rust-based unikernel enhancing reliability and ...",
          "url": "https://dl.acm.org/doi/10.1016/j.sysarc.2024.103199",
          "excerpts": [
            "To support memory isolation between user applications, user code, and kernel code, Unishyper designs the Zone mechanism on top of Intel MPK."
          ]
        },
        {
          "title": "[TeX] virtio-iommu.tex",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/tex/virtio-iommu.tex",
          "excerpts": [
            "The virtio-iommu device manages Direct Memory Access (DMA) from one or more endpoints. It may act both as a proxy for physical IOMMUs managing devices assigned ..."
          ]
        },
        {
          "title": "Intel® Trust Domain Extension Linux Guest Kernel Security ...",
          "url": "https://intel.github.io/ccc-linux-guest-hardening-docs/security-spec.html",
          "excerpts": [
            "May 27, 2025 — VirtIO drivers are built around the virtio ring. The ring contains descriptors, which are organized in a free list. The free list handling has ..."
          ]
        },
        {
          "title": "Memory Protection Unit (MPU)",
          "url": "https://documentation-service.arm.com/static/5ef61f08dbdee951c1ccdd48",
          "excerpts": [
            "Jul 8, 2016 — The MPU is configured by a series of memory mapped registers in the System Control Space. (SCS). This chapter lists the MPU registers, and ..."
          ]
        },
        {
          "title": "Using the RISC-V PMP with an Embedded RTOS to ...",
          "url": "https://www.youtube.com/watch?v=upkZZldpljA",
          "excerpts": [
            "Using the RISC-V PMP with an Embedded RTOS to Achieve Process Separation and Isolation. 2.9K views · 6 years ago ...more. RISC-V International."
          ]
        },
        {
          "title": "Enable Stack Protection for Shared Library",
          "url": "https://users.rust-lang.org/t/enable-stack-protection-for-shared-library/119533",
          "excerpts": [
            "I am trying to build a shared library. I want it to have a stack canary. I have installed rustc ... rustflags = [\"-Z\", \"stack-protector=strong\"]."
          ]
        },
        {
          "title": "Codegen Options - The rustc book",
          "url": "https://doc.rust-lang.org/rustc/codegen-options/index.html",
          "excerpts": [
            "When -C lto is not specified, LTO is disabled if codegen units is 1 or optimizations are disabled ( -C opt-level=0 ). That is: When -C lto is not specified:."
          ]
        },
        {
          "title": "Enforcing no-std and no-panic during build - Rust Internals",
          "url": "https://internals.rust-lang.org/t/enforcing-no-std-and-no-panic-during-build/14505",
          "excerpts": [
            "When compiling for no_std and in release mode, you can enforce that panic is never called with https://crates.io/crates/no-panics-whatsoever."
          ]
        },
        {
          "title": "StackProtector in rustc_target::spec",
          "url": "https://doc.rust-lang.org/beta/nightly-rustc/rustc_target/spec/enum.StackProtector.html",
          "excerpts": [
            "This triggers stack canary generation in functions which contain an array of a byte-sized type with more than eight elements."
          ]
        },
        {
          "title": "Follow up on stack protector research · Issue #3201 - GitHub",
          "url": "https://github.com/mobilecoinfoundation/mobilecoin/issues/3201",
          "excerpts": [
            "I'm trying to understand if there is a way to get stack protectors like this in rust. There is an old github issue that mentions this: rust-lang ..."
          ]
        },
        {
          "title": "Unishyper: A Rust-based unikernel enhancing reliability and ...",
          "url": "https://www.researchgate.net/publication/381383329_Unishyper_A_Rust-based_unikernel_enhancing_reliability_and_efficiency_of_embedded_systems",
          "excerpts": [
            "Iso-UniK leverages a recent hardware feature, named Intel Memory Protection Key (Intel MPK), to provide lightweight and efficient isolation ..."
          ]
        },
        {
          "title": "Creating a secure RTOS on Risc-V : r/embedded",
          "url": "https://www.reddit.com/r/embedded/comments/1ccy757/creating_a_secure_rtos_on_riscv/",
          "excerpts": [
            "I am building a simple secure RTOS with simple scheduler that run on Risc V , how can I make use of PMP to achieve isolation between tasks ,"
          ]
        },
        {
          "title": "Rust RFCS / unikernel security discussion",
          "url": "https://github.com/rust-lang/rfcs/issues/1005",
          "excerpts": [
            "Missing: unikernel hardening Previous thread: rust-lang/rust Now that the runtime is all but gone, no_std's purpose is a bit unclear. It's behind a feature gate, so figuring out exactly what it is and should do is part of the process to get it to be stable. Stabilize #!",
            "The Reenix paper: <http://scialex.github.io/reenix.pdf>\n\nhas some good insights, specifically on using the standard library in an osdev contex"
          ]
        },
        {
          "title": "libhermitMPK: Intra-Unikernel Isolation with Intel Memory Protection Keys",
          "url": "https://github.com/ssrg-vt/libhermitMPK",
          "excerpts": [
            "libhermitMPK: Intra-Unikernel Isolation with Intel Memory Protection Keys ... This work was implemented on the top of RustyHermit (https://github.com/hermitcore/ ...See more [rust-toolchain](/ssrg-vt/libhermitMPK/blob/master/rust-toolchain \"rust-toolchain\") | | [rust-toolchain](/ssrg-vt/libhermitMPK/blob/master/rust-toolchain \"rust-toolchain\") | [Added rust-toolchain file pointing to nightly. ](/ssrg-vt/libhermitMPK/commit/922862ea7aacdcbcf0cd43f5622ddb63faebc4ea \"Added rust-toolchain file pointing to nightly. This is useful for all of us not living on nightly as default.\") | Jul"
          ]
        },
        {
          "title": "Virtio and Vhost Architecture - Part 2 - Insu Jang",
          "url": "https://insujang.github.io/2021-03-15/virtio-and-vhost-architecture-part-2/",
          "excerpts": [
            "To reduce context switch overheads, vhost has been introduced to move the location of virtqueue from QEMU process to kernel space."
          ]
        },
        {
          "title": "Setting Up a KVM VM Host Server | Virtualization Guide",
          "url": "https://doc.opensuse.org/documentation/leap/archive/15.1/virtualization/html/book.virt/cha-qemu-host.html",
          "excerpts": [
            "... QEMU offers a way of improving the network performance using multiqueue. ... vectors for the virtio-net-pci device: -device virtio-net-pci,mq=on,vectors=2*N+2.",
            "As the number of virtual CPUs increases in VM Guests, QEMU offers a way of improving the network performance using multiqueue. Multiqueue virtio-net scales the ..."
          ]
        },
        {
          "title": "config_fw_cfg_sysfs",
          "url": "https://www.kernelconfig.io/config_fw_cfg_sysfs",
          "excerpts": [
            "Enable the exporting of the QEMU firmware configuration (fw_cfg) file entries via sysfs. Entries are found under /sys/firmware/fw_cfg when this option is ..."
          ]
        },
        {
          "title": "Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg",
          "url": "https://kernel.googlesource.com/pub/scm/linux/kernel/git/stable/linux-stable-rc.git/+/refs/tags/v5.4.234/Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg",
          "excerpts": [
            "configuration data to the guest userspace. The authoritative guest-side hardware interface documentation."
          ]
        },
        {
          "title": "Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg",
          "url": "https://gitlab.kontron.com/imx/linux-imx/-/blob/YOCTO_BSP_PITX_IMX8M_R11/Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg",
          "excerpts": [
            "Each fw_cfg entry of type \"file\" has an associated 56-char, nul-terminated ASCII string which represents its name. While the fw_cfg device doesn't itself ..."
          ]
        },
        {
          "title": "49. Poll Mode Driver for Emulated Virtio NIC - Documentation",
          "url": "https://doc.dpdk.org/guides-20.02/nics/virtio.html",
          "excerpts": [
            "Properly set msix vectors when starting VM: Enable multi-queue when starting VM, and specify msix vectors in qemu cmdline. · In VM, insert vfio module in NOIOMMU ..."
          ]
        },
        {
          "title": "docker container bridged(?) via vHost User Interface socket ...",
          "url": "https://forum.netgate.com/topic/182757/docker-container-bridged-via-vhost-user-interface-socket-on-the-same-host",
          "excerpts": [
            "The vhost-user socket is the control socket for virtio between the VM and VPP, if you are using QEMU, you bind it to a character device and ..."
          ]
        },
        {
          "title": "QEMU/Advanced networking",
          "url": "https://wiki.archlinux.org/title/QEMU/Advanced_networking",
          "excerpts": [
            "The following describes how to bridge a virtual machine to a host interface such as eth0, which is probably the most common configuration."
          ]
        },
        {
          "title": "Qemu image namings - - EVE-NG",
          "url": "https://www.eve-ng.net/index.php/documentation/qemu-image-namings/",
          "excerpts": [
            "Foldername examples: firepower6-FTD-6.2.1. acs-5.8.1.4. Inside of image folder must be placed HDD image with correct name like: hda.qcow2 or virtioa.qcow2."
          ]
        },
        {
          "title": "Disk Images — QEMU documentation",
          "url": "https://qemu-project.gitlab.io/qemu/system/images.html",
          "excerpts": [
            "QEMU supports many disk image formats, including growable disk images (their size increase as non empty sectors are written), compressed and encrypted disk ..."
          ]
        },
        {
          "title": "QEMU disk image utility",
          "url": "https://qemu.readthedocs.io/en/master/tools/qemu-img.html",
          "excerpts": [
            "qemu-img allows you to create, convert and modify images offline. It can handle all image formats supported by QEMU."
          ]
        },
        {
          "title": "QEMU Firmware Configuration (fw_cfg) Device",
          "url": "https://www.qemu.org/docs/master/specs/fw_cfg.html",
          "excerpts": [
            "This hardware interface allows the guest to retrieve various data items\n(blobs) that can influence how the firmware configures itself, or may\ncontain tables to be installed for the guest OS. Examples include device\nboot order, ACPI and SMBIOS tables, virtual machine UUID, SMP and NUMA\ninformation, kernel/initrd images for direct (Linux) kernel booting, etc.",
            "Since v2.4, “file” fw\\_cfg items (i.e., items with selector keys above\n`FW_CFG_FILE_FIRST`, and with a corresponding entry in the fw\\_cfg file\ndirectory structure) may be inserted via the QEMU command line, using\nthe following synta",
            "\n```\n-fw_cfg [name=]<item_name>,file=<path>\n\n```",
            "Or:\n\n```\n-fw_cfg [name=]<item_name>,string=<string>\n\n```",
            "g>\n\n```\n\nSince v5.1, QEMU allows some objects to generate fw\\_cfg-specific content,\nthe content is then associated with a “file” item using the ‘gen\\_id’ option\nin the command line, using the following syntax:\n\n```\n-object <generator-type>,id=<generated_id>,[generator-specific-options] \\\n-fw_cfg [name=]<item_name>,gen_id=<generated_id>\n\n```",
            "All externally provided fw\\_cfg items are read-only to the guest",
            "Guest-side Hardware Interface",
            "Selector (Control) Register",
            "Write only",
            "Location: platform dependent (IOport or MMIO)",
            "Width: 16-bit",
            "Endianness: little-endian (if IOport), or big-endian (if MMIO)"
          ]
        },
        {
          "title": "QEMU_fw_cfg (OSDev Wiki)",
          "url": "https://wiki.osdev.org/QEMU_fw_cfg",
          "excerpts": [
            "To pass strings or files into QEMU to appear in the fw\\_cfg device, use one or more command line parameters like those shown below:",
            "```\n -fw_cfg name=opt/com.name.domain.your.example,string=1\n -fw_cfg name=opt/com.name.domain.your.example,file=example_filename.png\n -fw_cfg opt/com.name.domain.your.example,file=example_filename.png\n\n```",
            "QEMU fw cfg",
            "Names that begin with \"opt/\" are reserved for users. It is strongly recommended to use a RFQDN (Reversed Fully Qualified Domain Name) as a prefix for the resource name.",
            "In the example above, it is assumed you are the owner of the domain \"your.domain.name.com\", and your resource is named \"example\".",
            "Following this advice guarantees that there will never be a conflict, since only one entity can own a given domain name.",
            "To detect the presence of QEMU, you should read CPUID leaf 0x40000000 and check ... If you read \"QEMU\" from the data port, the fw_cfg device is present.",
            "The \"name=\" part of the argument may be omitted, as shown in the third example above.",
            "Detecting QEMU",
            "It is a very good idea for your OS to first make sure it is running under QEMU, before attempting to access the fw\\_cfg device.",
            "Accessing I/O ports for devices that don't exist on a real machine may have undefined behavior.",
            "QEMU provides three I/O ports on x86, or three MMIO addresses on other architectures",
            "*FW\\_CFG\\_PORT\\_SEL* is a 16-bit port at I/O address 0x510. You write a \"selector\" to this port to control which data will be read when reading *FW\\_CFG\\_PORT\\_DATA",
            "*FW\\_CFG\\_PORT\\_DATA* is an 8-bit port at I/O address 0x511. You read one byte at a time of the file from this port, after selecting a file by writing a value to *FW\\_CFG\\_PORT\\_SEL",
            "*FW\\_CFG\\_PORT\\_DMA* is a 32-bit port at I/O address 0x514. DMA is covered later in this article",
            "### Selectors",
            "```\n  #define FW_CFG_SIGNATURE    0x0000\n  #define FW_CFG_ID           0x0001\n  #define FW_CFG_FILE_DIR     0x0019\n\n```",
            "Listed above are a few fixed-purpose selectors for probing for the existence of the device, and for getting the number of files present in the device.",
            "### Enumerating Files",
            "The first four bytes of the *FW\\_CFG\\_FILE\\_DIR* selector will be a 32-bit big-endian number which is the number of files present in the directo",
            "Following the count, there is a sequence of entries with the following structure:",
            "th the following structure:\n\n```\n  struct FWCfgFile {\t\t/* an individual file entry, 64 bytes total */\n      uint32_t size;\t\t/* size of referenced fw_cfg item, big-endian */\n      uint16_t select;\t\t/* selector key of fw_cfg item, big-endian */\n      uint16_t reserved;\n      char name[56];\t\t/* fw_cfg item name, NUL-terminated ascii */\n  };",
            "These can be read one at a time by reading *sizeof(FWCfgFile)* bytes into a single variable of type *FWCfgFile* and examined one by one until you find a structure with the desired name.",
            "### Reading files",
            "Once you have found the *FWCfgFile* structure which has the desired name, read the *select* field from the structure. It will contain the big-endian selector for the file. After byte-swapping the selector, output it to *FW\\_CFG\\_PORT\\_SEL*, then read *size* bytes from *FW\\_CFG\\_PORT\\_DATA* and write them to the desired location in memory.",
            "An example implementation is provided in the references below."
          ]
        },
        {
          "title": "QEMU fw_cfg - Somlo CMU (Contrib.cm.u.edu)",
          "url": "http://www.contrib.andrew.cmu.edu/~somlo/QEMU_fw_cfg/",
          "excerpts": [
            "The fw\\_cfg device allows QEMU to prepare various data items (or \"blobs\") in host memory, and offers a guest-side interface allowing the guest to read (using port or memory-mapped I/O operations) the blobs after having been launched by the host"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.0 - Index of /",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.0/cs03/virtio-v1.0-cs03.html",
          "excerpts": [
            "Queue Size corresponds to the maximum number of buffers in the virtqueue. Queue Size value is always a power of 2. The maximum Queue Size value is 32768. This ...See more"
          ]
        },
        {
          "title": "libvirt: virtio-net multiqueue — Nova Specs documentation",
          "url": "https://specs.openstack.org/openstack/nova-specs/specs/liberty/implemented/libvirt-virtiomq.html",
          "excerpts": [
            "Sep 4, 2015 — Guest OS is limited to ~200 MSI vectors. Each NIC queue requires a MSI vector, as well as any virtio device or assigned PCI device."
          ]
        },
        {
          "title": "A journey to the vhost-users realm",
          "url": "https://www.redhat.com/en/blog/journey-vhost-users-realm",
          "excerpts": [
            "Sep 24, 2019 — This post is a technical deep dive into the vhost-user/virtio-pmd architecture targeting high performance userland networking using DPDK.",
            "Sep 24, 2019 — The main difference between the vhost-user library and the vhost-net kernel driver is the communication channel. While the vhost-net kernel ..."
          ]
        },
        {
          "title": "Vhost-user Protocol — QEMU 10.0.93 documentation - Read the Docs",
          "url": "https://qemu.readthedocs.io/en/master/interop/vhost-user.html",
          "excerpts": [
            "The front-end enables queues by sending message VHOST_USER_SET_VRING_ENABLE . vhost-user-net has historically automatically enabled the first queue pair."
          ]
        },
        {
          "title": "23. Vhost Sample Application - Documentation - DPDK",
          "url": "https://doc.dpdk.org/guides-1.8/sample_app_ug/vhost.html",
          "excerpts": [
            "The Linux* Kernel vhost-net module was developed as an offload mechanism for virtio-net. The vhost-net module enables KVM (QEMU) to offload the servicing of ..."
          ]
        },
        {
          "title": "View topic - [SOLVED] KVM / QEMU - Bridge interface",
          "url": "https://forums.gentoo.org/viewtopic-t-1144185-start-0.html",
          "excerpts": [
            "Oct 23, 2021 — With the bridge, the interface br0 must be configured with your IP address. So yes, it is to be expected that you lose the connection if you add ..."
          ]
        },
        {
          "title": "[Solved] Network Bridging Issues with Qemu/KVM VMs and ...",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=216015",
          "excerpts": [
            "Aug 17, 2016 — I am trying to bridge a Qemu/kvm VM guest so that it has an IP address and can work with the rest of my 192.168.1.x network."
          ]
        },
        {
          "title": "OASIS Virtio Specification Repository",
          "url": "https://github.com/oasis-tcs/virtio-spec",
          "excerpts": [
            "The purpose of virtio and this specification is that virtual environments and guests should have a straightforward, efficient, standard and extensible mechanism for virtual devices, rather than boutique per-environment or per-OS mechanisms.",
            "This repository includes the [authoritative source](https://github.com/oasis-tcs/virtio-spec/releases) of the VIRTIO (Virtual I/O) Specification document"
          ]
        },
        {
          "title": "eBPF RSS virtio-net support",
          "url": "https://www.qemu.org/docs/master/devel/ebpf_rss.html",
          "excerpts": [
            "RSS(Receive Side Scaling) is used to distribute network packets to guest virtqueues by calculating packet hash. ... tap,vhost=on & virtio-net-pci,rss=on,hash=off."
          ]
        },
        {
          "title": "Add support for vhost-user-net · Issue #8 · cloud-hypervisor ...",
          "url": "https://github.com/cloud-hypervisor/cloud-hypervisor/issues/8",
          "excerpts": [
            "May 14, 2019 — In order to let users of cloud-hypervisor plug their own network backend, the VMM should support vhost-user-net daemons through a global ..."
          ]
        },
        {
          "title": "Network",
          "url": "https://crosvm.dev/book/devices/net.html",
          "excerpts": [
            "To network a crosvm, set up a TAP interface on the host, configure routing, and start crosvm with the tap-name. Guest network is visible if VIRTIO_NET is ..."
          ]
        },
        {
          "title": "Command line options and configuration files",
          "url": "https://crosvm.dev/book/running_crosvm/options.html",
          "excerpts": [
            "Crosvm can be configured via command-line options (key-value pairs) or JSON configuration files using the --cfg argument. Command-line options take precedence."
          ]
        },
        {
          "title": "Way to pass parameters or share a directory/file to a qemu ...",
          "url": "https://stackoverflow.com/questions/56535142/way-to-pass-parameters-or-share-a-directory-file-to-a-qemu-kvm-launched-vm-on-ce",
          "excerpts": [
            "If that guest is running a Linux kernel with CONFIG_FW_CFG_SYSFS enabled, you will be able to read out the information from sysfs. An example:."
          ]
        },
        {
          "title": "Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg",
          "url": "https://git.doit.wisc.edu/SWIFT/linux-ldos/-/blob/master/Documentation/ABI/testing/sysfs-firmware-qemu_fw_cfg?ref_type=heads",
          "excerpts": [
            "A few weeks ago, QEMU switched docs/specs/fw_cfg.txt to be docs/specs/fw_cfg.rst, so update the reference in the kernel docs to reflect this."
          ]
        },
        {
          "title": "crosvm - Architecture",
          "url": "https://chromium.googlesource.com/crosvm/crosvm/+/refs/heads/factory-brya-14517.B-chromeos/ARCHITECTURE.md",
          "excerpts": [
            "A typical session of crosvm starts in main.rs where command line parsing is done to build up a Config structure. The Config is used by run_config in linux/mod."
          ]
        },
        {
          "title": "3. Vhost Library - Documentation",
          "url": "https://doc.dpdk.org/guides/prog_guide/vhost_lib.html",
          "excerpts": [
            "When DPDK vhost-user acts as the client it will keep trying to reconnect to the server (QEMU) until it succeeds. This is useful in two cases: When QEMU is not ..."
          ]
        },
        {
          "title": "46. Vhost Library - Documentation",
          "url": "https://doc.dpdk.org/guides-19.11/prog_guide/vhost_lib.html",
          "excerpts": [
            "When DPDK vhost-user acts as the client it will keep trying to reconnect to the server (QEMU) until it succeeds. This is useful in two cases: When QEMU is not ..."
          ]
        },
        {
          "title": "How to use virtio-fs — Cloud Hypervisor documentation",
          "url": "https://intelkevinputnam.github.io/cloud-hypervisor-docs-HTML/docs/fs.html",
          "excerpts": [
            "virtio-fs, also known as vhost-user-fs is a virtual device defined by the VIRTIO specification which allows any VMM to perform filesystem sharing."
          ]
        },
        {
          "title": "How to create a multiqueue TAP device in Linux",
          "url": "https://stackoverflow.com/questions/72268735/how-to-create-a-multiqueue-tap-device-in-linux",
          "excerpts": [
            "The script creates a tap device tap0 with 8 queues, adds tap0 to bridge br0, and sets tap0 up.",
            "You can use ethtool -l [ifname] in your VM. NETWORK=\" -netdev tap,ifname=${TAP:-tap0},id=n1,script=no,downscript=no,queues=4 -device virtio-net-pci,mac=AA:BB:CC:DD:CA:B1,netdev=n1,mq=on,vectors=10\"",
            "Important options are\nvirtio-net-pci and\nmq=on,vectors=10 . Information about the vectors parameter https://www.linux-kvm.org/page/Multiqueue",
            "The docs say that for\nqueues=N the value for\nvectors should be \"2N+2\". Therefore, in your example with\nqueues=4 we should use\nvectors=10 ."
          ]
        },
        {
          "title": "Harold/Harry Hodge — Getting started with Firecracker (Firecracker networking and config)",
          "url": "https://harryhodge.co.uk/posts/2024/01/getting-started-with-firecracker/",
          "excerpts": [
            "\"subnet\": \"192.168.30.0/24\"",
            "\"rangeStart\": \"192.168.30.32\"",
            "\"boot_args\": \"... ip=192.168.30.39::192.168.30.1:255.255.255.0::eth0:off\"",
            ". CNI operates with network namespaces so we create unique namespaces for each VM interface.",
            "1{",
            "\"name\": \"firecracker\",",
            "\"cniVersion\": \"1.0.0\",",
            "\"plugins\": [",
            " \"type\": \"bridge\",",
            "\"name\": \"firecracker-bridge\",",
            "\"bridge\": \"fcbr0\",",
            "\"isGateway\": true,",
            "\"ipMasq\": true,",
            "\"ipam\": {",
            "\"type\": \"host-local\",",
            "\"resolvConf\": \"/etc/resolv.conf\",",
            "\"dataDir\": \"/srv/vm/networks\",",
            "\"subnet\": \"192.168.30.0/24\",",
            "\"rangeStart\": \"192.168.30.32\",",
            "\"gateway\": \"192.168.30.1\"",
            "\"gateway\": \"192.168.30.1\"",
            "},",
            "  \"type\": \"firewall\"",
            ",",
            "  \"type\": \"tc-redirect-tap\"",
            "]",
            "}",
            "}"
          ]
        },
        {
          "title": "How do I use linked-list-allocator in Rust?",
          "url": "https://stackoverflow.com/questions/78645054/how-do-i-use-linked-list-allocator-in-rust",
          "excerpts": [
            "}"
          ]
        },
        {
          "title": "Crate x86_64 - Rust",
          "url": "https://docs.rs/x86_64",
          "excerpts": [
            "This crate provides x86_64 specific functions and data structures, and access to various system registers."
          ]
        },
        {
          "title": "Page Tables | Writing an OS in Rust (First Edition)",
          "url": "https://os.phil-opp.com/page-tables/",
          "excerpts": [
            "Dec 9, 2015 — In this post we will create a paging module, which allows us to access and modify the 4-level page table. We will explore recursive page table mapping.See more"
          ]
        },
        {
          "title": "Introduction to Paging | Writing an OS in Rust",
          "url": "https://os.phil-opp.com/paging-introduction/",
          "excerpts": [
            "Jan 14, 2019 — A 4-level page table makes the translation of virtual addresses expensive because each translation requires four memory accesses. To improve ...See more"
          ]
        },
        {
          "title": "Virtio 1.2 is Coming! - Alibaba Cloud Community",
          "url": "https://www.alibabacloud.com/blog/virtio-1-2-is-coming_599615",
          "excerpts": [
            "Virto spec defines a set of virtqueue mechanisms to enable communication between guest and host. This mechanism can be applied to various ..."
          ]
        },
        {
          "title": "Virt Queue Types - NVIDIA Docs",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv190/Virt+Queue+Types",
          "excerpts": [
            "Descriptor table – occupies the descriptor area. Available ring – occupies the driver area. Used ring – occupies the device area. Each of these ..."
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 | StandICT.eu 2026",
          "url": "https://standict.eu/standards-repository/standard/virtual-io-device-virtio-version-11",
          "excerpts": [
            "This standard describes the specifications of the “virtio” family of devices. These devices are found in virtual environments, yet by design they look like ..."
          ]
        },
        {
          "title": "Linux Kernel vs DPDK: HTTP Performance Showdown",
          "url": "https://talawah.io/blog/linux-kernel-vs-dpdk-http-performance-showdown/",
          "excerpts": [
            "Jul 4, 2022 — An entire network interface is dedicated to a single application, and aggressive busy polling is used to achieve high throughput and low latency ..."
          ]
        },
        {
          "title": "OSDev/Phil-opp IDT & paging",
          "url": "https://os.phil-opp.com/catching-exceptions/",
          "excerpts": [
            "Jun 25, 2016 — Thus, the hardware can automatically load the corresponding IDT entry for each exception. The Exception Table in the OSDev wiki shows the IDT ...See more For example, the caller could add a non-present entry by executing: `idt.set_handler(11, handler_fn).set_present(false)`. ",
            ")Loading the IDT\n\nNow we’re able to create new interrupt descriptor tables with registered handler functions. We just need a way to load an IDT, so that the CPU uses it. The x86 architecture uses a special register to store the active IDT and its length. In order to load a new IDT we need to update this register through the [lidt](https://www.felixcloutier.com/x86/lgdt:lidt) instruction. The `lidt` instruction expects a pointer to a special data structure, which specifies the start address of the IDT and its length:\n\n| Type | Name | Description |\n| --- | --- | --- |\n| u16 | Limit | The maximum addressable byte in the table. Equal to the table size in bytes minus 1. |\n| u64 | Offset | Virtual start address of the table. |\n\nThis structure is already contained [in the x86\\_64 crate](https://docs.rs/x86_64/0.1.0/x86_64/instructions/tables/struct.DescriptorTablePointer.html), so we don’t need to create it ourselves. The same is true for the [lidt function](https://docs.rs/x86_64/0.1.0/x86_64/instructions/tables/fn.lidt.html)."
          ]
        },
        {
          "title": "rust-osdev/x86_64 - Tables and IDT/GDT setup (excerpt from tables.rs)",
          "url": "https://github.com/rust-osdev/x86_64/blob/master/src/instructions/tables.rs",
          "excerpts": [
            "/// Load an IDT. ///",
            "pub unsafe fn lidt(idt: &DescriptorTablePointer) {",
            "/// Get the address of the current GDT. #[inline]\npub fn sgdt() -> DescriptorTablePointer {",
            "pub unsafe fn lgdt(gdt: &DescriptorTablePointer) {"
          ]
        },
        {
          "title": "Virt Queue Types - NVIDIA Docs",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2407/Virt+Queue+Types",
          "excerpts": [
            "Aug 14, 2024 — Split VQ format is the only format supported by version 1.0 of the virtio spec. In split VQ mode, each VQ is separated into three parts:."
          ]
        },
        {
          "title": "Low-Latency GPU Packet Processing",
          "url": "https://eunomia.dev/others/cuda-tutorial/13-low-latency-gpu-packet-processing/",
          "excerpts": [
            "Packet Capture: Receive packets from the network interface · Batching: Collect multiple packets to amortize transfer and launch costs · Transfer ..."
          ]
        },
        {
          "title": "Virtio, vhost, vhost-user, vsock, etc. | Lencerf's Walk",
          "url": "https://lencerf.github.io/post/2023-05-28-virtio-vhost-vsock/",
          "excerpts": [
            "May 28, 2023 — On the other hand, vhost-user off-loads data processing to another user space program, which is called a backend device. The VMM and the device ..."
          ]
        },
        {
          "title": "[PDF] What's coming in VIRTIO 1.2 - Stefan Hajnoczi",
          "url": "https://vmsplice.net/~stefan/stefanha-fosdem-2022.pdf",
          "excerpts": [
            "Sound card for multichannel PCM audio playback and capture with up to 384 kHz sample rate and. 64-bit sample depth. Reuses parts of High Definition Audio (HDA) ..."
          ]
        },
        {
          "title": "Ultra Low Latency: The Ultimate Guide to Lightning-Fast ...",
          "url": "https://www.videosdk.live/developer-hub/webtransport/ultra-low-latency",
          "excerpts": [
            "In 2025, an ultra low latency trading system aims for round-trip times well below 10 microseconds, targeting single-digit microseconds or even nanoseconds for ..."
          ]
        },
        {
          "title": "Virtio-net Feature Bits - NVIDIA Docs",
          "url": "https://docs.nvidia.com/networking/display/bluefieldvirtionetv2410/Virtio-net+Feature+Bits",
          "excerpts": [
            "For more information on these bits, refer to the VIRTIO Version 1.2 Specifications. © Copyright 2024, NVIDIA. Last updated on Nov 12, 2024."
          ]
        },
        {
          "title": "8. AVP Poll Mode Driver - Documentation",
          "url": "https://doc.dpdk.org/guides/nics/avp.html",
          "excerpts": [
            "This provides DPDK applications running in virtual machines with significantly improved throughput and latency over other device types. The AVP device ..."
          ]
        },
        {
          "title": "bootloader - crates.io: Rust Package Registry",
          "url": "https://crates.io/crates/bootloader/range/%5E0.9.30",
          "excerpts": [
            "map_physical_memory : Maps the complete physical memory in the virtual address space and passes a physical_memory_offset field in the BootInfo ."
          ]
        },
        {
          "title": "OS Experiment in Rust (part 1): Creating a UEFI Loader",
          "url": "https://blog.malware.re/2023/08/20/rust-os-part1/index.html",
          "excerpts": [
            "Aug 20, 2023 — x86-64 UEFI Boot Process. Typically, the UEFI firmware in an x86-64 system will boot up, initialize the hardware, and then search the boot ..."
          ]
        },
        {
          "title": "Features/MQ virtio net - Fedora Project Wiki",
          "url": "https://fedoraproject.org/wiki/Features/MQ_virtio_net",
          "excerpts": [
            "Mar 15, 2013 — Multiqueue virtio-net provides an approach that scales the network performance as the increasing of the number of vcpus by allowing them to transfer packets ..."
          ]
        },
        {
          "title": "Building a Linux Kernel Driver using Rust",
          "url": "https://rust-exercises.ferrous-systems.com/latest/book/building-linux-kernel-driver",
          "excerpts": [
            "Building a Linux Kernel Driver using Rust. In this example we're going to build a very basic Linux Kernel driver in Rust, compile a Linux ..."
          ]
        },
        {
          "title": "Towards Practical Formal Verification for a General-Purpose ...",
          "url": "https://asterinas.github.io/2025/02/13/towards-practical-formal-verification-for-a-general-purpose-os-in-rust.html",
          "excerpts": [
            "Feb 13, 2025 — For Asterinas, the TCB is the OSTD, a (unofficial) Rust standard library for OS development. OSTD offers a set of safe abstractions enabling the ..."
          ]
        },
        {
          "title": "understanding priority arrays in linux kernel - Stack Overflow",
          "url": "https://stackoverflow.com/questions/14201296/understanding-priority-arrays-in-linux-kernel",
          "excerpts": [
            "The priority arrays also contain a priority bitmap used to efficiently discover the highest priority runnable task in the system."
          ]
        },
        {
          "title": "Implementing Real-Time Operating Systems",
          "url": "https://www.geeksforgeeks.org/operating-systems/implementing-real-time-operating-systems/",
          "excerpts": [
            "Jul 23, 2025 — A real-time operating system (RTOS) is a type of operating system that is designed to provide deterministic, timely processing of real-time events."
          ]
        },
        {
          "title": "Writing a kernel driver with Rust",
          "url": "https://not-matthias.github.io/kernel-driver-with-rust/",
          "excerpts": [
            "Writing a kernel driver with Rust",
            "#! [no_std",
            "#[panic_handler]",
            "fn panic ( _info : & PanicInfo ) -> ! {",
            "{\n    loop"
          ]
        }
      ],
      "reasoning": "The finegrained field virtio_network_datapath_model centers on a low-latency, deterministic VirtIO-based datapath with pure polling behavior, a PMD-like host-side approach, and a 1:1 queue-to-core mapping, typically realized via virtio-net MQ with a vhost-user backend. The excerpts most directly support this model by confirming key elements:\n- VirtIO network devices and their MQ-capable configurations, including virtio-net and the involvement of vhost backends (vhost-user) to bypass kernel datapath overhead and enable user-space/nested backends for high performance.\n- The architecture and behavior of virtio queues (split/packed), the descriptor rings (Descriptor Table, Available Ring, Used Ring), and features like VIRTQ_DESC_F_INDIRECT, VIRTQ_F_EVENT_IDX, and notification semantics that influence whether to poll or interrupt. These are foundational to a pure polling datapath where the guest driver avoids interrupts and relies on continuous polling of the queue state.\n- Practical guidance on topology and pinning (MQ virtio-net, per-CPU queues, and queue-to-core binding) to achieve high throughput with low tail latency, which underpins the proposed one-queue-per-core design. Several excerpts discuss multi-queue strategies and the performance benefits of MQ in virtualized networks, including the benefits of isolating work to dedicated cores and the use of vhost-user as the host-side backend to reduce context switching and kernel involvement.\n- The host-side backends and architecture (vhost-user, vhost-net, and related MQ discussions) illustrate practical pathways to implement the host-side datapath that the finegrained value envisions, including the potential for zero-copy paths and reduced interrupt handling via polling. These elements align with the proposed model where the host backend handles I/O efficiently and the guest uses a polling strategy to minimize latency.\n- The cited sources also outline how to configure and optimize virtio queues, including queue depth, multi-queue configurations, and the interaction between guest and host components, which are essential for achieving sub-10 microsecond P99.99 latency in a real-time setting.\n\nCollectively, these excerpts validate the feasibility and design patterns for a VirtIO-based, polling-driven datapath with 1:1 queue-to-core binding and an efficient host backend (vhost-user) that minimize latency variance and eliminate interrupt-driven delays in the datapath described by the finegrained field value.",
      "confidence": "high"
    },
    {
      "field": "virtio_network_datapath_model.host_backend_strategy",
      "citations": [
        {
          "title": "Features/VirtioVhostUser",
          "url": "https://wiki.qemu.org/Features/VirtioVhostUser",
          "excerpts": [
            "tdev:\n\n```\n$ qemu -M accel=kvm -cpu host -m 1G \\\n       -object memory-backend-file,id=mem0,mem-path=/var/tmp/foo,size=1G,share=on \\\n       -numa node,memdev=mem0 \\\n       -drive if=virtio,file=test.img,format=raw \\\n       -chardev socket,id=chardev0,path=vhost-user.sock \\\n       -netdev vhost-user,chardev=chardev0,id=netdev0 \\\n       -device virtio-net-pci,netdev=netdev0\n```\n",
            "VM2 sees a regular virtio-net device. VM2's QEMU uses the existing vhost-user feature as if it were talking to a host userspace vhost-user backend. VM1's QEMU tunnels the vhost-user protocol messages from VM1's QEMU to the new virtio-vhost-user device so that guest software in VM1 can act as the vhost-user backend."
          ]
        },
        {
          "title": "Tuning a real-time kernel",
          "url": "https://ubuntu.com/blog/real-time-kernel-tuning",
          "excerpts": [
            "nohz_full=<list of isolated cpus> reduces the number of scheduling-clock interrupts, improving energy efficiency and reducing OS jitter. When ..."
          ]
        },
        {
          "title": "A journey to the vhost-users realm",
          "url": "https://www.redhat.com/en/blog/journey-vhost-users-realm",
          "excerpts": [
            "Sep 24, 2019 — This post is a technical deep dive into the vhost-user/virtio-pmd architecture targeting high performance userland networking using DPDK.",
            "Sep 24, 2019 — The main difference between the vhost-user library and the vhost-net kernel driver is the communication channel. While the vhost-net kernel ..."
          ]
        },
        {
          "title": "Vhost-user Protocol — QEMU 10.0.93 documentation - Read the Docs",
          "url": "https://qemu.readthedocs.io/en/master/interop/vhost-user.html",
          "excerpts": [
            "The front-end enables queues by sending message VHOST_USER_SET_VRING_ENABLE . vhost-user-net has historically automatically enabled the first queue pair."
          ]
        },
        {
          "title": "23. Vhost Sample Application - Documentation - DPDK",
          "url": "https://doc.dpdk.org/guides-1.8/sample_app_ug/vhost.html",
          "excerpts": [
            "The Linux* Kernel vhost-net module was developed as an offload mechanism for virtio-net. The vhost-net module enables KVM (QEMU) to offload the servicing of ..."
          ]
        },
        {
          "title": "[Solved] Network Bridging Issues with Qemu/KVM VMs and ...",
          "url": "https://bbs.archlinux.org/viewtopic.php?id=216015",
          "excerpts": [
            "Aug 17, 2016 — I am trying to bridge a Qemu/kvm VM guest so that it has an IP address and can work with the rest of my 192.168.1.x network."
          ]
        },
        {
          "title": "Add support for vhost-user-net · Issue #8 · cloud-hypervisor ...",
          "url": "https://github.com/cloud-hypervisor/cloud-hypervisor/issues/8",
          "excerpts": [
            "May 14, 2019 — In order to let users of cloud-hypervisor plug their own network backend, the VMM should support vhost-user-net daemons through a global ..."
          ]
        },
        {
          "title": "UsingVhost",
          "url": "https://www.linux-kvm.org/page/UsingVhost",
          "excerpts": [
            "VhostNet provides better latency ( 10% less than e1000 on my system) and greater throughput (8x the normal virtio, around 7~8 Gigabits/sec here) for network."
          ]
        }
      ],
      "reasoning": "The most directly relevant information comes from excerpts that explicitly discuss vhost-user as the preferred host-side backend and describe how it bypasses the kernel's networking stack to enable high-performance, low-latency paths. One excerpt presents a concrete example showing how vhost-user back ends enable virtio-net with host-side processing, including the notion of a daemon/backend running outside QEMU and the use of vhost-user to attach a device to a user-space backend. This aligns with the claim that vhost-user is the recommended and high-performance host backend for latency-sensitive workloads, and it mentions the kernel-bypass, shared memory via a Unix domain socket, and a high-performance context for packet processing.\n\nAnother excerpt reinforces the same idea by describing vhost-user as a back-end that moves virtio-net processing into a user-space process, highlighting the separation from QEMU and the back-end's role in handling the data plane. This supports the notion of zero-copy and reduced host-kernel involvement, which are central to the proposed benefit profile.\n\nA follow-on excerpt explicitly discusses vhost-user in the context of a broader vhost-user back-end ecosystem, including back-end options and the protocol, further substantiating that vhost-user is the preferred high-performance mechanism for virtio networking in virtualized environments.\n\nAdditional excerpts illustrate practical implications and usage, such as vhost-user back ends enabling shared memory and inter-process communication patterns that underpin zero-copy data paths, which is precisely what the field value claims as the performance-enabling mechanism.\n\nCollectively, these excerpts directly support the core claims: (1) vhost-user is the recommended host-side backend for high-performance, low-latency virtio-net paths; (2) it enables kernel-bypass and zero-copy data flows by sharing memory regions over a Unix domain socket to a host user-space process; (3) this arrangement is suitable for latency-sensitive workloads and can be integrated with DPDK-based apps or virtual switches.",
      "confidence": "high"
    },
    {
      "field": "virtio_network_datapath_model.io_model",
      "citations": [
        {
          "title": "Linux Network Performance Ultimate Guide | kiennt26's home",
          "url": "https://ntk148v.github.io/posts/linux-network-performance-ultimate-guide/",
          "excerpts": [
            "Tuning a NIC for optimum throughput and latency is a complex process with many factors to consider. There is no generic configuration that can ..."
          ]
        },
        {
          "title": "Linux NAPI and Virtio Documentation",
          "url": "https://docs.kernel.org/networking/napi.html",
          "excerpts": [
            "Threaded NAPI is an operating mode that uses dedicated kernel\nthreads rather than software IRQ context for NAPI processing.",
            "o enable busy polling, there are two choices:\n\n1. `/proc/sys/net/core/busy_poll` can be set with a time in useconds to busy\n   loop waiting for"
          ]
        },
        {
          "title": "15. Poll Mode Driver for Emulated Virtio NIC - Documentation",
          "url": "https://doc.dpdk.org/guides-16.04/nics/virtio.html",
          "excerpts": [
            "In this release, the virtio PMD driver provides the basic functionality of packet reception and transmission."
          ]
        },
        {
          "title": "Vhost-user Protocol — QEMU 10.0.93 documentation - Read the Docs",
          "url": "https://qemu.readthedocs.io/en/master/interop/vhost-user.html",
          "excerpts": [
            "The front-end enables queues by sending message VHOST_USER_SET_VRING_ENABLE . vhost-user-net has historically automatically enabled the first queue pair."
          ]
        }
      ],
      "reasoning": "The target field value advocates a Purist polling posture on the hot datapath to achieve ultra-low latency, while recognizing a hybrid approach as a transitional strategy. The most directly relevant passages describe a Poll Mode Driver (PMD) as a dedicated thread that continually polls virtio queues to minimize latency, which aligns with the field's emphasis on a dedicated polling thread in the hot path. Additional passages discuss NAPI-based polling models as an established approach to reduce interrupt-driven overhead, including the use of busy polling as a technique to trade CPU cycles for lower latency. The combination of PMD-centric polling for the critical path and the surrounding discussion of NAPI-based polling provides a clear evidence base for a pure or quasi-pure polling strategy on the hot datapath, matching the field value's core claim. The cited DP/DPDK material reinforces how virtio networking can be arranged to minimize interrupts by moving data-path handling into a polling-based or hybrid model, including guidance on multi-queue/polling configurations and related latency considerations. Taken together, these excerpts substantiate the idea of a high-determinism IO path that relies on continuous polling (via a PMD) on the hot path, with interrupts used only to seed or transition states rather than dominate the data-path latency.",
      "confidence": "high"
    },
    {
      "field": "virtio_network_datapath_model.queue_topology_and_pinning",
      "citations": [
        {
          "title": "Intel KVM Tuning Guide: virtio-net & vhost-net",
          "url": "https://www.intel.com/content/www/us/en/developer/articles/guide/kvm-tuning-guide-on-xeon-based-systems.html",
          "excerpts": [
            "As the number of vCPUs increases, it is desirable to enable multi-queue virtio-net to boost performance of network packet handling. One vCPU can be dedicated to one virtqueue so that the overall parallelism and throughput can be improved significantly.",
            "Virtio-net is the main virtual ethernet network interface for virtual machine in KVM/Qemu environment. Its functionality is described next. A frontend driver sits in the virtual machine kernel. Then guest workload issues network packets into the frontend device/driver via the virtqueues, which is shared ring buffer between frontend and backend. Then a backend driver is implemented as a user land driver in Qemu. Finally, it receives network packets from the virtqueues and transfers them to the physical network.",
            "Enable vhost-net",
            "Enable Multiqueue",
            "\nUser land Qemu virtio-net driver has some disadvantages.\nContext switch between kernel and Qemu is needed to handle packets. This is an expensive operation, causes a lot of processing time and packet process latency. Vhost-net is a kernel acceleration module for virtio-net, which offloads the virtio-net data plane handling from user land to kernel. It leverages Linux ioeventfd mechanism, any guest access to the virtqueue memory mapped notification address will directly invoke vhost-net worker thread to handle the packets. Meanwhile, Linux kernel irqfd mechanism is also leveraged so that IRQs can be injected into guest by any thread on host. These asynchronous mechanisms improve virtio-net performance a lot. Use “vhost=on” in the “-netdev tap” Qemu command line option to enable vhost-net, like the following example\n\n```\n-netdev tap,id=nic0,vhost=on,script=$IFUP_SCRIPT,downscript=no \\\n\n-device virtio-net-pci,mq=on,netdev=nic0,mac=$MAC\n```",
            "Enable Zero Copy Transmission**\n\nTo enable transmission zero copy in Linux kernel vhost-net driver, use the following commands:\n\n```\n# modprobe -r vhost_net\n\n# modprobe vhost_net experimental_zcopytx=1\n```\n\nAnd the following to verify it has been set successfully:\n\n```\n# cat /sys/modules/vhost_net/parameters/experimental_zcopytx\n```\n\n**Enable Queue Polling**\n\nVhost-net is also possible to enable Queue Polling for better performance. The basic idea is to busy wait for a while after all packets has been handled to see if there is any new packet arrivin"
          ]
        },
        {
          "title": "Features/MQ virtio net - Fedora Project Wiki",
          "url": "https://fedoraproject.org/wiki/Features/MQ_virtio_net",
          "excerpts": [
            "Mar 15, 2013 — Multiqueue virtio-net provides an approach that scales the network performance as the increasing of the number of vcpus by allowing them to transfer packets ..."
          ]
        },
        {
          "title": "Multiqueue virtio-net",
          "url": "https://www.linux-kvm.org/page/Multiqueue",
          "excerpts": [
            "* macvtap/tap: For single queue virtio-net, one socket of macvtap/tap was abstracted as a queue for both tx and rx. We can reuse and extend this abstraction to allow macvtap/tap can dequeue and enqueue packets from multiple sockets. Then each socket can be treated as a tx and rx, and macvtap/tap is fact a multi-queue device in the host. The host network codes can then transmit and receive packets in parallel.",
            "* vhost: The parallelism could be done through using multiple vhost threads to handle multiple sockets. Currently, there's two choices in design. + 1:1 mapping between vhost threads and sockets. This method does not need vhost changes and just launch the the same number of vhost threads as queues. Each vhost thread is just used to handle one tx ring and rx ring just as they are used for single queue virtio-net. + M:N mapping between vhost threads and sockets. This methods allow a single vhost thread to poll more than one tx/rx rings and sockests and use separated threads to handle tx and rx request"
          ]
        }
      ],
      "reasoning": "- The most directly relevant material states that enabling multicast queueing for virtio-net (multi-queue) can boost parallelism and mentions dedicating a vCPU to a virtqueue, which aligns with the design goal of a highly parallel datapath with isolation: a single vCPU per virtqueue reduces cross-queue contention and keeps processing hot on a specific core. This speaks to both parallelism and fixed topology tying queues to CPUs. \n- Additional sources describe the broad goal and architecture of multiqueue virtio-net, emphasizing parallelism across vCPUs and the role of vhost and multiple queues to scale performance. These passages support the idea that the design uses multiple virtqueues to distribute work and exploit parallelism across cores. \n- Further evidence discusses enabling multiqueue and related configurations (e.g., per-CPU queues, multiple virtqueues, and the mapping between queues and processing threads), which reinforces the field value that there is a deliberate 1:1 or per-core/topology alignment to optimize latency and isolation. \n- Supporting material from MQ virtio-net discussions and examples shows practical deployment considerations (e.g., vhost backends, per-CPU queues, and multiqueue configurations) that underpin the proposed datapath topology and pinning strategy. \n- Some sources explicitly reference per-CPU queues or per-core parallelism, reinforcing the idea of isolating and pinning work to dedicated cores for deterministic latency in a high-throughput environment. \n- Collectively, these excerpts corroborate the core claim that VirtIO MQ is used to enable parallel packet processing with careful topology and pinning to isolated cores, matching the described topology where each vCPU is assigned to a dedicated queue pair and where isolation/pinning is a design goal. \n",
      "confidence": "high"
    },
    {
      "field": "virtio_network_datapath_model.descriptor_and_interrupt_management",
      "citations": [
        {
          "title": "Virtqueues and virtio ring: How the data travels",
          "url": "https://www.redhat.com/en/blog/virtqueues-and-virtio-ring-how-data-travels",
          "excerpts": [
            "The struct layout is:",
            "```\nstruct virtq_avail {              struct virtq_used {\n  le16 flags;                       le16 flags;\n  le16 idx;                         le16 idx;\n  le16 ring[ /* Queue Size */ ];    struct virtq_used_elem ring[Q. size];\n  le16 used_event;                  le16 avail_event;\n};                                };\n\n```"
          ]
        },
        {
          "title": "Virtual I/O Device (VIRTIO) Version 1.1 - OASIS Open",
          "url": "https://docs.oasis-open.org/virtio/virtio/v1.1/csprd01/virtio-v1.1-csprd01-diff.html",
          "excerpts": [
            "VIRTIO_F_EVENT_IDX would interrupt send a used buffer notification to the driver after the first buffer is used (and again after the 65536th buffer, etc). 2.6.8 ...",
            "\nThere are two parts to device  virtqueue  operation: supplying new available  buffers to\nthe device, and processing used buffers from the device. Note: As an example, the simplest virtio network device has two virtqueues: the\ntransmit virtqueue and the receive virtqueue. The driver adds outgoing\n(device-readable) packets to the transmit virtqueue, and then frees them\nafter they are used. Similarly, incoming (device-writable) buffers are\nadded to the receive virtqueue, and processed after they are used.",
            "the simplest virtio network device has two virtqueues: the\ntransmit virtqueue and the receive virtqueue."
          ]
        },
        {
          "title": "virtio_queue - Rust",
          "url": "https://docs.rs/virtio-queue",
          "excerpts": [
            "Struct to maintain information and manipulate state of a virtio queue for multi-threaded context. Reader: Provides high-level interface over the sequence of ..."
          ]
        },
        {
          "title": "devices::virtio::wl - Rust",
          "url": "https://crosvm.dev/doc/devices/virtio/wl/index.html",
          "excerpts": [
            "Each WlVfd represents one virtual file descriptor created by either the guest or the host. Virtual file descriptors contain actual file descriptors, either a ..."
          ]
        }
      ],
      "reasoning": "The most directly relevant parts describe the fundamental layout and management of virtio queues. The two excerpts that reveal the descriptor ring layout and the separation into Descriptor Area, Available Ring, and Used Ring establish the concrete data structures that underlie descriptor management in Virtio. These sections show how the virtqueue is organized and accessed, forming the basis for any latency-focused descriptor/interrupt strategy. They support the notion that an RTOS using Virtio will reason about the exact placement of buffers and how the driver and device coordinate through these rings, which is essential to minimize batching-related tail latency.\n\nThe excerpt that discusses the VIRTIO_F_EVENT_IDX feature and the notification semantics ties directly to the policy of suppressing interrupts and controlling when the device signals readiness. By negotiating and applying VIRTIO_F_EVENT_IDX, the system can reduce unnecessary interrupts and rely on polling or batched completion signaling, which aligns with the target architecture's goal of very low and predictable tail latency.\n\nOther excerpts describe the availability and usage of the virtqueue rings in practice (Avail Ring and Used Ring in the driver/device flow) and provide concrete descriptions of how descriptors are advanced and how used descriptors are reported back. These details corroborate the strategy of minimal batching and explicit polling behavior as a latency-oriented design choice.\n\nThe Rust-focused excerpts about virtio-queue abstractions and their multi-threaded usage supplement the operational picture by showing how software can safely manipulate virtio queues in a no_std or constrained environment, which is consistent with the overall goal of a deterministic, low-latency datapath in a unikernel/RTOS context.\n",
      "confidence": "high"
    }
  ],
  "outputSchema": {
    "type": "object",
    "properties": {
      "project_overview": {
        "type": "string",
        "description": "A high-level summary of the project's objective to create an application-specific, unikernel-style Real-Time Operating System (RTOS) in Rust for a high-frequency network packet processor."
      },
      "executive_summary": {
        "type": "string",
        "description": "A brief executive summary of the key architectural decisions, technology choices, and risk assessments presented in the technical blueprint."
      },
      "boot_and_initialization_specification": {
        "type": "object",
        "properties": {
          "bootloader_choice": {
            "type": "string",
            "description": "The selected bootloader approach (e.g., 'bootimage' toolchain or Multiboot2 specification)."
          },
          "boot_process_flow": {
            "type": "string",
            "description": "The step-by-step flow from CPU reset to kernel entry, including firmware handoff."
          },
          "cpu_mode_transition": {
            "type": "string",
            "description": "Details of the CPU transition from 16-bit real mode through 32-bit protected mode to 64-bit long mode."
          },
          "memory_map_handoff": {
            "type": "string",
            "description": "The mechanism for passing the physical memory map from the bootloader to the kernel (e.g., via the BootInfo struct)."
          },
          "kernel_entry_details": {
            "type": "string",
            "description": "Specification of the kernel's `_start` function, its ABI, and initial stack setup."
          }
        },
        "required": [
          "bootloader_choice",
          "boot_process_flow",
          "cpu_mode_transition",
          "memory_map_handoff",
          "kernel_entry_details"
        ],
        "additionalProperties": false
      },
      "physical_memory_management": {
        "type": "object",
        "properties": {
          "frame_allocator_algorithm": {
            "type": "string",
            "description": "The chosen algorithm for the physical memory manager (e.g., Bitmap Allocator, Buddy Allocator) and its justification."
          },
          "data_structures": {
            "type": "string",
            "description": "The key data structures used by the frame allocator, such as bitmaps or free lists."
          },
          "initialization_process": {
            "type": "string",
            "description": "How the allocator is initialized using the memory map provided by the bootloader."
          },
          "dma_considerations": {
            "type": "string",
            "description": "How the allocator provides physically contiguous memory required for VirtIO DMA operations."
          }
        },
        "required": [
          "frame_allocator_algorithm",
          "data_structures",
          "initialization_process",
          "dma_considerations"
        ],
        "additionalProperties": false
      },
      "virtual_memory_management": {
        "type": "object",
        "properties": {
          "paging_architecture": {
            "type": "string",
            "description": "The design of the virtual memory system, based on x86_64 4-level paging."
          },
          "address_space_layout": {
            "type": "string",
            "description": "The concrete layout of the virtual address space, including the higher-half kernel mapping and user-space regions."
          },
          "large_page_strategy": {
            "type": "string",
            "description": "The strategy for using 2MiB and 1GiB large pages to reduce TLB misses and improve performance."
          },
          "tlb_management_policy": {
            "type": "string",
            "description": "The policy for managing the Translation Lookaside Buffer (TLB), including CR3 management and the avoidance of TLB shootdowns."
          },
          "page_fault_handling": {
            "type": "string",
            "description": "The policy for handling page faults, which is to panic on any fault as they indicate a critical bug."
          }
        },
        "required": [
          "paging_architecture",
          "address_space_layout",
          "large_page_strategy",
          "tlb_management_policy",
          "page_fault_handling"
        ],
        "additionalProperties": false
      },
      "heap_allocator_integration": {
        "type": "object",
        "properties": {
          "chosen_allocator_crate": {
            "type": "string",
            "description": "The selected `no_std` heap allocator crate (e.g., 'talc', 'linked_list_allocator') and the rationale."
          },
          "initialization_strategy": {
            "type": "string",
            "description": "How the heap is initialized over a pre-mapped virtual memory region provided by the pager."
          },
          "synchronization_mechanism": {
            "type": "string",
            "description": "The thread-safety mechanism used by the allocator, such as spinlocks."
          },
          "out_of_memory_policy": {
            "type": "string",
            "description": "The defined behavior for when the heap runs out of memory, consistent with RTOS constraints (e.g., panic)."
          }
        },
        "required": [
          "chosen_allocator_crate",
          "initialization_strategy",
          "synchronization_mechanism",
          "out_of_memory_policy"
        ],
        "additionalProperties": false
      },
      "scheduler_design": {
        "type": "object",
        "properties": {
          "scheduling_algorithm": {
            "type": "string",
            "description": "The chosen scheduling algorithm, justified for deterministic performance (e.g., Fixed-Priority Preemptive Scheduling)."
          },
          "run_queue_data_structure": {
            "type": "string",
            "description": "The design of the run queue, such as a bitmap over an array of priority queues, to ensure O(1) selection of the next task."
          },
          "priority_inversion_mitigation": {
            "type": "string",
            "description": "The chosen strategy for handling priority inversion (e.g., Priority Inheritance or Priority Ceiling Protocol)."
          },
          "timer_and_preemption_integration": {
            "type": "string",
            "description": "The policy for time slicing, preemption points, and integration with a high-resolution timer source."
          }
        },
        "required": [
          "scheduling_algorithm",
          "run_queue_data_structure",
          "priority_inversion_mitigation",
          "timer_and_preemption_integration"
        ],
        "additionalProperties": false
      },
      "task_control_block_and_lifecycle": {
        "type": "object",
        "properties": {
          "tcb_struct_definition": {
            "type": "string",
            "description": "The definition of the Task Control Block (TCB) struct, including fields for state, priority, stack pointer, page table root, and CPU affinity."
          },
          "task_lifecycle_states": {
            "type": "string",
            "description": "The defined states of a task's lifecycle, such as Ready, Running, Blocked, and Exited."
          },
          "lifecycle_management": {
            "type": "string",
            "description": "The API and process for managing task transitions, including creation, blocking/unblocking, and termination."
          },
          "stack_management": {
            "type": "string",
            "description": "The strategy for managing task stacks, including sizing, guard pages, and overflow detection."
          }
        },
        "required": [
          "tcb_struct_definition",
          "task_lifecycle_states",
          "lifecycle_management",
          "stack_management"
        ],
        "additionalProperties": false
      },
      "context_switching_mechanism": {
        "type": "object",
        "properties": {
          "register_save_restore_set": {
            "type": "string",
            "description": "The full set of registers that are saved and restored during a context switch, including GPRs, segment registers, RFLAGS, and RIP/RSP."
          },
          "fpu_simd_state_handling": {
            "type": "string",
            "description": "The policy for handling FPU/SSE/AVX state, detailing the choice between lazy and eager policies and the use of XSAVE/XRSTOR instructions."
          },
          "switch_paths": {
            "type": "string",
            "description": "The distinction between the interrupt-driven context switch path and the voluntary (syscall-driven) switch path."
          },
          "implementation_details": {
            "type": "string",
            "description": "The implementation method using Rust's inline assembly (`asm!`) and considerations for ABI safety and stack unwinding."
          }
        },
        "required": [
          "register_save_restore_set",
          "fpu_simd_state_handling",
          "switch_paths",
          "implementation_details"
        ],
        "additionalProperties": false
      },
      "concurrency_primitives_implementation": {
        "type": "object",
        "properties": {
          "spinlock_design": {
            "type": "string",
            "description": "The design of spinlocks using atomics, including fairness vs. latency trade-offs (e.g., Test-and-Set vs. Ticket locks)."
          },
          "lock_free_data_structures": {
            "type": "string",
            "description": "The design of lock-free structures like MPSC/SPSC ring buffers for the packet datapath."
          },
          "memory_ordering_rationale": {
            "type": "string",
            "description": "The rationale for choosing specific memory orderings (Acquire/Release/SeqCst) to ensure correctness without unnecessary performance penalties."
          },
          "backoff_strategies": {
            "type": "string",
            "description": "Strategies to avoid contention and cache-line ping-pong, such as using `core::hint::spin_loop()` and exponential backoff."
          }
        },
        "required": [
          "spinlock_design",
          "lock_free_data_structures",
          "memory_ordering_rationale",
          "backoff_strategies"
        ],
        "additionalProperties": false
      },
      "interrupt_and_fault_handling_framework": {
        "type": "object",
        "properties": {
          "descriptor_tables_setup": {
            "type": "string",
            "description": "The process for creating and loading the GDT, TSS, and IDT."
          },
          "critical_fault_handling": {
            "type": "string",
            "description": "The use of the Interrupt Stack Table (IST) to provide a reliable stack for handling critical exceptions like Double Faults and NMIs."
          },
          "interrupt_flow_and_nesting": {
            "type": "string",
            "description": "The path from a hardware interrupt to handler execution, including rules for masking, unmasking, and nesting to ensure determinism."
          },
          "deferred_processing_strategy": {
            "type": "string",
            "description": "The strategy of keeping interrupt handlers minimal and deferring longer work to a non-interrupt context (bottom-half) to maintain low latency."
          }
        },
        "required": [
          "descriptor_tables_setup",
          "critical_fault_handling",
          "interrupt_flow_and_nesting",
          "deferred_processing_strategy"
        ],
        "additionalProperties": false
      },
      "system_call_interface_design": {
        "type": "object",
        "properties": {
          "mechanism_choice": {
            "type": "string",
            "description": "The chosen mechanism for system calls, specifically the fast `SYSCALL`/`SYSRET` instructions on x86_64."
          },
          "abi_definition": {
            "type": "string",
            "description": "The definition of the Application Binary Interface (ABI), including register conventions for arguments and return values."
          },
          "minimal_syscall_set": {
            "type": "string",
            "description": "The definition of the essential system calls required for the RTOS, such as `task_create`, `yield`, and `sleep`."
          },
          "performance_optimizations": {
            "type": "string",
            "description": "Strategies to mitigate syscall overhead, such as implementing vDSO-like fast paths for read-only operations."
          }
        },
        "required": [
          "mechanism_choice",
          "abi_definition",
          "minimal_syscall_set",
          "performance_optimizations"
        ],
        "additionalProperties": false
      },
      "virtio_abstraction_layer": {
        "type": "object",
        "properties": {
          "integration_crate": {
            "type": "string",
            "description": "The chosen crate for VirtIO guest drivers, specifically the `no_std` compatible `virtio-drivers` crate."
          },
          "device_discovery": {
            "type": "string",
            "description": "The design for discovering VirtIO devices over MMIO, which involves parsing the Device Tree Blob (DTB) provided by QEMU."
          },
          "device_initialization": {
            "type": "string",
            "description": "The standardized step-by-step flow for initializing a VirtIO device, including feature negotiation and status updates."
          },
          "virtqueue_setup": {
            "type": "string",
            "description": "The process for allocating, aligning, and registering virtqueues with the device."
          }
        },
        "required": [
          "integration_crate",
          "device_discovery",
          "device_initialization",
          "virtqueue_setup"
        ],
        "additionalProperties": false
      },
      "virtio_network_datapath_model": {
        "type": "object",
        "properties": {
          "io_model": {
            "type": "string",
            "description": "The chosen I/O model for handling virtio-net completions, which is a hybrid NAPI-like model that uses polling for the hot path to minimize latency."
          },
          "queue_topology_and_pinning": {
            "type": "string",
            "description": "The use of multi-queue virtio-net, with each queue pinned to a dedicated, isolated vCPU to maximize parallelism and cache efficiency."
          },
          "host_backend_strategy": {
            "type": "string",
            "description": "The recommended host-side backend, which is `vhost-user` connected to a DPDK-based application for the highest performance."
          },
          "descriptor_and_interrupt_management": {
            "type": "string",
            "description": "The strategy for managing descriptors in batches and suppressing interrupts via `VIRTIO_F_EVENT_IDX` to trade throughput for minimal latency."
          }
        },
        "required": [
          "io_model",
          "queue_topology_and_pinning",
          "host_backend_strategy",
          "descriptor_and_interrupt_management"
        ],
        "additionalProperties": false
      },
      "code_and_effort_estimation": {
        "type": "object",
        "properties": {
          "subsystem": {
            "type": "string",
            "description": "The name of the RTOS component or subsystem."
          },
          "estimated_loc_rust": {
            "type": "string",
            "description": "The estimated Lines of Code (LoC) in Rust for the subsystem, presented as a range."
          },
          "estimated_person_months": {
            "type": "string",
            "description": "The estimated effort in person-months to complete the subsystem, assuming senior Rust systems engineers."
          },
          "key_assumptions_and_rationale": {
            "type": "string",
            "description": "The rationale and key assumptions behind the LoC and effort estimates for the subsystem."
          }
        },
        "required": [
          "subsystem",
          "estimated_loc_rust",
          "estimated_person_months",
          "key_assumptions_and_rationale"
        ],
        "additionalProperties": false
      },
      "technology_and_crate_recommendations": {
        "type": "object",
        "properties": {
          "component_type": {
            "type": "string",
            "description": "The category of the component, such as Bootloader, Heap Allocator, or VirtIO Drivers."
          },
          "recommended_crate_tool": {
            "type": "string",
            "description": "The specific Rust crate or tool recommended for the component."
          },
          "reason_for_choice": {
            "type": "string",
            "description": "A brief justification for why the recommended crate or tool was chosen."
          }
        },
        "required": [
          "component_type",
          "recommended_crate_tool",
          "reason_for_choice"
        ],
        "additionalProperties": false
      },
      "risk_analysis_and_mitigation": {
        "type": "object",
        "properties": {
          "risk_description": {
            "type": "string",
            "description": "A description of a top technical challenge or risk for the project."
          },
          "mitigation_strategy": {
            "type": "string",
            "description": "A concrete mitigation strategy to address and de-risk the identified challenge."
          }
        },
        "required": [
          "risk_description",
          "mitigation_strategy"
        ],
        "additionalProperties": false
      },
      "host_and_qemu_tuning_guide": {
        "type": "object",
        "properties": {
          "host_cpu_isolation": {
            "type": "string",
            "description": "Host kernel parameters and tools (`isolcpus`, `nohz_full`, cpusets) to shield vCPU threads from host OS jitter."
          },
          "host_power_and_irq_management": {
            "type": "string",
            "description": "Configuration to disable CPU power-saving states (C-states) and to manage IRQ affinity, directing interrupts away from isolated cores."
          },
          "host_memory_configuration": {
            "type": "string",
            "description": "Setup for hugepages to reduce TLB misses and memory pre-faulting to avoid first-touch latency."
          },
          "qemu_cli_configuration": {
            "type": "string",
            "description": "Specific QEMU command-line arguments for CPU pinning, NUMA alignment, and memory backend setup."
          },
          "network_backend_setup": {
            "type": "string",
            "description": "Steps for configuring high-performance network backends, including TAP devices with vhost-kernel and sockets for vhost-user."
          }
        },
        "required": [
          "host_cpu_isolation",
          "host_power_and_irq_management",
          "host_memory_configuration",
          "qemu_cli_configuration",
          "network_backend_setup"
        ],
        "additionalProperties": false
      },
      "security_and_isolation_strategy": {
        "type": "object",
        "properties": {
          "intra_unikernel_isolation": {
            "type": "string",
            "description": "The strategy for creating memory safety boundaries within the single address space, using hardware features like Intel MPK."
          },
          "device_access_control": {
            "type": "string",
            "description": "Application of the Principle of Least Privilege to VirtIO device access, including the use of IOMMU for DMA protection."
          },
          "compile_time_hardening": {
            "type": "string",
            "description": "Techniques for hardening the binary at compile time, such as static linking, LTO, and setting `panic=abort`."
          },
          "deterministic_debugging": {
            "type": "string",
            "description": "The approach for including debugging hooks and diagnostics that have zero performance cost in release builds, using conditional compilation."
          }
        },
        "required": [
          "intra_unikernel_isolation",
          "device_access_control",
          "compile_time_hardening",
          "deterministic_debugging"
        ],
        "additionalProperties": false
      },
      "configuration_and_packaging_interface": {
        "type": "object",
        "properties": {
          "configuration_schema": {
            "type": "string",
            "description": "The design of a declarative configuration schema (e.g., TOML) for defining VM properties, devices, and network backends."
          },
          "host_cli_orchestration": {
            "type": "string",
            "description": "The role of a host-side Rust CLI in parsing the configuration, setting up host resources (TAP devices, bridges), and generating the QEMU command line."
          },
          "guest_configuration_passing": {
            "type": "string",
            "description": "The chosen mechanism for deterministically passing configuration from the host to the guest at boot, recommending the QEMU `fw_cfg` device."
          },
          "idempotency_and_failure_modes": {
            "type": "string",
            "description": "Strategies for ensuring the provisioning process is idempotent and handles failures explicitly, with the CLI managing the full lifecycle of host resources."
          }
        },
        "required": [
          "configuration_schema",
          "host_cli_orchestration",
          "guest_configuration_passing",
          "idempotency_and_failure_modes"
        ],
        "additionalProperties": false
      }
    },
    "required": [
      "project_overview",
      "executive_summary",
      "boot_and_initialization_specification",
      "physical_memory_management",
      "virtual_memory_management",
      "heap_allocator_integration",
      "scheduler_design",
      "task_control_block_and_lifecycle",
      "context_switching_mechanism",
      "concurrency_primitives_implementation",
      "interrupt_and_fault_handling_framework",
      "system_call_interface_design",
      "virtio_abstraction_layer",
      "virtio_network_datapath_model",
      "code_and_effort_estimation",
      "technology_and_crate_recommendations",
      "risk_analysis_and_mitigation",
      "host_and_qemu_tuning_guide",
      "security_and_isolation_strategy",
      "configuration_and_packaging_interface"
    ],
    "additionalProperties": false
  }
}